astropy_astropy-1.3.0: 36	
===================================================================	
test_basic_nddata: 16	
----------------------------	

arr = numpy.zeros((11, 11))
arr[(5, 5)] = 1
ndd = NDData(arr)
test_kernel = Gaussian2DKernel(1)
result = convolve(ndd, test_kernel)
(x, y) = numpy.mgrid[:11, :11]
tempResult = exp(((- 0.5) * (((x - 5) ** 2) + ((y - 5) ** 2))))
	
===================================================================	
wpwaCDM.de_density_scale: 858	
----------------------------	

' Evaluates the redshift dependence of the dark energy density.\n\n        Parameters\n        ----------\n        z : array-like\n          Input redshifts.\n\n        Returns\n        -------\n        I : ndarray, or float if input scalar\n          The scaling of the energy density of dark energy with redshift.\n\n        Notes\n        -----\n        The scaling factor, I, is defined by :math:`\\\\rho(z) = \\\\rho_0 I`,\n        and in this case is given by\n\n        .. math::\n\n          a_p = \\frac{1}{1 + z_p}\n\n          I = \\left(1 + z\\right)^{3 \\left(1 + w_p + a_p w_a\\right)}\n          \\exp \\left(-3 w_a \\frac{z}{1+z}\\right)\n        '
if isiterable(z):
    z = numpy.asarray(z)
zp1 = (1.0 + z)
apiv = (1.0 / (1.0 + self._zp))
tempResult = exp(((((- 3.0) * self._wa) * z) / zp1))
	
===================================================================	
w0waCDM.de_density_scale: 784	
----------------------------	

' Evaluates the redshift dependence of the dark energy density.\n\n        Parameters\n        ----------\n        z : array-like\n          Input redshifts.\n\n        Returns\n        -------\n        I : ndarray, or float if input scalar\n          The scaling of the energy density of dark energy with redshift.\n\n        Notes\n        -----\n        The scaling factor, I, is defined by :math:`\\\\rho(z) = \\\\rho_0 I`,\n        and in this case is given by\n\n        .. math::\n\n          I = \\left(1 + z\\right)^{3 \\left(1 + w_0 + w_a\\right)}\n          \\exp \\left(-3 w_a \\frac{z}{1+z}\\right)\n\n        '
if isiterable(z):
    z = numpy.asarray(z)
zp1 = (1.0 + z)
tempResult = exp(((((- 3) * self._wa) * z) / zp1))
	
===================================================================	
w0wzCDM.de_density_scale: 902	
----------------------------	

' Evaluates the redshift dependence of the dark energy density.\n\n        Parameters\n        ----------\n        z : array-like\n          Input redshifts.\n\n        Returns\n        -------\n        I : ndarray, or float if input scalar\n          The scaling of the energy density of dark energy with redshift.\n\n        Notes\n        -----\n        The scaling factor, I, is defined by :math:`\\\\rho(z) = \\\\rho_0 I`,\n        and in this case is given by\n\n        .. math::\n\n          I = \\left(1 + z\\right)^{3 \\left(1 + w_0 - w_z\\right)}\n          \\exp \\left(-3 w_z z\\right)\n        '
if isiterable(z):
    z = numpy.asarray(z)
zp1 = (1.0 + z)
tempResult = exp((((- 3.0) * self._wz) * z))
	
===================================================================	
FLRW.de_density_scale: 344	
----------------------------	

' Evaluates the redshift dependence of the dark energy density.\n\n        Parameters\n        ----------\n        z : array-like\n          Input redshifts.\n\n        Returns\n        -------\n        I : ndarray, or float if input scalar\n          The scaling of the energy density of dark energy with redshift.\n\n        Notes\n        -----\n        The scaling factor, I, is defined by :math:`\\rho(z) = \\rho_0 I`,\n        and is given by\n\n        .. math::\n\n            I = \\exp \\left( 3 \\int_{a}^1 \\frac{ da^{\\prime} }{ a^{\\prime} }\n            \\left[ 1 + w\\left( a^{\\prime} \\right) \\right] \\right)\n\n        It will generally helpful for subclasses to overload this method if\n        the integral can be done analytically for the particular dark\n        energy equation of state that they implement.\n        '
from scipy.integrate import quad
if isiterable(z):
    z = numpy.asarray(z)
    ival = numpy.array([quad(self._w_integrand, 0, log((1 + redshift)))[0] for redshift in z])
    tempResult = exp((3 * ival))
	
===================================================================	
Sersic1D.evaluate: 225	
----------------------------	

'One dimensional Sersic profile function.'
if (cls._gammaincinv is None):
    try:
        from scipy.special import gammaincinv
        cls._gammaincinv = gammaincinv
    except ValueError:
        raise ImportError('Sersic1D model requires scipy > 0.11.')
tempResult = exp(((- cls._gammaincinv((2 * n), 0.5)) * (((r / r_eff) ** (1 / n)) - 1)))
	
===================================================================	
MexicanHat1D.evaluate: 586	
----------------------------	

'One dimensional Mexican Hat model function'
xx_ww = (((x - x_0) ** 2) / (2 * (sigma ** 2)))
tempResult = exp((- xx_ww))
	
===================================================================	
MexicanHat2D.evaluate: 605	
----------------------------	

'Two dimensional Mexican Hat model function'
rr_ww = ((((x - x_0) ** 2) + ((y - y_0) ** 2)) / (2 * (sigma ** 2)))
tempResult = exp((- rr_ww))
	
===================================================================	
Gaussian1D.fit_deriv: 36	
----------------------------	

'\n        Gaussian1D model function derivatives.\n        '
tempResult = exp((((- 0.5) / (stddev ** 2)) * ((x - mean) ** 2)))
	
===================================================================	
Gaussian2D.evaluate: 105	
----------------------------	

'Two dimensional Gaussian function'
cost2 = (numpy.cos(theta) ** 2)
sint2 = (numpy.sin(theta) ** 2)
sin2t = numpy.sin((2.0 * theta))
xstd2 = (x_stddev ** 2)
ystd2 = (y_stddev ** 2)
xdiff = (x - x_mean)
ydiff = (y - y_mean)
a = (0.5 * ((cost2 / xstd2) + (sint2 / ystd2)))
b = (0.5 * ((sin2t / xstd2) - (sin2t / ystd2)))
c = (0.5 * ((sint2 / xstd2) + (cost2 / ystd2)))
tempResult = exp((- (((a * (xdiff ** 2)) + ((b * xdiff) * ydiff)) + (c * (ydiff ** 2)))))
	
===================================================================	
Sersic2D.evaluate: 705	
----------------------------	

'Two dimensional Sersic profile function.'
if (cls._gammaincinv is None):
    try:
        from scipy.special import gammaincinv
        cls._gammaincinv = gammaincinv
    except ValueError:
        raise ImportError('Sersic2D model requires scipy > 0.11.')
bn = cls._gammaincinv((2.0 * n), 0.5)
(a, b) = (r_eff, ((1 - ellip) * r_eff))
(cos_theta, sin_theta) = (numpy.cos(theta), numpy.sin(theta))
x_maj = (((x - x_0) * cos_theta) + ((y - y_0) * sin_theta))
x_min = (((- (x - x_0)) * sin_theta) + ((y - y_0) * cos_theta))
z = numpy.sqrt((((x_maj / a) ** 2) + ((x_min / b) ** 2)))
tempResult = exp(((- bn) * ((z ** (1 / n)) - 1)))
	
===================================================================	
Gaussian2D.fit_deriv: 127	
----------------------------	

'Two dimensional Gaussian function derivative with respect to parameters'
cost = numpy.cos(theta)
sint = numpy.sin(theta)
cost2 = (numpy.cos(theta) ** 2)
sint2 = (numpy.sin(theta) ** 2)
cos2t = numpy.cos((2.0 * theta))
sin2t = numpy.sin((2.0 * theta))
xstd2 = (x_stddev ** 2)
ystd2 = (y_stddev ** 2)
xstd3 = (x_stddev ** 3)
ystd3 = (y_stddev ** 3)
xdiff = (x - x_mean)
ydiff = (y - y_mean)
xdiff2 = (xdiff ** 2)
ydiff2 = (ydiff ** 2)
a = (0.5 * ((cost2 / xstd2) + (sint2 / ystd2)))
b = (0.5 * ((sin2t / xstd2) - (sin2t / ystd2)))
c = (0.5 * ((sint2 / xstd2) + (cost2 / ystd2)))
tempResult = exp((- (((a * xdiff2) + ((b * xdiff) * ydiff)) + (c * ydiff2))))
	
===================================================================	
Gaussian1D.evaluate: 31	
----------------------------	

'\n        Gaussian1D model function.\n        '
tempResult = exp((((- 0.5) * ((x - mean) ** 2)) / (stddev ** 2)))
	
===================================================================	
ExponentialCutoffPowerLaw1D.evaluate: 67	
----------------------------	

'One dimensional exponential cutoff power law model function'
xx = (x / x_0)
tempResult = exp(((- x) / x_cutoff))
	
===================================================================	
ExponentialCutoffPowerLaw1D.fit_deriv: 74	
----------------------------	

'One dimensional exponential cutoff power law derivative with respect to parameters'
xx = (x / x_0)
xc = (x / x_cutoff)
tempResult = exp((- xc))
	
===================================================================	
module: 9	
----------------------------	

'\nHere are all the test parameters and values for the each\n`~astropy.modeling.FittableModel` defined. There is a dictionary for 1D and a\ndictionary for 2D models.\n\nExplanation of keywords of the dictionaries:\n\n"parameters" : list or dict\n    Model parameters, the model is tested with. Make sure you keep the right\n    order.  For polynomials you can also use a dict to specify the\n    coefficients. See examples below.\n\n"x_values" : list\n    x values where the model is evaluated.\n\n"y_values" : list\n    Reference y values for the in x_values given positions.\n\n"z_values" : list\n    Reference z values for the in x_values and y_values given positions.\n    (2D model option)\n\n"x_lim" : list\n    x test range for the model fitter. Depending on the model this can differ\n    e.g. the PowerLaw model should be tested over a few magnitudes.\n\n"y_lim" : list\n    y test range for the model fitter. Depending on the model this can differ\n    e.g. the PowerLaw model should be tested over a few magnitudes.  (2D model\n    option)\n\n"log_fit" : bool\n    PowerLaw models should be tested over a few magnitudes. So log_fit should\n    be true.\n\n"requires_scipy" : bool\n    If a model requires scipy (Bessel functions etc.) set this flag.\n\n"integral" : float\n    Approximate value of the integral in the range x_lim (and y_lim).\n\n"deriv_parameters" : list\n    If given the test of the derivative will use these parameters to create a\n    model (optional)\n\n"deriv_initial" : list\n    If given the test of the derivative will use these parameters as initial\n    values for the fit (optional)\n'
from __future__ import absolute_import, division, print_function, unicode_literals
from ..functional_models import Gaussian1D, Sine1D, Box1D, Linear1D, Lorentz1D, MexicanHat1D, Trapezoid1D, Const1D, Moffat1D, Gaussian2D, Const2D, Box2D, MexicanHat2D, TrapezoidDisk2D, AiryDisk2D, Moffat2D, Disk2D, Ring2D, Sersic1D, Sersic2D, Voigt1D, Planar2D
from ..polynomial import Polynomial1D, Polynomial2D
from ..powerlaws import PowerLaw1D, BrokenPowerLaw1D, ExponentialCutoffPowerLaw1D, LogParabola1D
import numpy as np
models_1D = {Gaussian1D: {'parameters': [1, 0, 1], 'x_values': [0, numpy.sqrt(2), (- numpy.sqrt(2))], 'y_values': [1.0, 0.367879, 0.367879], 'x_lim': [(- 10), 10], 'integral': numpy.sqrt((2 * numpy.pi))}, Sine1D: {'parameters': [1, 0.1, 0], 'x_values': [0, 2.5], 'y_values': [0, 1], 'x_lim': [(- 10), 10], 'integral': 0}, Box1D: {'parameters': [1, 0, 10], 'x_values': [(- 5), 5, 0, (- 10), 10], 'y_values': [1, 1, 1, 0, 0], 'x_lim': [(- 10), 10], 'integral': 10}, Linear1D: {'parameters': [1, 0], 'x_values': [0, numpy.pi, 42, (- 1)], 'y_values': [0, numpy.pi, 42, (- 1)], 'x_lim': [(- 10), 10], 'integral': 0}, Lorentz1D: {'parameters': [1, 0, 1], 'x_values': [0, (- 1), 1, 0.5, (- 0.5)], 'y_values': [1.0, 0.2, 0.2, 0.5, 0.5], 'x_lim': [(- 10), 10], 'integral': 1}, MexicanHat1D: {'parameters': [1, 0, 1], 'x_values': [0, 1, (- 1), 3, (- 3)], 'y_values': [1.0, 0.0, 0.0, (- 0.088872), (- 0.088872)], 'x_lim': [(- 20), 20], 'integral': 0}, Trapezoid1D: {'parameters': [1, 0, 2, 1], 'x_values': [0, 1, (- 1), 1.5, (- 1.5), 2, 2], 'y_values': [1, 1, 1, 0.5, 0.5, 0, 0], 'x_lim': [(- 10), 10], 'integral': 3}, Const1D: {'parameters': [1], 'x_values': [(- 1), 1, numpy.pi, (- 42.0), 0], 'y_values': [1, 1, 1, 1, 1], 'x_lim': [(- 10), 10], 'integral': 20}, Moffat1D: {'parameters': [1, 0, 1, 2], 'x_values': [0, 1, (- 1), 3, (- 3)], 'y_values': [1.0, 0.25, 0.25, 0.01, 0.01], 'x_lim': [(- 10), 10], 'integral': 1, 'deriv_parameters': [23.4, 1.2, 2.1, 2.3], 'deriv_initial': [10, 1, 1, 1]}, PowerLaw1D: {'parameters': [1, 1, 2], 'constraints': {'fixed': {'x_0': True}}, 'x_values': [1, 10, 100], 'y_values': [1.0, 0.01, 0.0001], 'x_lim': [1, 10], 'log_fit': True, 'integral': 0.99}, BrokenPowerLaw1D: {'parameters': [1, 1, 2, 3], 'constraints': {'fixed': {'x_break': True}}, 'x_values': [0.1, 1, 10, 100], 'y_values': [100.0, 1.0, 0.001, 1e-06], 'x_lim': [0.1, 100], 'log_fit': True}, ExponentialCutoffPowerLaw1D: {'parameters': [1, 1, 2, 3], 'constraints': {'fixed': {'x_0': True}}, 'x_values': [0.1, 1, 10, 100], 'y_values': [96.72161, 0.716531311, 0.000356739933, 3.3382378e-19], 'x_lim': [0.01, 100], 'log_fit': True}, LogParabola1D: {'parameters': [1, 2, 3, 0.1], 'constraints': {'fixed': {'x_0': True}}, 'x_values': [0.1, 1, 10, 100], 'y_values': [3260.89063, 7.62472488, 0.00617440488, 1.73160572e-06], 'x_lim': [0.1, 100], 'log_fit': True}, Polynomial1D: {'parameters': {'degree': 2, 'c0': 1.0, 'c1': 1.0, 'c2': 1.0}, 'x_values': [1, 10, 100], 'y_values': [3, 111, 10101], 'x_lim': [(- 3), 3]}, Sersic1D: {'parameters': [1, 20, 4], 'x_values': [0.1, 1, 10, 100], 'y_values': [278.629391, 56.979143, 3.38788244, 0.0223941982], 'requires_scipy': True, 'x_lim': [0, 10], 'log_fit': True}, Voigt1D: {'parameters': [0, 1, 0.5, 0.9], 'x_values': [0, 2, 4, 8, 10], 'y_values': [0.520935, 0.017205, 0.003998, 0.000983, 0.000628], 'x_lim': [(- 3), 3]}}
tempResult = exp(1)
	
===================================================================	
module: 9	
----------------------------	

'\nHere are all the test parameters and values for the each\n`~astropy.modeling.FittableModel` defined. There is a dictionary for 1D and a\ndictionary for 2D models.\n\nExplanation of keywords of the dictionaries:\n\n"parameters" : list or dict\n    Model parameters, the model is tested with. Make sure you keep the right\n    order.  For polynomials you can also use a dict to specify the\n    coefficients. See examples below.\n\n"x_values" : list\n    x values where the model is evaluated.\n\n"y_values" : list\n    Reference y values for the in x_values given positions.\n\n"z_values" : list\n    Reference z values for the in x_values and y_values given positions.\n    (2D model option)\n\n"x_lim" : list\n    x test range for the model fitter. Depending on the model this can differ\n    e.g. the PowerLaw model should be tested over a few magnitudes.\n\n"y_lim" : list\n    y test range for the model fitter. Depending on the model this can differ\n    e.g. the PowerLaw model should be tested over a few magnitudes.  (2D model\n    option)\n\n"log_fit" : bool\n    PowerLaw models should be tested over a few magnitudes. So log_fit should\n    be true.\n\n"requires_scipy" : bool\n    If a model requires scipy (Bessel functions etc.) set this flag.\n\n"integral" : float\n    Approximate value of the integral in the range x_lim (and y_lim).\n\n"deriv_parameters" : list\n    If given the test of the derivative will use these parameters to create a\n    model (optional)\n\n"deriv_initial" : list\n    If given the test of the derivative will use these parameters as initial\n    values for the fit (optional)\n'
from __future__ import absolute_import, division, print_function, unicode_literals
from ..functional_models import Gaussian1D, Sine1D, Box1D, Linear1D, Lorentz1D, MexicanHat1D, Trapezoid1D, Const1D, Moffat1D, Gaussian2D, Const2D, Box2D, MexicanHat2D, TrapezoidDisk2D, AiryDisk2D, Moffat2D, Disk2D, Ring2D, Sersic1D, Sersic2D, Voigt1D, Planar2D
from ..polynomial import Polynomial1D, Polynomial2D
from ..powerlaws import PowerLaw1D, BrokenPowerLaw1D, ExponentialCutoffPowerLaw1D, LogParabola1D
import numpy as np
models_1D = {Gaussian1D: {'parameters': [1, 0, 1], 'x_values': [0, numpy.sqrt(2), (- numpy.sqrt(2))], 'y_values': [1.0, 0.367879, 0.367879], 'x_lim': [(- 10), 10], 'integral': numpy.sqrt((2 * numpy.pi))}, Sine1D: {'parameters': [1, 0.1, 0], 'x_values': [0, 2.5], 'y_values': [0, 1], 'x_lim': [(- 10), 10], 'integral': 0}, Box1D: {'parameters': [1, 0, 10], 'x_values': [(- 5), 5, 0, (- 10), 10], 'y_values': [1, 1, 1, 0, 0], 'x_lim': [(- 10), 10], 'integral': 10}, Linear1D: {'parameters': [1, 0], 'x_values': [0, numpy.pi, 42, (- 1)], 'y_values': [0, numpy.pi, 42, (- 1)], 'x_lim': [(- 10), 10], 'integral': 0}, Lorentz1D: {'parameters': [1, 0, 1], 'x_values': [0, (- 1), 1, 0.5, (- 0.5)], 'y_values': [1.0, 0.2, 0.2, 0.5, 0.5], 'x_lim': [(- 10), 10], 'integral': 1}, MexicanHat1D: {'parameters': [1, 0, 1], 'x_values': [0, 1, (- 1), 3, (- 3)], 'y_values': [1.0, 0.0, 0.0, (- 0.088872), (- 0.088872)], 'x_lim': [(- 20), 20], 'integral': 0}, Trapezoid1D: {'parameters': [1, 0, 2, 1], 'x_values': [0, 1, (- 1), 1.5, (- 1.5), 2, 2], 'y_values': [1, 1, 1, 0.5, 0.5, 0, 0], 'x_lim': [(- 10), 10], 'integral': 3}, Const1D: {'parameters': [1], 'x_values': [(- 1), 1, numpy.pi, (- 42.0), 0], 'y_values': [1, 1, 1, 1, 1], 'x_lim': [(- 10), 10], 'integral': 20}, Moffat1D: {'parameters': [1, 0, 1, 2], 'x_values': [0, 1, (- 1), 3, (- 3)], 'y_values': [1.0, 0.25, 0.25, 0.01, 0.01], 'x_lim': [(- 10), 10], 'integral': 1, 'deriv_parameters': [23.4, 1.2, 2.1, 2.3], 'deriv_initial': [10, 1, 1, 1]}, PowerLaw1D: {'parameters': [1, 1, 2], 'constraints': {'fixed': {'x_0': True}}, 'x_values': [1, 10, 100], 'y_values': [1.0, 0.01, 0.0001], 'x_lim': [1, 10], 'log_fit': True, 'integral': 0.99}, BrokenPowerLaw1D: {'parameters': [1, 1, 2, 3], 'constraints': {'fixed': {'x_break': True}}, 'x_values': [0.1, 1, 10, 100], 'y_values': [100.0, 1.0, 0.001, 1e-06], 'x_lim': [0.1, 100], 'log_fit': True}, ExponentialCutoffPowerLaw1D: {'parameters': [1, 1, 2, 3], 'constraints': {'fixed': {'x_0': True}}, 'x_values': [0.1, 1, 10, 100], 'y_values': [96.72161, 0.716531311, 0.000356739933, 3.3382378e-19], 'x_lim': [0.01, 100], 'log_fit': True}, LogParabola1D: {'parameters': [1, 2, 3, 0.1], 'constraints': {'fixed': {'x_0': True}}, 'x_values': [0.1, 1, 10, 100], 'y_values': [3260.89063, 7.62472488, 0.00617440488, 1.73160572e-06], 'x_lim': [0.1, 100], 'log_fit': True}, Polynomial1D: {'parameters': {'degree': 2, 'c0': 1.0, 'c1': 1.0, 'c2': 1.0}, 'x_values': [1, 10, 100], 'y_values': [3, 111, 10101], 'x_lim': [(- 3), 3]}, Sersic1D: {'parameters': [1, 20, 4], 'x_values': [0.1, 1, 10, 100], 'y_values': [278.629391, 56.979143, 3.38788244, 0.0223941982], 'requires_scipy': True, 'x_lim': [0, 10], 'log_fit': True}, Voigt1D: {'parameters': [0, 1, 0.5, 0.9], 'x_values': [0, 2, 4, 8, 10], 'y_values': [0.520935, 0.017205, 0.003998, 0.000983, 0.000628], 'x_lim': [(- 3), 3]}}
tempResult = exp(1)
	
===================================================================	
test_fit_with_fixed_and_bound_constraints: 286	
----------------------------	

"\n    Regression test for https://github.com/astropy/astropy/issues/2235\n\n    Currently doesn't test that the fit is any *good*--just that parameters\n    stay within their given constraints.\n    "
m = models.Gaussian1D(amplitude=3, mean=4, stddev=1, bounds={'mean': (4, 5)}, fixed={'amplitude': True})
x = numpy.linspace(0, 10, 10)
tempResult = exp(((- (x ** 2)) / 2))
	
===================================================================	
TestNonLinearConstraints.func: 81	
----------------------------	

tempResult = exp((((- 0.5) / (p[2] ** 2)) * ((x - p[1]) ** 2)))
	
===================================================================	
TestNonLinearConstraints.compmodel: 68	
----------------------------	

tempResult = exp((((- 0.5) / (p[1] ** 2)) * ((x - p[0]) ** 2)))
	
===================================================================	
Test1DFittingWithOutlierRemoval.func: 406	
----------------------------	

tempResult = exp((((- 0.5) * ((x - p[1]) ** 2)) / (p[2] ** 2)))
	
===================================================================	
TestNonLinearFitters.func1: 234	
----------------------------	

tempResult = exp((((- 0.5) / (p[2] ** 2)) * ((x - p[1]) ** 2)))
	
===================================================================	
TestNonLinearFitters.func: 205	
----------------------------	

tempResult = exp((((- 0.5) / (p[2] ** 2)) * ((x - p[1]) ** 2)))
	
===================================================================	
TestJointFitter.model: 145	
----------------------------	

tempResult = exp((((- 0.5) / (p[1] ** 2)) * ((x - p[0]) ** 2)))
	
===================================================================	
Test2DFittingWithOutlierRemoval.Gaussian_2D: 433	
----------------------------	

tempResult = exp(((((- 0.5) * ((pos[0] - p[2]) ** 2)) / (p[4] ** 2)) - ((0.5 * ((pos[1] - p[1]) ** 2)) / (p[3] ** 2))))
	
===================================================================	
rayleightest: 67	
----------------------------	

' Performs the Rayleigh test of uniformity.\n\n    This test is  used to identify a non-uniform distribution, i.e. it is\n    designed for detecting an unimodal deviation from uniformity. More\n    precisely, it assumes the following hypotheses:\n    - H0 (null hypothesis): The population is distributed uniformly around the\n    circle.\n    - H1 (alternative hypothesis): The population is not distributed uniformly\n    around the circle.\n    Small p-values suggest to reject the null hypothesis.\n\n    Parameters\n    ----------\n    data : numpy.ndarray or Quantity\n        Array of circular (directional) data, which is assumed to be in\n        radians whenever ``data`` is ``numpy.ndarray``.\n    axis : int, optional\n        Axis along which the Rayleigh test will be performed.\n    weights : numpy.ndarray, optional\n        In case of grouped data, the i-th element of ``weights`` represents a\n        weighting factor for each group such that ``np.sum(weights, axis)``\n        equals the number of observations.\n        See [1], remark 1.4, page 22, for detailed explanation.\n\n    Returns\n    -------\n    p-value : float or dimensionless Quantity\n        p-value.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from astropy.stats import rayleightest\n    >>> from astropy import units as u\n    >>> data = np.array([130, 90, 0, 145])*u.deg\n    >>> rayleightest(data) # doctest: +FLOAT_CMP\n    <Quantity 0.2563487733797317>\n\n    References\n    ----------\n    .. [1] S. R. Jammalamadaka, A. SenGupta. "Topics in Circular Statistics".\n       Series on Multivariate Analysis, Vol. 5, 2001.\n    .. [2] C. Agostinelli, U. Lund. "Circular Statistics from \'Topics in\n       Circular Statistics (2001)\'". 2015.\n       <https://cran.r-project.org/web/packages/CircStats/CircStats.pdf>\n    .. [3] M. Chirstman., C. Miller. "Testing a Sample of Directions for\n       Uniformity." Lecture Notes, STA 6934/5805. University of Florida, 2007.\n    .. [4] D. Wilkie. "Rayleigh Test for Randomness of Circular Data". Applied\n       Statistics. 1983.\n       <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.211.4762>\n    '
n = numpy.size(data, axis=axis)
Rbar = _length(data, 1, 0.0, axis, weights)
z = ((n * Rbar) * Rbar)
tmp = 1.0
if (n < 50):
    tmp = ((1.0 + (((2.0 * z) - (z * z)) / (4.0 * n))) - (((((24.0 * z) - (132.0 * (z ** 2.0))) + (76.0 * (z ** 3.0))) - (9.0 * (z ** 4.0))) / ((288.0 * n) * n)))
tempResult = exp((- z))
	
===================================================================	
trig_sum: 62	
----------------------------	

"Compute (approximate) trigonometric sums for a number of frequencies\n    This routine computes weighted sine and cosine sums:\n        S_j = sum_i { h_i * sin(2 pi * f_j * t_i) }\n        C_j = sum_i { h_i * cos(2 pi * f_j * t_i) }\n    Where f_j = freq_factor * (f0 + j * df) for the values j in 1 ... N.\n    The sums can be computed either by a brute force O[N^2] method, or\n    by an FFT-based O[Nlog(N)] method.\n\n    Parameters\n    ----------\n    t : array_like\n        array of input times\n    h : array_like\n        array weights for the sum\n    df : float\n        frequency spacing\n    N : int\n        number of frequency bins to return\n    f0 : float (optional, default=0)\n        The low frequency to use\n    freq_factor : float (optional, default=1)\n        Factor which multiplies the frequency\n    use_fft : bool\n        if True, use the approximate FFT algorithm to compute the result.\n        This uses the FFT with Press & Rybicki's Lagrangian extirpolation.\n    oversampling : int (default = 5)\n        oversampling freq_factor for the approximation; roughly the number of\n        time samples across the highest-frequency sinusoid. This parameter\n        contains the tradeoff between accuracy and speed. Not referenced\n        if use_fft is False.\n    Mfft : int\n        The number of adjacent points to use in the FFT approximation.\n        Not referenced if use_fft is False.\n\n    Returns\n    -------\n    S, C : ndarrays\n        summation arrays for frequencies f = df * np.arange(1, N + 1)\n    "
df *= freq_factor
f0 *= freq_factor
if (df <= 0):
    raise ValueError('df must be positive')
(t, h) = map(numpy.ravel, numpy.broadcast_arrays(t, h))
if use_fft:
    Mfft = int(Mfft)
    if (Mfft <= 0):
        raise ValueError('Mfft must be positive')
    Nfft = bitceil((N * oversampling))
    t0 = t.min()
    if (f0 > 0):
        tempResult = exp((((2j * numpy.pi) * f0) * (t - t0)))
	
===================================================================	
trig_sum: 68	
----------------------------	

"Compute (approximate) trigonometric sums for a number of frequencies\n    This routine computes weighted sine and cosine sums:\n        S_j = sum_i { h_i * sin(2 pi * f_j * t_i) }\n        C_j = sum_i { h_i * cos(2 pi * f_j * t_i) }\n    Where f_j = freq_factor * (f0 + j * df) for the values j in 1 ... N.\n    The sums can be computed either by a brute force O[N^2] method, or\n    by an FFT-based O[Nlog(N)] method.\n\n    Parameters\n    ----------\n    t : array_like\n        array of input times\n    h : array_like\n        array weights for the sum\n    df : float\n        frequency spacing\n    N : int\n        number of frequency bins to return\n    f0 : float (optional, default=0)\n        The low frequency to use\n    freq_factor : float (optional, default=1)\n        Factor which multiplies the frequency\n    use_fft : bool\n        if True, use the approximate FFT algorithm to compute the result.\n        This uses the FFT with Press & Rybicki's Lagrangian extirpolation.\n    oversampling : int (default = 5)\n        oversampling freq_factor for the approximation; roughly the number of\n        time samples across the highest-frequency sinusoid. This parameter\n        contains the tradeoff between accuracy and speed. Not referenced\n        if use_fft is False.\n    Mfft : int\n        The number of adjacent points to use in the FFT approximation.\n        Not referenced if use_fft is False.\n\n    Returns\n    -------\n    S, C : ndarrays\n        summation arrays for frequencies f = df * np.arange(1, N + 1)\n    "
df *= freq_factor
f0 *= freq_factor
if (df <= 0):
    raise ValueError('df must be positive')
(t, h) = map(numpy.ravel, numpy.broadcast_arrays(t, h))
if use_fft:
    Mfft = int(Mfft)
    if (Mfft <= 0):
        raise ValueError('Mfft must be positive')
    Nfft = bitceil((N * oversampling))
    t0 = t.min()
    if (f0 > 0):
        h = (h * numpy.exp((((2j * numpy.pi) * f0) * (t - t0))))
    tnorm = ((((t - t0) * Nfft) * df) % Nfft)
    grid = extirpolate(tnorm, h, Nfft, Mfft)
    fftgrid = numpy.fft.ifft(grid)[:N]
    if (t0 != 0):
        f = (f0 + (df * numpy.arange(N)))
        tempResult = exp((((2j * numpy.pi) * t0) * f))
	
===================================================================	
test_fitness_function_results: 94	
----------------------------	

'Test results for several fitness functions'
rng = numpy.random.RandomState(42)
t = rng.randn(100)
edges = bayesian_blocks(t, fitness='events')
assert_allclose(edges, [(- 2.6197451), (- 0.71094865), 0.36866702, 1.85227818])
t[80:] = t[:20]
edges = bayesian_blocks(t, fitness='events', p0=0.01)
assert_allclose(edges, [(- 2.6197451), (- 0.47432431), (- 0.46202823), 1.85227818])
dt = 0.01
t = (dt * numpy.arange(1000))
x = numpy.zeros(len(t))
N = (len(t) // 10)
x[rng.randint(0, len(t), N)] = 1
x[rng.randint(0, (len(t) // 2), N)] = 1
edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)
assert_allclose(edges, [0, 5.105, 9.99])
t = (100 * rng.rand(20))
tempResult = exp(((- 0.5) * ((t - 50) ** 2)))
	
===================================================================	
test_measures_fitness_heteroscedastic: 37	
----------------------------	

rng = numpy.random.RandomState(1)
t = numpy.linspace(0, 1, 11)
tempResult = exp((((- 0.5) * ((t - 0.5) ** 2)) / (0.01 ** 2)))
	
===================================================================	
test_measures_fitness_homoscedastic: 28	
----------------------------	

rng = numpy.random.RandomState(rseed)
t = numpy.linspace(0, 1, 11)
tempResult = exp((((- 0.5) * ((t - 0.5) ** 2)) / (0.01 ** 2)))
	
===================================================================	
test_equivalency_context: 316	
----------------------------	

with units.set_enabled_equivalencies(units.dimensionless_angles()):
    phase = units.Quantity(1.0, units.cycle)
    tempResult = exp((1j * phase))
	
===================================================================	
test_equivalency_context: 318	
----------------------------	

with units.set_enabled_equivalencies(units.dimensionless_angles()):
    phase = units.Quantity(1.0, units.cycle)
    assert_allclose(numpy.exp((1j * phase)), 1.0)
    Omega = (units.cycle / (1.0 * units.minute))
    tempResult = exp((((1j * Omega) * 60.0) * units.second))
	
===================================================================	
TestInplaceUfuncs.test_one_argument_ufunc_inplace_2: 475	
----------------------------	

'Check inplace works with non-quantity input and quantity output'
s = (value * units.m)
check = s
numpy.absolute(value, out=s)
assert (check is s)
assert numpy.all((check.value == numpy.absolute(value)))
assert (check.unit is units.dimensionless_unscaled)
numpy.sqrt(value, out=s)
assert (check is s)
assert numpy.all((check.value == numpy.sqrt(value)))
assert (check.unit is units.dimensionless_unscaled)
tempResult = exp(value, out=s)
	
===================================================================	
TestInplaceUfuncs.test_one_argument_ufunc_inplace_2: 477	
----------------------------	

'Check inplace works with non-quantity input and quantity output'
s = (value * units.m)
check = s
numpy.absolute(value, out=s)
assert (check is s)
assert numpy.all((check.value == numpy.absolute(value)))
assert (check.unit is units.dimensionless_unscaled)
numpy.sqrt(value, out=s)
assert (check is s)
assert numpy.all((check.value == numpy.sqrt(value)))
assert (check.unit is units.dimensionless_unscaled)
numpy.exp(value, out=s)
assert (check is s)
tempResult = exp(value)
	
===================================================================	
InvertedLogStretch.__call__: 164	
----------------------------	

values = _prepare(values, clip=clip, out=out)
numpy.multiply(values, numpy.log((self.exp + 1.0)), out=values)
tempResult = exp(values, out=values)
	
***************************************************	
scipy_scipy-0.19.0: 312	
===================================================================	
_analytical_solution: 80	
----------------------------	

'\n    Analytical solution to the linear differential equations dy/dt = a*y.\n\n    The solution is only valid if `a` is diagonalizable.\n\n    Returns a 2-d array with shape (len(t), len(y0)).\n    '
(lam, v) = numpy.linalg.eig(a)
c = numpy.linalg.solve(v, y0)
tempResult = exp((lam * t.reshape((- 1), 1)))
	
===================================================================	
exp_sol: 37	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
exp_sol: 37	
----------------------------	

tempResult = exp((x - 2))
	
===================================================================	
exp_sol: 37	
----------------------------	

tempResult = exp((- 2))
	
===================================================================	
CoupledDecay.verify: 392	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
tempResult = exp(((- lmbd[0]) * t))
	
===================================================================	
CoupledDecay.verify: 393	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
tempResult = exp(((- lmbd[1]) * t))
	
===================================================================	
CoupledDecay.verify: 394	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
e1 = numpy.exp(((- lmbd[1]) * t))
tempResult = exp(((- lmbd[2]) * t))
	
===================================================================	
Rbf._h_gaussian: 24	
----------------------------	

tempResult = exp((- (((1.0 / self.epsilon) * r) ** 2)))
	
===================================================================	
module: 54	
----------------------------	

' Nose test generators\n\nNeed function load / save / roundtrip tests\n\n'
from __future__ import division, print_function, absolute_import
import os
from os.path import join as pjoin, dirname
from glob import glob
from io import BytesIO
from tempfile import mkdtemp
from scipy._lib.six import u, text_type, string_types
import warnings
import shutil
import gzip
from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_raises, run_module_suite, assert_
import numpy as np
from numpy import array
import scipy.sparse as SP
import scipy.io.matlab.byteordercodes as boc
from scipy.io.matlab.miobase import matdims, MatWriteError, MatReadError
from scipy.io.matlab.mio import mat_reader_factory, loadmat, savemat, whosmat
from scipy.io.matlab.mio5 import MatlabObject, MatFile5Writer, MatFile5Reader, MatlabFunction, varmats_from_mat, to_writeable, EmptyStructMarker
from scipy.io.matlab import mio5_params as mio5p
test_data_path = pjoin(dirname(__file__), 'data')

def mlarr(*args, **kwargs):
    'Convenience function to return matlab-compatible 2D array.'
    arr = numpy.array(*args, **kwargs)
    arr.shape = matdims(arr)
    return arr
theta = ((numpy.pi / 4) * np.arange(9, dtype=float).reshape(1, 9))
case_table4 = [{'name': 'double', 'classes': {'testdouble': 'double'}, 'expected': {'testdouble': theta}}]
case_table4.append({'name': 'string', 'classes': {'teststring': 'char'}, 'expected': {'teststring': array([u('"Do nine men interpret?" "Nine men," I nod.')])}})
case_table4.append({'name': 'complex', 'classes': {'testcomplex': 'double'}, 'expected': {'testcomplex': (numpy.cos(theta) + (1j * numpy.sin(theta)))}})
A = numpy.zeros((3, 5))
A[0] = list(range(1, 6))
A[:, 0] = list(range(1, 4))
case_table4.append({'name': 'matrix', 'classes': {'testmatrix': 'double'}, 'expected': {'testmatrix': A}})
case_table4.append({'name': 'sparse', 'classes': {'testsparse': 'sparse'}, 'expected': {'testsparse': scipy.sparse.coo_matrix(A)}})
B = A.astype(complex)
B[(0, 0)] += 1j
case_table4.append({'name': 'sparsecomplex', 'classes': {'testsparsecomplex': 'sparse'}, 'expected': {'testsparsecomplex': scipy.sparse.coo_matrix(B)}})
case_table4.append({'name': 'multi', 'classes': {'theta': 'double', 'a': 'double'}, 'expected': {'theta': theta, 'a': A}})
case_table4.append({'name': 'minus', 'classes': {'testminus': 'double'}, 'expected': {'testminus': mlarr((- 1))}})
case_table4.append({'name': 'onechar', 'classes': {'testonechar': 'char'}, 'expected': {'testonechar': array([u('r')])}})
CA = mlarr(([], mlarr([1]), mlarr([[1, 2]]), mlarr([[1, 2, 3]])), dtype=object).reshape(1, (- 1))
CA[(0, 0)] = array([u('This cell contains this string and 3 arrays of increasing length')])
case_table5 = [{'name': 'cell', 'classes': {'testcell': 'cell'}, 'expected': {'testcell': CA}}]
CAE = mlarr((mlarr(1), mlarr(2), mlarr([]), mlarr([]), mlarr(3)), dtype=object).reshape(1, (- 1))
objarr = numpy.empty((1, 1), dtype=object)
objarr[(0, 0)] = mlarr(1)
case_table5.append({'name': 'scalarcell', 'classes': {'testscalarcell': 'cell'}, 'expected': {'testscalarcell': objarr}})
case_table5.append({'name': 'emptycell', 'classes': {'testemptycell': 'cell'}, 'expected': {'testemptycell': CAE}})
case_table5.append({'name': 'stringarray', 'classes': {'teststringarray': 'char'}, 'expected': {'teststringarray': array([u('one  '), u('two  '), u('three')])}})
case_table5.append({'name': '3dmatrix', 'classes': {'test3dmatrix': 'double'}, 'expected': {'test3dmatrix': numpy.transpose(numpy.reshape(list(range(1, 25)), (4, 3, 2)))}})
tempResult = exp(1)
	
===================================================================	
dft: 235	
----------------------------	

'\n    Discrete Fourier transform matrix.\n\n    Create the matrix that computes the discrete Fourier transform of a\n    sequence [1]_.  The n-th primitive root of unity used to generate the\n    matrix is exp(-2*pi*i/n), where i = sqrt(-1).\n\n    Parameters\n    ----------\n    n : int\n        Size the matrix to create.\n    scale : str, optional\n        Must be None, \'sqrtn\', or \'n\'.\n        If `scale` is \'sqrtn\', the matrix is divided by `sqrt(n)`.\n        If `scale` is \'n\', the matrix is divided by `n`.\n        If `scale` is None (the default), the matrix is not normalized, and the\n        return value is simply the Vandermonde matrix of the roots of unity.\n\n    Returns\n    -------\n    m : (n, n) ndarray\n        The DFT matrix.\n\n    Notes\n    -----\n    When `scale` is None, multiplying a vector by the matrix returned by\n    `dft` is mathematically equivalent to (but much less efficient than)\n    the calculation performed by `scipy.fftpack.fft`.\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] "DFT matrix", http://en.wikipedia.org/wiki/DFT_matrix\n\n    Examples\n    --------\n    >>> from scipy.linalg import dft\n    >>> np.set_printoptions(precision=5, suppress=True)\n    >>> x = np.array([1, 2, 3, 0, 3, 2, 1, 0])\n    >>> m = dft(8)\n    >>> m.dot(x)   # Compute the DFT of x\n    array([ 12.+0.j,  -2.-2.j,   0.-4.j,  -2.+2.j,   4.+0.j,  -2.-2.j,\n            -0.+4.j,  -2.+2.j])\n\n    Verify that ``m.dot(x)`` is the same as ``fft(x)``.\n\n    >>> from scipy.fftpack import fft\n    >>> fft(x)     # Same result as m.dot(x)\n    array([ 12.+0.j,  -2.-2.j,   0.-4.j,  -2.+2.j,   4.+0.j,  -2.-2.j,\n             0.+4.j,  -2.+2.j])\n    '
if (scale not in [None, 'sqrtn', 'n']):
    raise ValueError(("scale must be None, 'sqrtn', or 'n'; %r is not valid." % (scale,)))
tempResult = exp(((((- 2j) * numpy.pi) * numpy.arange(n)) / n))
	
===================================================================	
_fractional_power_superdiag_entry: 101	
----------------------------	

'\n    Compute a superdiagonal entry of a fractional matrix power.\n\n    This is Eq. (5.6) in [1]_.\n\n    Parameters\n    ----------\n    l1 : complex\n        A diagonal entry of the matrix.\n    l2 : complex\n        A diagonal entry of the matrix.\n    t12 : complex\n        A superdiagonal entry of the matrix.\n    p : float\n        A fractional power.\n\n    Returns\n    -------\n    f12 : complex\n        A superdiagonal entry of the fractional matrix power.\n\n    Notes\n    -----\n    Care has been taken to return a real number if possible when\n    all of the inputs are real numbers.\n\n    References\n    ----------\n    .. [1] Nicholas J. Higham and Lijing lin (2011)\n           "A Schur-Pade Algorithm for Fractional Powers of a Matrix."\n           SIAM Journal on Matrix Analysis and Applications,\n           32 (3). pp. 1056-1078. ISSN 0895-4798\n\n    '
if (l1 == l2):
    f12 = ((t12 * p) * (l1 ** (p - 1)))
elif (abs((l2 - l1)) > (abs((l1 + l2)) / 2)):
    f12 = ((t12 * ((l2 ** p) - (l1 ** p))) / (l2 - l1))
else:
    z = ((l2 - l1) / (l2 + l1))
    log_l1 = numpy.log(l1)
    log_l2 = numpy.log(l2)
    arctanh_z = numpy.arctanh(z)
    tempResult = exp(((p / 2) * (log_l2 + log_l1)))
	
===================================================================	
TestFractionalMatrixPower.test_random_matrices_and_powers: 299	
----------------------------	

numpy.random.seed(1234)
nsamples = 20
for i in range(nsamples):
    n = random.randrange(1, 5)
    p = numpy.random.randn()
    tempResult = exp(random.randrange((- 4), 5))
	
===================================================================	
_exp_fjb: 55	
----------------------------	

tempResult = exp((B[1] * x))
	
===================================================================	
_exp_fcn: 49	
----------------------------	

tempResult = exp((B[1] * x))
	
===================================================================	
_exp_fjd: 52	
----------------------------	

tempResult = exp((B[1] * x))
	
===================================================================	
TestODR.explicit_fjb: 35	
----------------------------	

tempResult = exp((B[2] * x))
	
===================================================================	
TestODR.multi_fcn: 68	
----------------------------	

if (x < 0.0).any():
    raise OdrStop
theta = ((pi * B[3]) / 2.0)
ctheta = numpy.cos(theta)
stheta = numpy.sin(theta)
tempResult = exp((- B[2]))
	
===================================================================	
TestODR.explicit_fjd: 30	
----------------------------	

tempResult = exp((B[2] * x))
	
===================================================================	
TestODR.explicit_fcn: 26	
----------------------------	

tempResult = exp((B[2] * x))
	
===================================================================	
Metropolis.accept_reject: 183	
----------------------------	

tempResult = exp(((- (energy_new - energy_old)) * self.beta))
	
===================================================================	
ExponentialFittingProblem.fun: 116	
----------------------------	

tempResult = exp((p[1] * self.x))
	
===================================================================	
ExponentialFittingProblem.jac: 121	
----------------------------	

J = numpy.empty((self.m, self.n))
J[:, 0] = 1
tempResult = exp((p[1] * self.x))
	
===================================================================	
ExponentialFittingProblem.__init__: 109	
----------------------------	

numpy.random.seed(random_seed)
self.m = n_points
self.n = 2
self.p0 = numpy.zeros(2)
self.x = numpy.linspace(x_range[0], x_range[1], n_points)
tempResult = exp((b * self.x))
	
===================================================================	
TestLineSearch._scalar_func_2: 50	
----------------------------	

self.fcount += 1
tempResult = exp(((- 4) * s))
	
===================================================================	
TestLineSearch._scalar_func_2: 51	
----------------------------	

self.fcount += 1
p = (numpy.exp(((- 4) * s)) + (s ** 2))
tempResult = exp(((- 4) * s))
	
===================================================================	
TestCurveFit.f11: 306	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestCurveFit.func111: 394	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestCurveFit.f_double_gauss: 235	
----------------------------	

tempResult = exp(((- ((x - x0) ** 2)) / (2.0 * (sigma ** 2))))
	
===================================================================	
TestCurveFit.f_double_gauss: 235	
----------------------------	

tempResult = exp(((- ((x - x1) ** 2)) / (2.0 * (sigma ** 2))))
	
===================================================================	
TestCurveFit.jacp: 390	
----------------------------	

rotn = numpy.array([[(1.0 / numpy.sqrt(2)), ((- 1.0) / numpy.sqrt(2)), 0], [(1.0 / numpy.sqrt(2)), (1.0 / numpy.sqrt(2)), 0], [0, 0, 1.0]])
tempResult = exp(((- b) * x))
	
===================================================================	
TestCurveFit.jac11: 397	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestCurveFit.func11: 364	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestCurveFit.jac: 335	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestCurveFit.f1: 295	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestFixedPoint.test_lambertw: 476	
----------------------------	

tempResult = exp(((- 2.0) * xx))
	
===================================================================	
TestFixedPoint.test_lambertw: 477	
----------------------------	

xxroot = fixed_point((lambda xx: (numpy.exp(((- 2.0) * xx)) / 2.0)), 1.0, args=(), xtol=1e-12, maxiter=500)
tempResult = exp(((- 2.0) * xxroot))
	
===================================================================	
TestCurveFit.jac1: 367	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestLeastSq.func: 177	
----------------------------	

tempResult = exp(((- ((x - p[1]) ** 2)) / (2.0 * (p[2] ** 2))))
	
===================================================================	
TestCurveFit.f1111: 332	
----------------------------	

tempResult = exp(((- b) * x))
	
===================================================================	
TestCurveFit.funcp: 386	
----------------------------	

rotn = numpy.array([[(1.0 / numpy.sqrt(2)), ((- 1.0) / numpy.sqrt(2)), 0], [(1.0 / numpy.sqrt(2)), (1.0 / numpy.sqrt(2)), 0], [0, 0, 1.0]])
tempResult = exp(((- b) * x))
	
===================================================================	
F6: 54	
----------------------------	

(x1, x2) = x
J0 = numpy.array([[(- 4.256), 14.7], [0.8394989, 0.59964207]])
tempResult = exp(x1)
	
===================================================================	
F4_powell: 42	
----------------------------	

A = 10000.0
tempResult = exp((- x[0]))
	
===================================================================	
F4_powell: 42	
----------------------------	

A = 10000.0
tempResult = exp((- x[1]))
	
===================================================================	
logit: 14	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
CheckOptimize.grad: 52	
----------------------------	

self.gradcalls += 1
log_pdot = numpy.dot(self.F, x)
tempResult = exp(log_pdot)
	
===================================================================	
CheckOptimize.grad: 53	
----------------------------	

self.gradcalls += 1
log_pdot = numpy.dot(self.F, x)
logZ = numpy.log(sum(numpy.exp(log_pdot)))
tempResult = exp((log_pdot - logZ))
	
===================================================================	
CheckOptimize.func: 44	
----------------------------	

self.funccalls += 1
if (self.funccalls > 6000):
    raise RuntimeError('too many iterations in optimization routine')
log_pdot = numpy.dot(self.F, x)
tempResult = exp(log_pdot)
	
===================================================================	
TestBrute.f3: 736	
----------------------------	

(x, y) = z
(a, b, c, d, e, f, g, h, i, j, k, l, scale) = params
tempResult = exp(((- (((x - k) ** 2) + ((y - l) ** 2))) / scale))
	
===================================================================	
der_logit: 17	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
der_logit: 17	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
CheckOptimize.hess: 58	
----------------------------	

log_pdot = numpy.dot(self.F, x)
tempResult = exp(log_pdot)
	
===================================================================	
CheckOptimize.hess: 59	
----------------------------	

log_pdot = numpy.dot(self.F, x)
logZ = numpy.log(sum(numpy.exp(log_pdot)))
tempResult = exp((log_pdot - logZ))
	
===================================================================	
TestBrute.f2: 731	
----------------------------	

(x, y) = z
(a, b, c, d, e, f, g, h, i, j, k, l, scale) = params
tempResult = exp(((- (((x - h) ** 2) + ((y - i) ** 2))) / scale))
	
===================================================================	
TestApproxDerivativesDense.jac_parametrized: 113	
----------------------------	

tempResult = exp((c0 * x[0]))
	
===================================================================	
TestApproxDerivativesDense.jac_parametrized: 113	
----------------------------	

tempResult = exp((c1 * x[1]))
	
===================================================================	
TestApproxDerivativesDense.fun_scalar_vector: 89	
----------------------------	

tempResult = exp(x[0])
	
===================================================================	
TestApproxDerivativesDense.fun_parametrized: 110	
----------------------------	

tempResult = exp((c0 * x[0]))
	
===================================================================	
TestApproxDerivativesDense.fun_parametrized: 110	
----------------------------	

tempResult = exp((c1 * x[1]))
	
===================================================================	
TestApproxDerivativesDense.wrong_dimensions_fun: 98	
----------------------------	

tempResult = exp(x)
	
===================================================================	
TestApproxDerivativesDense.jac_scalar_vector: 92	
----------------------------	

tempResult = exp(x[0])
	
===================================================================	
buttap: 951	
----------------------------	

'Return (z,p,k) for analog prototype of Nth-order Butterworth filter.\n\n    The filter will have an angular (e.g. rad/s) cutoff frequency of 1.\n\n    See Also\n    --------\n    butter : Filter design function using this prototype\n\n    '
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
z = numpy.array([])
m = numpy.arange(((- N) + 1), N, 2)
tempResult = exp((((1j * pi) * m) / (2 * N)))
	
===================================================================	
group_delay: 129	
----------------------------	

'Compute the group delay of a digital filter.\n\n    The group delay measures by how many samples amplitude envelopes of\n    various spectral components of a signal are delayed by a filter.\n    It is formally defined as the derivative of continuous (unwrapped) phase::\n\n               d        jw\n     D(w) = - -- arg H(e)\n              dw\n\n    Parameters\n    ----------\n    system : tuple of array_like (b, a)\n        Numerator and denominator coefficients of a filter transfer function.\n    w : {None, int, array-like}, optional\n        If None (default), then compute at 512 frequencies equally spaced\n        around the unit circle.\n        If a single integer, then compute at that many frequencies.\n        If array, compute the delay at the frequencies given\n        (in radians/sample).\n    whole : bool, optional\n        Normally, frequencies are computed from 0 to the Nyquist frequency,\n        pi radians/sample (upper-half of unit-circle).  If `whole` is True,\n        compute frequencies from 0 to ``2*pi`` radians/sample.\n\n    Returns\n    -------\n    w : ndarray\n        The normalized frequencies at which the group delay was computed,\n        in radians/sample.\n    gd : ndarray\n        The group delay.\n\n    Notes\n    -----\n    The similar function in MATLAB is called `grpdelay`.\n\n    If the transfer function :math:`H(z)` has zeros or poles on the unit\n    circle, the group delay at corresponding frequencies is undefined.\n    When such a case arises the warning is raised and the group delay\n    is set to 0 at those frequencies.\n\n    For the details of numerical computation of the group delay refer to [1]_.\n\n    .. versionadded: 0.16.0\n\n    See Also\n    --------\n    freqz : Frequency response of a digital filter\n\n    References\n    ----------\n    .. [1] Richard G. Lyons, "Understanding Digital Signal Processing,\n           3rd edition", p. 830.\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> b, a = signal.iirdesign(0.1, 0.3, 5, 50, ftype=\'cheby1\')\n    >>> w, gd = signal.group_delay((b, a))\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.title(\'Digital filter group delay\')\n    >>> plt.plot(w, gd)\n    >>> plt.ylabel(\'Group delay [samples]\')\n    >>> plt.xlabel(\'Frequency [rad/sample]\')\n    >>> plt.show()\n\n    '
if (w is None):
    w = 512
if isinstance(w, int):
    if whole:
        w = numpy.linspace(0, (2 * pi), w, endpoint=False)
    else:
        w = numpy.linspace(0, pi, w, endpoint=False)
w = numpy.atleast_1d(w)
(b, a) = map(numpy.atleast_1d, system)
c = numpy.convolve(b, a[::(- 1)])
cr = (c * numpy.arange(c.size))
tempResult = exp(((- 1j) * w))
	
===================================================================	
minimum_phase: 218	
----------------------------	

'Convert a linear-phase FIR filter to minimum phase\n\n    Parameters\n    ----------\n    h : array\n        Linear-phase FIR filter coefficients.\n    method : {\'hilbert\', \'homomorphic\'}\n        The method to use:\n\n            \'homomorphic\' (default)\n                This method [4]_ [5]_ works best with filters with an\n                odd number of taps, and the resulting minimum phase filter\n                will have a magnitude response that approximates the square\n                root of the the original filter\'s magnitude response.\n\n            \'hilbert\'\n                This method [1]_ is designed to be used with equiripple\n                filters (e.g., from `remez`) with unity or zero gain\n                regions.\n\n    n_fft : int\n        The number of points to use for the FFT. Should be at least a\n        few times larger than the signal length (see Notes).\n\n    Returns\n    -------\n    h_minimum : array\n        The minimum-phase version of the filter, with length\n        ``(length(h) + 1) // 2``.\n\n    See Also\n    --------\n    firwin\n    firwin2\n    remez\n\n    Notes\n    -----\n    Both the Hilbert [1]_ or homomorphic [4]_ [5]_ methods require selection\n    of an FFT length to estimate the complex cepstrum of the filter.\n\n    In the case of the Hilbert method, the deviation from the ideal\n    spectrum ``epsilon`` is related to the number of stopband zeros\n    ``n_stop`` and FFT length ``n_fft`` as::\n\n        epsilon = 2. * n_stop / n_fft\n\n    For example, with 100 stopband zeros and a FFT length of 2048,\n    ``epsilon = 0.0976``. If we conservatively assume that the number of\n    stopband zeros is one less than the filter length, we can take the FFT\n    length to be the next power of 2 that satisfies ``epsilon=0.01`` as::\n\n        n_fft = 2 ** int(np.ceil(np.log2(2 * (len(h) - 1) / 0.01)))\n\n    This gives reasonable results for both the Hilbert and homomorphic\n    methods, and gives the value used when ``n_fft=None``.\n\n    Alternative implementations exist for creating minimum-phase filters,\n    including zero inversion [2]_ and spectral factorization [3]_ [4]_.\n    For more information, see:\n\n        http://dspguru.com/dsp/howtos/how-to-design-minimum-phase-fir-filters\n\n    Examples\n    --------\n    Create an optimal linear-phase filter, then convert it to minimum phase:\n\n    >>> from scipy.signal import remez, minimum_phase, freqz, group_delay\n    >>> import matplotlib.pyplot as plt\n    >>> freq = [0, 0.2, 0.3, 1.0]\n    >>> desired = [1, 0]\n    >>> h_linear = remez(151, freq, desired, Hz=2.)\n\n    Convert it to minimum phase:\n\n    >>> h_min_hom = minimum_phase(h_linear, method=\'homomorphic\')\n    >>> h_min_hil = minimum_phase(h_linear, method=\'hilbert\')\n\n    Compare the three filters:\n\n    >>> fig, axs = plt.subplots(4, figsize=(4, 8))\n    >>> for h, style, color in zip((h_linear, h_min_hom, h_min_hil),\n    ...                            (\'-\', \'-\', \'--\'), (\'k\', \'r\', \'c\')):\n    ...     w, H = freqz(h)\n    ...     w, gd = group_delay((h, 1))\n    ...     w /= np.pi\n    ...     axs[0].plot(h, color=color, linestyle=style)\n    ...     axs[1].plot(w, np.abs(H), color=color, linestyle=style)\n    ...     axs[2].plot(w, 20 * np.log10(np.abs(H)), color=color, linestyle=style)\n    ...     axs[3].plot(w, gd, color=color, linestyle=style)\n    >>> for ax in axs:\n    ...     ax.grid(True, color=\'0.5\')\n    ...     ax.fill_between(freq[1:3], *ax.get_ylim(), color=\'#ffeeaa\', zorder=1)\n    >>> axs[0].set(xlim=[0, len(h_linear) - 1], ylabel=\'Amplitude\', xlabel=\'Samples\')\n    >>> axs[1].legend([\'Linear\', \'Min-Hom\', \'Min-Hil\'], title=\'Phase\')\n    >>> for ax, ylim in zip(axs[1:], ([0, 1.1], [-150, 10], [-60, 60])):\n    ...     ax.set(xlim=[0, 1], ylim=ylim, xlabel=\'Frequency\')\n    >>> axs[1].set(ylabel=\'Magnitude\')\n    >>> axs[2].set(ylabel=\'Magnitude (dB)\')\n    >>> axs[3].set(ylabel=\'Group delay\')\n    >>> plt.tight_layout()\n\n    References\n    ----------\n    .. [1] N. Damera-Venkata and B. L. Evans, "Optimal design of real and\n           complex minimum phase digital FIR filters," Acoustics, Speech,\n           and Signal Processing, 1999. Proceedings., 1999 IEEE International\n           Conference on, Phoenix, AZ, 1999, pp. 1145-1148 vol.3.\n           doi: 10.1109/ICASSP.1999.756179\n    .. [2] X. Chen and T. W. Parks, "Design of optimal minimum phase FIR\n           filters by direct factorization," Signal Processing,\n           vol. 10, no. 4, pp. 369383, Jun. 1986.\n    .. [3] T. Saramaki, "Finite Impulse Response Filter Design," in\n           Handbook for Digital Signal Processing, chapter 4,\n           New York: Wiley-Interscience, 1993.\n    .. [4] J. S. Lim, Advanced Topics in Signal Processing.\n           Englewood Cliffs, N.J.: Prentice Hall, 1988.\n    .. [5] A. V. Oppenheim, R. W. Schafer, and J. R. Buck,\n           "Discrete-Time Signal Processing," 2nd edition.\n           Upper Saddle River, N.J.: Prentice Hall, 1999.\n    '
h = numpy.asarray(h)
if numpy.iscomplexobj(h):
    raise ValueError('Complex filters not supported')
if ((h.ndim != 1) or (h.size <= 2)):
    raise ValueError('h must be 1D and at least 2 samples long')
n_half = (len(h) // 2)
if (not numpy.allclose(h[(- n_half):][::(- 1)], h[:n_half])):
    warnings.warn('h does not appear to by symmetric, conversion may fail', RuntimeWarning)
if ((not isinstance(method, string_types)) or (method not in ('homomorphic', 'hilbert'))):
    raise ValueError(('method must be "homomorphic" or "hilbert", got %r' % (method,)))
if (n_fft is None):
    n_fft = (2 ** int(numpy.ceil(numpy.log2(((2 * (len(h) - 1)) / 0.01)))))
n_fft = int(n_fft)
if (n_fft < len(h)):
    raise ValueError(('n_fft must be at least len(h)==%s' % len(h)))
if (method == 'hilbert'):
    w = (numpy.arange(n_fft) * (((2 * numpy.pi) / n_fft) * n_half))
    tempResult = exp((1j * w))
	
===================================================================	
minimum_phase: 240	
----------------------------	

'Convert a linear-phase FIR filter to minimum phase\n\n    Parameters\n    ----------\n    h : array\n        Linear-phase FIR filter coefficients.\n    method : {\'hilbert\', \'homomorphic\'}\n        The method to use:\n\n            \'homomorphic\' (default)\n                This method [4]_ [5]_ works best with filters with an\n                odd number of taps, and the resulting minimum phase filter\n                will have a magnitude response that approximates the square\n                root of the the original filter\'s magnitude response.\n\n            \'hilbert\'\n                This method [1]_ is designed to be used with equiripple\n                filters (e.g., from `remez`) with unity or zero gain\n                regions.\n\n    n_fft : int\n        The number of points to use for the FFT. Should be at least a\n        few times larger than the signal length (see Notes).\n\n    Returns\n    -------\n    h_minimum : array\n        The minimum-phase version of the filter, with length\n        ``(length(h) + 1) // 2``.\n\n    See Also\n    --------\n    firwin\n    firwin2\n    remez\n\n    Notes\n    -----\n    Both the Hilbert [1]_ or homomorphic [4]_ [5]_ methods require selection\n    of an FFT length to estimate the complex cepstrum of the filter.\n\n    In the case of the Hilbert method, the deviation from the ideal\n    spectrum ``epsilon`` is related to the number of stopband zeros\n    ``n_stop`` and FFT length ``n_fft`` as::\n\n        epsilon = 2. * n_stop / n_fft\n\n    For example, with 100 stopband zeros and a FFT length of 2048,\n    ``epsilon = 0.0976``. If we conservatively assume that the number of\n    stopband zeros is one less than the filter length, we can take the FFT\n    length to be the next power of 2 that satisfies ``epsilon=0.01`` as::\n\n        n_fft = 2 ** int(np.ceil(np.log2(2 * (len(h) - 1) / 0.01)))\n\n    This gives reasonable results for both the Hilbert and homomorphic\n    methods, and gives the value used when ``n_fft=None``.\n\n    Alternative implementations exist for creating minimum-phase filters,\n    including zero inversion [2]_ and spectral factorization [3]_ [4]_.\n    For more information, see:\n\n        http://dspguru.com/dsp/howtos/how-to-design-minimum-phase-fir-filters\n\n    Examples\n    --------\n    Create an optimal linear-phase filter, then convert it to minimum phase:\n\n    >>> from scipy.signal import remez, minimum_phase, freqz, group_delay\n    >>> import matplotlib.pyplot as plt\n    >>> freq = [0, 0.2, 0.3, 1.0]\n    >>> desired = [1, 0]\n    >>> h_linear = remez(151, freq, desired, Hz=2.)\n\n    Convert it to minimum phase:\n\n    >>> h_min_hom = minimum_phase(h_linear, method=\'homomorphic\')\n    >>> h_min_hil = minimum_phase(h_linear, method=\'hilbert\')\n\n    Compare the three filters:\n\n    >>> fig, axs = plt.subplots(4, figsize=(4, 8))\n    >>> for h, style, color in zip((h_linear, h_min_hom, h_min_hil),\n    ...                            (\'-\', \'-\', \'--\'), (\'k\', \'r\', \'c\')):\n    ...     w, H = freqz(h)\n    ...     w, gd = group_delay((h, 1))\n    ...     w /= np.pi\n    ...     axs[0].plot(h, color=color, linestyle=style)\n    ...     axs[1].plot(w, np.abs(H), color=color, linestyle=style)\n    ...     axs[2].plot(w, 20 * np.log10(np.abs(H)), color=color, linestyle=style)\n    ...     axs[3].plot(w, gd, color=color, linestyle=style)\n    >>> for ax in axs:\n    ...     ax.grid(True, color=\'0.5\')\n    ...     ax.fill_between(freq[1:3], *ax.get_ylim(), color=\'#ffeeaa\', zorder=1)\n    >>> axs[0].set(xlim=[0, len(h_linear) - 1], ylabel=\'Amplitude\', xlabel=\'Samples\')\n    >>> axs[1].legend([\'Linear\', \'Min-Hom\', \'Min-Hil\'], title=\'Phase\')\n    >>> for ax, ylim in zip(axs[1:], ([0, 1.1], [-150, 10], [-60, 60])):\n    ...     ax.set(xlim=[0, 1], ylim=ylim, xlabel=\'Frequency\')\n    >>> axs[1].set(ylabel=\'Magnitude\')\n    >>> axs[2].set(ylabel=\'Magnitude (dB)\')\n    >>> axs[3].set(ylabel=\'Group delay\')\n    >>> plt.tight_layout()\n\n    References\n    ----------\n    .. [1] N. Damera-Venkata and B. L. Evans, "Optimal design of real and\n           complex minimum phase digital FIR filters," Acoustics, Speech,\n           and Signal Processing, 1999. Proceedings., 1999 IEEE International\n           Conference on, Phoenix, AZ, 1999, pp. 1145-1148 vol.3.\n           doi: 10.1109/ICASSP.1999.756179\n    .. [2] X. Chen and T. W. Parks, "Design of optimal minimum phase FIR\n           filters by direct factorization," Signal Processing,\n           vol. 10, no. 4, pp. 369383, Jun. 1986.\n    .. [3] T. Saramaki, "Finite Impulse Response Filter Design," in\n           Handbook for Digital Signal Processing, chapter 4,\n           New York: Wiley-Interscience, 1993.\n    .. [4] J. S. Lim, Advanced Topics in Signal Processing.\n           Englewood Cliffs, N.J.: Prentice Hall, 1988.\n    .. [5] A. V. Oppenheim, R. W. Schafer, and J. R. Buck,\n           "Discrete-Time Signal Processing," 2nd edition.\n           Upper Saddle River, N.J.: Prentice Hall, 1999.\n    '
h = numpy.asarray(h)
if numpy.iscomplexobj(h):
    raise ValueError('Complex filters not supported')
if ((h.ndim != 1) or (h.size <= 2)):
    raise ValueError('h must be 1D and at least 2 samples long')
n_half = (len(h) // 2)
if (not numpy.allclose(h[(- n_half):][::(- 1)], h[:n_half])):
    warnings.warn('h does not appear to by symmetric, conversion may fail', RuntimeWarning)
if ((not isinstance(method, string_types)) or (method not in ('homomorphic', 'hilbert'))):
    raise ValueError(('method must be "homomorphic" or "hilbert", got %r' % (method,)))
if (n_fft is None):
    n_fft = (2 ** int(numpy.ceil(numpy.log2(((2 * (len(h) - 1)) / 0.01)))))
n_fft = int(n_fft)
if (n_fft < len(h)):
    raise ValueError(('n_fft must be at least len(h)==%s' % len(h)))
if (method == 'hilbert'):
    w = (numpy.arange(n_fft) * (((2 * numpy.pi) / n_fft) * n_half))
    H = numpy.real((fft(h, n_fft) * numpy.exp((1j * w))))
    dp = (max(H) - 1)
    ds = (0 - min(H))
    S = (4.0 / ((numpy.sqrt(((1 + dp) + ds)) + numpy.sqrt(((1 - dp) + ds))) ** 2))
    H += ds
    H *= S
    H = numpy.sqrt(H, out=H)
    H += 1e-10
    h_minimum = _dhtm(H)
else:
    h_temp = numpy.abs(fft(h, n_fft))
    h_temp += (1e-07 * h_temp[(h_temp > 0)].min())
    numpy.log(h_temp, out=h_temp)
    h_temp *= 0.5
    h_temp = ifft(h_temp).real
    win = numpy.zeros(n_fft)
    win[0] = 1
    stop = ((len(h) + 1) // 2)
    win[1:stop] = 2
    if (len(h) % 2):
        win[stop] = 1
    h_temp *= win
    tempResult = exp(fft(h_temp))
	
===================================================================	
_dhtm: 196	
----------------------------	

'Compute the modified 1D discrete Hilbert transform\n\n    Parameters\n    ----------\n    mag : ndarray\n        The magnitude spectrum. Should be 1D with an even length, and\n        preferably a fast length for FFT/IFFT.\n    '
sig = numpy.zeros(len(mag))
midpt = (len(mag) // 2)
sig[1:midpt] = 1
sig[(midpt + 1):] = (- 1)
tempResult = exp(fft((sig * ifft(numpy.log(mag)))))
	
===================================================================	
firwin2: 118	
----------------------------	

'\n    FIR filter design using the window method.\n\n    From the given frequencies `freq` and corresponding gains `gain`,\n    this function constructs an FIR filter with linear phase and\n    (approximately) the given frequency response.\n\n    Parameters\n    ----------\n    numtaps : int\n        The number of taps in the FIR filter.  `numtaps` must be less than\n        `nfreqs`.\n    freq : array_like, 1D\n        The frequency sampling points. Typically 0.0 to 1.0 with 1.0 being\n        Nyquist.  The Nyquist frequency can be redefined with the argument\n        `nyq`.\n        The values in `freq` must be nondecreasing.  A value can be repeated\n        once to implement a discontinuity.  The first value in `freq` must\n        be 0, and the last value must be `nyq`.\n    gain : array_like\n        The filter gains at the frequency sampling points. Certain\n        constraints to gain values, depending on the filter type, are applied,\n        see Notes for details.\n    nfreqs : int, optional\n        The size of the interpolation mesh used to construct the filter.\n        For most efficient behavior, this should be a power of 2 plus 1\n        (e.g, 129, 257, etc).  The default is one more than the smallest\n        power of 2 that is not less than `numtaps`.  `nfreqs` must be greater\n        than `numtaps`.\n    window : string or (string, float) or float, or None, optional\n        Window function to use. Default is "hamming".  See\n        `scipy.signal.get_window` for the complete list of possible values.\n        If None, no window function is applied.\n    nyq : float, optional\n        Nyquist frequency.  Each frequency in `freq` must be between 0 and\n        `nyq` (inclusive).\n    antisymmetric : bool, optional\n        Whether resulting impulse response is symmetric/antisymmetric.\n        See Notes for more details.\n\n    Returns\n    -------\n    taps : ndarray\n        The filter coefficients of the FIR filter, as a 1-D array of length\n        `numtaps`.\n\n    See also\n    --------\n    firls\n    firwin\n    minimum_phase\n    remez\n\n    Notes\n    -----\n    From the given set of frequencies and gains, the desired response is\n    constructed in the frequency domain.  The inverse FFT is applied to the\n    desired response to create the associated convolution kernel, and the\n    first `numtaps` coefficients of this kernel, scaled by `window`, are\n    returned.\n\n    The FIR filter will have linear phase. The type of filter is determined by\n    the value of \'numtaps` and `antisymmetric` flag.\n    There are four possible combinations:\n\n       - odd  `numtaps`, `antisymmetric` is False, type I filter is produced\n       - even `numtaps`, `antisymmetric` is False, type II filter is produced\n       - odd  `numtaps`, `antisymmetric` is True, type III filter is produced\n       - even `numtaps`, `antisymmetric` is True, type IV filter is produced\n\n    Magnitude response of all but type I filters are subjects to following\n    constraints:\n\n       - type II  -- zero at the Nyquist frequency\n       - type III -- zero at zero and Nyquist frequencies\n       - type IV  -- zero at zero frequency\n\n    .. versionadded:: 0.9.0\n\n    References\n    ----------\n    .. [1] Oppenheim, A. V. and Schafer, R. W., "Discrete-Time Signal\n       Processing", Prentice-Hall, Englewood Cliffs, New Jersey (1989).\n       (See, for example, Section 7.4.)\n\n    .. [2] Smith, Steven W., "The Scientist and Engineer\'s Guide to Digital\n       Signal Processing", Ch. 17. http://www.dspguide.com/ch17/1.htm\n\n    Examples\n    --------\n    A lowpass FIR filter with a response that is 1 on [0.0, 0.5], and\n    that decreases linearly on [0.5, 1.0] from 1 to 0:\n\n    >>> from scipy import signal\n    >>> taps = signal.firwin2(150, [0.0, 0.5, 1.0], [1.0, 1.0, 0.0])\n    >>> print(taps[72:78])\n    [-0.02286961 -0.06362756  0.57310236  0.57310236 -0.06362756 -0.02286961]\n\n    '
if (len(freq) != len(gain)):
    raise ValueError('freq and gain must be of same length.')
if ((nfreqs is not None) and (numtaps >= nfreqs)):
    raise ValueError(('ntaps must be less than nfreqs, but firwin2 was called with ntaps=%d and nfreqs=%s' % (numtaps, nfreqs)))
if ((freq[0] != 0) or (freq[(- 1)] != nyq)):
    raise ValueError('freq must start with 0 and end with `nyq`.')
d = numpy.diff(freq)
if (d < 0).any():
    raise ValueError('The values in freq must be nondecreasing.')
d2 = (d[:(- 1)] + d[1:])
if (d2 == 0).any():
    raise ValueError('A value in freq must not occur more than twice.')
if antisymmetric:
    if ((numtaps % 2) == 0):
        ftype = 4
    else:
        ftype = 3
elif ((numtaps % 2) == 0):
    ftype = 2
else:
    ftype = 1
if ((ftype == 2) and (gain[(- 1)] != 0.0)):
    raise ValueError('A Type II filter must have zero gain at the Nyquist rate.')
elif ((ftype == 3) and ((gain[0] != 0.0) or (gain[(- 1)] != 0.0))):
    raise ValueError('A Type III filter must have zero gain at zero and Nyquist rates.')
elif ((ftype == 4) and (gain[0] != 0.0)):
    raise ValueError('A Type IV filter must have zero gain at zero rate.')
if (nfreqs is None):
    nfreqs = (1 + (2 ** int(ceil(log(numtaps, 2)))))
eps = np.finfo(float).eps
for k in range(len(freq)):
    if ((k < (len(freq) - 1)) and (freq[k] == freq[(k + 1)])):
        freq[k] = (freq[k] - eps)
        freq[(k + 1)] = (freq[(k + 1)] + eps)
x = numpy.linspace(0.0, nyq, nfreqs)
fx = numpy.interp(x, freq, gain)
tempResult = exp(((((((- (numtaps - 1)) / 2.0) * 1j) * numpy.pi) * x) / nyq))
	
===================================================================	
ricker: 131	
----------------------------	

'\n    Return a Ricker wavelet, also known as the "Mexican hat wavelet".\n\n    It models the function:\n\n        ``A (1 - x^2/a^2) exp(-x^2/2 a^2)``,\n\n    where ``A = 2/sqrt(3a)pi^1/4``.\n\n    Parameters\n    ----------\n    points : int\n        Number of points in `vector`.\n        Will be centered around 0.\n    a : scalar\n        Width parameter of the wavelet.\n\n    Returns\n    -------\n    vector : (N,) ndarray\n        Array of length `points` in shape of ricker curve.\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> points = 100\n    >>> a = 4.0\n    >>> vec2 = signal.ricker(points, a)\n    >>> print(len(vec2))\n    100\n    >>> plt.plot(vec2)\n    >>> plt.show()\n\n    '
A = (2 / (numpy.sqrt((3 * a)) * (numpy.pi ** 0.25)))
wsq = (a ** 2)
vec = (numpy.arange(0, points) - ((points - 1.0) / 2))
xsq = (vec ** 2)
mod = (1 - (xsq / wsq))
tempResult = exp(((- xsq) / (2 * wsq)))
	
===================================================================	
chebwin: 225	
----------------------------	

'Return a Dolph-Chebyshev window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    at : float\n        Attenuation (in dB).\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value always normalized to 1\n\n    Notes\n    -----\n    This window optimizes for the narrowest main lobe width for a given order\n    `M` and sidelobe equiripple attenuation `at`, using Chebyshev\n    polynomials.  It was originally developed by Dolph to optimize the\n    directionality of radio antenna arrays.\n\n    Unlike most windows, the Dolph-Chebyshev is defined in terms of its\n    frequency response:\n\n    .. math:: W(k) = \\frac\n              {\\cos\\{M \\cos^{-1}[\\beta \\cos(\\frac{\\pi k}{M})]\\}}\n              {\\cosh[M \\cosh^{-1}(\\beta)]}\n\n    where\n\n    .. math:: \\beta = \\cosh \\left [\\frac{1}{M}\n              \\cosh^{-1}(10^\\frac{A}{20}) \\right ]\n\n    and 0 <= abs(k) <= M-1. A is the attenuation in decibels (`at`).\n\n    The time domain window is then generated using the IFFT, so\n    power-of-two `M` are the fastest to generate, and prime number `M` are\n    the slowest.\n\n    The equiripple condition in the frequency domain creates impulses in the\n    time domain, which appear at the ends of the window.\n\n    References\n    ----------\n    .. [1] C. Dolph, "A current distribution for broadside arrays which\n           optimizes the relationship between beam width and side-lobe level",\n           Proceedings of the IEEE, Vol. 34, Issue 6\n    .. [2] Peter Lynch, "The Dolph-Chebyshev Window: A Simple Optimal Filter",\n           American Meteorological Society (April 1997)\n           http://mathsci.ucd.ie/~plynch/Publications/Dolph.pdf\n    .. [3] F. J. Harris, "On the use of windows for harmonic analysis with the\n           discrete Fourier transforms", Proceedings of the IEEE, Vol. 66,\n           No. 1, January 1978\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.chebwin(51, at=100)\n    >>> plt.plot(window)\n    >>> plt.title("Dolph-Chebyshev window (100 dB)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the Dolph-Chebyshev window (100 dB)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if (numpy.abs(at) < 45):
    warnings.warn('This window is not suitable for spectral analysis for attenuation values lower than about 45dB because the equivalent noise bandwidth of a Chebyshev window does not grow monotonically with increasing sidelobe attenuation when the attenuation is smaller than about 45 dB.')
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
order = (M - 1.0)
beta = numpy.cosh(((1.0 / order) * numpy.arccosh((10 ** (numpy.abs(at) / 20.0)))))
k = (numpy.r_[0:M] * 1.0)
x = (beta * numpy.cos(((numpy.pi * k) / M)))
p = numpy.zeros(x.shape)
p[(x > 1)] = numpy.cosh((order * numpy.arccosh(x[(x > 1)])))
p[(x < (- 1))] = ((1 - (2 * (order % 2))) * numpy.cosh((order * numpy.arccosh((- x[(x < (- 1))])))))
p[(numpy.abs(x) <= 1)] = numpy.cos((order * numpy.arccos(x[(numpy.abs(x) <= 1)])))
if (M % 2):
    w = numpy.real(scipy.fftpack.fft(p))
    n = ((M + 1) // 2)
    w = w[:n]
    w = numpy.concatenate((w[(n - 1):0:(- 1)], w))
else:
    tempResult = exp((((1j * numpy.pi) / M) * numpy.r_[0:M]))
	
===================================================================	
exponential: 265	
----------------------------	

'Return an exponential (or Poisson) window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    center : float, optional\n        Parameter defining the center location of the window function.\n        The default value if not given is ``center = (M-1) / 2``.  This\n        parameter must take its default value for symmetric windows.\n    tau : float, optional\n        Parameter defining the decay.  For ``center = 0`` use\n        ``tau = -(M-1) / ln(x)`` if ``x`` is the fraction of the window\n        remaining at the end.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n    The Exponential window is defined as\n\n    .. math::  w(n) = e^{-|n-center| / \\tau}\n\n    References\n    ----------\n    S. Gade and H. Herlufsen, "Windows to FFT analysis (Part I)",\n    Technical Review 3, Bruel & Kjaer, 1987.\n\n    Examples\n    --------\n    Plot the symmetric window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> M = 51\n    >>> tau = 3.0\n    >>> window = signal.exponential(M, tau=tau)\n    >>> plt.plot(window)\n    >>> plt.title("Exponential Window (tau=3.0)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -35, 0])\n    >>> plt.title("Frequency response of the Exponential window (tau=3.0)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    This function can also generate non-symmetric windows:\n\n    >>> tau2 = -(M-1) / np.log(0.01)\n    >>> window2 = signal.exponential(M, 0, tau2, False)\n    >>> plt.figure()\n    >>> plt.plot(window2)\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n    '
if (sym and (center is not None)):
    raise ValueError('If sym==True, center must be None.')
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
if (center is None):
    center = ((M - 1) / 2)
n = numpy.arange(0, M)
tempResult = exp(((- numpy.abs((n - center))) / tau))
	
===================================================================	
general_gaussian: 201	
----------------------------	

'Return a window with a generalized Gaussian shape.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    p : float\n        Shape parameter.  p = 1 is identical to `gaussian`, p = 0.5 is\n        the same shape as the Laplace distribution.\n    sig : float\n        The standard deviation, sigma.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n    The generalized Gaussian window is defined as\n\n    .. math::  w(n) = e^{ -\\frac{1}{2}\\left|\\frac{n}{\\sigma}\\right|^{2p} }\n\n    the half-power point is at\n\n    .. math::  (2 \\log(2))^{1/(2 p)} \\sigma\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.general_gaussian(51, p=1.5, sig=7)\n    >>> plt.plot(window)\n    >>> plt.title(r"Generalized Gaussian window (p=1.5, $\\sigma$=7)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title(r"Freq. resp. of the gen. Gaussian "\n    ...           "window (p=1.5, $\\sigma$=7)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
n = (numpy.arange(0, M) - ((M - 1.0) / 2.0))
tempResult = exp(((- 0.5) * (numpy.abs((n / sig)) ** (2 * p))))
	
===================================================================	
gaussian: 192	
----------------------------	

'Return a Gaussian window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    std : float\n        The standard deviation, sigma.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n    The Gaussian window is defined as\n\n    .. math::  w(n) = e^{ -\\frac{1}{2}\\left(\\frac{n}{\\sigma}\\right)^2 }\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.gaussian(51, std=7)\n    >>> plt.plot(window)\n    >>> plt.title(r"Gaussian window ($\\sigma$=7)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title(r"Frequency response of the Gaussian window ($\\sigma$=7)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
n = (numpy.arange(0, M) - ((M - 1.0) / 2.0))
sig2 = ((2 * std) * std)
tempResult = exp(((- (n ** 2)) / sig2))
	
===================================================================	
Test_dfreqresp.test_auto: 347	
----------------------------	

system = TransferFunction(1, [1, (- 0.2)], dt=0.1)
w = [0.1, 1, 10, 100]
(w, H) = dfreqresp(system, w=w)
tempResult = exp((w * 1j))
	
===================================================================	
Test_bode.test_auto: 397	
----------------------------	

system = TransferFunction(0.3, [1, (- 0.2)], dt=0.1)
w = numpy.array([0.1, 0.5, 1, numpy.pi])
(w2, mag, phase) = dbode(system, w=w)
tempResult = exp((w * 1j))
	
===================================================================	
TestGroupDelay.test_singular: 1436	
----------------------------	

tempResult = exp(((1j * 0.1) * pi))
	
===================================================================	
TestGroupDelay.test_singular: 1437	
----------------------------	

z1 = numpy.exp(((1j * 0.1) * pi))
tempResult = exp(((1j * 0.25) * pi))
	
===================================================================	
TestGroupDelay.test_singular: 1438	
----------------------------	

z1 = numpy.exp(((1j * 0.1) * pi))
z2 = numpy.exp(((1j * 0.25) * pi))
tempResult = exp(((1j * 0.5) * pi))
	
===================================================================	
TestGroupDelay.test_singular: 1439	
----------------------------	

z1 = numpy.exp(((1j * 0.1) * pi))
z2 = numpy.exp(((1j * 0.25) * pi))
p1 = numpy.exp(((1j * 0.5) * pi))
tempResult = exp(((1j * 0.8) * pi))
	
===================================================================	
TestCplxPair.test_output_order: 31	
----------------------------	

assert_allclose(_cplxpair([(1 + 1j), (1 - 1j)]), [(1 - 1j), (1 + 1j)])
a = [(1 + 1j), (1 + 1j), 1, (1 - 1j), (1 - 1j), 2]
b = [(1 - 1j), (1 + 1j), (1 - 1j), (1 + 1j), 1, 2]
assert_allclose(_cplxpair(a), b)
tempResult = exp((((2j * pi) * array([4, 3, 5, 2, 6, 1, 0])) / 7))
	
===================================================================	
TestZpk2Sos.test_basic: 245	
----------------------------	

for pairing in ('nearest', 'keep_odd'):
    z = [(- 1), (- 1)]
    p = [(0.57149 + 0.2936j), (0.57149 - 0.2936j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 2, 1, 1, (- 1.14298), 0.4128]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [1j, (- 1j)]
    p = [0.9, (- 0.9), 0.7j, (- 0.7j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 0, 1, 1, 0, (+ 0.49)], [1, 0, 0, 1, 0, (- 0.81)]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = []
    p = [0.8, ((- 0.5) + 0.25j), ((- 0.5) - 0.25j)]
    k = 1.0
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1.0, 0.0, 0.0, 1.0, 1.0, 0.3125], [1.0, 0.0, 0.0, 1.0, (- 0.8), 0.0]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [1.0, 1.0, 0.9j, (- 0.9j)]
    p = [(0.99 + 0.01j), (0.99 - 0.01j), (0.1 + 0.9j), (0.1 - 0.9j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 0, 0.81, 1, (- 0.2), 0.82], [1, (- 2), 1, 1, (- 1.98), 0.9802]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [(0.9 + 0.1j), (0.9 - 0.1j), (- 0.9)]
    p = [(0.75 + 0.25j), (0.75 - 0.25j), 0.9]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    if (pairing == 'keep_odd'):
        sos2 = [[1, (- 1.8), 0.82, 1, (- 1.5), 0.625], [1, 0.9, 0, 1, (- 0.9), 0]]
        assert_array_almost_equal(sos, sos2, decimal=4)
    else:
        sos2 = [[1, 0.9, 0, 1, (- 1.5), 0.625], [1, (- 1.8), 0.82, 1, (- 0.9), 0]]
        assert_array_almost_equal(sos, sos2, decimal=4)
    z = [((- 0.309) + 0.9511j), ((- 0.309) - 0.9511j), (0.809 + 0.5878j), ((+ 0.809) - 0.5878j), ((- 1.0) + 0j)]
    p = [((- 0.3026) + 0.9312j), ((- 0.3026) - 0.9312j), (0.7922 + 0.5755j), ((+ 0.7922) - 0.5755j), ((- 0.9791) + 0j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 1, 0, 1, (+ 0.97915), 0], [1, 0.61803, 1, 1, (+ 0.60515), 0.95873], [1, (- 1.61803), 1, 1, (- 1.5843), 0.95873]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [((- 1) - 1.4142j), ((- 1) + 1.4142j), ((- 0.625) - 1.0533j), ((- 0.625) + 1.0533j)]
    p = [((- 0.2) - 0.6782j), ((- 0.2) + 0.6782j), ((- 0.1) - 0.5385j), ((- 0.1) + 0.5385j)]
    k = 4
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[4, 8, 12, 1, 0.2, 0.3], [1, 1.25, 1.5, 1, 0.4, 0.5]]
    assert_allclose(sos, sos2, rtol=0.0001, atol=0.0001)
    z = []
    p = [0.2, ((- 0.5) + 0.25j), ((- 0.5) - 0.25j)]
    k = 1.0
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1.0, 0.0, 0.0, 1.0, (- 0.2), 0.0], [1.0, 0.0, 0.0, 1.0, 1.0, 0.3125]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    deg2rad = (numpy.pi / 180.0)
    k = 1.0
    thetas = [22.5, 45, 77.5]
    mags = [0.8, 0.6, 0.9]
    tempResult = exp(((theta * deg2rad) * 1j))
	
===================================================================	
TestZpk2Sos.test_basic: 247	
----------------------------	

for pairing in ('nearest', 'keep_odd'):
    z = [(- 1), (- 1)]
    p = [(0.57149 + 0.2936j), (0.57149 - 0.2936j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 2, 1, 1, (- 1.14298), 0.4128]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [1j, (- 1j)]
    p = [0.9, (- 0.9), 0.7j, (- 0.7j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 0, 1, 1, 0, (+ 0.49)], [1, 0, 0, 1, 0, (- 0.81)]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = []
    p = [0.8, ((- 0.5) + 0.25j), ((- 0.5) - 0.25j)]
    k = 1.0
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1.0, 0.0, 0.0, 1.0, 1.0, 0.3125], [1.0, 0.0, 0.0, 1.0, (- 0.8), 0.0]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [1.0, 1.0, 0.9j, (- 0.9j)]
    p = [(0.99 + 0.01j), (0.99 - 0.01j), (0.1 + 0.9j), (0.1 - 0.9j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 0, 0.81, 1, (- 0.2), 0.82], [1, (- 2), 1, 1, (- 1.98), 0.9802]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [(0.9 + 0.1j), (0.9 - 0.1j), (- 0.9)]
    p = [(0.75 + 0.25j), (0.75 - 0.25j), 0.9]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    if (pairing == 'keep_odd'):
        sos2 = [[1, (- 1.8), 0.82, 1, (- 1.5), 0.625], [1, 0.9, 0, 1, (- 0.9), 0]]
        assert_array_almost_equal(sos, sos2, decimal=4)
    else:
        sos2 = [[1, 0.9, 0, 1, (- 1.5), 0.625], [1, (- 1.8), 0.82, 1, (- 0.9), 0]]
        assert_array_almost_equal(sos, sos2, decimal=4)
    z = [((- 0.309) + 0.9511j), ((- 0.309) - 0.9511j), (0.809 + 0.5878j), ((+ 0.809) - 0.5878j), ((- 1.0) + 0j)]
    p = [((- 0.3026) + 0.9312j), ((- 0.3026) - 0.9312j), (0.7922 + 0.5755j), ((+ 0.7922) - 0.5755j), ((- 0.9791) + 0j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 1, 0, 1, (+ 0.97915), 0], [1, 0.61803, 1, 1, (+ 0.60515), 0.95873], [1, (- 1.61803), 1, 1, (- 1.5843), 0.95873]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [((- 1) - 1.4142j), ((- 1) + 1.4142j), ((- 0.625) - 1.0533j), ((- 0.625) + 1.0533j)]
    p = [((- 0.2) - 0.6782j), ((- 0.2) + 0.6782j), ((- 0.1) - 0.5385j), ((- 0.1) + 0.5385j)]
    k = 4
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[4, 8, 12, 1, 0.2, 0.3], [1, 1.25, 1.5, 1, 0.4, 0.5]]
    assert_allclose(sos, sos2, rtol=0.0001, atol=0.0001)
    z = []
    p = [0.2, ((- 0.5) + 0.25j), ((- 0.5) - 0.25j)]
    k = 1.0
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1.0, 0.0, 0.0, 1.0, (- 0.2), 0.0], [1.0, 0.0, 0.0, 1.0, 1.0, 0.3125]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    deg2rad = (numpy.pi / 180.0)
    k = 1.0
    thetas = [22.5, 45, 77.5]
    mags = [0.8, 0.6, 0.9]
    z = numpy.array([numpy.exp(((theta * deg2rad) * 1j)) for theta in thetas])
    z = numpy.concatenate((z, numpy.conj(z)))
    tempResult = exp(((theta * deg2rad) * 1j))
	
===================================================================	
TestZpk2Sos.test_basic: 252	
----------------------------	

for pairing in ('nearest', 'keep_odd'):
    z = [(- 1), (- 1)]
    p = [(0.57149 + 0.2936j), (0.57149 - 0.2936j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 2, 1, 1, (- 1.14298), 0.4128]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [1j, (- 1j)]
    p = [0.9, (- 0.9), 0.7j, (- 0.7j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 0, 1, 1, 0, (+ 0.49)], [1, 0, 0, 1, 0, (- 0.81)]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = []
    p = [0.8, ((- 0.5) + 0.25j), ((- 0.5) - 0.25j)]
    k = 1.0
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1.0, 0.0, 0.0, 1.0, 1.0, 0.3125], [1.0, 0.0, 0.0, 1.0, (- 0.8), 0.0]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [1.0, 1.0, 0.9j, (- 0.9j)]
    p = [(0.99 + 0.01j), (0.99 - 0.01j), (0.1 + 0.9j), (0.1 - 0.9j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 0, 0.81, 1, (- 0.2), 0.82], [1, (- 2), 1, 1, (- 1.98), 0.9802]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [(0.9 + 0.1j), (0.9 - 0.1j), (- 0.9)]
    p = [(0.75 + 0.25j), (0.75 - 0.25j), 0.9]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    if (pairing == 'keep_odd'):
        sos2 = [[1, (- 1.8), 0.82, 1, (- 1.5), 0.625], [1, 0.9, 0, 1, (- 0.9), 0]]
        assert_array_almost_equal(sos, sos2, decimal=4)
    else:
        sos2 = [[1, 0.9, 0, 1, (- 1.5), 0.625], [1, (- 1.8), 0.82, 1, (- 0.9), 0]]
        assert_array_almost_equal(sos, sos2, decimal=4)
    z = [((- 0.309) + 0.9511j), ((- 0.309) - 0.9511j), (0.809 + 0.5878j), ((+ 0.809) - 0.5878j), ((- 1.0) + 0j)]
    p = [((- 0.3026) + 0.9312j), ((- 0.3026) - 0.9312j), (0.7922 + 0.5755j), ((+ 0.7922) - 0.5755j), ((- 0.9791) + 0j)]
    k = 1
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1, 1, 0, 1, (+ 0.97915), 0], [1, 0.61803, 1, 1, (+ 0.60515), 0.95873], [1, (- 1.61803), 1, 1, (- 1.5843), 0.95873]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    z = [((- 1) - 1.4142j), ((- 1) + 1.4142j), ((- 0.625) - 1.0533j), ((- 0.625) + 1.0533j)]
    p = [((- 0.2) - 0.6782j), ((- 0.2) + 0.6782j), ((- 0.1) - 0.5385j), ((- 0.1) + 0.5385j)]
    k = 4
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[4, 8, 12, 1, 0.2, 0.3], [1, 1.25, 1.5, 1, 0.4, 0.5]]
    assert_allclose(sos, sos2, rtol=0.0001, atol=0.0001)
    z = []
    p = [0.2, ((- 0.5) + 0.25j), ((- 0.5) - 0.25j)]
    k = 1.0
    sos = zpk2sos(z, p, k, pairing=pairing)
    sos2 = [[1.0, 0.0, 0.0, 1.0, (- 0.2), 0.0], [1.0, 0.0, 0.0, 1.0, 1.0, 0.3125]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    deg2rad = (numpy.pi / 180.0)
    k = 1.0
    thetas = [22.5, 45, 77.5]
    mags = [0.8, 0.6, 0.9]
    z = numpy.array([numpy.exp(((theta * deg2rad) * 1j)) for theta in thetas])
    z = numpy.concatenate((z, numpy.conj(z)))
    p = numpy.array([(mag * numpy.exp(((theta * deg2rad) * 1j))) for (theta, mag) in zip(thetas, mags)])
    p = numpy.concatenate((p, numpy.conj(p)))
    sos = zpk2sos(z, p, k)
    sos2 = [[1, (- 1.41421), 1, 1, (- 0.84853), 0.36], [1, (- 1.84776), 1, 1, (- 1.47821), 0.64], [1, (- 0.43288), 1, 1, (- 0.38959), 0.81]]
    assert_array_almost_equal(sos, sos2, decimal=4)
    tempResult = exp(((theta * deg2rad) * 1j))
	
===================================================================	
TestFirwin.check_response: 37	
----------------------------	

N = len(h)
alpha = (0.5 * (N - 1))
m = (numpy.arange(0, N) - alpha)
for (freq, expected) in expected_response:
    tempResult = exp(((((- 1j) * numpy.pi) * m) * freq))
	
===================================================================	
Test_lsim2.test_05: 330	
----------------------------	

A = numpy.array([[(- 1.0), 0.0], [0.0, (- 2.0)]])
B = numpy.array([[1.0, 0.0], [0.0, 1.0]])
C = numpy.array([1.0, 0.0])
D = numpy.zeros((1, 2))
t = numpy.linspace(0, 10.0, 101)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', BadCoefficients)
    (tout, y, x) = lsim2((A, B, C, D), T=t, X0=[1.0, 1.0])
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_05: 331	
----------------------------	

A = numpy.array([[(- 1.0), 0.0], [0.0, (- 2.0)]])
B = numpy.array([[1.0, 0.0], [0.0, 1.0]])
C = numpy.array([1.0, 0.0])
D = numpy.zeros((1, 2))
t = numpy.linspace(0, 10.0, 101)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', BadCoefficients)
    (tout, y, x) = lsim2((A, B, C, D), T=t, X0=[1.0, 1.0])
expected_y = numpy.exp((- tout))
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_05: 332	
----------------------------	

A = numpy.array([[(- 1.0), 0.0], [0.0, (- 2.0)]])
B = numpy.array([[1.0, 0.0], [0.0, 1.0]])
C = numpy.array([1.0, 0.0])
D = numpy.zeros((1, 2))
t = numpy.linspace(0, 10.0, 101)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', BadCoefficients)
    (tout, y, x) = lsim2((A, B, C, D), T=t, X0=[1.0, 1.0])
expected_y = numpy.exp((- tout))
expected_x0 = numpy.exp((- tout))
tempResult = exp(((- 2.0) * tout))
	
===================================================================	
_TestStepFuncs.test_01: 409	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system)
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_06: 340	
----------------------------	

system = ([1.0], [1.0, 2.0, 1.0])
(tout, y, x) = lsim2(system, X0=[1.0, 0.0])
tempResult = exp((- tout))
	
===================================================================	
_TestImpulseFuncs.test_03: 364	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system, X0=3.0)
tempResult = exp((- tout))
	
===================================================================	
TestLsim.test_jordan_block: 260	
----------------------------	

A = numpy.mat('-1. 1.; 0. -1.')
B = numpy.mat('0.; 1.')
C = numpy.mat('1. 0.')
system = self.lti_nowarn(A, B, C, 0.0)
t = numpy.linspace(0, 5)
u = numpy.zeros_like(t)
(tout, y, x) = lsim(system, u, t, X0=[0.0, 1.0])
tempResult = exp((- tout))
	
===================================================================	
_TestStepFuncs.test_06: 443	
----------------------------	

system = ([1.0], [1.0, 2.0, 1.0])
(tout, y) = self.func(system)
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_01: 294	
----------------------------	

t = numpy.linspace(0, 10, 1001)
u = numpy.zeros_like(t)
system = ([1.0], [1.0, 1.0])
(tout, y, x) = lsim2(system, u, t, X0=[1.0])
tempResult = exp((- tout))
	
===================================================================	
_TestImpulseFuncs.test_02: 358	
----------------------------	

system = ([1.0], [1.0, 1.0])
n = 21
t = numpy.linspace(0, 2.0, n)
(tout, y) = self.func(system, T=t)
assert_equal(tout.shape, (n,))
assert_almost_equal(tout, t)
tempResult = exp((- t))
	
===================================================================	
_TestStepFuncs.test_04: 431	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system, X0=[3.0])
tempResult = exp((- tout))
	
===================================================================	
_TestStepFuncs.test_02: 419	
----------------------------	

system = ([1.0], [1.0, 1.0])
n = 21
t = numpy.linspace(0, 2.0, n)
(tout, y) = self.func(system, T=t)
assert_equal(tout.shape, (n,))
assert_almost_equal(tout, t)
tempResult = exp((- t))
	
===================================================================	
Test_lsim2.test_04: 318	
----------------------------	

t = numpy.linspace(0, 10, 1001)
u = numpy.zeros_like(t)
system = ([1.0], [1.0, 2.0, 1.0])
(tout, y, x) = lsim2(system, u, t, X0=[1.0, 0.0])
tempResult = exp((- tout))
	
===================================================================	
_TestImpulseFuncs.test_01: 348	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system)
tempResult = exp((- tout))
	
===================================================================	
_gen_gaussians: 14	
----------------------------	

xdata = np.arange(0, total_length).astype(float)
out_data = numpy.zeros(total_length, dtype=float)
for (ind, sigma) in enumerate(sigmas):
    tmp = ((xdata - center_locs[ind]) / sigma)
    tempResult = exp((- (tmp ** 2)))
	
===================================================================	
TestDecimate._test_phaseshift: 1238	
----------------------------	

rate = 120
rates_to = [15, 20, 30, 40]
t_tot = int(100)
t = (numpy.arange(((rate * t_tot) + 1)) / float(rate))
freqs = ((numpy.array(rates_to) * 0.8) / 2)
tempResult = exp(((((1j * 2) * numpy.pi) * freqs[:, numpy.newaxis]) * t))
	
===================================================================	
TestDecimate._test_phaseshift: 1242	
----------------------------	

rate = 120
rates_to = [15, 20, 30, 40]
t_tot = int(100)
t = (numpy.arange(((rate * t_tot) + 1)) / float(rate))
freqs = ((numpy.array(rates_to) * 0.8) / 2)
d = (numpy.exp(((((1j * 2) * numpy.pi) * freqs[:, numpy.newaxis]) * t)) * scipy.signal.tukey(t.size, 0.1))
for rate_to in rates_to:
    q = (rate // rate_to)
    t_to = (numpy.arange(((rate_to * t_tot) + 1)) / float(rate_to))
    tempResult = exp(((((1j * 2) * numpy.pi) * freqs[:, numpy.newaxis]) * t_to))
	
===================================================================	
TestCSpline1DEval.test_complex: 465	
----------------------------	

x = numpy.arange(2)
y = numpy.zeros(x.shape, dtype=numpy.complex64)
T = 10.0
f = (1.0 / T)
tempResult = exp((((2j * numpy.pi) * f) * x))
	
===================================================================	
TestSTFT.test_roundtrip_padded_FFT: 1002	
----------------------------	

numpy.random.seed(1234)
settings = [('hann', 1024, 256, 128, 512), ('hann', 1024, 256, 128, 501), ('boxcar', 100, 10, 0, 33), (('tukey', 0.5), 1152, 256, 64, 1024)]
for (window, N, nperseg, noverlap, nfft) in settings:
    t = numpy.arange(N)
    x = (10 * numpy.random.randn(t.size))
    tempResult = exp(((1j * numpy.pi) / 4))
	
===================================================================	
_eq_10_42: 396	
----------------------------	

'\n    Equation (10.42) of Functions of Matrices: Theory and Computation.\n\n    Notes\n    -----\n    This is a helper function for _fragment_2_1 of expm_2009.\n    Equation (10.42) is on page 251 in the section on Schur algorithms.\n    In particular, section 10.4.3 explains the Schur-Parlett algorithm.\n    expm([[lam_1, t_12], [0, lam_1])\n    =\n    [[exp(lam_1), t_12*exp((lam_1 + lam_2)/2)*sinch((lam_1 - lam_2)/2)],\n    [0, exp(lam_2)]\n    '
a = (0.5 * (lam_1 + lam_2))
b = (0.5 * (lam_1 - lam_2))
tempResult = exp(a)
	
===================================================================	
_expm: 334	
----------------------------	

if isinstance(A, (list, tuple)):
    A = numpy.asarray(A)
if ((len(A.shape) != 2) or (A.shape[0] != A.shape[1])):
    raise ValueError('expected a square matrix')
if (A.shape == (1, 1)):
    tempResult = exp(A[(0, 0)])
	
===================================================================	
_fragment_2_1: 403	
----------------------------	

'\n    A helper function for expm_2009.\n\n    Notes\n    -----\n    The argument X is modified in-place, but this modification is not the same\n    as the returned value of the function.\n    This function also takes pains to do things in ways that are compatible\n    with sparse matrices, for example by avoiding fancy indexing\n    and by using methods of the matrices whenever possible instead of\n    using functions of the numpy or scipy libraries themselves.\n\n    '
n = X.shape[0]
diag_T = numpy.ravel(T.diagonal().copy())
scale = (2 ** (- s))
tempResult = exp((scale * diag_T))
	
===================================================================	
_fragment_2_1: 409	
----------------------------	

'\n    A helper function for expm_2009.\n\n    Notes\n    -----\n    The argument X is modified in-place, but this modification is not the same\n    as the returned value of the function.\n    This function also takes pains to do things in ways that are compatible\n    with sparse matrices, for example by avoiding fancy indexing\n    and by using methods of the matrices whenever possible instead of\n    using functions of the numpy or scipy libraries themselves.\n\n    '
n = X.shape[0]
diag_T = numpy.ravel(T.diagonal().copy())
scale = (2 ** (- s))
exp_diag = numpy.exp((scale * diag_T))
for k in range(n):
    X[(k, k)] = exp_diag[k]
for i in range((s - 1), (- 1), (- 1)):
    X = X.dot(X)
    scale = (2 ** (- i))
    tempResult = exp((scale * diag_T))
	
===================================================================	
_expm_multiply_interval_core_1: 258	
----------------------------	

'\n    A helper function, for the case q > s and q % s == 0.\n    '
d = (q // s)
input_shape = X.shape[1:]
K_shape = (((m_star + 1),) + input_shape)
K = numpy.empty(K_shape, dtype=X.dtype)
for i in range(s):
    Z = X[(i * d)]
    K[0] = Z
    high_p = 0
    for k in range(1, (d + 1)):
        F = K[0]
        c1 = _exact_inf_norm(F)
        for p in range(1, (m_star + 1)):
            if (p > high_p):
                K[p] = ((h * A.dot(K[(p - 1)])) / float(p))
            coeff = float(pow(k, p))
            F = (F + (coeff * K[p]))
            inf_norm_K_p_1 = _exact_inf_norm(K[p])
            c2 = (coeff * inf_norm_K_p_1)
            if ((c1 + c2) <= (tol * _exact_inf_norm(F))):
                break
            c1 = c2
        tempResult = exp(((k * h) * mu))
	
===================================================================	
_expm_multiply_simple_core: 79	
----------------------------	

'\n    A helper function.\n    '
if balance:
    raise NotImplementedError
if (tol is None):
    u_d = (2 ** (- 53))
    tol = u_d
F = B
tempResult = exp(((t * mu) / float(s)))
	
===================================================================	
_expm_multiply_interval_core_2: 291	
----------------------------	

'\n    A helper function, for the case q > s and q % s > 0.\n    '
d = (q // s)
j = (q // d)
r = (q - (d * j))
input_shape = X.shape[1:]
K_shape = (((m_star + 1),) + input_shape)
K = numpy.empty(K_shape, dtype=X.dtype)
for i in range((j + 1)):
    Z = X[(i * d)]
    K[0] = Z
    high_p = 0
    if (i < j):
        effective_d = d
    else:
        effective_d = r
    for k in range(1, (effective_d + 1)):
        F = K[0]
        c1 = _exact_inf_norm(F)
        for p in range(1, (m_star + 1)):
            if (p == (high_p + 1)):
                K[p] = ((h * A.dot(K[(p - 1)])) / float(p))
                high_p = p
            coeff = float(pow(k, p))
            F = (F + (coeff * K[p]))
            inf_norm_K_p_1 = _exact_inf_norm(K[p])
            c2 = (coeff * inf_norm_K_p_1)
            if ((c1 + c2) <= (tol * _exact_inf_norm(F))):
                break
            c1 = c2
        tempResult = exp(((k * h) * mu))
	
===================================================================	
TestExpmActionSimple.test_complex: 117	
----------------------------	

A = numpy.array([[1j, 1j], [0, 1j]], dtype=complex)
B = numpy.array([1j, 1j])
observed = expm_multiply(A, B)
tempResult = exp(1j)
	
===================================================================	
TestExpmActionSimple.test_complex: 117	
----------------------------	

A = numpy.array([[1j, 1j], [0, 1j]], dtype=complex)
B = numpy.array([1j, 1j])
observed = expm_multiply(A, B)
tempResult = exp(1j)
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 173	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
tempResult = exp(numpy.arange(5))
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 177	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(numpy.arange(5)), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
A = scipy.sparse.diags(((- 1j) * numpy.arange(5)), format='csr', dtype=complex)
B = numpy.ones(5, dtype=int)
tempResult = exp(((- 1j) * numpy.arange(5)))
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 181	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(numpy.arange(5)), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
A = scipy.sparse.diags(((- 1j) * numpy.arange(5)), format='csr', dtype=complex)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(((- 1j) * numpy.arange(5))), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = (1j * numpy.ones(5, dtype=complex))
tempResult = exp(numpy.arange(5))
	
===================================================================	
TestExpM.test_burkardt_1: 141	
----------------------------	

tempResult = exp(1)
	
===================================================================	
TestExpM.test_burkardt_1: 142	
----------------------------	

exp1 = numpy.exp(1)
tempResult = exp(2)
	
===================================================================	
TestExpM.test_burkardt_4: 167	
----------------------------	

A = numpy.array([[(- 49), 24], [(- 64), 31]], dtype=float)
U = numpy.array([[3, 1], [4, 2]], dtype=float)
V = numpy.array([[1, ((- 1) / 2)], [(- 2), (3 / 2)]], dtype=float)
w = numpy.array([(- 17), (- 1)], dtype=float)
tempResult = exp(w)
	
===================================================================	
TestExpM.test_burkardt_7: 185	
----------------------------	

tempResult = exp(1)
	
===================================================================	
TestExpM.test_burkardt_8: 193	
----------------------------	

tempResult = exp(4)
	
===================================================================	
TestExpM.test_burkardt_8: 194	
----------------------------	

exp4 = numpy.exp(4)
tempResult = exp(16)
	
===================================================================	
TestExpM.test_burkardt_6: 178	
----------------------------	

tempResult = exp(1)
	
===================================================================	
TestExpM.test_burkardt_3: 155	
----------------------------	

tempResult = exp(1)
	
===================================================================	
TestExpM.test_burkardt_3: 156	
----------------------------	

exp1 = numpy.exp(1)
tempResult = exp(39)
	
===================================================================	
count_neighbors_consistency.test_multiple_radius: 571	
----------------------------	

tempResult = exp(numpy.linspace(numpy.log(0.01), numpy.log(10), 3))
	
===================================================================	
logsumexp: 22	
----------------------------	

'Compute the log of the sum of exponentials of input elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : None or int or tuple of ints, optional\n        Axis or axes over which the sum is taken. By default `axis` is None,\n        and all elements are summed.\n\n        .. versionadded:: 0.11.0\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original array.\n\n        .. versionadded:: 0.15.0\n    b : array-like, optional\n        Scaling factor for exp(`a`) must be of the same shape as `a` or\n        broadcastable to `a`. These values may be negative in order to\n        implement subtraction.\n\n        .. versionadded:: 0.12.0\n    return_sign : bool, optional\n        If this is set to True, the result will be a pair containing sign\n        information; if False, results that are negative will be returned\n        as NaN. Default is False (no sign information).\n\n        .. versionadded:: 0.16.0\n    Returns\n    -------\n    res : ndarray\n        The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically\n        more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``\n        is returned.\n    sgn : ndarray\n        If return_sign is True, this will be an array of floating-point\n        numbers matching res and +1, 0, or -1 depending on the sign\n        of the result. If False, only one result is returned.\n\n    See Also\n    --------\n    numpy.logaddexp, numpy.logaddexp2\n\n    Notes\n    -----\n    Numpy has a logaddexp function which is very similar to `logsumexp`, but\n    only handles two arguments. `logaddexp.reduce` is similar to this\n    function, but may be less stable.\n\n    Examples\n    --------\n    >>> from scipy.special import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n\n    With weights\n\n    >>> a = np.arange(10)\n    >>> b = np.arange(10, 0, -1)\n    >>> logsumexp(a, b=b)\n    9.9170178533034665\n    >>> np.log(np.sum(b*np.exp(a)))\n    9.9170178533034647\n\n    Returning a sign flag\n\n    >>> logsumexp([1,2],b=[1,-1],return_sign=True)\n    (1.5413248546129181, -1.0)\n\n    Notice that `logsumexp` does not directly support masked arrays. To use it\n    on a masked array, convert the mask into zero weights:\n\n    >>> a = np.ma.array([np.log(2), 2, np.log(3)],\n    ...                  mask=[False, True, False])\n    >>> b = (~a.mask).astype(int)\n    >>> logsumexp(a.data, b=b), np.log(5)\n    1.6094379124341005, 1.6094379124341005\n\n    '
a = _asarray_validated(a, check_finite=False)
if (b is not None):
    (a, b) = numpy.broadcast_arrays(a, b)
    if numpy.any((b == 0)):
        a = (a + 0.0)
        a[(b == 0)] = (- numpy.inf)
a_max = numpy.amax(a, axis=axis, keepdims=True)
if (a_max.ndim > 0):
    a_max[(~ numpy.isfinite(a_max))] = 0
elif (not numpy.isfinite(a_max)):
    a_max = 0
if (b is not None):
    b = numpy.asarray(b)
    tempResult = exp((a - a_max))
	
===================================================================	
logsumexp: 24	
----------------------------	

'Compute the log of the sum of exponentials of input elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : None or int or tuple of ints, optional\n        Axis or axes over which the sum is taken. By default `axis` is None,\n        and all elements are summed.\n\n        .. versionadded:: 0.11.0\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original array.\n\n        .. versionadded:: 0.15.0\n    b : array-like, optional\n        Scaling factor for exp(`a`) must be of the same shape as `a` or\n        broadcastable to `a`. These values may be negative in order to\n        implement subtraction.\n\n        .. versionadded:: 0.12.0\n    return_sign : bool, optional\n        If this is set to True, the result will be a pair containing sign\n        information; if False, results that are negative will be returned\n        as NaN. Default is False (no sign information).\n\n        .. versionadded:: 0.16.0\n    Returns\n    -------\n    res : ndarray\n        The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically\n        more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``\n        is returned.\n    sgn : ndarray\n        If return_sign is True, this will be an array of floating-point\n        numbers matching res and +1, 0, or -1 depending on the sign\n        of the result. If False, only one result is returned.\n\n    See Also\n    --------\n    numpy.logaddexp, numpy.logaddexp2\n\n    Notes\n    -----\n    Numpy has a logaddexp function which is very similar to `logsumexp`, but\n    only handles two arguments. `logaddexp.reduce` is similar to this\n    function, but may be less stable.\n\n    Examples\n    --------\n    >>> from scipy.special import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n\n    With weights\n\n    >>> a = np.arange(10)\n    >>> b = np.arange(10, 0, -1)\n    >>> logsumexp(a, b=b)\n    9.9170178533034665\n    >>> np.log(np.sum(b*np.exp(a)))\n    9.9170178533034647\n\n    Returning a sign flag\n\n    >>> logsumexp([1,2],b=[1,-1],return_sign=True)\n    (1.5413248546129181, -1.0)\n\n    Notice that `logsumexp` does not directly support masked arrays. To use it\n    on a masked array, convert the mask into zero weights:\n\n    >>> a = np.ma.array([np.log(2), 2, np.log(3)],\n    ...                  mask=[False, True, False])\n    >>> b = (~a.mask).astype(int)\n    >>> logsumexp(a.data, b=b), np.log(5)\n    1.6094379124341005, 1.6094379124341005\n\n    '
a = _asarray_validated(a, check_finite=False)
if (b is not None):
    (a, b) = numpy.broadcast_arrays(a, b)
    if numpy.any((b == 0)):
        a = (a + 0.0)
        a[(b == 0)] = (- numpy.inf)
a_max = numpy.amax(a, axis=axis, keepdims=True)
if (a_max.ndim > 0):
    a_max[(~ numpy.isfinite(a_max))] = 0
elif (not numpy.isfinite(a_max)):
    a_max = 0
if (b is not None):
    b = numpy.asarray(b)
    tmp = (b * numpy.exp((a - a_max)))
else:
    tempResult = exp((a - a_max))
	
===================================================================	
TestCephes.test_pdtrc: 676	
----------------------------	

val = scipy.special._ufuncs.pdtrc(0, 1)
tempResult = exp((- 1))
	
===================================================================	
TestLegendreFunctions.test_clpmn_close_to_real_3: 2202	
----------------------------	

eps = 1e-10
m = 1
n = 3
x = 0.5
clp_plus = scipy.special.clpmn(m, n, (x + (1j * eps)), 3)[0][(m, n)]
clp_minus = scipy.special.clpmn(m, n, (x - (1j * eps)), 3)[0][(m, n)]
tempResult = exp((((- 0.5j) * m) * numpy.pi))
	
===================================================================	
TestLegendreFunctions.test_clpmn_close_to_real_3: 2202	
----------------------------	

eps = 1e-10
m = 1
n = 3
x = 0.5
clp_plus = scipy.special.clpmn(m, n, (x + (1j * eps)), 3)[0][(m, n)]
clp_minus = scipy.special.clpmn(m, n, (x - (1j * eps)), 3)[0][(m, n)]
tempResult = exp(((0.5j * m) * numpy.pi))
	
===================================================================	
TestErf.test_dawsn_consistent: 1286	
----------------------------	

tempResult = exp(((- z) * z))
	
===================================================================	
TestCephes.test_pdtr: 670	
----------------------------	

val = scipy.special._ufuncs.pdtr(0, 1)
tempResult = exp((- 1))
	
===================================================================	
TestErf.test_erfcx_consistent: 1280	
----------------------------	

tempResult = exp((z * z))
	
===================================================================	
f: 15	
----------------------------	

tempResult = exp(loggamma(z))
	
===================================================================	
test_logsumexp_b: 36	
----------------------------	

a = numpy.arange(200)
b = numpy.arange(200, 0, (- 1))
tempResult = exp(a)
	
===================================================================	
test_logsumexp_b: 48	
----------------------------	

a = numpy.arange(200)
b = numpy.arange(200, 0, (- 1))
desired = numpy.log(numpy.sum((b * numpy.exp(a))))
assert_almost_equal(logsumexp(a, b=b), desired)
a = [1000, 1000]
b = [1.2, 1.2]
desired = (1000 + numpy.log((2 * 1.2)))
assert_almost_equal(logsumexp(a, b=b), desired)
x = numpy.array(([1e-40] * 100000))
b = numpy.linspace(1, 1000, 100000)
logx = numpy.log(x)
X = numpy.vstack((x, x))
logX = numpy.vstack((logx, logx))
B = numpy.vstack((b, b))
tempResult = exp(logsumexp(logX, b=B))
	
===================================================================	
test_logsumexp_b: 49	
----------------------------	

a = numpy.arange(200)
b = numpy.arange(200, 0, (- 1))
desired = numpy.log(numpy.sum((b * numpy.exp(a))))
assert_almost_equal(logsumexp(a, b=b), desired)
a = [1000, 1000]
b = [1.2, 1.2]
desired = (1000 + numpy.log((2 * 1.2)))
assert_almost_equal(logsumexp(a, b=b), desired)
x = numpy.array(([1e-40] * 100000))
b = numpy.linspace(1, 1000, 100000)
logx = numpy.log(x)
X = numpy.vstack((x, x))
logX = numpy.vstack((logx, logx))
B = numpy.vstack((b, b))
assert_array_almost_equal(numpy.exp(logsumexp(logX, b=B)), (B * X).sum())
tempResult = exp(logsumexp(logX, b=B, axis=0))
	
===================================================================	
test_logsumexp_b: 50	
----------------------------	

a = numpy.arange(200)
b = numpy.arange(200, 0, (- 1))
desired = numpy.log(numpy.sum((b * numpy.exp(a))))
assert_almost_equal(logsumexp(a, b=b), desired)
a = [1000, 1000]
b = [1.2, 1.2]
desired = (1000 + numpy.log((2 * 1.2)))
assert_almost_equal(logsumexp(a, b=b), desired)
x = numpy.array(([1e-40] * 100000))
b = numpy.linspace(1, 1000, 100000)
logx = numpy.log(x)
X = numpy.vstack((x, x))
logX = numpy.vstack((logx, logx))
B = numpy.vstack((b, b))
assert_array_almost_equal(numpy.exp(logsumexp(logX, b=B)), (B * X).sum())
assert_array_almost_equal(numpy.exp(logsumexp(logX, b=B, axis=0)), (B * X).sum(axis=0))
tempResult = exp(logsumexp(logX, b=B, axis=1))
	
===================================================================	
test_logsumexp: 9	
----------------------------	

a = numpy.arange(200)
tempResult = exp(a)
	
===================================================================	
test_logsumexp: 22	
----------------------------	

a = numpy.arange(200)
desired = numpy.log(numpy.sum(numpy.exp(a)))
assert_almost_equal(logsumexp(a), desired)
b = [1000, 1000]
desired = (1000.0 + numpy.log(2.0))
assert_almost_equal(logsumexp(b), desired)
n = 1000
b = (numpy.ones(n) * 10000)
desired = (10000.0 + numpy.log(n))
assert_almost_equal(logsumexp(b), desired)
x = numpy.array(([1e-40] * 1000000))
logx = numpy.log(x)
X = numpy.vstack([x, x])
logX = numpy.vstack([logx, logx])
tempResult = exp(logsumexp(logX))
	
===================================================================	
test_logsumexp: 23	
----------------------------	

a = numpy.arange(200)
desired = numpy.log(numpy.sum(numpy.exp(a)))
assert_almost_equal(logsumexp(a), desired)
b = [1000, 1000]
desired = (1000.0 + numpy.log(2.0))
assert_almost_equal(logsumexp(b), desired)
n = 1000
b = (numpy.ones(n) * 10000)
desired = (10000.0 + numpy.log(n))
assert_almost_equal(logsumexp(b), desired)
x = numpy.array(([1e-40] * 1000000))
logx = numpy.log(x)
X = numpy.vstack([x, x])
logX = numpy.vstack([logx, logx])
assert_array_almost_equal(numpy.exp(logsumexp(logX)), X.sum())
tempResult = exp(logsumexp(logX, axis=0))
	
===================================================================	
test_logsumexp: 24	
----------------------------	

a = numpy.arange(200)
desired = numpy.log(numpy.sum(numpy.exp(a)))
assert_almost_equal(logsumexp(a), desired)
b = [1000, 1000]
desired = (1000.0 + numpy.log(2.0))
assert_almost_equal(logsumexp(b), desired)
n = 1000
b = (numpy.ones(n) * 10000)
desired = (10000.0 + numpy.log(n))
assert_almost_equal(logsumexp(b), desired)
x = numpy.array(([1e-40] * 1000000))
logx = numpy.log(x)
X = numpy.vstack([x, x])
logX = numpy.vstack([logx, logx])
assert_array_almost_equal(numpy.exp(logsumexp(logX)), X.sum())
assert_array_almost_equal(numpy.exp(logsumexp(logX, axis=0)), X.sum(axis=0))
tempResult = exp(logsumexp(logX, axis=1))
	
===================================================================	
test_spence_circle: 297	
----------------------------	


def spence(z):
    return complex(mpmath.polylog(2, (1 - z)))
r = numpy.linspace(0.5, 1.5)
theta = numpy.linspace(0, (2 * pi))
tempResult = exp((1j * theta))
	
===================================================================	
TestSystematic.test_lanczos_sum_expg_scaled: 890	
----------------------------	

maxgamma = 171.6243769563027
tempResult = exp(1)
	
===================================================================	
test_expi_complex: 24	
----------------------------	

dataset = []
for r in numpy.logspace((- 99), 2, 10):
    for p in numpy.linspace(0, (2 * numpy.pi), 30):
        tempResult = exp((1j * p))
	
===================================================================	
test_loggamma_taylor: 201	
----------------------------	

r = numpy.logspace((- 16), numpy.log10(LOGGAMMA_TAYLOR_RADIUS), 10)
theta = numpy.linspace(0, (2 * numpy.pi), 20)
(r, theta) = numpy.meshgrid(r, theta)
tempResult = exp((1j * theta))
	
===================================================================	
test_loggamma_taylor_transition: 188	
----------------------------	

r = (LOGGAMMA_TAYLOR_RADIUS + numpy.array([(- 0.1), (- 0.01), 0, 0.01, 0.1]))
theta = numpy.linspace(0, (2 * numpy.pi), 20)
(r, theta) = numpy.meshgrid(r, theta)
tempResult = exp((1j * theta))
	
===================================================================	
hermite_recursion: 363	
----------------------------	

H = numpy.zeros((n, nodes.size))
tempResult = exp(((- 0.5) * (nodes ** 2)))
	
===================================================================	
Y11: 19	
----------------------------	

tempResult = exp((1j * theta))
	
===================================================================	
Yn11: 13	
----------------------------	

tempResult = exp(((- 1j) * theta))
	
===================================================================	
ks_twosamp: 496	
----------------------------	

"\n    Computes the Kolmogorov-Smirnov test on two samples.\n\n    Missing values are discarded.\n\n    Parameters\n    ----------\n    data1 : array_like\n        First data set\n    data2 : array_like\n        Second data set\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Indicates the alternative hypothesis.  Default is 'two-sided'.\n\n    Returns\n    -------\n    d : float\n        Value of the Kolmogorov Smirnov test\n    p : float\n        Corresponding p-value.\n\n    "
(data1, data2) = (numpy.ma.asarray(data1), numpy.ma.asarray(data2))
(n1, n2) = (data1.count(), data2.count())
n = ((n1 * n2) / float((n1 + n2)))
mix = numpy.ma.concatenate((data1.compressed(), data2.compressed()))
mixsort = mix.argsort(kind='mergesort')
csum = np.where((mixsort < n1), (1.0 / n1), ((- 1.0) / n2)).cumsum()
if (len(numpy.unique(mix)) < (n1 + n2)):
    csum = csum[numpy.r_[(np.diff(mix[mixsort]).nonzero()[0], (- 1))]]
alternative = str(alternative).lower()[0]
if (alternative == 't'):
    d = ma.abs(csum).max()
    prob = scipy.special.kolmogorov((numpy.sqrt(n) * d))
elif (alternative == 'l'):
    d = (- csum.min())
    tempResult = exp((((- 2) * n) * (d ** 2)))
	
===================================================================	
ks_twosamp: 499	
----------------------------	

"\n    Computes the Kolmogorov-Smirnov test on two samples.\n\n    Missing values are discarded.\n\n    Parameters\n    ----------\n    data1 : array_like\n        First data set\n    data2 : array_like\n        Second data set\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Indicates the alternative hypothesis.  Default is 'two-sided'.\n\n    Returns\n    -------\n    d : float\n        Value of the Kolmogorov Smirnov test\n    p : float\n        Corresponding p-value.\n\n    "
(data1, data2) = (numpy.ma.asarray(data1), numpy.ma.asarray(data2))
(n1, n2) = (data1.count(), data2.count())
n = ((n1 * n2) / float((n1 + n2)))
mix = numpy.ma.concatenate((data1.compressed(), data2.compressed()))
mixsort = mix.argsort(kind='mergesort')
csum = np.where((mixsort < n1), (1.0 / n1), ((- 1.0) / n2)).cumsum()
if (len(numpy.unique(mix)) < (n1 + n2)):
    csum = csum[numpy.r_[(np.diff(mix[mixsort]).nonzero()[0], (- 1))]]
alternative = str(alternative).lower()[0]
if (alternative == 't'):
    d = ma.abs(csum).max()
    prob = scipy.special.kolmogorov((numpy.sqrt(n) * d))
elif (alternative == 'l'):
    d = (- csum.min())
    prob = numpy.exp((((- 2) * n) * (d ** 2)))
elif (alternative == 'g'):
    d = csum.max()
    tempResult = exp((((- 2) * n) * (d ** 2)))
	
===================================================================	
gmean: 72	
----------------------------	

'\n    Compute the geometric mean along the specified axis.\n\n    Returns the geometric average of the array elements.\n    That is:  n-th root of (x1 * x2 * ... * xn)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the geometric mean is computed. Default is 0.\n        If None, compute over the whole array `a`.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed. If dtype is not specified, it defaults to the\n        dtype of a, unless a has an integer dtype with a precision less than\n        that of the default platform integer. In that case, the default\n        platform integer is used.\n\n    Returns\n    -------\n    gmean : ndarray\n        see dtype parameter above\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.average : Weighted average\n    hmean : Harmonic mean\n\n    Notes\n    -----\n    The geometric average is computed over a single dimension of the input\n    array, axis=0 by default, or all values in the array if axis=None.\n    float64 intermediate and return values are used for integer inputs.\n\n    Use masked arrays to ignore any non-finite values in the input or that\n    arise in the calculations such as Not a Number and infinity because masked\n    arrays automatically mask any non-finite values.\n\n    '
if (not isinstance(a, numpy.ndarray)):
    log_a = numpy.log(numpy.array(a, dtype=dtype))
elif dtype:
    if isinstance(a, numpy.ma.MaskedArray):
        log_a = numpy.log(numpy.ma.asarray(a, dtype=dtype))
    else:
        log_a = numpy.log(numpy.asarray(a, dtype=dtype))
else:
    log_a = numpy.log(a)
tempResult = exp(log_a.mean(axis=axis))
	
===================================================================	
von_mises_cdf_normalapprox: 22	
----------------------------	

tempResult = exp(k)
	
===================================================================	
lomax_gen._sf: 2166	
----------------------------	

tempResult = exp(((- c) * scipy.special.log1p(x)))
	
===================================================================	
gilbrat_gen._ppf: 1757	
----------------------------	

tempResult = exp(_norm_ppf(q))
	
===================================================================	
genextreme_gen._logpdf: 987	
----------------------------	

cx = _lazywhere(((x == x) & (c != 0)), (x, c), (lambda x, c: (c * x)), 0.0)
logex2 = scipy.special.log1p((- cx))
logpex2 = self._loglogcdf(x, c)
tempResult = exp(logpex2)
	
===================================================================	
gompertz_gen._entropy: 1231	
----------------------------	

tempResult = exp(c)
	
===================================================================	
fatiguelife_gen._pdf: 720	
----------------------------	

tempResult = exp(self._logpdf(x, c))
	
===================================================================	
beta_gen._pdf: 212	
----------------------------	

tempResult = exp(self._logpdf(x, a, b))
	
===================================================================	
hypsecant_gen._cdf: 1374	
----------------------------	

tempResult = exp(x)
	
===================================================================	
kappa4_gen.f311: 1882	
----------------------------	

'cdf = np.exp(-np.exp(-x))\n               logcdf = ...\n            '
tempResult = exp((- x))
	
===================================================================	
invgamma_gen._pdf: 1408	
----------------------------	

tempResult = exp(self._logpdf(x, a))
	
===================================================================	
logistic_gen._pdf: 1629	
----------------------------	

tempResult = exp(self._logpdf(x))
	
===================================================================	
gumbel_l_gen._pdf: 1263	
----------------------------	

tempResult = exp(self._logpdf(x))
	
===================================================================	
ncf_gen._pdf_skip: 1989	
----------------------------	

(n1, n2) = (dfn, dfd)
term = (((((- nc) / 2) + (((nc * n1) * x) / (2 * (n2 + (n1 * x))))) + scipy.special.gammaln((n1 / 2.0))) + scipy.special.gammaln((1 + (n2 / 2.0))))
term -= scipy.special.gammaln(((n1 + n2) / 2.0))
tempResult = exp(term)
	
===================================================================	
invweibull_gen._cdf: 1474	
----------------------------	

xc1 = numpy.power(x, (- c))
tempResult = exp((- xc1))
	
===================================================================	
gengamma_gen._pdf: 1153	
----------------------------	

tempResult = exp(self._logpdf(x, a, c))
	
===================================================================	
kappa4_gen._cdf: 1863	
----------------------------	

tempResult = exp(self._logcdf(x, h, k))
	
===================================================================	
exponweib_gen._pdf: 668	
----------------------------	

tempResult = exp(self._logpdf(x, a, c))
	
===================================================================	
kappa4_gen._pdf: 1840	
----------------------------	

tempResult = exp(self._logpdf(x, h, k))
	
===================================================================	
lognorm_gen._ppf: 1720	
----------------------------	

tempResult = exp((s * _norm_ppf(q)))
	
===================================================================	
foldnorm_gen._stats: 812	
----------------------------	

c2 = (c * c)
tempResult = exp(((- 0.5) * c2))
	
===================================================================	
invgauss_gen._pdf: 1447	
----------------------------	

tempResult = exp((((- 1.0) / (2 * x)) * (((x - mu) / mu) ** 2)))
	
===================================================================	
f_gen._pdf: 765	
----------------------------	

tempResult = exp(self._logpdf(x, dfn, dfd))
	
===================================================================	
halfnorm_gen._pdf: 1349	
----------------------------	

tempResult = exp((((- x) * x) / 2.0))
	
===================================================================	
genpareto_gen._pdf: 922	
----------------------------	

tempResult = exp(self._logpdf(x, c))
	
===================================================================	
invweibull_gen._pdf: 1469	
----------------------------	

xc1 = numpy.power(x, ((- c) - 1.0))
xc2 = numpy.power(x, (- c))
tempResult = exp((- xc2))
	
===================================================================	
halflogistic_gen._pdf: 1316	
----------------------------	

tempResult = exp(self._logpdf(x))
	
===================================================================	
dweibull_gen._pdf: 570	
----------------------------	

ax = abs(x)
tempResult = exp((- (ax ** c)))
	
===================================================================	
halflogistic_gen._logpdf: 1319	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
powerlognorm_gen._ppf: 2285	
----------------------------	

tempResult = exp(((- s) * _norm_ppf(pow((1.0 - q), (1.0 / c)))))
	
===================================================================	
loggamma_gen._cdf: 1663	
----------------------------	

tempResult = exp(x)
	
===================================================================	
gumbel_r_gen._logpdf: 1241	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
exponnorm_gen._cdf: 649	
----------------------------	

invK = (1.0 / K)
expval = (invK * ((0.5 * invK) - x))
tempResult = exp(expval)
	
===================================================================	
genextreme_gen._logcdf: 994	
----------------------------	

tempResult = exp(self._loglogcdf(x, c))
	
===================================================================	
lognorm_gen._rvs: 1705	
----------------------------	

tempResult = exp((s * self._random_state.standard_normal(self._size)))
	
===================================================================	
laplace_gen._pdf: 1529	
----------------------------	

tempResult = exp((- abs(x)))
	
===================================================================	
exponnorm_gen._sf: 654	
----------------------------	

invK = (1.0 / K)
expval = (invK * ((0.5 * invK) - x))
tempResult = exp(expval)
	
===================================================================	
gilbrat_gen._rvs: 1745	
----------------------------	

tempResult = exp(self._random_state.standard_normal(self._size))
	
===================================================================	
chi2_gen._pdf: 484	
----------------------------	

tempResult = exp(self._logpdf(x, df))
	
===================================================================	
levy_l_gen._pdf: 1568	
----------------------------	

ax = abs(x)
tempResult = exp(((- 1) / (2 * ax)))
	
===================================================================	
gumbel_l_gen._logsf: 1275	
----------------------------	

tempResult = exp(x)
	
===================================================================	
kappa4_gen.f2: 1855	
----------------------------	

'pdf = np.exp(-x)*(1.0 - h*np.exp(-x))**(1.0/h - 1.0)\n               logpdf = ...\n            '
tempResult = exp((- x))
	
===================================================================	
argus_gen._pdf: 2789	
----------------------------	

'\n        Return PDF of the argus function\n        '
y = (1.0 - (x ** 2))
tempResult = exp((((- (chi ** 2)) * y) / 2))
	
===================================================================	
frechet_r_gen._pdf: 827	
----------------------------	

tempResult = exp((- pow(x, c)))
	
===================================================================	
genlogistic_gen._logpdf: 893	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
halfgennorm_gen._pdf: 2758	
----------------------------	

tempResult = exp(self._logpdf(x, beta))
	
===================================================================	
laplace_gen._cdf: 1532	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
laplace_gen._cdf: 1532	
----------------------------	

tempResult = exp(x)
	
===================================================================	
kappa4_gen.f21: 1878	
----------------------------	

'cdf = (1.0 - h*np.exp(-x))**(1.0/h)\n               logcdf = ...\n            '
tempResult = exp((- x))
	
===================================================================	
burr12_gen._pdf: 375	
----------------------------	

tempResult = exp(self._logpdf(x, c, d))
	
===================================================================	
gennorm_gen._pdf: 2727	
----------------------------	

tempResult = exp(self._logpdf(x, beta))
	
===================================================================	
exponpow_gen._logpdf: 692	
----------------------------	

xb = (x ** b)
tempResult = exp(xb)
	
===================================================================	
maxwell_gen._pdf: 1778	
----------------------------	

tempResult = exp((((- x) * x) / 2.0))
	
===================================================================	
nakagami_gen._pdf: 1939	
----------------------------	

tempResult = exp((((- nu) * x) * x))
	
===================================================================	
lognorm_gen._stats: 1729	
----------------------------	

tempResult = exp((s * s))
	
===================================================================	
ncf_gen._munp: 2004	
----------------------------	

val = (((dfn * 1.0) / dfd) ** n)
term = ((scipy.special.gammaln((n + (0.5 * dfn))) + scipy.special.gammaln(((0.5 * dfd) - n))) - scipy.special.gammaln((dfd * 0.5)))
tempResult = exp((((- nc) / 2.0) + term))
	
===================================================================	
genexpon_gen._pdf: 960	
----------------------------	

tempResult = exp(((((- a) - b) * x) + ((b * (- scipy.special.expm1(((- c) * x)))) / c)))
	
===================================================================	
gumbel_r_gen._logcdf: 1247	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
_norm_pdf: 41	
----------------------------	

tempResult = exp(((- (x ** 2)) / 2.0))
	
===================================================================	
chi_gen._pdf: 456	
----------------------------	

tempResult = exp(self._logpdf(x, df))
	
===================================================================	
nct_gen._pdf: 2071	
----------------------------	

n = (df * 1.0)
nc = (nc * 1.0)
x2 = (x * x)
ncx2 = ((nc * nc) * x2)
fac1 = (n + x2)
trm1 = (((n / 2.0) * numpy.log(n)) + scipy.special.gammaln((n + 1)))
trm1 -= ((((n * numpy.log(2)) + ((nc * nc) / 2.0)) + ((n / 2.0) * numpy.log(fac1))) + scipy.special.gammaln((n / 2.0)))
tempResult = exp(trm1)
	
===================================================================	
levy_gen._pdf: 1549	
----------------------------	

tempResult = exp(((- 1) / (2 * x)))
	
===================================================================	
frechet_l_gen._cdf: 862	
----------------------------	

tempResult = exp((- pow((- x), c)))
	
===================================================================	
gennorm_gen._stats: 2748	
----------------------------	

(c1, c3, c5) = scipy.special.gammaln([(1.0 / beta), (3.0 / beta), (5.0 / beta)])
tempResult = exp((c3 - c1))
	
===================================================================	
gennorm_gen._stats: 2748	
----------------------------	

(c1, c3, c5) = scipy.special.gammaln([(1.0 / beta), (3.0 / beta), (5.0 / beta)])
tempResult = exp(((c5 + c1) - (2.0 * c3)))
	
===================================================================	
gumbel_r_gen._pdf: 1238	
----------------------------	

tempResult = exp(self._logpdf(x))
	
===================================================================	
gumbel_l_gen._sf: 1278	
----------------------------	

tempResult = exp((- numpy.exp(x)))
	
===================================================================	
gumbel_l_gen._sf: 1278	
----------------------------	

tempResult = exp(x)
	
===================================================================	
expon_gen._pdf: 600	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
gumbel_l_gen._logpdf: 1266	
----------------------------	

tempResult = exp(x)
	
===================================================================	
genlogistic_gen._pdf: 890	
----------------------------	

tempResult = exp(self._logpdf(x, c))
	
===================================================================	
genextreme_gen._pdf: 981	
----------------------------	

tempResult = exp(self._logpdf(x, c))
	
===================================================================	
genlogistic_gen._cdf: 896	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
rice_gen._munp: 2409	
----------------------------	

nd2 = (n / 2.0)
n1 = (1 + nd2)
b2 = ((b * b) / 2.0)
tempResult = exp((- b2))
	
===================================================================	
lognorm_gen._pdf: 1708	
----------------------------	

tempResult = exp(self._logpdf(x, s))
	
===================================================================	
rayleigh_gen._sf: 2342	
----------------------------	

tempResult = exp(self._logsf(r))
	
===================================================================	
gilbrat_gen._pdf: 1748	
----------------------------	

tempResult = exp(self._logpdf(x))
	
===================================================================	
gompertz_gen._pdf: 1219	
----------------------------	

tempResult = exp(self._logpdf(x, c))
	
===================================================================	
burr12_gen._sf: 387	
----------------------------	

tempResult = exp(self._logsf(x, c, d))
	
===================================================================	
exponpow_gen._sf: 699	
----------------------------	

tempResult = exp((- scipy.special.expm1((x ** b))))
	
===================================================================	
dweibull_gen._cdf: 578	
----------------------------	

tempResult = exp((- (abs(x) ** c)))
	
===================================================================	
genextreme_gen._cdf: 997	
----------------------------	

tempResult = exp(self._logcdf(x, c))
	
===================================================================	
logistic_gen._logpdf: 1632	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
loggamma_gen._pdf: 1660	
----------------------------	

tempResult = exp((((c * x) - numpy.exp(x)) - scipy.special.gammaln(c)))
	
===================================================================	
loggamma_gen._pdf: 1660	
----------------------------	

tempResult = exp(x)
	
===================================================================	
kappa4_gen.f31: 1859	
----------------------------	

'pdf = np.exp(-x-np.exp(-x))\n               logpdf = ...\n            '
tempResult = exp((- x))
	
===================================================================	
rice_gen._pdf: 2403	
----------------------------	

tempResult = exp((((- (x - b)) * (x - b)) / 2.0))
	
===================================================================	
gumbel_r_gen._cdf: 1244	
----------------------------	

tempResult = exp((- numpy.exp((- x))))
	
===================================================================	
gumbel_r_gen._cdf: 1244	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
recipinvgauss_gen._cdf: 2425	
----------------------------	

trm1 = ((1.0 / mu) - x)
trm2 = ((1.0 / mu) + x)
isqx = (1.0 / numpy.sqrt(x))
tempResult = exp((2.0 / mu))
	
===================================================================	
_digammainv: 1049	
----------------------------	

_em = 0.5772156649015329
func = (lambda x: (scipy.special.digamma(x) - y))
if (y > (- 0.125)):
    tempResult = exp(y)
	
===================================================================	
_digammainv: 1054	
----------------------------	

_em = 0.5772156649015329
func = (lambda x: (scipy.special.digamma(x) - y))
if (y > (- 0.125)):
    x0 = (numpy.exp(y) + 0.5)
    if (y < 10):
        value = scipy.optimize.newton(func, x0, tol=1e-10)
        return value
elif (y > (- 3)):
    tempResult = exp((y / 2.332))
	
===================================================================	
pearson3_gen._pdf: 2211	
----------------------------	

tempResult = exp(self._logpdf(x, skew))
	
===================================================================	
truncexpon_gen._munp: 2547	
----------------------------	

if (n == 1):
    tempResult = exp((- b))
	
===================================================================	
truncexpon_gen._munp: 2549	
----------------------------	

if (n == 1):
    return ((1 - ((b + 1) * numpy.exp((- b)))) / (- scipy.special.expm1((- b))))
elif (n == 2):
    tempResult = exp((- b))
	
===================================================================	
frechet_l_gen._pdf: 856	
----------------------------	

tempResult = exp((- pow((- x), c)))
	
===================================================================	
truncexpon_gen._entropy: 2554	
----------------------------	

tempResult = exp(b)
	
===================================================================	
gamma_gen._pdf: 1069	
----------------------------	

tempResult = exp(self._logpdf(x, a))
	
===================================================================	
vonmises_gen._pdf: 2651	
----------------------------	

tempResult = exp((kappa * numpy.cos(x)))
	
===================================================================	
exponpow_gen._pdf: 688	
----------------------------	

tempResult = exp(self._logpdf(x, b))
	
===================================================================	
johnsonsb_gen._ppf: 1501	
----------------------------	

tempResult = exp((((- 1.0) / b) * (_norm_ppf(q) - a)))
	
===================================================================	
dgamma_gen._pdf: 536	
----------------------------	

ax = abs(x)
tempResult = exp((- ax))
	
===================================================================	
rayleigh_gen._pdf: 2330	
----------------------------	

tempResult = exp(self._logpdf(r))
	
===================================================================	
recipinvgauss_gen._pdf: 2416	
----------------------------	

tempResult = exp(((- ((1 - (mu * x)) ** 2.0)) / ((2 * x) * (mu ** 2.0))))
	
===================================================================	
t_gen._pdf: 2022	
----------------------------	

r = numpy.asarray((df * 1.0))
tempResult = exp((scipy.special.gammaln(((r + 1) / 2)) - scipy.special.gammaln((r / 2))))
	
===================================================================	
invgauss_gen._cdf: 1455	
----------------------------	

fac = numpy.sqrt((1.0 / x))
C1 = _norm_cdf(((fac * (x - mu)) / mu))
tempResult = exp((1.0 / mu))
	
===================================================================	
invgauss_gen._cdf: 1455	
----------------------------	

fac = numpy.sqrt((1.0 / x))
C1 = _norm_cdf(((fac * (x - mu)) / mu))
tempResult = exp((1.0 / mu))
	
===================================================================	
frechet_r_gen._sf: 836	
----------------------------	

tempResult = exp((- pow(x, c)))
	
===================================================================	
betaprime_gen._pdf: 300	
----------------------------	

tempResult = exp(self._logpdf(x, a, b))
	
===================================================================	
truncexpon_gen._pdf: 2534	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
expon_gen._sf: 612	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
gumbel_l_gen._cdf: 1269	
----------------------------	

tempResult = exp(x)
	
===================================================================	
geom_gen._sf: 151	
----------------------------	

tempResult = exp(self._logsf(x, p))
	
===================================================================	
_ncx2_pdf: 246	
----------------------------	

tempResult = exp(_ncx2_log_pdf(x, df, nc))
	
===================================================================	
multinomial_gen.pmf: 969	
----------------------------	

'\n        Multinomial probability mass function.\n\n        Parameters\n        ----------\n        x : array_like\n            Quantiles, with the last axis of `x` denoting the components.\n            Each quantile must be a symmetric positive definite matrix.\n        %(_doc_default_callparams)s\n\n        Returns\n        -------\n        pmf : ndarray or scalar\n            Probability density function evaluated at `x`\n\n        Notes\n        -----\n        %(_doc_callparams_note)s\n        '
tempResult = exp(self.logpmf(x, n, p))
	
===================================================================	
multivariate_normal_gen.pdf: 188	
----------------------------	

'\n        Multivariate normal probability density function.\n\n        Parameters\n        ----------\n        x : array_like\n            Quantiles, with the last axis of `x` denoting the components.\n        %(_mvn_doc_default_callparams)s\n\n        Returns\n        -------\n        pdf : ndarray\n            Probability density function evaluated at `x`\n\n        Notes\n        -----\n        %(_mvn_doc_callparams_note)s\n\n        '
(dim, mean, cov) = self._process_parameters(None, mean, cov)
x = self._process_quantiles(x, dim)
psd = _PSD(cov, allow_singular=allow_singular)
tempResult = exp(self._logpdf(x, mean, psd.U, psd.log_pdet, psd.rank))
	
===================================================================	
multivariate_normal_frozen.pdf: 219	
----------------------------	

tempResult = exp(self.logpdf(x))
	
===================================================================	
dirichlet_gen.pdf: 436	
----------------------------	

'\n        The Dirichlet probability density function.\n\n        Parameters\n        ----------\n        x : array_like\n            Quantiles, with the last axis of `x` denoting the components.\n        %(_dirichlet_doc_default_callparams)s\n\n        Returns\n        -------\n        pdf : ndarray\n            The probability density function evaluated at `x`.\n\n        '
alpha = _dirichlet_check_parameters(alpha)
x = _dirichlet_check_input(alpha, x)
tempResult = exp(self._logpdf(x, alpha))
	
===================================================================	
wishart_gen.pdf: 590	
----------------------------	

'\n        Wishart probability density function.\n\n        Parameters\n        ----------\n        x : array_like\n            Quantiles, with the last axis of `x` denoting the components.\n            Each quantile must be a symmetric positive definite matrix.\n        %(_doc_default_callparams)s\n\n        Returns\n        -------\n        pdf : ndarray\n            Probability density function evaluated at `x`\n\n        Notes\n        -----\n        %(_doc_callparams_note)s\n\n        '
tempResult = exp(self.logpdf(x, df, scale))
	
===================================================================	
matrix_normal_gen.pdf: 331	
----------------------------	

'\n        Matrix normal probability density function.\n\n        Parameters\n        ----------\n        X : array_like\n            Quantiles, with the last two axes of `X` denoting the components.\n        %(_matnorm_doc_default_callparams)s\n\n        Returns\n        -------\n        pdf : ndarray\n            Probability density function evaluated at `X`\n\n        Notes\n        -----\n        %(_matnorm_doc_callparams_note)s\n\n        '
tempResult = exp(self.logpdf(X, mean, rowcov, colcov))
	
===================================================================	
invwishart_frozen.pdf: 878	
----------------------------	

tempResult = exp(self.logpdf(x))
	
===================================================================	
invwishart_gen.pdf: 783	
----------------------------	

'\n        Inverse Wishart probability density function.\n\n        Parameters\n        ----------\n        x : array_like\n            Quantiles, with the last axis of `x` denoting the components.\n            Each quantile must be a symmetric positive definite matrix.\n\n        %(_doc_default_callparams)s\n\n        Returns\n        -------\n        pdf : ndarray\n            Probability density function evaluated at `x`\n\n        Notes\n        -----\n        %(_doc_callparams_note)s\n\n        '
tempResult = exp(self.logpdf(x, df, scale))
	
===================================================================	
matrix_normal_frozen.pdf: 363	
----------------------------	

tempResult = exp(self.logpdf(X))
	
===================================================================	
wishart_frozen.pdf: 691	
----------------------------	

tempResult = exp(self.logpdf(x))
	
===================================================================	
TestBeta.test_logpdf_ticket_1866: 967	
----------------------------	

(alpha, beta) = (267, 1472)
x = numpy.array([0.2, 0.5, 0.6])
b = scipy.stats.beta(alpha, beta)
assert_allclose(b.logpdf(x).sum(), (- 1201.699061824062))
tempResult = exp(b.logpdf(x))
	
===================================================================	
TestGumbelL.test_logcdf_logsf: 1019	
----------------------------	

x = numpy.linspace((- 100), (- 4))
y = scipy.stats.gumbel_l.logcdf(x)
z = scipy.stats.gumbel_l.logsf(x)
tempResult = exp(y)
	
===================================================================	
TestBetaPrime.test_logpdf: 976	
----------------------------	

(alpha, beta) = (267, 1472)
x = numpy.array([0.2, 0.5, 0.6])
b = scipy.stats.betaprime(alpha, beta)
assert_(np.isfinite(b.logpdf(x)).all())
tempResult = exp(b.logpdf(x))
	
===================================================================	
TestNBinom.test_pmf: 214	
----------------------------	

tempResult = exp(scipy.stats.nbinom.logpmf(700, 721, 0.52))
	
===================================================================	
test_norm_logcdf: 1843	
----------------------------	

x = (- numpy.asarray(list(range(0, 120, 4))))
expected = [(- 0.69314718), (- 10.36010149), (- 35.01343716), (- 75.410673), (- 131.69539607), (- 203.91715537), (- 292.098721), (- 396.25241451), (- 516.38564863), (- 652.50322759), (- 804.60844201), (- 972.70364403), (- 1156.7905731), (- 1356.87055173), (- 1572.94460885), (- 1805.01356068), (- 2053.07806561), (- 2317.13866238), (- 2597.19579746), (- 2893.24984493), (- 3205.30112136), (- 3533.34989701), (- 3877.39640444), (- 4237.44084522), (- 4613.4833952), (- 5005.52420869), (- 5413.56342187), (- 5837.60115548), (- 6277.63751711), (- 6733.67260303)]
assert_allclose(stats.norm().logcdf(x), expected, atol=1e-08)
assert_allclose(stats.norm().logcdf((x + 1e-14j)).real, expected, atol=1e-08)
deriv = (stats.norm.logcdf((x + 1e-10j)) / 1e-10).imag
tempResult = exp((scipy.stats.norm.logpdf(x) - scipy.stats.norm.logcdf(x)))
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1604	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
tempResult = exp((- 0.25))
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1612	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
assert_allclose(lc, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_min.sf(x, a, scale=b)
tempResult = exp((- 0.25))
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1616	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
assert_allclose(lc, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_min.sf(x, a, scale=b)
assert_allclose(s, numpy.exp((- 0.25)))
ls = scipy.stats.weibull_min.logsf(x, a, scale=b)
assert_allclose(ls, (- 0.25))
s = scipy.stats.weibull_min.sf(30, 2, scale=3)
tempResult = exp((- 100))
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1621	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
assert_allclose(lc, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_min.sf(x, a, scale=b)
assert_allclose(s, numpy.exp((- 0.25)))
ls = scipy.stats.weibull_min.logsf(x, a, scale=b)
assert_allclose(ls, (- 0.25))
s = scipy.stats.weibull_min.sf(30, 2, scale=3)
assert_allclose(s, numpy.exp((- 100)))
ls = scipy.stats.weibull_min.logsf(30, 2, scale=3)
assert_allclose(ls, (- 100))
x = (- 1.5)
p = scipy.stats.weibull_max.pdf(x, a, scale=b)
tempResult = exp((- 0.25))
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1625	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
assert_allclose(lc, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_min.sf(x, a, scale=b)
assert_allclose(s, numpy.exp((- 0.25)))
ls = scipy.stats.weibull_min.logsf(x, a, scale=b)
assert_allclose(ls, (- 0.25))
s = scipy.stats.weibull_min.sf(30, 2, scale=3)
assert_allclose(s, numpy.exp((- 100)))
ls = scipy.stats.weibull_min.logsf(30, 2, scale=3)
assert_allclose(ls, (- 100))
x = (- 1.5)
p = scipy.stats.weibull_max.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_max.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_max.cdf(x, a, scale=b)
tempResult = exp((- 0.25))
	
===================================================================	
TestMultivariateNormal.test_large_pseudo_determinant: 108	
----------------------------	

large_total_log = 1000.0
npos = 100
nzero = 2
tempResult = exp((large_total_log / npos))
	
===================================================================	
TestMultivariateNormal.test_lnB: 262	
----------------------------	

alpha = numpy.array([1, 1, 1])
desired = 0.5
tempResult = exp(_lnB(alpha))
	
===================================================================	
CoupledDecay.verify: 392	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
tempResult = exp(((- lmbd[0]) * t))
	
===================================================================	
CoupledDecay.verify: 393	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
tempResult = exp(((- lmbd[1]) * t))
	
===================================================================	
CoupledDecay.verify: 394	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
e1 = numpy.exp(((- lmbd[1]) * t))
tempResult = exp(((- lmbd[2]) * t))
	
===================================================================	
Test_lsim2.test_05: 330	
----------------------------	

A = numpy.array([[(- 1.0), 0.0], [0.0, (- 2.0)]])
B = numpy.array([[1.0, 0.0], [0.0, 1.0]])
C = numpy.array([1.0, 0.0])
D = numpy.zeros((1, 2))
t = numpy.linspace(0, 10.0, 101)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', BadCoefficients)
    (tout, y, x) = lsim2((A, B, C, D), T=t, X0=[1.0, 1.0])
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_05: 331	
----------------------------	

A = numpy.array([[(- 1.0), 0.0], [0.0, (- 2.0)]])
B = numpy.array([[1.0, 0.0], [0.0, 1.0]])
C = numpy.array([1.0, 0.0])
D = numpy.zeros((1, 2))
t = numpy.linspace(0, 10.0, 101)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', BadCoefficients)
    (tout, y, x) = lsim2((A, B, C, D), T=t, X0=[1.0, 1.0])
expected_y = numpy.exp((- tout))
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_05: 332	
----------------------------	

A = numpy.array([[(- 1.0), 0.0], [0.0, (- 2.0)]])
B = numpy.array([[1.0, 0.0], [0.0, 1.0]])
C = numpy.array([1.0, 0.0])
D = numpy.zeros((1, 2))
t = numpy.linspace(0, 10.0, 101)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', BadCoefficients)
    (tout, y, x) = lsim2((A, B, C, D), T=t, X0=[1.0, 1.0])
expected_y = numpy.exp((- tout))
expected_x0 = numpy.exp((- tout))
tempResult = exp(((- 2.0) * tout))
	
===================================================================	
_TestStepFuncs.test_01: 409	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system)
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_06: 340	
----------------------------	

system = ([1.0], [1.0, 2.0, 1.0])
(tout, y, x) = lsim2(system, X0=[1.0, 0.0])
tempResult = exp((- tout))
	
===================================================================	
_TestImpulseFuncs.test_03: 364	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system, X0=3.0)
tempResult = exp((- tout))
	
===================================================================	
TestLsim.test_jordan_block: 260	
----------------------------	

A = numpy.mat('-1. 1.; 0. -1.')
B = numpy.mat('0.; 1.')
C = numpy.mat('1. 0.')
system = self.lti_nowarn(A, B, C, 0.0)
t = numpy.linspace(0, 5)
u = numpy.zeros_like(t)
(tout, y, x) = lsim(system, u, t, X0=[0.0, 1.0])
tempResult = exp((- tout))
	
===================================================================	
_TestStepFuncs.test_06: 443	
----------------------------	

system = ([1.0], [1.0, 2.0, 1.0])
(tout, y) = self.func(system)
tempResult = exp((- tout))
	
===================================================================	
Test_lsim2.test_01: 294	
----------------------------	

t = numpy.linspace(0, 10, 1001)
u = numpy.zeros_like(t)
system = ([1.0], [1.0, 1.0])
(tout, y, x) = lsim2(system, u, t, X0=[1.0])
tempResult = exp((- tout))
	
===================================================================	
_TestImpulseFuncs.test_02: 358	
----------------------------	

system = ([1.0], [1.0, 1.0])
n = 21
t = numpy.linspace(0, 2.0, n)
(tout, y) = self.func(system, T=t)
assert_equal(tout.shape, (n,))
assert_almost_equal(tout, t)
tempResult = exp((- t))
	
===================================================================	
_TestStepFuncs.test_04: 431	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system, X0=[3.0])
tempResult = exp((- tout))
	
===================================================================	
_TestStepFuncs.test_02: 419	
----------------------------	

system = ([1.0], [1.0, 1.0])
n = 21
t = numpy.linspace(0, 2.0, n)
(tout, y) = self.func(system, T=t)
assert_equal(tout.shape, (n,))
assert_almost_equal(tout, t)
tempResult = exp((- t))
	
===================================================================	
Test_lsim2.test_04: 318	
----------------------------	

t = numpy.linspace(0, 10, 1001)
u = numpy.zeros_like(t)
system = ([1.0], [1.0, 2.0, 1.0])
(tout, y, x) = lsim2(system, u, t, X0=[1.0, 0.0])
tempResult = exp((- tout))
	
===================================================================	
_TestImpulseFuncs.test_01: 348	
----------------------------	

system = ([1.0], [1.0, 1.0])
(tout, y) = self.func(system)
tempResult = exp((- tout))
	
===================================================================	
hermite_recursion: 363	
----------------------------	

H = numpy.zeros((n, nodes.size))
tempResult = exp(((- 0.5) * (nodes ** 2)))
	
===================================================================	
CoupledDecay.verify: 392	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
tempResult = exp(((- lmbd[0]) * t))
	
===================================================================	
CoupledDecay.verify: 393	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
tempResult = exp(((- lmbd[1]) * t))
	
===================================================================	
CoupledDecay.verify: 394	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
e1 = numpy.exp(((- lmbd[1]) * t))
tempResult = exp(((- lmbd[2]) * t))
	
===================================================================	
test_roots_genlaguerre: 564	
----------------------------	

rootf = (lambda a: (lambda n, mu: scipy.special.roots_genlaguerre(n, a, mu)))
evalf = (lambda a: (lambda n, x: scipy.special.orthogonal.eval_genlaguerre(n, a, x)))
tempResult = exp((- x))
	
===================================================================	
CoupledDecay.verify: 392	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
tempResult = exp(((- lmbd[0]) * t))
	
===================================================================	
CoupledDecay.verify: 393	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
tempResult = exp(((- lmbd[1]) * t))
	
===================================================================	
CoupledDecay.verify: 394	
----------------------------	

lmbd = numpy.array(self.lmbd)
d10 = (lmbd[1] - lmbd[0])
d21 = (lmbd[2] - lmbd[1])
d20 = (lmbd[2] - lmbd[0])
e0 = numpy.exp(((- lmbd[0]) * t))
e1 = numpy.exp(((- lmbd[1]) * t))
tempResult = exp(((- lmbd[2]) * t))
	
===================================================================	
test_roots_genlaguerre: 564	
----------------------------	

rootf = (lambda a: (lambda n, mu: scipy.special.roots_genlaguerre(n, a, mu)))
evalf = (lambda a: (lambda n, x: scipy.special.orthogonal.eval_genlaguerre(n, a, x)))
tempResult = exp((- x))
	
***************************************************	
sklearn_sklearn-0.18.0: 118	
===================================================================	
objective: 157	
----------------------------	

tempResult = exp(((AB[0] * F) + AB[1]))
	
===================================================================	
_SigmoidCalibration.predict: 192	
----------------------------	

'Predict new data by linear interpolation.\n\n        Parameters\n        ----------\n        T : array-like, shape (n_samples,)\n            Data to predict from.\n\n        Returns\n        -------\n        T_ : array, shape (n_samples,)\n            The predicted data.\n        '
T = column_or_1d(T)
tempResult = exp(((self.a_ * T) + self.b_))
	
===================================================================	
grad: 166	
----------------------------	

tempResult = exp(((AB[0] * F) + AB[1]))
	
===================================================================	
LinearDiscriminantAnalysis.predict_proba: 176	
----------------------------	

'Estimate probability.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        C : array, shape (n_samples, n_classes)\n            Estimated probabilities.\n        '
prob = self.decision_function(X)
prob *= (- 1)
tempResult = exp(prob, prob)
	
===================================================================	
QuadraticDiscriminantAnalysis.predict_proba: 277	
----------------------------	

'Return posterior probabilities of classification.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Array of samples/test vectors.\n\n        Returns\n        -------\n        C : array, shape = [n_samples, n_classes]\n            Posterior probabilities of classification per class.\n        '
values = self._decision_function(X)
tempResult = exp((values - values.max(axis=1)[:, numpy.newaxis]))
	
===================================================================	
BernoulliNB._joint_log_likelihood: 273	
----------------------------	

'Calculate the posterior log probability of the samples X'
check_is_fitted(self, 'classes_')
X = check_array(X, accept_sparse='csr')
if (self.binarize is not None):
    X = binarize(X, threshold=self.binarize)
(n_classes, n_features) = self.feature_log_prob_.shape
(n_samples, n_features_X) = X.shape
if (n_features_X != n_features):
    raise ValueError(('Expected input with %d features, got %d instead' % (n_features, n_features_X)))
tempResult = exp(self.feature_log_prob_)
	
===================================================================	
BaseNB.predict_proba: 38	
----------------------------	

'\n        Return probability estimates for the test vector X.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n\n        Returns\n        -------\n        C : array-like, shape = [n_samples, n_classes]\n            Returns the probability of the samples for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute `classes_`.\n        '
tempResult = exp(self.predict_log_proba(X))
	
===================================================================	
make_low_rank_matrix: 276	
----------------------------	

"Generate a mostly low rank matrix with bell-shaped singular values\n\n    Most of the variance can be explained by a bell-shaped curve of width\n    effective_rank: the low rank part of the singular values profile is::\n\n        (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\n    The remaining singular values' tail is fat, decreasing as::\n\n        tail_strength * exp(-0.1 * i / effective_rank).\n\n    The low rank part of the profile can be considered the structured\n    signal part of the data while the tail can be considered the noisy\n    part of the data that cannot be summarized by a low number of linear\n    components (singular vectors).\n\n    This kind of singular profiles is often seen in practice, for instance:\n     - gray level pictures of faces\n     - TF-IDF vectors of text documents crawled from the web\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    effective_rank : int, optional (default=10)\n        The approximate number of singular vectors required to explain most of\n        the data by linear combinations.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The matrix.\n    "
generator = check_random_state(random_state)
n = min(n_samples, n_features)
(u, _) = scipy.linalg.qr(generator.randn(n_samples, n), mode='economic')
(v, _) = scipy.linalg.qr(generator.randn(n_features, n), mode='economic')
singular_ind = numpy.arange(n, dtype=numpy.float64)
tempResult = exp(((- 1.0) * ((singular_ind / effective_rank) ** 2)))
	
===================================================================	
make_low_rank_matrix: 277	
----------------------------	

"Generate a mostly low rank matrix with bell-shaped singular values\n\n    Most of the variance can be explained by a bell-shaped curve of width\n    effective_rank: the low rank part of the singular values profile is::\n\n        (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\n    The remaining singular values' tail is fat, decreasing as::\n\n        tail_strength * exp(-0.1 * i / effective_rank).\n\n    The low rank part of the profile can be considered the structured\n    signal part of the data while the tail can be considered the noisy\n    part of the data that cannot be summarized by a low number of linear\n    components (singular vectors).\n\n    This kind of singular profiles is often seen in practice, for instance:\n     - gray level pictures of faces\n     - TF-IDF vectors of text documents crawled from the web\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    effective_rank : int, optional (default=10)\n        The approximate number of singular vectors required to explain most of\n        the data by linear combinations.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The matrix.\n    "
generator = check_random_state(random_state)
n = min(n_samples, n_features)
(u, _) = scipy.linalg.qr(generator.randn(n_samples, n), mode='economic')
(v, _) = scipy.linalg.qr(generator.randn(n_features, n), mode='economic')
singular_ind = numpy.arange(n, dtype=numpy.float64)
low_rank = ((1 - tail_strength) * numpy.exp(((- 1.0) * ((singular_ind / effective_rank) ** 2))))
tempResult = exp((((- 0.1) * singular_ind) / effective_rank))
	
===================================================================	
_exp: 73	
----------------------------	

tempResult = exp(((- (x ** 2)) / 2))
	
===================================================================	
LatentDirichletAllocation.perplexity: 268	
----------------------------	

'Calculate approximate perplexity for data X.\n\n        Perplexity is defined as exp(-1. * log-likelihood per word)\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, [n_samples, n_features]\n            Document word matrix.\n\n        doc_topic_distr : None or array, shape=(n_samples, n_topics)\n            Document topic distribution.\n            If it is None, it will be generated by applying transform on X.\n\n        Returns\n        -------\n        score : float\n            Perplexity score.\n        '
if (not hasattr(self, 'components_')):
    raise NotFittedError("no 'components_' attribute in model. Please fit model first.")
X = self._check_non_neg_array(X, 'LatentDirichletAllocation.perplexity')
if (doc_topic_distr is None):
    doc_topic_distr = self.transform(X)
else:
    (n_samples, n_topics) = doc_topic_distr.shape
    if (n_samples != X.shape[0]):
        raise ValueError('Number of samples in X and doc_topic_distr do not match.')
    if (n_topics != self.n_topics):
        raise ValueError('Number of topics does not match.')
current_samples = X.shape[0]
bound = self._approx_bound(X, doc_topic_distr, sub_sampling)
if sub_sampling:
    word_cnt = (X.sum() * (float(self.total_samples) / current_samples))
else:
    word_cnt = X.sum()
perword_bound = (bound / word_cnt)
tempResult = exp(((- 1.0) * perword_bound))
	
===================================================================	
_update_doc_distribution: 26	
----------------------------	

'E-step: update document-topic distribution.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape=(n_samples, n_features)\n        Document word matrix.\n\n    exp_topic_word_distr : dense matrix, shape=(n_topics, n_features)\n        Exponential value of expection of log topic word distribution.\n        In the literature, this is `exp(E[log(beta)])`.\n\n    doc_topic_prior : float\n        Prior of document topic distribution `theta`.\n\n    max_iters : int\n        Max number of iterations for updating document topic distribution in\n        the E-step.\n\n    mean_change_tol : float\n        Stopping tolerance for updating document topic distribution in E-setp.\n\n    cal_sstats : boolean\n        Parameter that indicate to calculate sufficient statistics or not.\n        Set `cal_sstats` to `True` when we need to run M-step.\n\n    random_state : RandomState instance or None\n        Parameter that indicate how to initialize document topic distribution.\n        Set `random_state` to None will initialize document topic distribution\n        to a constant number.\n\n    Returns\n    -------\n    (doc_topic_distr, suff_stats) :\n        `doc_topic_distr` is unnormalized topic distribution for each document.\n        In the literature, this is `gamma`. we can calculate `E[log(theta)]`\n        from it.\n        `suff_stats` is expected sufficient statistics for the M-step.\n            When `cal_sstats == False`, this will be None.\n\n    '
is_sparse_x = scipy.sparse.issparse(X)
(n_samples, n_features) = X.shape
n_topics = exp_topic_word_distr.shape[0]
if random_state:
    doc_topic_distr = random_state.gamma(100.0, 0.01, (n_samples, n_topics))
else:
    doc_topic_distr = numpy.ones((n_samples, n_topics))
tempResult = exp(_dirichlet_expectation_2d(doc_topic_distr))
	
===================================================================	
LatentDirichletAllocation._init_latent_vars: 103	
----------------------------	

'Initialize latent variables.'
self.random_state_ = check_random_state(self.random_state)
self.n_batch_iter_ = 1
self.n_iter_ = 0
if (self.doc_topic_prior is None):
    self.doc_topic_prior_ = (1.0 / self.n_topics)
else:
    self.doc_topic_prior_ = self.doc_topic_prior
if (self.topic_word_prior is None):
    self.topic_word_prior_ = (1.0 / self.n_topics)
else:
    self.topic_word_prior_ = self.topic_word_prior
init_gamma = 100.0
init_var = (1.0 / init_gamma)
self.components_ = self.random_state_.gamma(init_gamma, init_var, (self.n_topics, n_features))
tempResult = exp(_dirichlet_expectation_2d(self.components_))
	
===================================================================	
LatentDirichletAllocation._em_step: 133	
----------------------------	

'EM update for 1 iteration.\n\n        update `_component` by batch VB or online VB.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape=(n_samples, n_features)\n            Document word matrix.\n\n        total_samples : integer\n            Total umber of documents. It is only used when\n            batch_update is `False`.\n\n        batch_update : boolean\n            Parameter that controls updating method.\n            `True` for batch learning, `False` for online learning.\n\n        parallel : joblib.Parallel\n            Pre-initialized instance of joblib.Parallel\n\n        Returns\n        -------\n        doc_topic_distr : array, shape=(n_samples, n_topics)\n            Unnormalized document topic distribution.\n        '
(_, suff_stats) = self._e_step(X, cal_sstats=True, random_init=True, parallel=parallel)
if batch_update:
    self.components_ = (self.topic_word_prior_ + suff_stats)
else:
    weight = numpy.power((self.learning_offset + self.n_batch_iter_), (- self.learning_decay))
    doc_ratio = (float(total_samples) / X.shape[0])
    self.components_ *= (1 - weight)
    self.components_ += (weight * (self.topic_word_prior_ + (doc_ratio * suff_stats)))
tempResult = exp(_dirichlet_expectation_2d(self.components_))
	
===================================================================	
test_lda_score_perplexity: 211	
----------------------------	

(n_topics, X) = _build_sparse_mtx()
lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10, random_state=0)
distr = lda.fit_transform(X)
perplexity_1 = lda.perplexity(X, distr, sub_sampling=False)
score = lda.score(X)
tempResult = exp(((- 1.0) * (score / numpy.sum(X.data))))
	
===================================================================	
test_dirichlet_expectation: 226	
----------------------------	

'Test Cython version of Dirichlet expectation calculation.'
x = numpy.logspace((- 100), 10, 10000)
expectation = numpy.empty_like(x)
_dirichlet_expectation_1d(x, 0, expectation)
tempResult = exp((psi(x) - psi(numpy.sum(x))))
	
===================================================================	
test_pca_score: 275	
----------------------------	

(n, p) = (1000, 3)
rng = numpy.random.RandomState(0)
X = ((rng.randn(n, p) * 0.1) + numpy.array([3, 4, 5]))
for solver in solver_list:
    pca = PCA(n_components=2, svd_solver=solver)
    pca.fit(X)
    ll1 = pca.score(X)
    tempResult = exp(1)
	
===================================================================	
MultinomialDeviance._score_to_proba: 405	
----------------------------	

tempResult = exp((score - logsumexp(score, axis=1)[:, numpy.newaxis]))
	
===================================================================	
ExponentialLoss.__call__: 425	
----------------------------	

pred = pred.ravel()
if (sample_weight is None):
    tempResult = exp(((- ((2.0 * y) - 1.0)) * pred))
	
===================================================================	
ExponentialLoss.__call__: 427	
----------------------------	

pred = pred.ravel()
if (sample_weight is None):
    return numpy.mean(numpy.exp(((- ((2.0 * y) - 1.0)) * pred)))
else:
    tempResult = exp(((- ((2 * y) - 1)) * pred))
	
===================================================================	
ExponentialLoss._update_terminal_region: 439	
----------------------------	

terminal_region = numpy.where((terminal_regions == leaf))[0]
pred = pred.take(terminal_region, axis=0)
y = y.take(terminal_region, axis=0)
sample_weight = sample_weight.take(terminal_region, axis=0)
y_ = ((2.0 * y) - 1.0)
tempResult = exp(((- y_) * pred))
	
===================================================================	
ExponentialLoss._update_terminal_region: 440	
----------------------------	

terminal_region = numpy.where((terminal_regions == leaf))[0]
pred = pred.take(terminal_region, axis=0)
y = y.take(terminal_region, axis=0)
sample_weight = sample_weight.take(terminal_region, axis=0)
y_ = ((2.0 * y) - 1.0)
numerator = numpy.sum(((y_ * sample_weight) * numpy.exp(((- y_) * pred))))
tempResult = exp(((- y_) * pred))
	
===================================================================	
ExponentialLoss.negative_gradient: 431	
----------------------------	

y_ = (- ((2.0 * y) - 1.0))
tempResult = exp((y_ * pred.ravel()))
	
===================================================================	
MultinomialDeviance.negative_gradient: 388	
----------------------------	

'Compute negative gradient for the ``k``-th class. '
tempResult = exp((pred[:, k] - logsumexp(pred, axis=1)))
	
===================================================================	
AdaBoostClassifier._boost_real: 156	
----------------------------	

'Implement a single boost using the SAMME.R real algorithm.'
estimator = self._make_estimator(random_state=random_state)
estimator.fit(X, y, sample_weight=sample_weight)
y_predict_proba = estimator.predict_proba(X)
if (iboost == 0):
    self.classes_ = getattr(estimator, 'classes_', None)
    self.n_classes_ = len(self.classes_)
y_predict = self.classes_.take(numpy.argmax(y_predict_proba, axis=1), axis=0)
incorrect = (y_predict != y)
estimator_error = numpy.mean(numpy.average(incorrect, weights=sample_weight, axis=0))
if (estimator_error <= 0):
    return (sample_weight, 1.0, 0.0)
n_classes = self.n_classes_
classes = self.classes_
y_codes = numpy.array([((- 1.0) / (n_classes - 1)), 1.0])
y_coding = y_codes.take((classes == y[:, numpy.newaxis]))
proba = y_predict_proba
proba[(proba < np.finfo(proba.dtype).eps)] = np.finfo(proba.dtype).eps
estimator_weight = (((- 1.0) * self.learning_rate) * (((n_classes - 1.0) / n_classes) * inner1d(y_coding, numpy.log(y_predict_proba))))
if (not (iboost == (self.n_estimators - 1))):
    tempResult = exp((estimator_weight * ((sample_weight > 0) | (estimator_weight < 0))))
	
===================================================================	
AdaBoostClassifier._boost_discrete: 179	
----------------------------	

'Implement a single boost using the SAMME discrete algorithm.'
estimator = self._make_estimator(random_state=random_state)
estimator.fit(X, y, sample_weight=sample_weight)
y_predict = estimator.predict(X)
if (iboost == 0):
    self.classes_ = getattr(estimator, 'classes_', None)
    self.n_classes_ = len(self.classes_)
incorrect = (y_predict != y)
estimator_error = numpy.mean(numpy.average(incorrect, weights=sample_weight, axis=0))
if (estimator_error <= 0):
    return (sample_weight, 1.0, 0.0)
n_classes = self.n_classes_
if (estimator_error >= (1.0 - (1.0 / n_classes))):
    self.estimators_.pop((- 1))
    if (len(self.estimators_) == 0):
        raise ValueError('BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.')
    return (None, None, None)
estimator_weight = (self.learning_rate * (numpy.log(((1.0 - estimator_error) / estimator_error)) + numpy.log((n_classes - 1.0))))
if (not (iboost == (self.n_estimators - 1))):
    tempResult = exp(((estimator_weight * incorrect) * ((sample_weight > 0) | (estimator_weight < 0))))
	
===================================================================	
AdaBoostRegressor._boost: 320	
----------------------------	

'Implement a single boost for regression\n\n        Perform a single boost according to the AdaBoost.R2 algorithm and\n        return the updated sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. DOK and LIL are converted to CSR.\n\n        y : array-like of shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n\n        sample_weight : array-like of shape = [n_samples]\n            The current sample weights.\n\n        random_state : numpy.RandomState\n            The current random number generator\n\n        Returns\n        -------\n        sample_weight : array-like of shape = [n_samples] or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        estimator_weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        estimator_error : float\n            The regression error for the current boost.\n            If None then boosting has terminated early.\n        '
estimator = self._make_estimator(random_state=random_state)
cdf = sample_weight.cumsum()
cdf /= cdf[(- 1)]
uniform_samples = random_state.random_sample(X.shape[0])
bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
bootstrap_idx = numpy.array(bootstrap_idx, copy=False)
estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
y_predict = estimator.predict(X)
error_vect = numpy.abs((y_predict - y))
error_max = error_vect.max()
if (error_max != 0.0):
    error_vect /= error_max
if (self.loss == 'square'):
    error_vect **= 2
elif (self.loss == 'exponential'):
    tempResult = exp((- error_vect))
	
===================================================================	
AdaBoostClassifier.predict_proba: 253	
----------------------------	

'Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. DOK and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : array of shape = [n_samples]\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the `classes_` attribute.\n        '
check_is_fitted(self, 'n_classes_')
n_classes = self.n_classes_
X = self._validate_X_predict(X)
if (self.algorithm == 'SAMME.R'):
    proba = sum((_samme_proba(estimator, n_classes, X) for estimator in self.estimators_))
else:
    proba = sum(((estimator.predict_proba(X) * w) for (estimator, w) in zip(self.estimators_, self.estimator_weights_)))
proba /= self.estimator_weights_.sum()
tempResult = exp(((1.0 / (n_classes - 1)) * proba))
	
===================================================================	
AdaBoostClassifier.staged_predict_proba: 275	
----------------------------	

'Predict class probabilities for X.\n\n        The predicted class probabilities of an input sample is computed as\n        the weighted mean predicted class probabilities of the classifiers\n        in the ensemble.\n\n        This generator method yields the ensemble predicted class probabilities\n        after each iteration of boosting and therefore allows monitoring, such\n        as to determine the predicted class probabilities on a test set after\n        each boost.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. DOK and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : generator of array, shape = [n_samples]\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the `classes_` attribute.\n        '
X = self._validate_X_predict(X)
n_classes = self.n_classes_
proba = None
norm = 0.0
for (weight, estimator) in zip(self.estimator_weights_, self.estimators_):
    norm += weight
    if (self.algorithm == 'SAMME.R'):
        current_proba = _samme_proba(estimator, n_classes, X)
    else:
        current_proba = (estimator.predict_proba(X) * weight)
    if (proba is None):
        proba = current_proba
    else:
        proba += current_proba
    tempResult = exp(((1.0 / (n_classes - 1)) * (proba / norm)))
	
===================================================================	
test_probability: 130	
----------------------------	

rng = check_random_state(0)
(X_train, X_test, y_train, y_test) = train_test_split(iris.data, iris.target, random_state=rng)
with numpy.errstate(divide='ignore', invalid='ignore'):
    ensemble = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=rng).fit(X_train, y_train)
    assert_array_almost_equal(numpy.sum(ensemble.predict_proba(X_test), axis=1), numpy.ones(len(X_test)))
    tempResult = exp(ensemble.predict_log_proba(X_test))
	
===================================================================	
test_probability: 133	
----------------------------	

rng = check_random_state(0)
(X_train, X_test, y_train, y_test) = train_test_split(iris.data, iris.target, random_state=rng)
with numpy.errstate(divide='ignore', invalid='ignore'):
    ensemble = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=rng).fit(X_train, y_train)
    assert_array_almost_equal(numpy.sum(ensemble.predict_proba(X_test), axis=1), numpy.ones(len(X_test)))
    assert_array_almost_equal(ensemble.predict_proba(X_test), numpy.exp(ensemble.predict_log_proba(X_test)))
    ensemble = BaggingClassifier(base_estimator=LogisticRegression(), random_state=rng, max_samples=5).fit(X_train, y_train)
    assert_array_almost_equal(numpy.sum(ensemble.predict_proba(X_test), axis=1), numpy.ones(len(X_test)))
    tempResult = exp(ensemble.predict_log_proba(X_test))
	
===================================================================	
check_probability: 124	
----------------------------	

ForestClassifier = FOREST_CLASSIFIERS[name]
with numpy.errstate(divide='ignore'):
    clf = ForestClassifier(n_estimators=10, random_state=1, max_features=1, max_depth=1)
    clf.fit(iris.data, iris.target)
    assert_array_almost_equal(numpy.sum(clf.predict_proba(iris.data), axis=1), numpy.ones(iris.data.shape[0]))
    tempResult = exp(clf.predict_log_proba(iris.data))
	
===================================================================	
test_probability_exponential: 628	
----------------------------	

clf = GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1)
assert_raises(ValueError, clf.predict_proba, T)
clf.fit(X, y)
assert_array_equal(clf.predict(T), true_result)
y_proba = clf.predict_proba(T)
assert_true(numpy.all((y_proba >= 0.0)))
assert_true(numpy.all((y_proba <= 1.0)))
score = clf.decision_function(T).ravel()
tempResult = exp(((- 2) * score))
	
===================================================================	
test_binomial_deviance: 25	
----------------------------	

bd = BinomialDeviance(2)
assert_equal(bd(numpy.array([0.0]), numpy.array([0.0])), bd(numpy.array([1.0]), numpy.array([0.0])))
assert_almost_equal(bd(numpy.array([1.0, 1.0, 1.0]), numpy.array([100.0, 100.0, 100.0])), 0.0)
assert_almost_equal(bd(numpy.array([1.0, 0.0, 0.0]), numpy.array([100.0, (- 100.0), (- 100.0)])), 0)
alt_dev = (lambda y, pred: numpy.mean(numpy.logaddexp(0.0, (((- 2.0) * ((2.0 * y) - 1)) * pred))))
test_data = [(numpy.array([1.0, 1.0, 1.0]), numpy.array([100.0, 100.0, 100.0])), (numpy.array([0.0, 0.0, 0.0]), numpy.array([100.0, 100.0, 100.0])), (numpy.array([0.0, 0.0, 0.0]), numpy.array([(- 100.0), (- 100.0), (- 100.0)])), (numpy.array([1.0, 1.0, 1.0]), numpy.array([(- 100.0), (- 100.0), (- 100.0)]))]
for datum in test_data:
    assert_almost_equal(bd(*datum), alt_dev(*datum))
tempResult = exp(((2 * ((2 * y) - 1)) * pred))
	
===================================================================	
absolute_exponential: 14	
----------------------------	

"\n    Absolute exponential autocorrelation model.\n    (Ornstein-Uhlenbeck stochastic process)::\n\n                                          n\n        theta, d --> r(theta, d) = exp(  sum  - theta_i * |d_i| )\n                                        i = 1\n\n    Parameters\n    ----------\n    theta : array_like\n        An array with shape 1 (isotropic) or n (anisotropic) giving the\n        autocorrelation parameter(s).\n\n    d : array_like\n        An array with shape (n_eval, n_features) giving the componentwise\n        distances between locations x and x' at which the correlation model\n        should be evaluated.\n\n    Returns\n    -------\n    r : array_like\n        An array with shape (n_eval, ) containing the values of the\n        autocorrelation model.\n    "
theta = numpy.asarray(theta, dtype=numpy.float64)
d = numpy.abs(numpy.asarray(d, dtype=numpy.float64))
if (d.ndim > 1):
    n_features = d.shape[1]
else:
    n_features = 1
if (theta.size == 1):
    tempResult = exp(((- theta[0]) * numpy.sum(d, axis=1)))
	
===================================================================	
absolute_exponential: 18	
----------------------------	

"\n    Absolute exponential autocorrelation model.\n    (Ornstein-Uhlenbeck stochastic process)::\n\n                                          n\n        theta, d --> r(theta, d) = exp(  sum  - theta_i * |d_i| )\n                                        i = 1\n\n    Parameters\n    ----------\n    theta : array_like\n        An array with shape 1 (isotropic) or n (anisotropic) giving the\n        autocorrelation parameter(s).\n\n    d : array_like\n        An array with shape (n_eval, n_features) giving the componentwise\n        distances between locations x and x' at which the correlation model\n        should be evaluated.\n\n    Returns\n    -------\n    r : array_like\n        An array with shape (n_eval, ) containing the values of the\n        autocorrelation model.\n    "
theta = numpy.asarray(theta, dtype=numpy.float64)
d = numpy.abs(numpy.asarray(d, dtype=numpy.float64))
if (d.ndim > 1):
    n_features = d.shape[1]
else:
    n_features = 1
if (theta.size == 1):
    return numpy.exp(((- theta[0]) * numpy.sum(d, axis=1)))
elif (theta.size != n_features):
    raise ValueError(('Length of theta must be 1 or %s' % n_features))
else:
    tempResult = exp((- numpy.sum((theta.reshape(1, n_features) * d), axis=1)))
	
===================================================================	
squared_exponential: 29	
----------------------------	

"\n    Squared exponential correlation model (Radial Basis Function).\n    (Infinitely differentiable stochastic process, very smooth)::\n\n                                          n\n        theta, d --> r(theta, d) = exp(  sum  - theta_i * (d_i)^2 )\n                                        i = 1\n\n    Parameters\n    ----------\n    theta : array_like\n        An array with shape 1 (isotropic) or n (anisotropic) giving the\n        autocorrelation parameter(s).\n\n    d : array_like\n        An array with shape (n_eval, n_features) giving the componentwise\n        distances between locations x and x' at which the correlation model\n        should be evaluated.\n\n    Returns\n    -------\n    r : array_like\n        An array with shape (n_eval, ) containing the values of the\n        autocorrelation model.\n    "
theta = numpy.asarray(theta, dtype=numpy.float64)
d = numpy.asarray(d, dtype=numpy.float64)
if (d.ndim > 1):
    n_features = d.shape[1]
else:
    n_features = 1
if (theta.size == 1):
    tempResult = exp(((- theta[0]) * numpy.sum((d ** 2), axis=1)))
	
===================================================================	
squared_exponential: 33	
----------------------------	

"\n    Squared exponential correlation model (Radial Basis Function).\n    (Infinitely differentiable stochastic process, very smooth)::\n\n                                          n\n        theta, d --> r(theta, d) = exp(  sum  - theta_i * (d_i)^2 )\n                                        i = 1\n\n    Parameters\n    ----------\n    theta : array_like\n        An array with shape 1 (isotropic) or n (anisotropic) giving the\n        autocorrelation parameter(s).\n\n    d : array_like\n        An array with shape (n_eval, n_features) giving the componentwise\n        distances between locations x and x' at which the correlation model\n        should be evaluated.\n\n    Returns\n    -------\n    r : array_like\n        An array with shape (n_eval, ) containing the values of the\n        autocorrelation model.\n    "
theta = numpy.asarray(theta, dtype=numpy.float64)
d = numpy.asarray(d, dtype=numpy.float64)
if (d.ndim > 1):
    n_features = d.shape[1]
else:
    n_features = 1
if (theta.size == 1):
    return numpy.exp(((- theta[0]) * numpy.sum((d ** 2), axis=1)))
elif (theta.size != n_features):
    raise ValueError(('Length of theta must be 1 or %s' % n_features))
else:
    tempResult = exp((- numpy.sum((theta.reshape(1, n_features) * (d ** 2)), axis=1)))
	
===================================================================	
generalized_exponential: 51	
----------------------------	

"\n    Generalized exponential correlation model.\n    (Useful when one does not know the smoothness of the function to be\n    predicted.)::\n\n                                          n\n        theta, d --> r(theta, d) = exp(  sum  - theta_i * |d_i|^p )\n                                        i = 1\n\n    Parameters\n    ----------\n    theta : array_like\n        An array with shape 1+1 (isotropic) or n+1 (anisotropic) giving the\n        autocorrelation parameter(s) (theta, p).\n\n    d : array_like\n        An array with shape (n_eval, n_features) giving the componentwise\n        distances between locations x and x' at which the correlation model\n        should be evaluated.\n\n    Returns\n    -------\n    r : array_like\n        An array with shape (n_eval, ) with the values of the autocorrelation\n        model.\n    "
theta = numpy.asarray(theta, dtype=numpy.float64)
d = numpy.asarray(d, dtype=numpy.float64)
if (d.ndim > 1):
    n_features = d.shape[1]
else:
    n_features = 1
lth = theta.size
if ((n_features > 1) and (lth == 2)):
    theta = numpy.hstack([numpy.repeat(theta[0], n_features), theta[1]])
elif (lth != (n_features + 1)):
    raise Exception(('Length of theta must be 2 or %s' % (n_features + 1)))
else:
    theta = theta.reshape(1, lth)
td = (theta[:, 0:(- 1)].reshape(1, n_features) * (numpy.abs(d) ** theta[:, (- 1)]))
tempResult = exp((- numpy.sum(td, 1)))
	
===================================================================	
_BinaryGaussianProcessClassifierLaplace.fit: 59	
----------------------------	

'Fit Gaussian process classification model\n\n        Parameters\n        ----------\n        X : array-like, shape = (n_samples, n_features)\n            Training data\n\n        y : array-like, shape = (n_samples,)\n            Target values, must be binary\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
if (self.kernel is None):
    self.kernel_ = (C(1.0, constant_value_bounds='fixed') * RBF(1.0, length_scale_bounds='fixed'))
else:
    self.kernel_ = clone(self.kernel)
self.rng = check_random_state(self.random_state)
self.X_train_ = (numpy.copy(X) if self.copy_X_train else X)
label_encoder = LabelEncoder()
self.y_train_ = label_encoder.fit_transform(y)
self.classes_ = label_encoder.classes_
if (self.classes_.size > 2):
    raise ValueError(('%s supports only binary classification. y contains classes %s' % (self.__class__.__name__, self.classes_)))
elif (self.classes_.size == 1):
    raise ValueError('{0:s} requires 2 classes.'.format(self.__class__.__name__))
if ((self.optimizer is not None) and (self.kernel_.n_dims > 0)):

    def obj_func(theta, eval_gradient=True):
        if eval_gradient:
            (lml, grad) = self.log_marginal_likelihood(theta, eval_gradient=True)
            return ((- lml), (- grad))
        else:
            return (- self.log_marginal_likelihood(theta))
    optima = [self._constrained_optimization(obj_func, self.kernel_.theta, self.kernel_.bounds)]
    if (self.n_restarts_optimizer > 0):
        if (not np.isfinite(self.kernel_.bounds).all()):
            raise ValueError('Multiple optimizer restarts (n_restarts_optimizer>0) requires that all bounds are finite.')
        bounds = self.kernel_.bounds
        for iteration in range(self.n_restarts_optimizer):
            tempResult = exp(self.rng.uniform(bounds[:, 0], bounds[:, 1]))
	
===================================================================	
_BinaryGaussianProcessClassifierLaplace._posterior_mode: 124	
----------------------------	

"Mode-finding for binary Laplace GPC and fixed kernel.\n\n        This approximates the posterior of the latent function values for given\n        inputs and target observations with a Gaussian approximation and uses\n        Newton's iteration to find the mode of this approximation.\n        "
if (self.warm_start and hasattr(self, 'f_cached') and (self.f_cached.shape == self.y_train_.shape)):
    f = self.f_cached
else:
    f = numpy.zeros_like(self.y_train_, dtype=numpy.float64)
log_marginal_likelihood = (- numpy.inf)
for _ in range(self.max_iter_predict):
    tempResult = exp((- f))
	
===================================================================	
_BinaryGaussianProcessClassifierLaplace._posterior_mode: 133	
----------------------------	

"Mode-finding for binary Laplace GPC and fixed kernel.\n\n        This approximates the posterior of the latent function values for given\n        inputs and target observations with a Gaussian approximation and uses\n        Newton's iteration to find the mode of this approximation.\n        "
if (self.warm_start and hasattr(self, 'f_cached') and (self.f_cached.shape == self.y_train_.shape)):
    f = self.f_cached
else:
    f = numpy.zeros_like(self.y_train_, dtype=numpy.float64)
log_marginal_likelihood = (- numpy.inf)
for _ in range(self.max_iter_predict):
    pi = (1 / (1 + numpy.exp((- f))))
    W = (pi * (1 - pi))
    W_sr = numpy.sqrt(W)
    W_sr_K = (W_sr[:, numpy.newaxis] * K)
    B = (numpy.eye(W.shape[0]) + (W_sr_K * W_sr))
    L = cholesky(B, lower=True)
    b = ((W * f) + (self.y_train_ - pi))
    a = (b - (W_sr * cho_solve((L, True), W_sr_K.dot(b))))
    f = K.dot(a)
    tempResult = exp(((- ((self.y_train_ * 2) - 1)) * f))
	
===================================================================	
ExpSineSquared.__call__: 679	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
if (Y is None):
    dists = squareform(pdist(X, metric='euclidean'))
    arg = ((numpy.pi * dists) / self.periodicity)
    sin_of_arg = numpy.sin(arg)
    tempResult = exp(((- 2) * ((sin_of_arg / self.length_scale) ** 2)))
	
===================================================================	
ExpSineSquared.__call__: 684	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
if (Y is None):
    dists = squareform(pdist(X, metric='euclidean'))
    arg = ((numpy.pi * dists) / self.periodicity)
    sin_of_arg = numpy.sin(arg)
    K = numpy.exp(((- 2) * ((sin_of_arg / self.length_scale) ** 2)))
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist(X, Y, metric='euclidean')
    tempResult = exp(((- 2) * ((numpy.sin(((numpy.pi / self.periodicity) * dists)) / self.length_scale) ** 2)))
	
===================================================================	
PairwiseKernel.f: 782	
----------------------------	

tempResult = exp(gamma)
	
===================================================================	
Kernel.theta1: 123	
----------------------------	

'Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\n        Parameters\n        ----------\n        theta : array, shape (n_dims,)\n            The non-fixed, log-transformed hyperparameters of the kernel\n        '
params = self.get_params()
i = 0
for hyperparameter in self.hyperparameters:
    if hyperparameter.fixed:
        continue
    if (hyperparameter.n_elements > 1):
        tempResult = exp(theta[i:(i + hyperparameter.n_elements)])
	
===================================================================	
Kernel.theta1: 126	
----------------------------	

'Sets the (flattened, log-transformed) non-fixed hyperparameters.\n\n        Parameters\n        ----------\n        theta : array, shape (n_dims,)\n            The non-fixed, log-transformed hyperparameters of the kernel\n        '
params = self.get_params()
i = 0
for hyperparameter in self.hyperparameters:
    if hyperparameter.fixed:
        continue
    if (hyperparameter.n_elements > 1):
        params[hyperparameter.name] = numpy.exp(theta[i:(i + hyperparameter.n_elements)])
        i += hyperparameter.n_elements
    else:
        tempResult = exp(theta[i])
	
===================================================================	
Matern.__call__: 555	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
length_scale = _check_length_scale(X, self.length_scale)
if (Y is None):
    dists = pdist((X / length_scale), metric='euclidean')
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist((X / length_scale), (Y / length_scale), metric='euclidean')
if (self.nu == 0.5):
    tempResult = exp((- dists))
	
===================================================================	
Matern.__call__: 558	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
length_scale = _check_length_scale(X, self.length_scale)
if (Y is None):
    dists = pdist((X / length_scale), metric='euclidean')
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist((X / length_scale), (Y / length_scale), metric='euclidean')
if (self.nu == 0.5):
    K = numpy.exp((- dists))
elif (self.nu == 1.5):
    K = (dists * math.sqrt(3))
    tempResult = exp((- K))
	
===================================================================	
Matern.__call__: 561	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
length_scale = _check_length_scale(X, self.length_scale)
if (Y is None):
    dists = pdist((X / length_scale), metric='euclidean')
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist((X / length_scale), (Y / length_scale), metric='euclidean')
if (self.nu == 0.5):
    K = numpy.exp((- dists))
elif (self.nu == 1.5):
    K = (dists * math.sqrt(3))
    K = ((1.0 + K) * numpy.exp((- K)))
elif (self.nu == 2.5):
    K = (dists * math.sqrt(5))
    tempResult = exp((- K))
	
===================================================================	
Matern.__call__: 584	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
length_scale = _check_length_scale(X, self.length_scale)
if (Y is None):
    dists = pdist((X / length_scale), metric='euclidean')
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist((X / length_scale), (Y / length_scale), metric='euclidean')
if (self.nu == 0.5):
    K = numpy.exp((- dists))
elif (self.nu == 1.5):
    K = (dists * math.sqrt(3))
    K = ((1.0 + K) * numpy.exp((- K)))
elif (self.nu == 2.5):
    K = (dists * math.sqrt(5))
    K = (((1.0 + K) + ((K ** 2) / 3.0)) * numpy.exp((- K)))
else:
    K = dists
    K[(K == 0.0)] += np.finfo(float).eps
    tmp = (math.sqrt((2 * self.nu)) * K)
    K.fill(((2 ** (1.0 - self.nu)) / gamma(self.nu)))
    K *= (tmp ** self.nu)
    K *= kv(self.nu, tmp)
if (Y is None):
    K = squareform(K)
    numpy.fill_diagonal(K, 1)
if eval_gradient:
    if self.hyperparameter_length_scale.fixed:
        K_gradient = numpy.empty((X.shape[0], X.shape[0], 0))
        return (K, K_gradient)
    if self.anisotropic:
        D = (((X[:, numpy.newaxis, :] - X[numpy.newaxis, :, :]) ** 2) / (length_scale ** 2))
    else:
        D = squareform((dists ** 2))[:, :, numpy.newaxis]
    if (self.nu == 0.5):
        K_gradient = ((K[(..., numpy.newaxis)] * D) / numpy.sqrt(D.sum(2))[:, :, numpy.newaxis])
        K_gradient[(~ numpy.isfinite(K_gradient))] = 0
    elif (self.nu == 1.5):
        tempResult = exp((- numpy.sqrt((3 * D.sum((- 1))))))
	
===================================================================	
Matern.__call__: 587	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
length_scale = _check_length_scale(X, self.length_scale)
if (Y is None):
    dists = pdist((X / length_scale), metric='euclidean')
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist((X / length_scale), (Y / length_scale), metric='euclidean')
if (self.nu == 0.5):
    K = numpy.exp((- dists))
elif (self.nu == 1.5):
    K = (dists * math.sqrt(3))
    K = ((1.0 + K) * numpy.exp((- K)))
elif (self.nu == 2.5):
    K = (dists * math.sqrt(5))
    K = (((1.0 + K) + ((K ** 2) / 3.0)) * numpy.exp((- K)))
else:
    K = dists
    K[(K == 0.0)] += np.finfo(float).eps
    tmp = (math.sqrt((2 * self.nu)) * K)
    K.fill(((2 ** (1.0 - self.nu)) / gamma(self.nu)))
    K *= (tmp ** self.nu)
    K *= kv(self.nu, tmp)
if (Y is None):
    K = squareform(K)
    numpy.fill_diagonal(K, 1)
if eval_gradient:
    if self.hyperparameter_length_scale.fixed:
        K_gradient = numpy.empty((X.shape[0], X.shape[0], 0))
        return (K, K_gradient)
    if self.anisotropic:
        D = (((X[:, numpy.newaxis, :] - X[numpy.newaxis, :, :]) ** 2) / (length_scale ** 2))
    else:
        D = squareform((dists ** 2))[:, :, numpy.newaxis]
    if (self.nu == 0.5):
        K_gradient = ((K[(..., numpy.newaxis)] * D) / numpy.sqrt(D.sum(2))[:, :, numpy.newaxis])
        K_gradient[(~ numpy.isfinite(K_gradient))] = 0
    elif (self.nu == 1.5):
        K_gradient = ((3 * D) * numpy.exp((- numpy.sqrt((3 * D.sum((- 1))))))[(..., numpy.newaxis)])
    elif (self.nu == 2.5):
        tmp = numpy.sqrt((5 * D.sum((- 1))))[(..., numpy.newaxis)]
        tempResult = exp((- tmp))
	
===================================================================	
RBF.__call__: 510	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
length_scale = _check_length_scale(X, self.length_scale)
if (Y is None):
    dists = pdist((X / length_scale), metric='sqeuclidean')
    tempResult = exp(((- 0.5) * dists))
	
===================================================================	
RBF.__call__: 517	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
length_scale = _check_length_scale(X, self.length_scale)
if (Y is None):
    dists = pdist((X / length_scale), metric='sqeuclidean')
    K = numpy.exp(((- 0.5) * dists))
    K = squareform(K)
    numpy.fill_diagonal(K, 1)
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist((X / length_scale), (Y / length_scale), metric='sqeuclidean')
    tempResult = exp(((- 0.5) * dists))
	
===================================================================	
test_no_optimizer: 84	
----------------------------	

kernel = RBF(1.0)
gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None).fit(X, y)
tempResult = exp(gpr.kernel_.theta)
	
===================================================================	
test_prior: 69	
----------------------------	

for kernel in kernels:
    gpr = GaussianProcessRegressor(kernel=kernel)
    (y_mean, y_cov) = gpr.predict(X, return_cov=True)
    assert_almost_equal(y_mean, 0, 5)
    if (len(gpr.kernel.theta) > 1):
        tempResult = exp(kernel.theta[0])
	
===================================================================	
test_anisotropic_kernel: 99	
----------------------------	

rng = numpy.random.RandomState(0)
X = rng.uniform((- 1), 1, (50, 2))
y = (X[:, 0] + (0.1 * X[:, 1]))
kernel = RBF([1.0, 1.0])
gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
tempResult = exp(gpr.kernel_.theta[1])
	
===================================================================	
test_anisotropic_kernel: 99	
----------------------------	

rng = numpy.random.RandomState(0)
X = rng.uniform((- 1), 1, (50, 2))
y = (X[:, 0] + (0.1 * X[:, 1]))
kernel = RBF([1.0, 1.0])
gpr = GaussianProcessRegressor(kernel=kernel).fit(X, y)
tempResult = exp(gpr.kernel_.theta[0])
	
===================================================================	
test_set_get_params: 168	
----------------------------	

for kernel in kernels:
    index = 0
    params = kernel.get_params()
    for hyperparameter in kernel.hyperparameters:
        if (hyperparameter.bounds == 'fixed'):
            continue
        size = hyperparameter.n_elements
        if (size > 1):
            tempResult = exp(kernel.theta[index:(index + size)])
	
===================================================================	
test_set_get_params: 171	
----------------------------	

for kernel in kernels:
    index = 0
    params = kernel.get_params()
    for hyperparameter in kernel.hyperparameters:
        if (hyperparameter.bounds == 'fixed'):
            continue
        size = hyperparameter.n_elements
        if (size > 1):
            assert_almost_equal(numpy.exp(kernel.theta[index:(index + size)]), params[hyperparameter.name])
            index += size
        else:
            tempResult = exp(kernel.theta[index])
	
===================================================================	
test_set_get_params: 181	
----------------------------	

for kernel in kernels:
    index = 0
    params = kernel.get_params()
    for hyperparameter in kernel.hyperparameters:
        if (hyperparameter.bounds == 'fixed'):
            continue
        size = hyperparameter.n_elements
        if (size > 1):
            assert_almost_equal(numpy.exp(kernel.theta[index:(index + size)]), params[hyperparameter.name])
            index += size
        else:
            assert_almost_equal(numpy.exp(kernel.theta[index]), params[hyperparameter.name])
            index += 1
    index = 0
    value = 10
    for hyperparameter in kernel.hyperparameters:
        if (hyperparameter.bounds == 'fixed'):
            continue
        size = hyperparameter.n_elements
        if (size > 1):
            kernel.set_params(**{hyperparameter.name: ([value] * size)})
            tempResult = exp(kernel.theta[index:(index + size)])
	
===================================================================	
test_set_get_params: 185	
----------------------------	

for kernel in kernels:
    index = 0
    params = kernel.get_params()
    for hyperparameter in kernel.hyperparameters:
        if (hyperparameter.bounds == 'fixed'):
            continue
        size = hyperparameter.n_elements
        if (size > 1):
            assert_almost_equal(numpy.exp(kernel.theta[index:(index + size)]), params[hyperparameter.name])
            index += size
        else:
            assert_almost_equal(numpy.exp(kernel.theta[index]), params[hyperparameter.name])
            index += 1
    index = 0
    value = 10
    for hyperparameter in kernel.hyperparameters:
        if (hyperparameter.bounds == 'fixed'):
            continue
        size = hyperparameter.n_elements
        if (size > 1):
            kernel.set_params(**{hyperparameter.name: ([value] * size)})
            assert_almost_equal(numpy.exp(kernel.theta[index:(index + size)]), ([value] * size))
            index += size
        else:
            kernel.set_params(**{hyperparameter.name: value})
            tempResult = exp(kernel.theta[index])
	
===================================================================	
test_matern_kernel: 140	
----------------------------	

K = Matern(nu=1.5, length_scale=1.0)(X)
assert_array_almost_equal(numpy.diag(K), numpy.ones(X.shape[0]))
tempResult = exp((- euclidean_distances(X, X, squared=False)))
	
===================================================================	
LinearClassifierMixin._predict_proba_lr: 190	
----------------------------	

'Probability estimation for OvR logistic regression.\n\n        Positive class probabilities are computed as\n        1. / (1. + np.exp(-self.decision_function(X)));\n        multiclass is handled by normalizing that over all classes.\n        '
prob = self.decision_function(X)
prob *= (- 1)
tempResult = exp(prob, prob)
	
===================================================================	
_multinomial_loss: 109	
----------------------------	

'Computes multinomial loss and class probabilities.\n\n    Parameters\n    ----------\n    w : ndarray, shape (n_classes * n_features,) or\n        (n_classes * (n_features + 1),)\n        Coefficient vector.\n\n    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n        Training data.\n\n    Y : ndarray, shape (n_samples, n_classes)\n        Transformed labels according to the output of LabelBinarizer.\n\n    alpha : float\n        Regularization parameter. alpha is equal to 1 / C.\n\n    sample_weight : array-like, shape (n_samples,) optional\n        Array of weights that are assigned to individual samples.\n        If not provided, then each sample is given unit weight.\n\n    Returns\n    -------\n    loss : float\n        Multinomial loss.\n\n    p : ndarray, shape (n_samples, n_classes)\n        Estimated class probabilities.\n\n    w : ndarray, shape (n_classes, n_features)\n        Reshaped param vector excluding intercept terms.\n\n    Reference\n    ---------\n    Bishop, C. M. (2006). Pattern recognition and machine learning.\n    Springer. (Chapter 4.3.4)\n    '
n_classes = Y.shape[1]
n_features = X.shape[1]
fit_intercept = (w.size == (n_classes * (n_features + 1)))
w = w.reshape(n_classes, (- 1))
sample_weight = sample_weight[:, numpy.newaxis]
if fit_intercept:
    intercept = w[:, (- 1)]
    w = w[:, :(- 1)]
else:
    intercept = 0
p = safe_sparse_dot(X, w.T)
p += intercept
p -= logsumexp(p, axis=1)[:, numpy.newaxis]
loss = (- ((sample_weight * Y) * p).sum())
loss += ((0.5 * alpha) * squared_norm(w))
tempResult = exp(p, p)
	
===================================================================	
log_loss: 33	
----------------------------	

tempResult = exp(((- y) * p))
	
===================================================================	
test_multinomial_loss_ground_truth: 515	
----------------------------	

n_classes = 3
X = numpy.array([[1.1, 2.2], [2.2, (- 4.4)], [3.3, (- 2.2)], [1.1, 1.1]])
y = numpy.array([0, 1, 2, 0])
lbin = LabelBinarizer()
Y_bin = lbin.fit_transform(y)
weights = numpy.array([[0.1, 0.2, 0.3], [1.1, 1.2, (- 1.3)]])
intercept = numpy.array([1.0, 0, (- 0.2)])
sample_weights = numpy.array([0.8, 1, 1, 0.8])
prediction = (numpy.dot(X, weights) + intercept)
logsumexp_prediction = logsumexp(prediction, axis=1)
p = (prediction - logsumexp_prediction[:, numpy.newaxis])
loss_1 = (- ((sample_weights[:, np.newaxis] * p) * Y_bin).sum())
tempResult = exp(p)
	
===================================================================	
test_gradient_log: 796	
----------------------------	

loss = sklearn.linear_model.sgd_fast.Log()
tempResult = exp(1.0)
	
===================================================================	
test_gradient_log: 796	
----------------------------	

loss = sklearn.linear_model.sgd_fast.Log()
tempResult = exp((- 1.0))
	
===================================================================	
test_gradient_log: 796	
----------------------------	

loss = sklearn.linear_model.sgd_fast.Log()
tempResult = exp(1.0)
	
===================================================================	
test_gradient_log: 796	
----------------------------	

loss = sklearn.linear_model.sgd_fast.Log()
tempResult = exp((- 1.0))
	
===================================================================	
test_gradient_log: 798	
----------------------------	

loss = sklearn.linear_model.sgd_fast.Log()
cases = [(1.0, 1.0, ((- 1.0) / (numpy.exp(1.0) + 1.0))), (1.0, (- 1.0), (1.0 / (numpy.exp((- 1.0)) + 1.0))), ((- 1.0), (- 1.0), (1.0 / (numpy.exp(1.0) + 1.0))), ((- 1.0), 1.0, ((- 1.0) / (numpy.exp((- 1.0)) + 1.0))), (0.0, 1.0, (- 0.5)), (0.0, (- 1.0), 0.5), (17.9, (- 1.0), 1.0), ((- 17.9), 1.0, (- 1.0))]
_test_gradient_common(loss, cases)
tempResult = exp((- 18.1))
	
===================================================================	
test_gradient_log: 799	
----------------------------	

loss = sklearn.linear_model.sgd_fast.Log()
cases = [(1.0, 1.0, ((- 1.0) / (numpy.exp(1.0) + 1.0))), (1.0, (- 1.0), (1.0 / (numpy.exp((- 1.0)) + 1.0))), ((- 1.0), (- 1.0), (1.0 / (numpy.exp(1.0) + 1.0))), ((- 1.0), 1.0, ((- 1.0) / (numpy.exp((- 1.0)) + 1.0))), (0.0, 1.0, (- 0.5)), (0.0, (- 1.0), 0.5), (17.9, (- 1.0), 1.0), ((- 17.9), 1.0, (- 1.0))]
_test_gradient_common(loss, cases)
assert_almost_equal(loss.dloss(18.1, 1.0), (numpy.exp((- 18.1)) * (- 1.0)), 16)
tempResult = exp((- 18.1))
	
===================================================================	
test_binary_search: 96	
----------------------------	

random_state = check_random_state(0)
distances = random_state.randn(50, 2).astype(numpy.float32)
distances = numpy.abs(distances.dot(distances.T))
numpy.fill_diagonal(distances, 0.0)
desired_perplexity = 25.0
P = _binary_search_perplexity(distances, None, desired_perplexity, verbose=0)
P = numpy.maximum(P, np.finfo(np.double).eps)
tempResult = exp((- numpy.sum((P[i] * numpy.log(P[i])))))
	
===================================================================	
rbf_kernel: 227	
----------------------------	

'\n    Compute the rbf (gaussian) kernel between X and Y::\n\n        K(x, y) = exp(-gamma ||x-y||^2)\n\n    for each pair of rows x in X and y in Y.\n\n    Read more in the :ref:`User Guide <rbf_kernel>`.\n\n    Parameters\n    ----------\n    X : array of shape (n_samples_X, n_features)\n\n    Y : array of shape (n_samples_Y, n_features)\n\n    gamma : float, default None\n        If None, defaults to 1.0 / n_samples_X\n\n    Returns\n    -------\n    kernel_matrix : array of shape (n_samples_X, n_samples_Y)\n    '
(X, Y) = check_pairwise_arrays(X, Y)
if (gamma is None):
    gamma = (1.0 / X.shape[1])
K = euclidean_distances(X, Y, squared=True)
K *= (- gamma)
tempResult = exp(K, K)
	
===================================================================	
laplacian_kernel: 236	
----------------------------	

'Compute the laplacian kernel between X and Y.\n\n    The laplacian kernel is defined as::\n\n        K(x, y) = exp(-gamma ||x-y||_1)\n\n    for each pair of rows x in X and y in Y.\n    Read more in the :ref:`User Guide <laplacian_kernel>`.\n\n    .. versionadded:: 0.17\n\n    Parameters\n    ----------\n    X : array of shape (n_samples_X, n_features)\n\n    Y : array of shape (n_samples_Y, n_features)\n\n    gamma : float, default None\n        If None, defaults to 1.0 / n_samples_X\n\n    Returns\n    -------\n    kernel_matrix : array of shape (n_samples_X, n_samples_Y)\n    '
(X, Y) = check_pairwise_arrays(X, Y)
if (gamma is None):
    gamma = (1.0 / X.shape[1])
K = ((- gamma) * manhattan_distances(X, Y))
tempResult = exp(K, K)
	
===================================================================	
chi2_kernel: 267	
----------------------------	

'Computes the exponential chi-squared kernel X and Y.\n\n    The chi-squared kernel is computed between each pair of rows in X and Y.  X\n    and Y have to be non-negative. This kernel is most commonly applied to\n    histograms.\n\n    The chi-squared kernel is given by::\n\n        k(x, y) = exp(-gamma Sum [(x - y)^2 / (x + y)])\n\n    It can be interpreted as a weighted difference per entry.\n\n    Read more in the :ref:`User Guide <chi2_kernel>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples_X, n_features)\n\n    Y : array of shape (n_samples_Y, n_features)\n\n    gamma : float, default=1.\n        Scaling parameter of the chi2 kernel.\n\n    Returns\n    -------\n    kernel_matrix : array of shape (n_samples_X, n_samples_Y)\n\n    References\n    ----------\n    * Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\n      Local features and kernels for classification of texture and object\n      categories: A comprehensive study\n      International Journal of Computer Vision 2007\n      http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf\n\n    See also\n    --------\n    additive_chi2_kernel : The additive version of this kernel\n\n    sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation\n        to the additive version of this kernel.\n    '
K = additive_chi2_kernel(X, Y)
K *= gamma
tempResult = exp(K, K)
	
===================================================================	
test_chi_square_kernel: 311	
----------------------------	

rng = numpy.random.RandomState(0)
X = rng.random_sample((5, 4))
Y = rng.random_sample((10, 4))
K_add = additive_chi2_kernel(X, Y)
gamma = 0.1
K = chi2_kernel(X, Y, gamma=gamma)
assert_equal(K.dtype, numpy.float)
for (i, x) in enumerate(X):
    for (j, y) in enumerate(Y):
        chi2 = (- numpy.sum((((x - y) ** 2) / (x + y))))
        tempResult = exp((gamma * chi2))
	
===================================================================	
BaseMixture.predict_proba: 163	
----------------------------	

'Predict posterior probability of data per each component.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        resp : array, shape (n_samples, n_components)\n            Returns the probability of the sample for each Gaussian\n            (state) in the model.\n        '
self._check_is_fitted()
X = _check_X(X, None, self.means_.shape[1])
(_, log_resp) = self._estimate_log_prob_resp(X)
tempResult = exp(log_resp)
	
===================================================================	
BayesianGaussianMixture._compute_lower_bound: 196	
----------------------------	

'Estimate the lower bound of the model.\n\n        The lower bound on the likelihood (of the training data with respect to\n        the model) is used to detect the convergence and has to decrease at\n        each iteration.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n\n        log_prob_norm : float\n            Logarithm of the probability of each sample in X.\n\n        Returns\n        -------\n        lower_bound : float\n        '
(n_features,) = self.mean_prior_.shape
log_det_precisions_chol = (_compute_log_det_cholesky(self.precisions_cholesky_, self.covariance_type, n_features) - ((0.5 * n_features) * numpy.log(self.degrees_of_freedom_)))
if (self.covariance_type == 'tied'):
    log_wishart = (self.n_components * numpy.float64(_log_wishart_norm(self.degrees_of_freedom_, log_det_precisions_chol, n_features)))
else:
    log_wishart = numpy.sum(_log_wishart_norm(self.degrees_of_freedom_, log_det_precisions_chol, n_features))
if (self.weight_concentration_prior_type == 'dirichlet_process'):
    log_norm_weight = (- numpy.sum(betaln(self.weight_concentration_[0], self.weight_concentration_[1])))
else:
    log_norm_weight = _log_dirichlet_norm(self.weight_concentration_)
tempResult = exp(log_resp)
	
===================================================================	
BayesianGaussianMixture._m_step: 164	
----------------------------	

'M step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array-like, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n        '
(n_samples, _) = X.shape
tempResult = exp(log_resp)
	
===================================================================	
log_normalize: 30	
----------------------------	

'Normalized probabilities from unnormalized log-probabilites'
v = numpy.rollaxis(v, axis)
v = v.copy()
v -= v.max(axis=0)
out = logsumexp(v)
tempResult = exp((v - out))
	
===================================================================	
GaussianMixture._m_step: 191	
----------------------------	

'M step.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array-like, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n        '
(n_samples, _) = X.shape
tempResult = exp(log_resp)
	
===================================================================	
_GMMBase.score_samples: 90	
----------------------------	

'Return the per-sample likelihood of the data under the model.\n\n        Compute the log probability of X under the model and\n        return the posterior distribution (responsibilities) of each\n        mixture component for each element of X.\n\n        Parameters\n        ----------\n        X: array_like, shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        logprob : array_like, shape (n_samples,)\n            Log probabilities of each data point in X.\n\n        responsibilities : array_like, shape (n_samples, n_components)\n            Posterior probabilities of each mixture component for each\n            observation\n        '
check_is_fitted(self, 'means_')
X = check_array(X)
if (X.ndim == 1):
    X = X[:, numpy.newaxis]
if (X.size == 0):
    return (numpy.array([]), numpy.empty((0, self.n_components)))
if (X.shape[1] != self.means_.shape[1]):
    raise ValueError('The shape of X  is not compatible with self')
lpr = (log_multivariate_normal_density(X, self.means_, self.covars_, self.covariance_type) + numpy.log(self.weights_))
logprob = logsumexp(lpr, axis=1)
tempResult = exp((lpr - logprob[:, numpy.newaxis]))
	
===================================================================	
compute_kernel_slow: 101	
----------------------------	

d = numpy.sqrt(((Y[:, None, :] - X) ** 2).sum((- 1)))
norm = kernel_norm(h, X.shape[1], kernel)
if (kernel == 'gaussian'):
    tempResult = exp((((- 0.5) * (d * d)) / (h * h)))
	
===================================================================	
compute_kernel_slow: 107	
----------------------------	

d = numpy.sqrt(((Y[:, None, :] - X) ** 2).sum((- 1)))
norm = kernel_norm(h, X.shape[1], kernel)
if (kernel == 'gaussian'):
    return (norm * np.exp((((- 0.5) * (d * d)) / (h * h))).sum((- 1)))
elif (kernel == 'tophat'):
    return (norm * (d < h).sum((- 1)))
elif (kernel == 'epanechnikov'):
    return (norm * ((1.0 - ((d * d) / (h * h))) * (d < h)).sum((- 1)))
elif (kernel == 'exponential'):
    tempResult = exp(((- d) / h))
	
===================================================================	
check_results: 40	
----------------------------	

kde = KernelDensity(kernel=kernel, bandwidth=bandwidth, atol=atol, rtol=rtol)
log_dens = kde.fit(X).score_samples(Y)
tempResult = exp(log_dens)
	
===================================================================	
check_results: 41	
----------------------------	

kde = KernelDensity(kernel=kernel, bandwidth=bandwidth, atol=atol, rtol=rtol)
log_dens = kde.fit(X).score_samples(Y)
assert_allclose(numpy.exp(log_dens), dens_true, atol=atol, rtol=max(1e-07, rtol))
tempResult = exp(kde.score(Y))
	
===================================================================	
compute_kernel_slow: 15	
----------------------------	

d = numpy.sqrt(((Y[:, None, :] - X) ** 2).sum((- 1)))
norm = (kernel_norm(h, X.shape[1], kernel) / X.shape[0])
if (kernel == 'gaussian'):
    tempResult = exp((((- 0.5) * (d * d)) / (h * h)))
	
===================================================================	
compute_kernel_slow: 21	
----------------------------	

d = numpy.sqrt(((Y[:, None, :] - X) ** 2).sum((- 1)))
norm = (kernel_norm(h, X.shape[1], kernel) / X.shape[0])
if (kernel == 'gaussian'):
    return (norm * np.exp((((- 0.5) * (d * d)) / (h * h))).sum((- 1)))
elif (kernel == 'tophat'):
    return (norm * (d < h).sum((- 1)))
elif (kernel == 'epanechnikov'):
    return (norm * ((1.0 - ((d * d) / (h * h))) * (d < h)).sum((- 1)))
elif (kernel == 'exponential'):
    tempResult = exp(((- d) / h))
	
===================================================================	
compute_kernel_slow: 66	
----------------------------	

d = numpy.sqrt(((Y[:, None, :] - X) ** 2).sum((- 1)))
norm = kernel_norm(h, X.shape[1], kernel)
if (kernel == 'gaussian'):
    tempResult = exp((((- 0.5) * (d * d)) / (h * h)))
	
===================================================================	
compute_kernel_slow: 72	
----------------------------	

d = numpy.sqrt(((Y[:, None, :] - X) ** 2).sum((- 1)))
norm = kernel_norm(h, X.shape[1], kernel)
if (kernel == 'gaussian'):
    return (norm * np.exp((((- 0.5) * (d * d)) / (h * h))).sum((- 1)))
elif (kernel == 'tophat'):
    return (norm * (d < h).sum((- 1)))
elif (kernel == 'epanechnikov'):
    return (norm * ((1.0 - ((d * d) / (h * h))) * (d < h)).sum((- 1)))
elif (kernel == 'exponential'):
    tempResult = exp(((- d) / h))
	
===================================================================	
softmax: 26	
----------------------------	

'Compute the K-way softmax function inplace.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n        The input data.\n\n    Returns\n    -------\n    X_new : {array-like, sparse matrix}, shape (n_samples, n_features)\n        The transformed data.\n    '
tmp = (X - X.max(axis=1)[:, numpy.newaxis])
tempResult = exp(tmp, out=X)
	
===================================================================	
test_probability: 198	
----------------------------	

for clf in (sklearn.svm.SVC(probability=True, random_state=0, C=1.0), sklearn.svm.NuSVC(probability=True, random_state=0)):
    clf.fit(iris.data, iris.target)
    prob_predict = clf.predict_proba(iris.data)
    assert_array_almost_equal(numpy.sum(prob_predict, 1), numpy.ones(iris.data.shape[0]))
    assert_true((numpy.mean((numpy.argmax(prob_predict, 1) == clf.predict(iris.data))) > 0.9))
    tempResult = exp(clf.predict_log_proba(iris.data))
	
===================================================================	
test_sigmoid_calibration: 136	
----------------------------	

'Test calibration values with Platt sigmoid model'
exF = numpy.array([5, (- 4), 1.0])
exY = numpy.array([1, (- 1), (- 1)])
AB_lin_libsvm = numpy.array([(- 0.20261354391187855), 0.6523631498001051])
assert_array_almost_equal(AB_lin_libsvm, _sigmoid_calibration(exF, exY), 3)
tempResult = exp(((AB_lin_libsvm[0] * exF) + AB_lin_libsvm[1]))
	
===================================================================	
softmax: 88	
----------------------------	

tempResult = exp((- y_pred))
	
===================================================================	
test_lda_predict: 50	
----------------------------	

for test_case in solver_shrinkage:
    (solver, shrinkage) = test_case
    clf = LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)
    y_pred = clf.fit(X, y).predict(X)
    assert_array_equal(y_pred, y, ('solver %s' % solver))
    y_pred1 = clf.fit(X1, y).predict(X1)
    assert_array_equal(y_pred1, y, ('solver %s' % solver))
    y_proba_pred1 = clf.predict_proba(X1)
    assert_array_equal(((y_proba_pred1[:, 1] > 0.5) + 1), y, ('solver %s' % solver))
    y_log_proba_pred1 = clf.predict_log_proba(X1)
    tempResult = exp(y_log_proba_pred1)
	
===================================================================	
test_qda: 150	
----------------------------	

clf = QuadraticDiscriminantAnalysis()
y_pred = clf.fit(X6, y6).predict(X6)
assert_array_equal(y_pred, y6)
y_pred1 = clf.fit(X7, y6).predict(X7)
assert_array_equal(y_pred1, y6)
y_proba_pred1 = clf.predict_proba(X7)
assert_array_equal(((y_proba_pred1[:, 1] > 0.5) + 1), y6)
y_log_proba_pred1 = clf.predict_log_proba(X7)
tempResult = exp(y_log_proba_pred1)
	
===================================================================	
test_fast_predict: 243	
----------------------------	

rng = numpy.random.RandomState(123)
n_samples = (10 ** 3)
X_train = ((20.0 * rng.rand(n_samples)) - 10)
tempResult = exp((- X_train))
	
===================================================================	
test_skewed_chi2_sampler: 56	
----------------------------	

c = 0.03
X_c = (X + c)[:, numpy.newaxis, :]
Y_c = (Y + c)[numpy.newaxis, :, :]
log_kernel = ((((numpy.log(X_c) / 2.0) + (numpy.log(Y_c) / 2.0)) + numpy.log(2.0)) - numpy.log((X_c + Y_c)))
tempResult = exp(log_kernel.sum(axis=2))
	
===================================================================	
test_bnb: 312	
----------------------------	

X = numpy.array([[1, 1, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0], [0, 1, 0, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
Y = numpy.array([0, 0, 0, 1])
clf = BernoulliNB(alpha=1.0)
clf.fit(X, Y)
class_prior = numpy.array([0.75, 0.25])
tempResult = exp(clf.class_log_prior_)
	
===================================================================	
test_bnb: 314	
----------------------------	

X = numpy.array([[1, 1, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0], [0, 1, 0, 1, 0, 0], [0, 1, 1, 0, 0, 1]])
Y = numpy.array([0, 0, 0, 1])
clf = BernoulliNB(alpha=1.0)
clf.fit(X, Y)
class_prior = numpy.array([0.75, 0.25])
assert_array_almost_equal(numpy.exp(clf.class_log_prior_), class_prior)
feature_prob = numpy.array([[0.4, 0.8, 0.2, 0.4, 0.4, 0.2], [(1 / 3.0), (2 / 3.0), (2 / 3.0), (1 / 3.0), (1 / 3.0), (2 / 3.0)]])
tempResult = exp(clf.feature_log_prob_)
	
===================================================================	
test_discretenb_provide_prior: 230	
----------------------------	

for cls in [BernoulliNB, MultinomialNB]:
    clf = cls(class_prior=[0.5, 0.5])
    clf.fit([[0], [0], [1]], [0, 0, 1])
    tempResult = exp(clf.class_log_prior_)
	
===================================================================	
test_sample_weight_mnb: 268	
----------------------------	

clf = MultinomialNB()
clf.fit([[1, 2], [1, 2], [1, 0]], [0, 0, 1], sample_weight=[1, 1, 4])
assert_array_equal(clf.predict([[1, 0]]), [1])
tempResult = exp(clf.intercept_[0])
	
===================================================================	
test_discretenb_predict_proba: 215	
----------------------------	

X_bernoulli = [[1, 100, 0], [0, 1, 0], [0, 100, 1]]
X_multinomial = [[0, 1], [1, 3], [4, 0]]
y = [0, 0, 2]
for (cls, X) in zip([BernoulliNB, MultinomialNB], [X_bernoulli, X_multinomial]):
    clf = cls().fit(X, y)
    assert_equal(clf.predict(X[(- 1):]), 2)
    assert_equal(clf.predict_proba([X[0]]).shape, (1, 2))
    assert_array_almost_equal(clf.predict_proba(X[:2]).sum(axis=1), numpy.array([1.0, 1.0]), 6)
y = [0, 1, 2]
for (cls, X) in zip([BernoulliNB, MultinomialNB], [X_bernoulli, X_multinomial]):
    clf = cls().fit(X, y)
    assert_equal(clf.predict_proba(X[0:1]).shape, (1, 3))
    assert_equal(clf.predict_proba(X[:2]).shape, (2, 3))
    assert_almost_equal(numpy.sum(clf.predict_proba([X[1]])), 1)
    assert_almost_equal(numpy.sum(clf.predict_proba([X[(- 1)]])), 1)
    tempResult = exp(clf.class_log_prior_)
	
===================================================================	
test_discretenb_predict_proba: 216	
----------------------------	

X_bernoulli = [[1, 100, 0], [0, 1, 0], [0, 100, 1]]
X_multinomial = [[0, 1], [1, 3], [4, 0]]
y = [0, 0, 2]
for (cls, X) in zip([BernoulliNB, MultinomialNB], [X_bernoulli, X_multinomial]):
    clf = cls().fit(X, y)
    assert_equal(clf.predict(X[(- 1):]), 2)
    assert_equal(clf.predict_proba([X[0]]).shape, (1, 2))
    assert_array_almost_equal(clf.predict_proba(X[:2]).sum(axis=1), numpy.array([1.0, 1.0]), 6)
y = [0, 1, 2]
for (cls, X) in zip([BernoulliNB, MultinomialNB], [X_bernoulli, X_multinomial]):
    clf = cls().fit(X, y)
    assert_equal(clf.predict_proba(X[0:1]).shape, (1, 3))
    assert_equal(clf.predict_proba(X[:2]).shape, (2, 3))
    assert_almost_equal(numpy.sum(clf.predict_proba([X[1]])), 1)
    assert_almost_equal(numpy.sum(clf.predict_proba([X[(- 1)]])), 1)
    assert_almost_equal(numpy.sum(numpy.exp(clf.class_log_prior_)), 1)
    tempResult = exp(clf.intercept_)
	
===================================================================	
test_discretenb_uniform_prior: 223	
----------------------------	

for cls in [BernoulliNB, MultinomialNB]:
    clf = cls()
    clf.set_params(fit_prior=False)
    clf.fit([[0], [0], [1]], [0, 0, 1])
    tempResult = exp(clf.class_log_prior_)
	
===================================================================	
test_probability: 159	
----------------------------	

for (name, Tree) in CLF_TREES.items():
    clf = Tree(max_depth=1, max_features=1, random_state=42)
    clf.fit(iris.data, iris.target)
    prob_predict = clf.predict_proba(iris.data)
    assert_array_almost_equal(numpy.sum(prob_predict, 1), numpy.ones(iris.data.shape[0]), err_msg='Failed with {0}'.format(name))
    assert_array_equal(numpy.argmax(prob_predict, 1), clf.predict(iris.data), err_msg='Failed with {0}'.format(name))
    tempResult = exp(clf.predict_log_proba(iris.data))
	
===================================================================	
softmax: 251	
----------------------------	

'\n    Calculate the softmax function.\n\n    The softmax function is calculated by\n    np.exp(X) / np.sum(np.exp(X), axis=1)\n\n    This will cause overflow when large values are exponentiated.\n    Hence the largest value in each row is subtracted from each data\n    point to prevent this.\n\n    Parameters\n    ----------\n    X: array-like, shape (M, N)\n        Argument to the logistic function\n\n    copy: bool, optional\n        Copy X or not.\n\n    Returns\n    -------\n    out: array, shape (M, N)\n        Softmax function evaluated at every point in x\n    '
if copy:
    X = numpy.copy(X)
max_prob = np.max(X, axis=1).reshape(((- 1), 1))
X -= max_prob
tempResult = exp(X, X)
	
===================================================================	
logsumexp: 159	
----------------------------	

'Computes the sum of arr assuming arr is in the log domain.\n\n    Returns log(sum(exp(arr))) while minimizing the possibility of\n    over/underflow.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n    '
arr = numpy.rollaxis(arr, axis)
vmax = arr.max(axis=0)
tempResult = exp((arr - vmax))
	
===================================================================	
test_logsumexp: 70	
----------------------------	

x = numpy.array(([1e-40] * 1000000))
logx = numpy.log(x)
tempResult = exp(logsumexp(logx))
	
===================================================================	
test_logsumexp: 73	
----------------------------	

x = numpy.array(([1e-40] * 1000000))
logx = numpy.log(x)
assert_almost_equal(numpy.exp(logsumexp(logx)), x.sum())
X = numpy.vstack([x, x])
logX = numpy.vstack([logx, logx])
tempResult = exp(logsumexp(logX, axis=0))
	
===================================================================	
test_logsumexp: 74	
----------------------------	

x = numpy.array(([1e-40] * 1000000))
logx = numpy.log(x)
assert_almost_equal(numpy.exp(logsumexp(logx)), x.sum())
X = numpy.vstack([x, x])
logX = numpy.vstack([logx, logx])
assert_array_almost_equal(numpy.exp(logsumexp(logX, axis=0)), X.sum(axis=0))
tempResult = exp(logsumexp(logX, axis=1))
	
===================================================================	
naive_log_logistic: 242	
----------------------------	

tempResult = exp((- x))
	
===================================================================	
test_softmax: 393	
----------------------------	

rng = numpy.random.RandomState(0)
X = rng.randn(3, 5)
tempResult = exp(X)
	
===================================================================	
test_expit: 11	
----------------------------	

tempResult = exp((- 1000.0))
	
===================================================================	
test_expit: 12	
----------------------------	

assert_almost_equal(expit(1000.0), (1.0 / (1.0 + numpy.exp((- 1000.0)))), decimal=16)
tempResult = exp((- 1000.0))
	
===================================================================	
test_expit: 12	
----------------------------	

assert_almost_equal(expit(1000.0), (1.0 / (1.0 + numpy.exp((- 1000.0)))), decimal=16)
tempResult = exp((- 1000.0))
	
***************************************************	
matplotlib_matplotlib-2.0.0: 99	
===================================================================	
data_gen: 12	
----------------------------	

cnt = 0
while (cnt < 1000):
    cnt += 1
    t += 0.1
    tempResult = exp(((- t) / 10.0))
	
===================================================================	
module: 10	
----------------------------	

"\n========================\nMATPLOTLIB **UNCHAINED**\n========================\n\nComparative path demonstration of frequency from a fake signal of a pulsar.\n(mostly known because of the cover for Joy Division's Unknown Pleasures)\n\nAuthor: Nicolas P. Rougier\n"
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
fig = matplotlib.pyplot.figure(figsize=(8, 8), facecolor='black')
ax = matplotlib.pyplot.subplot(111, frameon=False)
data = numpy.random.uniform(0, 1, (64, 75))
X = numpy.linspace((- 1), 1, data.shape[(- 1)])
tempResult = exp((((- 4) * X) * X))
	
===================================================================	
module: 6	
----------------------------	

'\n===============================\nLegend using pre-defined labels\n===============================\n\nNotice how the legend labels are defined with the plots!\n'
import numpy as np
import matplotlib.pyplot as plt
a = b = numpy.arange(0, 3, 0.02)
tempResult = exp(a)
	
===================================================================	
module: 7	
----------------------------	

'\n===========================\nPlots with different scales\n===========================\n\nDemonstrate how to do two plots on the same axes with different left and\nright scales.\n\nThe trick is to use *two different axes* that share the same *x* axis.\nYou can use separate `matplotlib.ticker` formatters and locators as\ndesired since the two axes are independent.\n\nSuch axes are generated by calling the `Axes.twinx` method.  Likewise,\n`Axes.twiny` is available to generate axes that share a *y* axis but\nhave different top and bottom scales.\n\nThe twinx and twiny methods are also exposed as pyplot functions.\n\n'
import numpy as np
import matplotlib.pyplot as plt
(fig, ax1) = matplotlib.pyplot.subplots()
t = numpy.arange(0.01, 10.0, 0.01)
tempResult = exp(t)
	
===================================================================	
module: 10	
----------------------------	

'\n===========================\nFrontpage histogram example\n===========================\n\nThis example reproduces the frontpage histogram example.\n'
import matplotlib.pyplot as plt
import numpy as np
random_state = numpy.random.RandomState(19680801)
X = random_state.randn(10000)
(fig, ax) = matplotlib.pyplot.subplots()
ax.hist(X, bins=25, normed=True)
x = numpy.linspace((- 5), 5, 1000)
tempResult = exp(((- (x ** 2)) / 2))
	
===================================================================	
module: 6	
----------------------------	

'\nSimple demo of the fill function.\n'
import numpy as np
import matplotlib.pyplot as plt
x = numpy.linspace(0, 1, 500)
tempResult = exp(((- 5) * x))
	
===================================================================	
f: 9	
----------------------------	

s1 = numpy.cos(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
f: 8	
----------------------------	

'a damped exponential'
s1 = numpy.cos(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.001
t = numpy.arange(0.0, 10.0, dt)
tempResult = exp(((- t[:1000]) / 0.05))
	
===================================================================	
module: 10	
----------------------------	

'\nCompute the coherence of two signals\n'
import numpy as np
import matplotlib.pyplot as plt
matplotlib.pyplot.subplots_adjust(wspace=0.5)
dt = 0.01
t = numpy.arange(0, 30, dt)
nse1 = numpy.random.randn(len(t))
nse2 = numpy.random.randn(len(t))
tempResult = exp(((- t) / 0.05))
	
===================================================================	
module: 11	
----------------------------	

'\nCompute the cross spectral density of two signals\n'
import numpy as np
import matplotlib.pyplot as plt
(fig, (ax1, ax2)) = matplotlib.pyplot.subplots(2, 1)
fig.subplots_adjust(hspace=0.5)
dt = 0.01
t = numpy.arange(0, 30, dt)
nse1 = numpy.random.randn(len(t))
nse2 = numpy.random.randn(len(t))
tempResult = exp(((- t) / 0.05))
	
===================================================================	
module: 6	
----------------------------	

'\nDemo for the errorevery keyword to show data full accuracy data plots with\nfew errorbars.\n'
import numpy as np
import matplotlib.pyplot as plt
x = numpy.arange(0.1, 4, 0.1)
tempResult = exp((- x))
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
ax1 = fig.add_axes([0.1, 0.1, 0.4, 0.7])
ax2 = fig.add_axes([0.55, 0.1, 0.4, 0.7])
x = numpy.arange(0.0, 2.0, 0.02)
y1 = numpy.sin(((2 * numpy.pi) * x))
tempResult = exp((- x))
	
===================================================================	
module: 12	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
ax1 = fig.add_axes([0.1, 0.1, 0.4, 0.7])
ax2 = fig.add_axes([0.55, 0.1, 0.4, 0.7])
x = numpy.arange(0.0, 2.0, 0.02)
y1 = numpy.sin(((2 * numpy.pi) * x))
y2 = numpy.exp((- x))
(l1, l2) = ax1.plot(x, y1, 'rs-', x, y2, 'go')
y3 = numpy.sin(((4 * numpy.pi) * x))
tempResult = exp(((- 2) * x))
	
===================================================================	
f: 8	
----------------------------	

s1 = numpy.cos(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
module: 8	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
theta = numpy.arange(0, (8 * numpy.pi), 0.1)
a = 1
b = 0.2
for dt in numpy.arange(0, (2 * numpy.pi), (numpy.pi / 2.0)):
    tempResult = exp((b * theta))
	
===================================================================	
module: 9	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
theta = numpy.arange(0, (8 * numpy.pi), 0.1)
a = 1
b = 0.2
for dt in numpy.arange(0, (2 * numpy.pi), (numpy.pi / 2.0)):
    x = ((a * numpy.cos((theta + dt))) * numpy.exp((b * theta)))
    tempResult = exp((b * theta))
	
===================================================================	
module: 11	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
theta = numpy.arange(0, (8 * numpy.pi), 0.1)
a = 1
b = 0.2
for dt in numpy.arange(0, (2 * numpy.pi), (numpy.pi / 2.0)):
    x = ((a * numpy.cos((theta + dt))) * numpy.exp((b * theta)))
    y = ((a * numpy.sin((theta + dt))) * numpy.exp((b * theta)))
    dt = (dt + (numpy.pi / 4.0))
    tempResult = exp((b * theta))
	
===================================================================	
module: 12	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
theta = numpy.arange(0, (8 * numpy.pi), 0.1)
a = 1
b = 0.2
for dt in numpy.arange(0, (2 * numpy.pi), (numpy.pi / 2.0)):
    x = ((a * numpy.cos((theta + dt))) * numpy.exp((b * theta)))
    y = ((a * numpy.sin((theta + dt))) * numpy.exp((b * theta)))
    dt = (dt + (numpy.pi / 4.0))
    x2 = ((a * numpy.cos((theta + dt))) * numpy.exp((b * theta)))
    tempResult = exp((b * theta))
	
===================================================================	
module: 8	
----------------------------	

'\nRecursively find all objects that match some criteria\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.text as text
a = numpy.arange(0, 3, 0.02)
b = numpy.arange(0, 3, 0.02)
tempResult = exp(a)
	
===================================================================	
module: 7	
----------------------------	

"\nTo create plots that share a common axes (visually) you can set the\nhspace between the subplots close to zero (do not use zero itself).\nNormally you'll want to turn off the tick labels on all but one of the\naxes.\n\nIn this example the plots share a common xaxis but you can follow the\nsame logic to supply a common y axis.\n"
import matplotlib.pyplot as plt
import numpy as np
t = numpy.arange(0.0, 2.0, 0.01)
s1 = numpy.sin(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
module: 10	
----------------------------	

from numpy.random import uniform, seed
from matplotlib.mlab import griddata
import matplotlib.pyplot as plt
import numpy as np
seed(0)
npts = 200
x = uniform((- 2), 2, npts)
y = uniform((- 2), 2, npts)
tempResult = exp(((- (x ** 2)) - (y ** 2)))
	
===================================================================	
module: 6	
----------------------------	

'\nYou can use decreasing axes by flipping the normal order of the axis\nlimits\n'
import matplotlib.pyplot as plt
import numpy as np
t = numpy.arange(0.01, 5.0, 0.01)
tempResult = exp((- t))
	
===================================================================	
func3: 8	
----------------------------	

tempResult = exp((- ((x ** 2) + (y ** 2))))
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
t1 = numpy.arange(0.0, 2.0, 0.1)
t2 = numpy.arange(0.0, 2.0, 0.01)
tempResult = exp((- t2))
	
===================================================================	
module: 8	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
t1 = numpy.arange(0.0, 2.0, 0.1)
t2 = numpy.arange(0.0, 2.0, 0.01)
(l1,) = matplotlib.pyplot.plot(t2, numpy.exp((- t2)))
(l2, l3) = matplotlib.pyplot.plot(t2, numpy.sin(((2 * numpy.pi) * t2)), '--o', t1, numpy.log((1 + t1)), '.')
tempResult = exp((- t2))
	
===================================================================	
module: 7	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
matplotlib.pyplot.subplots_adjust(hspace=0.4)
t = numpy.arange(0.01, 20.0, 0.01)
matplotlib.pyplot.subplot(221)
tempResult = exp(((- t) / 5.0))
	
===================================================================	
module: 15	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
matplotlib.pyplot.subplots_adjust(hspace=0.4)
t = numpy.arange(0.01, 20.0, 0.01)
matplotlib.pyplot.subplot(221)
matplotlib.pyplot.semilogy(t, numpy.exp(((- t) / 5.0)))
matplotlib.pyplot.title('semilogy')
matplotlib.pyplot.grid(True)
matplotlib.pyplot.subplot(222)
matplotlib.pyplot.semilogx(t, numpy.sin(((2 * numpy.pi) * t)))
matplotlib.pyplot.title('semilogx')
matplotlib.pyplot.grid(True)
matplotlib.pyplot.subplot(223)
tempResult = exp(((- t) / 10.0))
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.01
t = numpy.arange(dt, 20.0, dt)
tempResult = exp(((- t) / 5.0))
	
===================================================================	
module: 10	
----------------------------	

"\nDemonstrate how to use major and minor tickers.\n\nThe two relevant userland classes are Locators and Formatters.\nLocators determine where the ticks are and formatters control the\nformatting of ticks.\n\nMinor ticks are off by default (NullLocator and NullFormatter).  You\ncan turn minor ticks on w/o labels by setting the minor locator.  You\ncan also turn labeling on for the minor ticker by setting the minor\nformatter\n\nMake a plot with major ticks that are multiples of 20 and minor ticks\nthat are multiples of 5.  Label major ticks with %d formatting but\ndon't label minor ticks\n\nThe MultipleLocator ticker class is used to place ticks on multiples of\nsome base.  The FormatStrFormatter uses a string format string (e.g.,\n'%d' or '%1.2f' or '%1.1f cm' ) to format the tick\n\nThe pyplot interface grid command changes the grid settings of the\nmajor ticks of the y and y axis together.  If you want to control the\ngrid of the minor ticks for a given axis, use for example\n\n  ax.xaxis.grid(True, which='minor')\n\nNote, you should not use the same locator between different Axis\nbecause the locator stores references to the Axis data and view limits\n\n"
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
majorLocator = MultipleLocator(20)
majorFormatter = FormatStrFormatter('%d')
minorLocator = MultipleLocator(5)
t = numpy.arange(0.0, 100.0, 0.1)
tempResult = exp(((- t) * 0.01))
	
===================================================================	
module: 8	
----------------------------	

'\nAutomatic tick selection for major and minor ticks.\n\nUse interactive pan and zoom to see how the tick intervals\nchange. There will be either 4 or 5 minor tick intervals\nper major interval, depending on the major interval.\n'
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
minorLocator = AutoMinorLocator()
t = numpy.arange(0.0, 100.0, 0.01)
tempResult = exp(((- t) * 0.01))
	
===================================================================	
module: 7	
----------------------------	

'\nDemonstrates similarities between pcolor, pcolormesh, imshow and pcolorfast\nfor drawing quadrilateral grids.\n\n'
import matplotlib.pyplot as plt
import numpy as np
(dx, dy) = (0.15, 0.05)
(y, x) = numpy.mgrid[(slice((- 3), (3 + dy), dy), slice((- 3), (3 + dx), dx))]
tempResult = exp(((- (x ** 2)) - (y ** 2)))
	
===================================================================	
module: 7	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.01
t = numpy.arange(0, 10, dt)
nse = numpy.random.randn(len(t))
tempResult = exp(((- t) / 0.05))
	
===================================================================	
module: 11	
----------------------------	

"This is a ported version of a MATLAB example from the signal\nprocessing toolbox that showed some difference at one time between\nMatplotlib's and MATLAB's scaling of the PSD.\n\nThis differs from psd_demo3.py in that this uses a complex signal,\nso we can see that complex PSD's work properly\n\n"
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
prng = numpy.random.RandomState(123456)
fs = 1000
t = numpy.linspace(0, 0.3, 301)
A = np.array([2, 8]).reshape((- 1), 1)
f = np.array([150, 140]).reshape((- 1), 1)
tempResult = exp((((2j * numpy.pi) * f) * t))
	
===================================================================	
module: 7	
----------------------------	

'\nYou can share the x or y axis limits for one axis with another by\npassing an axes instance as a sharex or sharey kwarg.\n\nChanging the axis limits on one axes will be reflected automatically\nin the other, and vice-versa, so when you navigate with the toolbar\nthe axes will follow each other on their shared axes.  Ditto for\nchanges in the axis scaling (e.g., log vs linear).  However, it is\npossible to have differences in tick labeling, e.g., you can selectively\nturn off the tick labels on one axes.\n\nThe example below shows how to customize the tick labels on the\nvarious axes.  Shared axes share the tick locator, tick formatter,\nview limits, and transformation (e.g., log, linear).  But the ticklabels\nthemselves do not share properties.  This is a feature and not a bug,\nbecause you may want to make the tick labels smaller on the upper\naxes, e.g., in the example below.\n\nIf you want to turn off the ticklabels for a given axes (e.g., on\nsubplot(211) or subplot(212), you cannot do the standard trick\n\n   setp(ax2, xticklabels=[])\n\nbecause this changes the tick Formatter, which is shared among all\naxes.  But you can alter the visibility of the labels, which is a\nproperty\n\n  setp( ax2.get_xticklabels(), visible=False)\n\n'
import matplotlib.pyplot as plt
import numpy as np
t = numpy.arange(0.01, 5.0, 0.01)
s1 = numpy.sin(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
module: 9	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
numpy.random.seed(0)
dt = 0.01
Fs = (1 / dt)
t = numpy.arange(0, 10, dt)
nse = numpy.random.randn(len(t))
tempResult = exp(((- t) / 0.05))
	
===================================================================	
bump: 15	
----------------------------	

x = (1 / (0.1 + numpy.random.random()))
y = ((2 * numpy.random.random()) - 0.5)
z = (10 / (0.1 + numpy.random.random()))
for i in range(m):
    w = (((i / float(m)) - y) * z)
    tempResult = exp(((- w) * w))
	
===================================================================	
module: 7	
----------------------------	

'\nSimple demo with multiple subplots.\n'
import numpy as np
import matplotlib.pyplot as plt
x1 = numpy.linspace(0.0, 5.0)
x2 = numpy.linspace(0.0, 2.0)
tempResult = exp((- x1))
	
===================================================================	
f: 8	
----------------------------	

s1 = numpy.sin(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
module: 33	
----------------------------	

'\nContour plots of unstructured triangular grids.\n'
import matplotlib.pyplot as plt
import matplotlib.tri as tri
import numpy as np
import math
n_angles = 48
n_radii = 8
min_radius = 0.25
radii = numpy.linspace(min_radius, 0.95, n_radii)
angles = numpy.linspace(0, (2 * math.pi), n_angles, endpoint=False)
angles = numpy.repeat(angles[(..., numpy.newaxis)], n_radii, axis=1)
angles[:, 1::2] += (math.pi / n_angles)
x = (radii * np.cos(angles)).flatten()
y = (radii * np.sin(angles)).flatten()
z = (np.cos(radii) * np.cos((angles * 3.0))).flatten()
triang = matplotlib.tri.Triangulation(x, y)
xmid = x[triang.triangles].mean(axis=1)
ymid = y[triang.triangles].mean(axis=1)
mask = numpy.where((((xmid * xmid) + (ymid * ymid)) < (min_radius * min_radius)), 1, 0)
triang.set_mask(mask)
matplotlib.pyplot.figure()
plt.gca().set_aspect('equal')
matplotlib.pyplot.tricontourf(triang, z)
matplotlib.pyplot.colorbar()
matplotlib.pyplot.tricontour(triang, z, colors='k')
matplotlib.pyplot.title('Contour plot of Delaunay triangulation')
xy = numpy.asarray([[(- 0.101), 0.872], [(- 0.08), 0.883], [(- 0.069), 0.888], [(- 0.054), 0.89], [(- 0.045), 0.897], [(- 0.057), 0.895], [(- 0.073), 0.9], [(- 0.087), 0.898], [(- 0.09), 0.904], [(- 0.069), 0.907], [(- 0.069), 0.921], [(- 0.08), 0.919], [(- 0.073), 0.928], [(- 0.052), 0.93], [(- 0.048), 0.942], [(- 0.062), 0.949], [(- 0.054), 0.958], [(- 0.069), 0.954], [(- 0.087), 0.952], [(- 0.087), 0.959], [(- 0.08), 0.966], [(- 0.085), 0.973], [(- 0.087), 0.965], [(- 0.097), 0.965], [(- 0.097), 0.975], [(- 0.092), 0.984], [(- 0.101), 0.98], [(- 0.108), 0.98], [(- 0.104), 0.987], [(- 0.102), 0.993], [(- 0.115), 1.001], [(- 0.099), 0.996], [(- 0.101), 1.007], [(- 0.09), 1.01], [(- 0.087), 1.021], [(- 0.069), 1.021], [(- 0.052), 1.022], [(- 0.052), 1.017], [(- 0.069), 1.01], [(- 0.064), 1.005], [(- 0.048), 1.005], [(- 0.031), 1.005], [(- 0.031), 0.996], [(- 0.04), 0.987], [(- 0.045), 0.98], [(- 0.052), 0.975], [(- 0.04), 0.973], [(- 0.026), 0.968], [(- 0.02), 0.954], [(- 0.006), 0.947], [0.003, 0.935], [0.006, 0.926], [0.005, 0.921], [0.022, 0.923], [0.033, 0.912], [0.029, 0.905], [0.017, 0.9], [0.012, 0.895], [0.027, 0.893], [0.019, 0.886], [0.001, 0.883], [(- 0.012), 0.884], [(- 0.029), 0.883], [(- 0.038), 0.879], [(- 0.057), 0.881], [(- 0.062), 0.876], [(- 0.078), 0.876], [(- 0.087), 0.872], [(- 0.03), 0.907], [(- 0.007), 0.905], [(- 0.057), 0.916], [(- 0.025), 0.933], [(- 0.077), 0.99], [(- 0.059), 0.993]])
x = numpy.degrees(xy[:, 0])
y = numpy.degrees(xy[:, 1])
x0 = (- 5)
y0 = 52
tempResult = exp(((- 0.01) * (((x - x0) * (x - x0)) + ((y - y0) * (y - y0)))))
	
===================================================================	
experiment_res: 15	
----------------------------	

' An analytic function representing experiment results '
x = (2.0 * x)
r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r1 / 10) ** 2))
	
===================================================================	
experiment_res: 15	
----------------------------	

' An analytic function representing experiment results '
x = (2.0 * x)
r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r2 / 10) ** 2))
	
===================================================================	
function_z: 15	
----------------------------	

' A function of 2 variables '
r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r1 / 10) ** 2))
	
===================================================================	
function_z: 15	
----------------------------	

' A function of 2 variables '
r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r2 / 10) ** 2))
	
===================================================================	
module: 16	
----------------------------	

'\nComparison of griddata and tricontour for an unstructured triangular grid.\n'
from __future__ import print_function
import matplotlib.pyplot as plt
import matplotlib.tri as tri
import numpy as np
import numpy.random as rnd
import matplotlib.mlab as mlab
import time
numpy.random.seed(0)
npts = 200
ngridx = 100
ngridy = 200
x = numpy.random.uniform((- 2), 2, npts)
y = numpy.random.uniform((- 2), 2, npts)
tempResult = exp(((- (x ** 2)) - (y ** 2)))
	
===================================================================	
module: 40	
----------------------------	

'\nPseudocolor plots of unstructured triangular grids.\n'
import matplotlib.pyplot as plt
import matplotlib.tri as tri
import numpy as np
import math
n_angles = 36
n_radii = 8
min_radius = 0.25
radii = numpy.linspace(min_radius, 0.95, n_radii)
angles = numpy.linspace(0, (2 * math.pi), n_angles, endpoint=False)
angles = numpy.repeat(angles[(..., numpy.newaxis)], n_radii, axis=1)
angles[:, 1::2] += (math.pi / n_angles)
x = (radii * np.cos(angles)).flatten()
y = (radii * np.sin(angles)).flatten()
z = (np.cos(radii) * np.cos((angles * 3.0))).flatten()
triang = matplotlib.tri.Triangulation(x, y)
xmid = x[triang.triangles].mean(axis=1)
ymid = y[triang.triangles].mean(axis=1)
mask = numpy.where((((xmid * xmid) + (ymid * ymid)) < (min_radius * min_radius)), 1, 0)
triang.set_mask(mask)
matplotlib.pyplot.figure()
plt.gca().set_aspect('equal')
matplotlib.pyplot.tripcolor(triang, z, shading='flat')
matplotlib.pyplot.colorbar()
matplotlib.pyplot.title('tripcolor of Delaunay triangulation, flat shading')
matplotlib.pyplot.figure()
plt.gca().set_aspect('equal')
matplotlib.pyplot.tripcolor(triang, z, shading='gouraud')
matplotlib.pyplot.colorbar()
matplotlib.pyplot.title('tripcolor of Delaunay triangulation, gouraud shading')
xy = numpy.asarray([[(- 0.101), 0.872], [(- 0.08), 0.883], [(- 0.069), 0.888], [(- 0.054), 0.89], [(- 0.045), 0.897], [(- 0.057), 0.895], [(- 0.073), 0.9], [(- 0.087), 0.898], [(- 0.09), 0.904], [(- 0.069), 0.907], [(- 0.069), 0.921], [(- 0.08), 0.919], [(- 0.073), 0.928], [(- 0.052), 0.93], [(- 0.048), 0.942], [(- 0.062), 0.949], [(- 0.054), 0.958], [(- 0.069), 0.954], [(- 0.087), 0.952], [(- 0.087), 0.959], [(- 0.08), 0.966], [(- 0.085), 0.973], [(- 0.087), 0.965], [(- 0.097), 0.965], [(- 0.097), 0.975], [(- 0.092), 0.984], [(- 0.101), 0.98], [(- 0.108), 0.98], [(- 0.104), 0.987], [(- 0.102), 0.993], [(- 0.115), 1.001], [(- 0.099), 0.996], [(- 0.101), 1.007], [(- 0.09), 1.01], [(- 0.087), 1.021], [(- 0.069), 1.021], [(- 0.052), 1.022], [(- 0.052), 1.017], [(- 0.069), 1.01], [(- 0.064), 1.005], [(- 0.048), 1.005], [(- 0.031), 1.005], [(- 0.031), 0.996], [(- 0.04), 0.987], [(- 0.045), 0.98], [(- 0.052), 0.975], [(- 0.04), 0.973], [(- 0.026), 0.968], [(- 0.02), 0.954], [(- 0.006), 0.947], [0.003, 0.935], [0.006, 0.926], [0.005, 0.921], [0.022, 0.923], [0.033, 0.912], [0.029, 0.905], [0.017, 0.9], [0.012, 0.895], [0.027, 0.893], [0.019, 0.886], [0.001, 0.883], [(- 0.012), 0.884], [(- 0.029), 0.883], [(- 0.038), 0.879], [(- 0.057), 0.881], [(- 0.062), 0.876], [(- 0.078), 0.876], [(- 0.087), 0.872], [(- 0.03), 0.907], [(- 0.007), 0.905], [(- 0.057), 0.916], [(- 0.025), 0.933], [(- 0.077), 0.99], [(- 0.059), 0.993]])
x = ((xy[:, 0] * 180) / 3.14159)
y = ((xy[:, 1] * 180) / 3.14159)
triangles = numpy.asarray([[67, 66, 1], [65, 2, 66], [1, 66, 2], [64, 2, 65], [63, 3, 64], [60, 59, 57], [2, 64, 3], [3, 63, 4], [0, 67, 1], [62, 4, 63], [57, 59, 56], [59, 58, 56], [61, 60, 69], [57, 69, 60], [4, 62, 68], [6, 5, 9], [61, 68, 62], [69, 68, 61], [9, 5, 70], [6, 8, 7], [4, 70, 5], [8, 6, 9], [56, 69, 57], [69, 56, 52], [70, 10, 9], [54, 53, 55], [56, 55, 53], [68, 70, 4], [52, 56, 53], [11, 10, 12], [69, 71, 68], [68, 13, 70], [10, 70, 13], [51, 50, 52], [13, 68, 71], [52, 71, 69], [12, 10, 13], [71, 52, 50], [71, 14, 13], [50, 49, 71], [49, 48, 71], [14, 16, 15], [14, 71, 48], [17, 19, 18], [17, 20, 19], [48, 16, 14], [48, 47, 16], [47, 46, 16], [16, 46, 45], [23, 22, 24], [21, 24, 22], [17, 16, 45], [20, 17, 45], [21, 25, 24], [27, 26, 28], [20, 72, 21], [25, 21, 72], [45, 72, 20], [25, 28, 26], [44, 73, 45], [72, 45, 73], [28, 25, 29], [29, 25, 31], [43, 73, 44], [73, 43, 40], [72, 73, 39], [72, 31, 25], [42, 40, 43], [31, 30, 29], [39, 73, 40], [42, 41, 40], [72, 33, 31], [32, 31, 33], [39, 38, 72], [33, 72, 38], [33, 38, 34], [37, 35, 38], [34, 38, 35], [35, 37, 36]])
xmid = x[triangles].mean(axis=1)
ymid = y[triangles].mean(axis=1)
x0 = (- 5)
y0 = 52
tempResult = exp(((- 0.01) * (((xmid - x0) * (xmid - x0)) + ((ymid - y0) * (ymid - y0)))))
	
===================================================================	
f: 9	
----------------------------	

s1 = numpy.sin(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
x = numpy.arange(0, 10, 0.005)
tempResult = exp(((- x) / 2.0))
	
===================================================================	
f: 6	
----------------------------	

tempResult = exp((- t))
	
===================================================================	
module: 6	
----------------------------	

'\n=============================\nDemo of the errorbar function\n=============================\n\nThis exhibits the most basic use of the error bar method.\nIn this case, constant values are provided for the error\nin both the x- and y-directions.\n'
import numpy as np
import matplotlib.pyplot as plt
x = numpy.arange(0.1, 4, 0.5)
tempResult = exp((- x))
	
===================================================================	
module: 6	
----------------------------	

'\n===================================================\nDemo of the different ways of specifying error bars\n===================================================\n\nErrors can be specified as a constant value (as shown in\n`errorbar_demo.py`). However, this example demonstrates\nhow they vary by specifying arrays of error values.\n\nIf the raw ``x`` and ``y`` data have length N, there are two options:\n\nArray of shape (N,):\n    Error varies for each point, but the error values are\n    symmetric (i.e. the lower and upper values are equal).\n\nArray of shape (2, N):\n    Error varies for each point, and the lower and upper limits\n    (in that order) are different (asymmetric case)\n\nIn addition, this example demonstrates how to use log\nscale with error bars.\n'
import numpy as np
import matplotlib.pyplot as plt
x = numpy.arange(0.1, 4, 0.5)
tempResult = exp((- x))
	
===================================================================	
module: 6	
----------------------------	

'\n===========================================================\nDemo of how to include upper and lower limits in error bars\n===========================================================\n\nIn matplotlib, errors bars can have "limits". Applying limits to the\nerror bars essentially makes the error unidirectional. Because of that,\nupper and lower limits can be applied in both the y- and x-directions\nvia the ``uplims``, ``lolims``, ``xuplims``, and ``xlolims`` parameters,\nrespectively. These parameters can be scalar or boolean arrays.\n\nFor example, if ``xlolims`` is ``True``, the x-error bars will only\nextend from the data towards increasing values. If ``uplims`` is an\narray filled with ``False`` except for the 4th and 7th values, all of the\ny-error bars will be bidirectional, except the 4th and 7th bars, which\nwill extend from the data towards decreasing y-values.\n'
import numpy as np
import matplotlib.pyplot as plt
x = numpy.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])
tempResult = exp((- x))
	
===================================================================	
module: 7	
----------------------------	

'\nSimple demo with multiple subplots.\n'
import numpy as np
import matplotlib.pyplot as plt
x1 = numpy.linspace(0.0, 5.0)
x2 = numpy.linspace(0.0, 2.0)
tempResult = exp((- x1))
	
===================================================================	
module: 7	
----------------------------	

'\nDemo using fontdict to control style of text and labels.\n'
import numpy as np
import matplotlib.pyplot as plt
font = {'family': 'serif', 'color': 'darkred', 'weight': 'normal', 'size': 16}
x = numpy.linspace(0.0, 5.0, 100)
tempResult = exp((- x))
	
===================================================================	
FourierDemoWindow.compute: 165	
----------------------------	

f = numpy.arange((- 6.0), 6.0, 0.02)
t = numpy.arange((- 2.0), 2.0, 0.01)
tempResult = exp(((- numpy.pi) * (t ** 2)))
	
===================================================================	
FourierDemoWindow.compute: 166	
----------------------------	

f = numpy.arange((- 6.0), 6.0, 0.02)
t = numpy.arange((- 2.0), 2.0, 0.01)
x = ((A * numpy.cos((((2 * numpy.pi) * f0) * t))) * numpy.exp(((- numpy.pi) * (t ** 2))))
tempResult = exp(((- numpy.pi) * ((f - f0) ** 2)))
	
===================================================================	
FourierDemoWindow.compute: 166	
----------------------------	

f = numpy.arange((- 6.0), 6.0, 0.02)
t = numpy.arange((- 2.0), 2.0, 0.01)
x = ((A * numpy.cos((((2 * numpy.pi) * f0) * t))) * numpy.exp(((- numpy.pi) * (t ** 2))))
tempResult = exp(((- numpy.pi) * ((f + f0) ** 2)))
	
===================================================================	
f: 10	
----------------------------	

s1 = numpy.cos(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
===================================================================	
SymLogNorm._inv_transform: 581	
----------------------------	

'\n        Inverse inplace Transformation.\n        '
masked = (numpy.abs(a) > (self.linthresh * self._linscale_adj))
sign = numpy.sign(a[masked])
tempResult = exp((((sign * a[masked]) / self.linthresh) - self._linscale_adj))
	
===================================================================	
logspace: 23	
----------------------------	

'\n    Return N values logarithmically spaced between xmin and xmax.\n\n    '
tempResult = exp(numpy.linspace(numpy.log(xmin), numpy.log(xmax), N))
	
===================================================================	
exp_safe: 668	
----------------------------	

'\n    Compute exponentials which safely underflow to zero.\n\n    Slow, but convenient to use. Note that numpy provides proper\n    floating point exception handling with access to the underlying\n    hardware.\n    '
if (type(x) is numpy.ndarray):
    tempResult = exp(numpy.clip(x, exp_safe_MIN, exp_safe_MAX))
	
===================================================================	
fftsurr: 653	
----------------------------	

'\n    Compute an FFT phase randomized surrogate of *x*.\n    '
if matplotlib.cbook.iterable(window):
    x = (window * detrend(x))
else:
    x = window(detrend(x))
z = numpy.fft.fft(x)
a = ((2.0 * numpy.pi) * 1j)
phase = (a * numpy.random.rand(len(x)))
tempResult = exp(phase)
	
===================================================================	
normpdf: 434	
----------------------------	

'Return the normal pdf evaluated at *x*; args provides *mu*, *sigma*'
(mu, sigma) = args
tempResult = exp(((- 0.5) * (((1.0 / sigma) * (x - mu)) ** 2)))
	
===================================================================	
bivariate_normal: 593	
----------------------------	

'\n    Bivariate Gaussian distribution for equal shape *X*, *Y*.\n\n    See `bivariate normal\n    <http://mathworld.wolfram.com/BivariateNormalDistribution.html>`_\n    at mathworld.\n    '
Xmu = (X - mux)
Ymu = (Y - muy)
rho = (sigmaxy / (sigmax * sigmay))
z = ((((Xmu ** 2) / (sigmax ** 2)) + ((Ymu ** 2) / (sigmay ** 2))) - ((((2 * rho) * Xmu) * Ymu) / (sigmax * sigmay)))
denom = ((((2 * numpy.pi) * sigmax) * sigmay) * numpy.sqrt((1 - (rho ** 2))))
tempResult = exp(((- z) / (2 * (1 - (rho ** 2)))))
	
===================================================================	
GaussianKDE.evaluate: 1572	
----------------------------	

'Evaluate the estimated pdf on a set of points.\n\n        Parameters\n        ----------\n        points : (# of dimensions, # of points)-array\n            Alternatively, a (# of dimensions,) vector can be passed in and\n            treated as a single point.\n\n        Returns\n        -------\n        values : (# of points,)-array\n            The values at each point.\n\n        Raises\n        ------\n        ValueError : if the dimensionality of the input points is different\n                     than the dimensionality of the KDE.\n\n        '
points = numpy.atleast_2d(points)
(dim, num_m) = np.array(points).shape
if (dim != self.dim):
    msg = ('points have dimension %s, dataset has dimension %s' % (dim, self.dim))
    raise ValueError(msg)
result = numpy.zeros((num_m,), dtype=numpy.float)
if (num_m >= self.num_dp):
    for i in range(self.num_dp):
        diff = (self.dataset[:, i, numpy.newaxis] - points)
        tdiff = numpy.dot(self.inv_cov, diff)
        energy = (numpy.sum((diff * tdiff), axis=0) / 2.0)
        tempResult = exp((- energy))
	
===================================================================	
GaussianKDE.evaluate: 1578	
----------------------------	

'Evaluate the estimated pdf on a set of points.\n\n        Parameters\n        ----------\n        points : (# of dimensions, # of points)-array\n            Alternatively, a (# of dimensions,) vector can be passed in and\n            treated as a single point.\n\n        Returns\n        -------\n        values : (# of points,)-array\n            The values at each point.\n\n        Raises\n        ------\n        ValueError : if the dimensionality of the input points is different\n                     than the dimensionality of the KDE.\n\n        '
points = numpy.atleast_2d(points)
(dim, num_m) = np.array(points).shape
if (dim != self.dim):
    msg = ('points have dimension %s, dataset has dimension %s' % (dim, self.dim))
    raise ValueError(msg)
result = numpy.zeros((num_m,), dtype=numpy.float)
if (num_m >= self.num_dp):
    for i in range(self.num_dp):
        diff = (self.dataset[:, i, numpy.newaxis] - points)
        tdiff = numpy.dot(self.inv_cov, diff)
        energy = (numpy.sum((diff * tdiff), axis=0) / 2.0)
        result = (result + numpy.exp((- energy)))
else:
    for i in range(num_m):
        diff = (self.dataset - points[:, i, numpy.newaxis])
        tdiff = numpy.dot(self.inv_cov, diff)
        energy = (numpy.sum((diff * tdiff), axis=0) / 2.0)
        tempResult = exp((- energy))
	
===================================================================	
Quiver._make_verts: 353	
----------------------------	

uv = (U + (V * 1j))
str_angles = isinstance(self.angles, six.string_types)
if (str_angles and ((self.angles == 'xy') and (self.scale_units == 'xy'))):
    (angles, lengths) = self._angles_lengths(U, V, eps=1)
elif (str_angles and ((self.angles == 'xy') or (self.scale_units == 'xy'))):
    eps = (np.abs(self.ax.dataLim.extents).max() * 0.001)
    (angles, lengths) = self._angles_lengths(U, V, eps=eps)
if (self.scale_units == 'xy'):
    a = lengths
else:
    a = numpy.absolute(uv)
if (self.scale is None):
    sn = max(10, math.sqrt(self.N))
    if (self.Umask is not numpy.ma.nomask):
        amean = a[(~ self.Umask)].mean()
    else:
        amean = a.mean()
    scale = (((1.8 * amean) * sn) / self.span)
if (self.scale_units is None):
    if (self.scale is None):
        self.scale = scale
    widthu_per_lenu = 1.0
else:
    if (self.scale_units == 'xy'):
        dx = 1
    else:
        dx = self._dots_per_unit(self.scale_units)
    widthu_per_lenu = (dx / self._trans_scale)
    if (self.scale is None):
        self.scale = (scale * widthu_per_lenu)
length = (a * (widthu_per_lenu / (self.scale * self.width)))
(X, Y) = self._h_arrows(length)
if (str_angles and (self.angles == 'xy')):
    theta = angles
elif (str_angles and (self.angles == 'uv')):
    theta = numpy.angle(uv)
else:
    theta = ma.masked_invalid(self.angles, copy=True).filled(0)
    theta = theta.ravel()
    theta *= (numpy.pi / 180.0)
theta.shape = (theta.shape[0], 1)
tempResult = exp((1j * theta))
	
===================================================================	
cloverleaf: 89	
----------------------------	

tempResult = exp(((10.0 - (20.0 * x)) / 3.0))
	
===================================================================	
cloverleaf: 90	
----------------------------	

ex = numpy.exp(((10.0 - (20.0 * x)) / 3.0))
tempResult = exp(((10.0 - (20.0 * y)) / 3.0))
	
===================================================================	
gentle: 59	
----------------------------	

tempResult = exp(((- 5.0625) * (((x - 0.5) ** 2) + ((y - 0.5) ** 2))))
	
===================================================================	
exponential: 44	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp(((- ((x2 * x2) + (y2 * y2))) / 4.0))
	
===================================================================	
exponential: 44	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp(((((- x1) * x1) / 49.0) - (y1 / 10.0)))
	
===================================================================	
exponential: 44	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp(((- ((x7 * x7) + (y3 * y3))) / 4.0))
	
===================================================================	
exponential: 44	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp((((- x4) * x4) - (y7 * y7)))
	
===================================================================	
steep: 64	
----------------------------	

tempResult = exp(((- 20.25) * (((x - 0.5) ** 2) + ((y - 0.5) ** 2))))
	
===================================================================	
gauss: 82	
----------------------------	

x = (5.0 - (10.0 * x))
y = (5.0 - (10.0 * y))
tempResult = exp((((- x) * x) / 2))
	
===================================================================	
gauss: 83	
----------------------------	

x = (5.0 - (10.0 * x))
y = (5.0 - (10.0 * y))
g1 = numpy.exp((((- x) * x) / 2))
tempResult = exp((((- y) * y) / 2))
	
===================================================================	
cosine_peak: 99	
----------------------------	

circle = numpy.hypot(((80 * x) - 40.0), ((90 * y) - 45.0))
tempResult = exp(((- 0.04) * circle))
	
===================================================================	
test_errorbar_limits: 1539	
----------------------------	

x = numpy.arange(0.5, 5.5, 0.5)
tempResult = exp((- x))
	
===================================================================	
bump: 1003	
----------------------------	

x = (1 / (0.1 + numpy.random.random()))
y = ((2 * numpy.random.random()) - 0.5)
z = (10 / (0.1 + numpy.random.random()))
for i in range(m):
    w = (((i / float(m)) - y) * z)
    tempResult = exp(((- w) * w))
	
===================================================================	
test_errorbar_shape: 1528	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.gca()
x = numpy.arange(0.1, 4, 0.5)
tempResult = exp((- x))
	
===================================================================	
test_errorbar: 1492	
----------------------------	

x = numpy.arange(0.1, 4, 0.5)
tempResult = exp((- x))
	
===================================================================	
test_grayscale_alpha: 100	
----------------------------	

'Masking images with NaN did not work for grayscale images'
(x, y) = numpy.ogrid[(- 2):2:0.1, (- 2):2:0.1]
tempResult = exp((- ((x ** 2) + (y ** 2))))
	
===================================================================	
cloverleaf: 83	
----------------------------	

tempResult = exp(((10.0 - (20.0 * x)) / 3.0))
	
===================================================================	
cloverleaf: 84	
----------------------------	

ex = numpy.exp(((10.0 - (20.0 * x)) / 3.0))
tempResult = exp(((10.0 - (20.0 * y)) / 3.0))
	
===================================================================	
gentle: 53	
----------------------------	

tempResult = exp(((- 5.0625) * (((x - 0.5) ** 2) + ((y - 0.5) ** 2))))
	
===================================================================	
exponential: 38	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp(((- ((x2 * x2) + (y2 * y2))) / 4.0))
	
===================================================================	
exponential: 38	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp(((((- x1) * x1) / 49.0) - (y1 / 10.0)))
	
===================================================================	
exponential: 38	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp(((- ((x7 * x7) + (y3 * y3))) / 4.0))
	
===================================================================	
exponential: 38	
----------------------------	

x = (x * 9)
y = (y * 9)
x1 = (x + 1.0)
x2 = (x - 2.0)
x4 = (x - 4.0)
x7 = (x - 7.0)
y1 = (x + 1.0)
y2 = (y - 2.0)
y3 = (y - 3.0)
y7 = (y - 7.0)
tempResult = exp((((- x4) * x4) - (y7 * y7)))
	
===================================================================	
steep: 58	
----------------------------	

tempResult = exp(((- 20.25) * (((x - 0.5) ** 2) + ((y - 0.5) ** 2))))
	
===================================================================	
gauss: 76	
----------------------------	

x = (5.0 - (10.0 * x))
y = (5.0 - (10.0 * y))
tempResult = exp((((- x) * x) / 2))
	
===================================================================	
gauss: 77	
----------------------------	

x = (5.0 - (10.0 * x))
y = (5.0 - (10.0 * y))
g1 = numpy.exp((((- x) * x) / 2))
tempResult = exp((((- y) * y) / 2))
	
===================================================================	
cosine_peak: 93	
----------------------------	

circle = numpy.hypot(((80 * x) - 40.0), ((90 * y) - 45.0))
tempResult = exp(((- 0.04) * circle))
	
===================================================================	
z1: 474	
----------------------------	

r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r1 / 10) ** 2))
	
===================================================================	
z1: 474	
----------------------------	

r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r2 / 10) ** 2))
	
===================================================================	
z: 408	
----------------------------	

r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r1 / 10) ** 2))
	
===================================================================	
z: 408	
----------------------------	

r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
tempResult = exp(((r2 / 10) ** 2))
	
===================================================================	
f: 84	
----------------------------	

s1 = numpy.cos(((2 * numpy.pi) * t))
tempResult = exp((- t))
	
***************************************************	
ipython_ipython-6.1.0: 0	
***************************************************	
pandas_pandas-0.19.2: 2	
===================================================================	
_get_center_of_mass: 969	
----------------------------	

valid_count = len([x for x in [com, span, halflife, alpha] if (x is not None)])
if (valid_count > 1):
    raise ValueError('com, span, halflife, and alpha are mutually exclusive')
if (com is not None):
    if (com < 0):
        raise ValueError('com must satisfy: com >= 0')
elif (span is not None):
    if (span < 1):
        raise ValueError('span must satisfy: span >= 1')
    com = ((span - 1) / 2.0)
elif (halflife is not None):
    if (halflife <= 0):
        raise ValueError('halflife must satisfy: halflife > 0')
    tempResult = exp((numpy.log(0.5) / halflife))
	
===================================================================	
TestSeriesApply.test_apply: 18	
----------------------------	

with numpy.errstate(all='ignore'):
    assert_series_equal(self.ts.apply(numpy.sqrt), numpy.sqrt(self.ts))
    import math
    tempResult = exp(self.ts)
	
***************************************************	
dask_dask-0.7.0: 0	
***************************************************	
nengo_nengo-2.0.0: 11	
===================================================================	
LIFRate.gain_bias: 139	
----------------------------	

'Compute the alpha and bias needed to satisfy max_rates, intercepts.\n\n        Returns gain (alpha) and offset (j_bias) values of neurons.\n\n        Parameters\n        ----------\n        max_rates : list of floats\n            Maximum firing rates of neurons.\n        intercepts : list of floats\n            X-intercepts of neurons.\n        '
inv_tau_ref = ((1.0 / self.tau_ref) if (self.tau_ref > 0) else numpy.inf)
if (max_rates > inv_tau_ref).any():
    raise ValueError(('Max rates must be below the inverse refractory period (%0.3f)' % inv_tau_ref))
tempResult = exp(((self.tau_ref - (1.0 / max_rates)) / self.tau_rc))
	
===================================================================	
Sigmoid.step_math: 107	
----------------------------	

'Compute rates in Hz for input current (incl. bias)'
tempResult = exp((- J))
	
===================================================================	
dft_half: 62	
----------------------------	

x = numpy.arange(n)
w = numpy.arange(((n // 2) + 1))
tempResult = exp(((((- 2j) * numpy.pi) / n) * (w[:, None] * x[None, :])))
	
===================================================================	
test_pdf: 10	
----------------------------	

s = 0.25
tempResult = exp((((- 0.5) * ((x + 0.5) ** 2)) / (s ** 2)))
	
===================================================================	
test_pdf: 10	
----------------------------	

s = 0.25
tempResult = exp((((- 0.5) * ((x - 0.5) ** 2)) / (s ** 2)))
	
===================================================================	
test_noise: 105	
----------------------------	

'Make sure that we can generate noise properly.'
n = 1000
(mean, std) = (0.1, 0.8)
noise = Signal(numpy.zeros(n), name='noise')
process = nengo.processes.StochasticProcess(nengo.dists.Gaussian(mean, std))
m = Model(dt=0.001)
m.operators += [Reset(noise), SimNoise(noise, process)]
sim = RefSimulator(None, model=m, seed=seed)
samples = numpy.zeros((100, n))
for i in range(100):
    sim.step()
    samples[i] = sim.signals[noise]
(h, xedges) = numpy.histogram(samples.flat, bins=51)
x = (0.5 * (xedges[:(- 1)] + xedges[1:]))
dx = numpy.diff(xedges)
tempResult = exp((((- 0.5) * ((x - mean) ** 2)) / (std ** 2)))
	
===================================================================	
test_filt: 65	
----------------------------	

dt = 0.001
tend = 3.0
t = (dt * numpy.arange((tend / dt)))
nt = len(t)
tau = (0.1 / dt)
u = rng.normal(size=nt)
tk = numpy.arange(0, (30 * tau))
tempResult = exp(((- tk) / tau))
	
===================================================================	
lowpass_filter: 42	
----------------------------	

nt = x.shape[(- 1)]
if (kind == 'expon'):
    t = numpy.arange(0, (5 * tau))
    tempResult = exp(((- t) / tau))
	
===================================================================	
lowpass_filter: 47	
----------------------------	

nt = x.shape[(- 1)]
if (kind == 'expon'):
    t = numpy.arange(0, (5 * tau))
    kern = (numpy.exp(((- t) / tau)) / tau)
    delay = tau
elif (kind == 'gauss'):
    std = (tau / 2.0)
    t = numpy.arange(((- 4) * std), (4 * std))
    tempResult = exp(((- 0.5) * ((t / std) ** 2)))
	
===================================================================	
lowpass_filter: 52	
----------------------------	

nt = x.shape[(- 1)]
if (kind == 'expon'):
    t = numpy.arange(0, (5 * tau))
    kern = (numpy.exp(((- t) / tau)) / tau)
    delay = tau
elif (kind == 'gauss'):
    std = (tau / 2.0)
    t = numpy.arange(((- 4) * std), (4 * std))
    kern = (numpy.exp(((- 0.5) * ((t / std) ** 2))) / numpy.sqrt(((2 * numpy.pi) * (std ** 2))))
    delay = (4 * std)
elif (kind == 'alpha'):
    alpha = (1.0 / tau)
    t = numpy.arange(0, (5 * tau))
    tempResult = exp(((- alpha) * t))
	
===================================================================	
test_cont2discrete_zoh: 28	
----------------------------	

taus = numpy.logspace(((- numpy.log10(dt)) - 1), (numpy.log10(dt) + 3), 30)
for tau in taus:
    (num, den) = ([1], [tau, 1])
    d = (- numpy.expm1(((- dt) / tau)))
    (num0, den0) = ([0, d], [1, (d - 1)])
    (num1, den1, _) = cont2discrete((num, den), dt)
    assert numpy.allclose(num0, num1)
    assert numpy.allclose(den0, den1)
for tau in taus:
    (num, den) = ([1], [(tau ** 2), (2 * tau), 1])
    a = (dt / tau)
    tempResult = exp((- a))
	
***************************************************	
sympy_sympy-1.0.0: 0	
***************************************************	
daducci_amico-dev: 0	
***************************************************	
aplpy_aplpy-1.1.1: 0	
***************************************************	
markovmodel_msmtools-1.0.2: 0	
***************************************************	
nilearn_nilearn-0.4.0: 5	
===================================================================	
_logistic: 91	
----------------------------	

'Compute the logistic function of the data: sum(sigmoid(yXw))\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_samples, n_features)\n        Design matrix.\n\n    y : ndarray, shape (n_samples,)\n        Target / response vector. Each entry must be +1 or -1.\n\n    w : ndarray, shape (n_features,)\n        Unmasked, ravelized input map.\n\n    Returns\n    -------\n    energy : float\n        Energy contribution due to logistic data-fit term.\n    '
z = (numpy.dot(X, w[:(- 1)]) + w[(- 1)])
yz = (y * z)
idx = (yz > 0)
out = numpy.empty_like(yz)
tempResult = exp((- yz[idx]))
	
===================================================================	
_logistic: 92	
----------------------------	

'Compute the logistic function of the data: sum(sigmoid(yXw))\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_samples, n_features)\n        Design matrix.\n\n    y : ndarray, shape (n_samples,)\n        Target / response vector. Each entry must be +1 or -1.\n\n    w : ndarray, shape (n_features,)\n        Unmasked, ravelized input map.\n\n    Returns\n    -------\n    energy : float\n        Energy contribution due to logistic data-fit term.\n    '
z = (numpy.dot(X, w[:(- 1)]) + w[(- 1)])
yz = (y * z)
idx = (yz > 0)
out = numpy.empty_like(yz)
out[idx] = numpy.log1p(numpy.exp((- yz[idx])))
tempResult = exp(yz[(~ idx)])
	
===================================================================	
_sigmoid: 80	
----------------------------	

'Helper function: return 1. / (1 + np.exp(-t))'
if copy:
    t = numpy.copy(t)
t *= (- 1.0)
tempResult = exp(t, t)
	
===================================================================	
create_graph_net_simulation_data: 30	
----------------------------	

'\n    Function to generate data\n\n    '
generator = check_random_state(random_state)
w = numpy.zeros((size, size, size))
for _ in range(n_points):
    point = (generator.randint(0, size), generator.randint(0, size), generator.randint(0, size))
    w[point] = 1.0
mask = numpy.ones((size, size, size), dtype=numpy.bool)
w = scipy.ndimage.gaussian_filter(w, sigma=1)
w = w[mask]
XX = generator.randn(n_samples, size, size, size)
noise = []
for i in range(n_samples):
    Xi = scipy.ndimage.filters.gaussian_filter(XX[i, :, :, :], smooth_X)
    Xi = Xi[mask]
    noise.append(Xi)
noise = numpy.array(noise)
if (task == 'regression'):
    y = generator.randn(n_samples)
elif (task == 'classification'):
    y = numpy.ones(n_samples)
    y[0::2] = (- 1)
X = numpy.dot(y[:, numpy.newaxis], w[numpy.newaxis])
tempResult = exp((snr / 20.0))
	
===================================================================	
_compute_weights_3d: 24	
----------------------------	

gradients = 0
for channel in range(0, data.shape[(- 1)]):
    gradients += (_compute_gradients_3d(data[(..., channel)], spacing) ** 2)
beta /= (10 * data.std())
gradients *= beta
tempResult = exp((- gradients))
	
***************************************************	
poliastro_poliastro-0.8.0: 2	
===================================================================	
F_to_nu: 37	
----------------------------	

'True anomaly from hyperbolic eccentric anomaly.\n\n    Parameters\n    ----------\n    F : float\n        Hyperbolic eccentric anomaly (rad).\n    ecc : float\n        Eccentricity (>1).\n\n    Returns\n    -------\n    nu : float\n        True anomaly (rad).\n\n    '
with astropy.units.set_enabled_equivalencies(astropy.units.dimensionless_angles()):
    tempResult = exp(F)
	
===================================================================	
F_to_nu: 37	
----------------------------	

'True anomaly from hyperbolic eccentric anomaly.\n\n    Parameters\n    ----------\n    F : float\n        Hyperbolic eccentric anomaly (rad).\n    ecc : float\n        Eccentricity (>1).\n\n    Returns\n    -------\n    nu : float\n        True anomaly (rad).\n\n    '
with astropy.units.set_enabled_equivalencies(astropy.units.dimensionless_angles()):
    tempResult = exp(F)
	
***************************************************	
skimage_skimage-0.13.0: 17	
===================================================================	
combine_stains: 339	
----------------------------	

'Stain to RGB color space conversion.\n\n    Parameters\n    ----------\n    stains : array_like\n        The image in stain color space, in a 3-D array of shape\n        ``(.., .., 3)``.\n    conv_matrix: ndarray\n        The stain separation matrix as described by G. Landini [1]_.\n\n    Returns\n    -------\n    out : ndarray\n        The image in RGB format, in a 3-D array of shape ``(.., .., 3)``.\n\n    Raises\n    ------\n    ValueError\n        If `stains` is not a 3-D array of shape ``(.., .., 3)``.\n\n    Notes\n    -----\n    Stain combination matrices available in the ``color`` module and their\n    respective colorspace:\n\n    * ``rgb_from_hed``: Hematoxylin + Eosin + DAB\n    * ``rgb_from_hdx``: Hematoxylin + DAB\n    * ``rgb_from_fgx``: Feulgen + Light Green\n    * ``rgb_from_bex``: Giemsa stain : Methyl Blue + Eosin\n    * ``rgb_from_rbd``: FastRed + FastBlue +  DAB\n    * ``rgb_from_gdx``: Methyl Green + DAB\n    * ``rgb_from_hax``: Hematoxylin + AEC\n    * ``rgb_from_bro``: Blue matrix Anilline Blue + Red matrix Azocarmine                        + Orange matrix Orange-G\n    * ``rgb_from_bpx``: Methyl Blue + Ponceau Fuchsin\n    * ``rgb_from_ahx``: Alcian Blue + Hematoxylin\n    * ``rgb_from_hpx``: Hematoxylin + PAS\n\n    References\n    ----------\n    .. [1] http://www.dentistry.bham.ac.uk/landinig/software/cdeconv/cdeconv.html\n\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.color import (separate_stains, combine_stains,\n    ...                            hdx_from_rgb, rgb_from_hdx)\n    >>> ihc = data.immunohistochemistry()\n    >>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n    >>> ihc_rgb = combine_stains(ihc_hdx, rgb_from_hdx)\n    '
from ..exposure import rescale_intensity
stains = util.dtype.img_as_float(stains)
logrgb2 = numpy.dot((- numpy.reshape(stains, ((- 1), 3))), conv_matrix)
tempResult = exp(logrgb2)
	
===================================================================	
deltaE_ciede2000: 75	
----------------------------	

'Color difference as given by the CIEDE 2000 standard.\n\n    CIEDE 2000 is a major revision of CIDE94.  The perceptual calibration is\n    largely based on experience with automotive paint on smooth surfaces.\n\n    Parameters\n    ----------\n    lab1 : array_like\n        reference color (Lab colorspace)\n    lab2 : array_like\n        comparison color (Lab colorspace)\n    kL : float (range), optional\n        lightness scale factor, 1 for "acceptably close"; 2 for "imperceptible"\n        see deltaE_cmc\n    kC : float (range), optional\n        chroma scale factor, usually 1\n    kH : float (range), optional\n        hue scale factor, usually 1\n\n    Returns\n    -------\n    deltaE : array_like\n        The distance between `lab1` and `lab2`\n\n    Notes\n    -----\n    CIEDE 2000 assumes parametric weighting factors for the lightness, chroma,\n    and hue (`kL`, `kC`, `kH` respectively).  These default to 1.\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Color_difference\n    .. [2] http://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf\n           (doi:10.1364/AO.33.008069)\n    .. [3] M. Melgosa, J. Quesada, and E. Hita, "Uniformity of some recent\n           color metrics tested with an accurate color-difference tolerance\n           dataset," Appl. Opt. 33, 8069-8077 (1994).\n    '
lab1 = numpy.asarray(lab1)
lab2 = numpy.asarray(lab2)
unroll = False
if ((lab1.ndim == 1) and (lab2.ndim == 1)):
    unroll = True
    if (lab1.ndim == 1):
        lab1 = lab1[None, :]
    if (lab2.ndim == 1):
        lab2 = lab2[None, :]
(L1, a1, b1) = numpy.rollaxis(lab1, (- 1))[:3]
(L2, a2, b2) = numpy.rollaxis(lab2, (- 1))[:3]
Cbar = (0.5 * (numpy.hypot(a1, b1) + numpy.hypot(a2, b2)))
c7 = (Cbar ** 7)
G = (0.5 * (1 - numpy.sqrt((c7 / (c7 + (25 ** 7))))))
scale = (1 + G)
(C1, h1) = _cart2polar_2pi((a1 * scale), b1)
(C2, h2) = _cart2polar_2pi((a2 * scale), b2)
Lbar = (0.5 * (L1 + L2))
tmp = ((Lbar - 50) ** 2)
SL = (1 + ((0.015 * tmp) / numpy.sqrt((20 + tmp))))
L_term = ((L2 - L1) / (kL * SL))
Cbar = (0.5 * (C1 + C2))
SC = (1 + (0.045 * Cbar))
C_term = ((C2 - C1) / (kC * SC))
h_diff = (h2 - h1)
h_sum = (h1 + h2)
CC = (C1 * C2)
dH = h_diff.copy()
dH[(h_diff > numpy.pi)] -= (2 * numpy.pi)
dH[(h_diff < (- numpy.pi))] += (2 * numpy.pi)
dH[(CC == 0.0)] = 0.0
dH_term = ((2 * numpy.sqrt(CC)) * numpy.sin((dH / 2)))
Hbar = h_sum.copy()
mask = numpy.logical_and((CC != 0.0), (numpy.abs(h_diff) > numpy.pi))
Hbar[(mask * (h_sum < (2 * numpy.pi)))] += (2 * numpy.pi)
Hbar[(mask * (h_sum >= (2 * numpy.pi)))] -= (2 * numpy.pi)
Hbar[(CC == 0.0)] *= 2
Hbar *= 0.5
T = ((((1 - (0.17 * numpy.cos((Hbar - numpy.deg2rad(30))))) + (0.24 * numpy.cos((2 * Hbar)))) + (0.32 * numpy.cos(((3 * Hbar) + numpy.deg2rad(6))))) - (0.2 * numpy.cos(((4 * Hbar) - numpy.deg2rad(63)))))
SH = (1 + ((0.015 * Cbar) * T))
H_term = (dH_term / (kH * SH))
c7 = (Cbar ** 7)
Rc = (2 * numpy.sqrt((c7 / (c7 + (25 ** 7)))))
tempResult = exp((- (((numpy.rad2deg(Hbar) - 275) / 25) ** 2)))
	
===================================================================	
adjust_sigmoid: 108	
----------------------------	

'Performs Sigmoid Correction on the input image.\n\n    Also known as Contrast Adjustment.\n    This function transforms the input image pixelwise according to the\n    equation ``O = 1/(1 + exp*(gain*(cutoff - I)))`` after scaling each pixel\n    to the range 0 to 1.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    cutoff : float\n        Cutoff of the sigmoid function that shifts the characteristic curve\n        in horizontal direction. Default value is 0.5.\n    gain : float\n        The constant multiplier in exponential\'s power of sigmoid function.\n        Default value is 10.\n    inv : bool\n        If True, returns the negative sigmoid correction. Defaults to False.\n\n    Returns\n    -------\n    out : ndarray\n        Sigmoid corrected output image.\n\n    See Also\n    --------\n    adjust_gamma\n\n    References\n    ----------\n    .. [1] Gustav J. Braun, "Image Lightness Rescaling Using Sigmoidal Contrast\n           Enhancement Functions",\n           http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf\n\n    '
_assert_non_negative(image)
dtype = image.dtype.type
scale = float((dtype_limits(image, True)[1] - dtype_limits(image, True)[0]))
if inv:
    tempResult = exp((gain * (cutoff - (image / scale))))
	
===================================================================	
adjust_sigmoid: 110	
----------------------------	

'Performs Sigmoid Correction on the input image.\n\n    Also known as Contrast Adjustment.\n    This function transforms the input image pixelwise according to the\n    equation ``O = 1/(1 + exp*(gain*(cutoff - I)))`` after scaling each pixel\n    to the range 0 to 1.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    cutoff : float\n        Cutoff of the sigmoid function that shifts the characteristic curve\n        in horizontal direction. Default value is 0.5.\n    gain : float\n        The constant multiplier in exponential\'s power of sigmoid function.\n        Default value is 10.\n    inv : bool\n        If True, returns the negative sigmoid correction. Defaults to False.\n\n    Returns\n    -------\n    out : ndarray\n        Sigmoid corrected output image.\n\n    See Also\n    --------\n    adjust_gamma\n\n    References\n    ----------\n    .. [1] Gustav J. Braun, "Image Lightness Rescaling Using Sigmoidal Contrast\n           Enhancement Functions",\n           http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf\n\n    '
_assert_non_negative(image)
dtype = image.dtype.type
scale = float((dtype_limits(image, True)[1] - dtype_limits(image, True)[0]))
if inv:
    out = ((1 - (1 / (1 + numpy.exp((gain * (cutoff - (image / scale))))))) * scale)
    return dtype(out)
tempResult = exp((gain * (cutoff - (image / scale))))
	
===================================================================	
_upsampled_dft: 15	
----------------------------	

'\n    Upsampled DFT by matrix multiplication.\n\n    This code is intended to provide the same result as if the following\n    operations were performed:\n        - Embed the array "data" in an array that is ``upsample_factor`` times\n          larger in each dimension.  ifftshift to bring the center of the\n          image to (1,1).\n        - Take the FFT of the larger array.\n        - Extract an ``[upsampled_region_size]`` region of the result, starting\n          with the ``[axis_offsets+1]`` element.\n\n    It achieves this result by computing the DFT in the output array without\n    the need to zeropad. Much faster and memory efficient than the zero-padded\n    FFT approach if ``upsampled_region_size`` is much smaller than\n    ``data.size * upsample_factor``.\n\n    Parameters\n    ----------\n    data : 2D ndarray\n        The input data array (DFT of original data) to upsample.\n    upsampled_region_size : integer or tuple of integers, optional\n        The size of the region to be sampled.  If one integer is provided, it\n        is duplicated up to the dimensionality of ``data``.\n    upsample_factor : integer, optional\n        The upsampling factor.  Defaults to 1.\n    axis_offsets : tuple of integers, optional\n        The offsets of the region to be sampled.  Defaults to None (uses\n        image center)\n\n    Returns\n    -------\n    output : 2D ndarray\n            The upsampled DFT of the specified region.\n    '
if (not hasattr(upsampled_region_size, '__iter__')):
    upsampled_region_size = ([upsampled_region_size] * data.ndim)
elif (len(upsampled_region_size) != data.ndim):
    raise ValueError("shape of upsampled region sizes must be equal to input data's number of dimensions.")
if (axis_offsets is None):
    axis_offsets = ([0] * data.ndim)
elif (len(axis_offsets) != data.ndim):
    raise ValueError("number of axis offsets must be equal to input data's number of dimensions.")
tempResult = exp((((((- 1j) * 2) * numpy.pi) / (data.shape[1] * upsample_factor)) * (np.fft.ifftshift(np.arange(data.shape[1]))[:, None] - np.floor((data.shape[1] / 2))).dot((numpy.arange(upsampled_region_size[1])[None, :] - axis_offsets[1]))))
	
===================================================================	
_upsampled_dft: 16	
----------------------------	

'\n    Upsampled DFT by matrix multiplication.\n\n    This code is intended to provide the same result as if the following\n    operations were performed:\n        - Embed the array "data" in an array that is ``upsample_factor`` times\n          larger in each dimension.  ifftshift to bring the center of the\n          image to (1,1).\n        - Take the FFT of the larger array.\n        - Extract an ``[upsampled_region_size]`` region of the result, starting\n          with the ``[axis_offsets+1]`` element.\n\n    It achieves this result by computing the DFT in the output array without\n    the need to zeropad. Much faster and memory efficient than the zero-padded\n    FFT approach if ``upsampled_region_size`` is much smaller than\n    ``data.size * upsample_factor``.\n\n    Parameters\n    ----------\n    data : 2D ndarray\n        The input data array (DFT of original data) to upsample.\n    upsampled_region_size : integer or tuple of integers, optional\n        The size of the region to be sampled.  If one integer is provided, it\n        is duplicated up to the dimensionality of ``data``.\n    upsample_factor : integer, optional\n        The upsampling factor.  Defaults to 1.\n    axis_offsets : tuple of integers, optional\n        The offsets of the region to be sampled.  Defaults to None (uses\n        image center)\n\n    Returns\n    -------\n    output : 2D ndarray\n            The upsampled DFT of the specified region.\n    '
if (not hasattr(upsampled_region_size, '__iter__')):
    upsampled_region_size = ([upsampled_region_size] * data.ndim)
elif (len(upsampled_region_size) != data.ndim):
    raise ValueError("shape of upsampled region sizes must be equal to input data's number of dimensions.")
if (axis_offsets is None):
    axis_offsets = ([0] * data.ndim)
elif (len(axis_offsets) != data.ndim):
    raise ValueError("number of axis offsets must be equal to input data's number of dimensions.")
col_kernel = numpy.exp((((((- 1j) * 2) * numpy.pi) / (data.shape[1] * upsample_factor)) * (np.fft.ifftshift(np.arange(data.shape[1]))[:, None] - np.floor((data.shape[1] / 2))).dot((numpy.arange(upsampled_region_size[1])[None, :] - axis_offsets[1]))))
tempResult = exp((((((- 1j) * 2) * numpy.pi) / (data.shape[0] * upsample_factor)) * (np.arange(upsampled_region_size[0])[:, None] - axis_offsets[0]).dot((numpy.fft.ifftshift(numpy.arange(data.shape[0]))[None, :] - numpy.floor((data.shape[0] / 2))))))
	
===================================================================	
_frangi_hessian_common_filter: 24	
----------------------------	

"This is an intermediate function for Frangi and Hessian filters.\n\n    Shares the common code for Frangi and Hessian functions.\n\n    Parameters\n    ----------\n    image : (N, M) ndarray\n        Array with input image data.\n    scale_range : 2-tuple of floats, optional\n        The range of sigmas used.\n    scale_step : float, optional\n        Step size between sigmas.\n    beta1 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to deviation from a blob-like structure.\n    beta2 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to areas of high variance/texture/structure.\n\n    Returns\n    -------\n    filtered_list : list\n        List of pre-filtered images.\n\n    "
from ..feature import hessian_matrix, hessian_matrix_eigvals
sigmas = numpy.arange(scale_range[0], scale_range[1], scale_step)
if numpy.any((numpy.asarray(sigmas) < 0.0)):
    raise ValueError('Sigma values less than zero are not valid')
beta1 = (2 * (beta1 ** 2))
beta2 = (2 * (beta2 ** 2))
filtered_array = numpy.zeros((sigmas.shape + image.shape))
lambdas_array = numpy.zeros((sigmas.shape + image.shape))
for (i, sigma) in enumerate(sigmas):
    (Drr, Drc, Dcc) = hessian_matrix(image, sigma, order='rc')
    Drr = ((sigma ** 2) * Drr)
    Drc = ((sigma ** 2) * Drc)
    Dcc = ((sigma ** 2) * Dcc)
    (lambda1, lambda2) = hessian_matrix_eigvals(Drr, Drc, Dcc)
    lambda1[(lambda1 == 0)] = 1e-10
    rb = ((lambda2 / lambda1) ** 2)
    s2 = ((lambda1 ** 2) + (lambda2 ** 2))
    tempResult = exp(((- rb) / beta1))
	
===================================================================	
_frangi_hessian_common_filter: 24	
----------------------------	

"This is an intermediate function for Frangi and Hessian filters.\n\n    Shares the common code for Frangi and Hessian functions.\n\n    Parameters\n    ----------\n    image : (N, M) ndarray\n        Array with input image data.\n    scale_range : 2-tuple of floats, optional\n        The range of sigmas used.\n    scale_step : float, optional\n        Step size between sigmas.\n    beta1 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to deviation from a blob-like structure.\n    beta2 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to areas of high variance/texture/structure.\n\n    Returns\n    -------\n    filtered_list : list\n        List of pre-filtered images.\n\n    "
from ..feature import hessian_matrix, hessian_matrix_eigvals
sigmas = numpy.arange(scale_range[0], scale_range[1], scale_step)
if numpy.any((numpy.asarray(sigmas) < 0.0)):
    raise ValueError('Sigma values less than zero are not valid')
beta1 = (2 * (beta1 ** 2))
beta2 = (2 * (beta2 ** 2))
filtered_array = numpy.zeros((sigmas.shape + image.shape))
lambdas_array = numpy.zeros((sigmas.shape + image.shape))
for (i, sigma) in enumerate(sigmas):
    (Drr, Drc, Dcc) = hessian_matrix(image, sigma, order='rc')
    Drr = ((sigma ** 2) * Drr)
    Drc = ((sigma ** 2) * Drc)
    Dcc = ((sigma ** 2) * Dcc)
    (lambda1, lambda2) = hessian_matrix_eigvals(Drr, Drc, Dcc)
    lambda1[(lambda1 == 0)] = 1e-10
    rb = ((lambda2 / lambda1) ** 2)
    s2 = ((lambda1 ** 2) + (lambda2 ** 2))
    tempResult = exp(((- s2) / beta2))
	
===================================================================	
gabor_kernel: 23	
----------------------------	

'Return complex 2D Gabor filter kernel.\n\n    Gabor kernel is a Gaussian kernel modulated by a complex harmonic function.\n    Harmonic function consists of an imaginary sine function and a real\n    cosine function. Spatial frequency is inversely proportional to the\n    wavelength of the harmonic and to the standard deviation of a Gaussian\n    kernel. The bandwidth is also inversely proportional to the standard\n    deviation.\n\n    Parameters\n    ----------\n    frequency : float\n        Spatial frequency of the harmonic function. Specified in pixels.\n    theta : float, optional\n        Orientation in radians. If 0, the harmonic is in the x-direction.\n    bandwidth : float, optional\n        The bandwidth captured by the filter. For fixed bandwidth, `sigma_x`\n        and `sigma_y` will decrease with increasing frequency. This value is\n        ignored if `sigma_x` and `sigma_y` are set by the user.\n    sigma_x, sigma_y : float, optional\n        Standard deviation in x- and y-directions. These directions apply to\n        the kernel *before* rotation. If `theta = pi/2`, then the kernel is\n        rotated 90 degrees so that `sigma_x` controls the *vertical* direction.\n    n_stds : scalar, optional\n        The linear size of the kernel is n_stds (3 by default) standard\n        deviations\n    offset : float, optional\n        Phase offset of harmonic function in radians.\n\n    Returns\n    -------\n    g : complex array\n        Complex filter kernel.\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Gabor_filter\n    .. [2] http://mplab.ucsd.edu/tutorials/gabor.pdf\n\n    Examples\n    --------\n    >>> from skimage.filters import gabor_kernel\n    >>> from skimage import io\n    >>> from matplotlib import pyplot as plt  # doctest: +SKIP\n\n    >>> gk = gabor_kernel(frequency=0.2)\n    >>> plt.figure()        # doctest: +SKIP\n    >>> io.imshow(gk.real)  # doctest: +SKIP\n    >>> io.show()           # doctest: +SKIP\n\n    >>> # more ripples (equivalent to increasing the size of the\n    >>> # Gaussian spread)\n    >>> gk = gabor_kernel(frequency=0.2, bandwidth=0.1)\n    >>> plt.figure()        # doctest: +SKIP\n    >>> io.imshow(gk.real)  # doctest: +SKIP\n    >>> io.show()           # doctest: +SKIP\n    '
if (sigma_x is None):
    sigma_x = (_sigma_prefactor(bandwidth) / frequency)
if (sigma_y is None):
    sigma_y = (_sigma_prefactor(bandwidth) / frequency)
x0 = numpy.ceil(max(numpy.abs(((n_stds * sigma_x) * numpy.cos(theta))), numpy.abs(((n_stds * sigma_y) * numpy.sin(theta))), 1))
y0 = numpy.ceil(max(numpy.abs(((n_stds * sigma_y) * numpy.cos(theta))), numpy.abs(((n_stds * sigma_x) * numpy.sin(theta))), 1))
(y, x) = numpy.mgrid[(- y0):(y0 + 1), (- x0):(x0 + 1)]
rotx = ((x * numpy.cos(theta)) + (y * numpy.sin(theta)))
roty = (((- x) * numpy.sin(theta)) + (y * numpy.cos(theta)))
g = numpy.zeros(y.shape, dtype=numpy.complex)
tempResult = exp(((- 0.5) * (((rotx ** 2) / (sigma_x ** 2)) + ((roty ** 2) / (sigma_y ** 2)))))
	
===================================================================	
gabor_kernel: 25	
----------------------------	

'Return complex 2D Gabor filter kernel.\n\n    Gabor kernel is a Gaussian kernel modulated by a complex harmonic function.\n    Harmonic function consists of an imaginary sine function and a real\n    cosine function. Spatial frequency is inversely proportional to the\n    wavelength of the harmonic and to the standard deviation of a Gaussian\n    kernel. The bandwidth is also inversely proportional to the standard\n    deviation.\n\n    Parameters\n    ----------\n    frequency : float\n        Spatial frequency of the harmonic function. Specified in pixels.\n    theta : float, optional\n        Orientation in radians. If 0, the harmonic is in the x-direction.\n    bandwidth : float, optional\n        The bandwidth captured by the filter. For fixed bandwidth, `sigma_x`\n        and `sigma_y` will decrease with increasing frequency. This value is\n        ignored if `sigma_x` and `sigma_y` are set by the user.\n    sigma_x, sigma_y : float, optional\n        Standard deviation in x- and y-directions. These directions apply to\n        the kernel *before* rotation. If `theta = pi/2`, then the kernel is\n        rotated 90 degrees so that `sigma_x` controls the *vertical* direction.\n    n_stds : scalar, optional\n        The linear size of the kernel is n_stds (3 by default) standard\n        deviations\n    offset : float, optional\n        Phase offset of harmonic function in radians.\n\n    Returns\n    -------\n    g : complex array\n        Complex filter kernel.\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Gabor_filter\n    .. [2] http://mplab.ucsd.edu/tutorials/gabor.pdf\n\n    Examples\n    --------\n    >>> from skimage.filters import gabor_kernel\n    >>> from skimage import io\n    >>> from matplotlib import pyplot as plt  # doctest: +SKIP\n\n    >>> gk = gabor_kernel(frequency=0.2)\n    >>> plt.figure()        # doctest: +SKIP\n    >>> io.imshow(gk.real)  # doctest: +SKIP\n    >>> io.show()           # doctest: +SKIP\n\n    >>> # more ripples (equivalent to increasing the size of the\n    >>> # Gaussian spread)\n    >>> gk = gabor_kernel(frequency=0.2, bandwidth=0.1)\n    >>> plt.figure()        # doctest: +SKIP\n    >>> io.imshow(gk.real)  # doctest: +SKIP\n    >>> io.show()           # doctest: +SKIP\n    '
if (sigma_x is None):
    sigma_x = (_sigma_prefactor(bandwidth) / frequency)
if (sigma_y is None):
    sigma_y = (_sigma_prefactor(bandwidth) / frequency)
x0 = numpy.ceil(max(numpy.abs(((n_stds * sigma_x) * numpy.cos(theta))), numpy.abs(((n_stds * sigma_y) * numpy.sin(theta))), 1))
y0 = numpy.ceil(max(numpy.abs(((n_stds * sigma_y) * numpy.cos(theta))), numpy.abs(((n_stds * sigma_x) * numpy.sin(theta))), 1))
(y, x) = numpy.mgrid[(- y0):(y0 + 1), (- x0):(x0 + 1)]
rotx = ((x * numpy.cos(theta)) + (y * numpy.sin(theta)))
roty = (((- x) * numpy.sin(theta)) + (y * numpy.cos(theta)))
g = numpy.zeros(y.shape, dtype=numpy.complex)
g[:] = numpy.exp(((- 0.5) * (((rotx ** 2) / (sigma_x ** 2)) + ((roty ** 2) / (sigma_y ** 2)))))
g /= (((2 * numpy.pi) * sigma_x) * sigma_y)
tempResult = exp((1j * ((((2 * numpy.pi) * frequency) * rotx) + offset)))
	
===================================================================	
TestLPIFilter2D.filt_func: 11	
----------------------------	

tempResult = exp(((- numpy.hypot(r, c)) / 1))
	
===================================================================	
TestColorMixer.test_sigmoid: 80	
----------------------------	

import math
alpha = 1.5
beta = 1.5
c1 = (1 / (1 + math.exp(beta)))
c2 = ((1 / (1 + math.exp((beta - alpha)))) - c1)
state = (self.state / 255.0)
skimage.io._plugins._colormixer.sigmoid_gamma(self.img, self.state, alpha, beta)
tempResult = exp((beta - (state * alpha)))
	
===================================================================	
test_mask: 93	
----------------------------	

length = 100
ramps = [numpy.linspace(0, (4 * numpy.pi), length), numpy.linspace(0, (8 * numpy.pi), length), numpy.linspace(0, (6 * numpy.pi), length)]
image = numpy.vstack(ramps)
mask_1d = numpy.ones((length,), dtype=numpy.bool)
mask_1d[0] = mask_1d[(- 1)] = False
for i in range(len(ramps)):
    mask = numpy.zeros(image.shape, dtype=numpy.bool)
    mask |= mask_1d.reshape(1, (- 1))
    mask[i, :] = False
    tempResult = exp((1j * image))
	
===================================================================	
check_wrap_around: 63	
----------------------------	

elements = 100
ramp = numpy.linspace(0, (12 * numpy.pi), elements)
ramp[(- 1)] = ramp[0]
image = ramp.reshape(tuple([(elements if (n == axis) else 1) for n in range(ndim)]))
tempResult = exp((1j * image))
	
===================================================================	
check_unwrap: 28	
----------------------------	

tempResult = exp((1j * image))
	
===================================================================	
_compute_weights_3d: 45	
----------------------------	

gradients = 0
for channel in range(0, data.shape[(- 1)]):
    gradients += (_compute_gradients_3d(data[(..., channel)], spacing) ** 2)
beta /= (10 * data.std())
if multichannel:
    beta /= numpy.sqrt(data.shape[(- 1)])
gradients *= beta
tempResult = exp((- gradients))
	
===================================================================	
_swirl_mapping: 97	
----------------------------	

(x, y) = xy.T
(x0, y0) = center
rho = numpy.sqrt((((x - x0) ** 2) + ((y - y0) ** 2)))
radius = ((radius / 5) * numpy.log(2))
tempResult = exp(((- rho) / radius))
	
***************************************************	
sunpy_sunpy-0.8.0: 0	
***************************************************	
spacetelescope_synphot-0.1: 2	
===================================================================	
etau_madau: 116	
----------------------------	

'Madau 1995 extinction for a galaxy at given redshift.\n    This is the Lyman-alpha prescription from the photo-z code BPZ.\n\n    The Lyman-alpha forest approximately has an effective\n    "throughput" which is a function of redshift and\n    rest-frame wavelength.\n    One would multiply the SEDs by this factor before\n    passing it through an instrument filter.\n\n    This approximation is from Footnote 3 of\n    :ref:`Madau et al. (1995) <synphot-ref-madau1995>`.\n    This is claimed accurate to 5%.\n    The scatter in this factor (due to different lines of sight)\n    is huge, as shown in Madau\'s Fig. 3 (top panel);\n    The figure\'s bottom panel shows a redshifted version of the\n    "exact" prescription.\n\n    Parameters\n    ----------\n    wave : array-like or `~astropy.units.quantity.Quantity`\n        Redshifted wavelength values.\n        Non-redshifted wavelength is ``wave / (1 + z)``.\n\n    z : number\n        Redshift.\n\n    kwargs : dict\n        Equivalencies for unit conversion, see\n        :func:`~synphot.units.validate_quantity`.\n\n    Returns\n    -------\n    extcurve : `ExtinctionCurve`\n        Extinction curve to apply to the redshifted spectrum.\n\n    '
if (not isinstance(z, numbers.Real)):
    raise exceptions.SynphotError('Redshift must be a real scalar number.')
if (numpy.isscalar(wave) or (len(wave) <= 1)):
    raise exceptions.SynphotError('Wavelength has too few data points')
wave = units.validate_quantity(wave, u.AA, **kwargs).value
ll = 912.0
c = numpy.array([0.0036, 0.0017, 0.0012, 0.00093])
el = numpy.array([1216, 1026, 973, 950], dtype=numpy.float)
tau = numpy.zeros_like(wave, dtype=numpy.float)
xe = (1.0 + z)
for i in range(len(el)):
    tau = numpy.where((wave <= (el[i] * xe)), (tau + (c[i] * ((wave / el[i]) ** 3.46))), tau)
xc = (wave / ll)
xc3 = (xc ** 3)
tau = numpy.where((wave <= (ll * xe)), ((((tau + ((0.25 * xc3) * ((xe ** 0.46) - (xc ** 0.46)))) + ((9.4 * (xc ** 1.5)) * ((xe ** 0.18) - (xc ** 0.18)))) - ((0.7 * xc3) * ((xc ** (- 1.32)) - (xe ** (- 1.32))))) - (0.023 * ((xe ** 1.68) - (xc ** 1.68)))), tau)
tempResult = exp((- tau))
	
===================================================================	
BaseSpectrum.barlam: 247	
----------------------------	

'Calculate :ref:`mean log wavelength <synphot-formula-barlam>`.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, `waveset` is used.\n\n        Returns\n        -------\n        bar_lam : `~astropy.units.quantity.Quantity`\n            Mean log wavelength.\n\n        '
x = self._validate_wavelengths(wavelengths).value
y = self(x).value
num = numpy.trapz(((y * numpy.log(x)) / x), x=x)
den = numpy.trapz((y / x), x=x)
if ((num == 0) or (den == 0)):
    bar_lam = 0.0
else:
    tempResult = exp(abs((num / den)))
	
***************************************************	
librosa_librosa-0.5.1: 13	
===================================================================	
tempo: 66	
----------------------------	

"Estimate the tempo (beats per minute)\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)] or None\n        audio time series\n\n    sr : number > 0 [scalar]\n        sampling rate of the time series\n\n    onset_envelope    : np.ndarray [shape=(n,)]\n        pre-computed onset strength envelope\n\n    hop_length : int > 0 [scalar]\n        hop length of the time series\n\n    start_bpm : float [scalar]\n        initial guess of the BPM\n\n    std_bpm : float > 0 [scalar]\n        standard deviation of tempo distribution\n\n    ac_size : float > 0 [scalar]\n        length (in seconds) of the auto-correlation window\n\n    max_tempo : float > 0 [scalar, optional]\n        If provided, only estimate tempo below this threshold\n\n    aggregate : callable [optional]\n        Aggregation function for estimating global tempo.\n        If `None`, then tempo is estimated independently for each frame.\n\n    Returns\n    -------\n    tempo : np.ndarray [scalar]\n        estimated tempo (beats per minute)\n\n    See Also\n    --------\n    librosa.onset.onset_strength\n    librosa.feature.tempogram\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    >>> # Estimate a static tempo\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> onset_env = librosa.onset.onset_strength(y, sr=sr)\n    >>> tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)\n    >>> tempo\n    array([129.199])\n\n    >>> # Or a dynamic tempo\n    >>> dtempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr,\n    ...                             aggregate=None)\n    >>> dtempo\n    array([ 143.555,  143.555,  143.555, ...,  161.499,  161.499,\n            172.266])\n\n\n    Plot the estimated tempo against the onset autocorrelation\n\n    >>> import matplotlib.pyplot as plt\n    >>> # Convert to scalar\n    >>> tempo = np.asscalar(tempo)\n    >>> # Compute 2-second windowed autocorrelation\n    >>> hop_length = 512\n    >>> ac = librosa.autocorrelate(onset_env, 2 * sr // hop_length)\n    >>> freqs = librosa.tempo_frequencies(len(ac), sr=sr,\n    ...                                   hop_length=hop_length)\n    >>> # Plot on a BPM axis.  We skip the first (0-lag) bin.\n    >>> plt.figure(figsize=(8,4))\n    >>> plt.semilogx(freqs[1:], librosa.util.normalize(ac)[1:],\n    ...              label='Onset autocorrelation', basex=2)\n    >>> plt.axvline(tempo, 0, 1, color='r', alpha=0.75, linestyle='--',\n    ...            label='Tempo: {:.2f} BPM'.format(tempo))\n    >>> plt.xlabel('Tempo (BPM)')\n    >>> plt.grid()\n    >>> plt.title('Static tempo estimation')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis('tight')\n\n    Plot dynamic tempo estimates over a tempogram\n\n    >>> plt.figure()\n    >>> tg = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr,\n    ...                                hop_length=hop_length)\n    >>> librosa.display.specshow(tg, x_axis='time', y_axis='tempo')\n    >>> plt.plot(librosa.frames_to_time(np.arange(len(dtempo))), dtempo,\n    ...          color='w', linewidth=1.5, label='Tempo estimate')\n    >>> plt.title('Dynamic tempo estimation')\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    "
if (start_bpm <= 0):
    raise ParameterError('start_bpm must be strictly positive')
win_length = numpy.asscalar(core.time_to_frames(ac_size, sr=sr, hop_length=hop_length))
tg = tempogram(y=y, sr=sr, onset_envelope=onset_envelope, hop_length=hop_length, win_length=win_length)
if (aggregate is not None):
    tg = aggregate(tg, axis=1, keepdims=True)
bpms = core.tempo_frequencies(tg.shape[0], hop_length=hop_length, sr=sr)
tempResult = exp(((- 0.5) * (((numpy.log2(bpms) - numpy.log2(start_bpm)) / std_bpm) ** 2)))
	
===================================================================	
__beat_local_score: 98	
----------------------------	

'Construct the local score for an onset envlope and given period'
tempResult = exp(((- 0.5) * (((numpy.arange((- period), (period + 1)) * 32.0) / period) ** 2)))
	
===================================================================	
estimate_tempo: 46	
----------------------------	

"Estimate the tempo (beats per minute) from an onset envelope\n\n    .. warning:: Deprecated in librosa 0.5\n                 Functionality is superseded by\n                 `librosa.beat.tempo`.\n\n    Parameters\n    ----------\n    onset_envelope    : np.ndarray [shape=(n,)]\n        onset strength envelope\n\n    sr : number > 0 [scalar]\n        sampling rate of the time series\n\n    hop_length : int > 0 [scalar]\n        hop length of the time series\n\n    start_bpm : float [scalar]\n        initial guess of the BPM\n\n    std_bpm : float > 0 [scalar]\n        standard deviation of tempo distribution\n\n    ac_size : float > 0 [scalar]\n        length (in seconds) of the auto-correlation window\n\n    duration : float > 0 [scalar]\n        length of signal (in seconds) to use in estimating tempo\n\n    offset : float > 0 [scalar]\n        offset (in seconds) of signal sample to use in estimating tempo\n\n\n    Returns\n    -------\n    tempo : float [scalar]\n        estimated tempo (beats per minute)\n\n\n    See Also\n    --------\n    librosa.onset.onset_strength\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> onset_env = librosa.onset.onset_strength(y, sr=sr)\n    >>> tempo = librosa.beat.estimate_tempo(onset_env, sr=sr)\n    >>> tempo\n    103.359375\n\n    Plot the estimated tempo against the onset autocorrelation\n\n    >>> import matplotlib.pyplot as plt\n    >>> # Compute 2-second windowed autocorrelation\n    >>> hop_length = 512\n    >>> ac = librosa.autocorrelate(onset_env, 2 * sr // hop_length)\n    >>> freqs = librosa.tempo_frequencies(len(ac), sr=sr,\n    ...                                   hop_length=hop_length)\n    >>> # Plot on a BPM axis.  We skip the first (0-lag) bin.\n    >>> plt.figure(figsize=(8,4))\n    >>> plt.semilogx(freqs[1:], librosa.util.normalize(ac)[1:],\n    ...              label='Onset autocorrelation', basex=2)\n    >>> plt.axvline(tempo, 0, 1, color='r', alpha=0.75, linestyle='--',\n    ...            label='Tempo: {:.2f} BPM'.format(tempo))\n    >>> plt.xlabel('Tempo (BPM)')\n    >>> plt.grid()\n    >>> plt.legend(frameon=True)\n    >>> plt.axis('tight')\n    "
if (start_bpm <= 0):
    raise ParameterError('start_bpm must be strictly positive')
fft_res = (float(sr) / hop_length)
maxcol = int(min((len(onset_envelope) - 1), numpy.round(((offset + duration) * fft_res))))
mincol = int(max(0, (maxcol - numpy.round((duration * fft_res)))))
ac_window = min(maxcol, numpy.round((ac_size * fft_res)))
x_corr = core.autocorrelate(onset_envelope[mincol:maxcol], ac_window)[1:]
bpms = core.tempo_frequencies(ac_window, hop_length=hop_length, sr=sr)[1:]
tempResult = exp(((- 0.5) * (((numpy.log2(bpms) - numpy.log2(start_bpm)) / std_bpm) ** 2)))
	
===================================================================	
chroma: 61	
----------------------------	

"Create a Filterbank matrix to convert STFT to chroma\n\n\n    Parameters\n    ----------\n    sr        : number > 0 [scalar]\n        audio sampling rate\n\n    n_fft     : int > 0 [scalar]\n        number of FFT bins\n\n    n_chroma  : int > 0 [scalar]\n        number of chroma bins\n\n    A440      : float > 0 [scalar]\n        Reference frequency for A440\n\n    ctroct    : float > 0 [scalar]\n\n    octwidth  : float > 0 or None [scalar]\n        `ctroct` and `octwidth` specify a dominance window -\n        a Gaussian weighting centered on `ctroct` (in octs, A0 = 27.5Hz)\n        and with a gaussian half-width of `octwidth`.\n        Set `octwidth` to `None` to use a flat weighting.\n\n    norm : float > 0 or np.inf\n        Normalization factor for each filter\n\n    base_c : bool\n        If True, the filter bank will start at 'C'.\n        If False, the filter bank will start at 'A'.\n\n    Returns\n    -------\n    wts : ndarray [shape=(n_chroma, 1 + n_fft / 2)]\n        Chroma filter matrix\n\n    See Also\n    --------\n    util.normalize\n    feature.chroma_stft\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    Examples\n    --------\n    Build a simple chroma filter bank\n\n    >>> chromafb = librosa.filters.chroma(22050, 4096)\n    array([[  1.689e-05,   3.024e-04, ...,   4.639e-17,   5.327e-17],\n           [  1.716e-05,   2.652e-04, ...,   2.674e-25,   3.176e-25],\n    ...,\n           [  1.578e-05,   3.619e-04, ...,   8.577e-06,   9.205e-06],\n           [  1.643e-05,   3.355e-04, ...,   1.474e-10,   1.636e-10]])\n\n    Use quarter-tones instead of semitones\n\n    >>> librosa.filters.chroma(22050, 4096, n_chroma=24)\n    array([[  1.194e-05,   2.138e-04, ...,   6.297e-64,   1.115e-63],\n           [  1.206e-05,   2.009e-04, ...,   1.546e-79,   2.929e-79],\n    ...,\n           [  1.162e-05,   2.372e-04, ...,   6.417e-38,   9.923e-38],\n           [  1.180e-05,   2.260e-04, ...,   4.697e-50,   7.772e-50]])\n\n\n    Equally weight all octaves\n\n    >>> librosa.filters.chroma(22050, 4096, octwidth=None)\n    array([[  3.036e-01,   2.604e-01, ...,   2.445e-16,   2.809e-16],\n           [  3.084e-01,   2.283e-01, ...,   1.409e-24,   1.675e-24],\n    ...,\n           [  2.836e-01,   3.116e-01, ...,   4.520e-05,   4.854e-05],\n           [  2.953e-01,   2.888e-01, ...,   7.768e-10,   8.629e-10]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> librosa.display.specshow(chromafb, x_axis='linear')\n    >>> plt.ylabel('Chroma filter')\n    >>> plt.title('Chroma filter bank')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n    "
wts = numpy.zeros((n_chroma, n_fft))
frequencies = numpy.linspace(0, sr, n_fft, endpoint=False)[1:]
frqbins = (n_chroma * hz_to_octs(frequencies, A440))
frqbins = numpy.concatenate(([(frqbins[0] - (1.5 * n_chroma))], frqbins))
binwidthbins = numpy.concatenate((numpy.maximum((frqbins[1:] - frqbins[:(- 1)]), 1.0), [1]))
D = np.subtract.outer(frqbins, np.arange(0, n_chroma, dtype='d')).T
n_chroma2 = numpy.round((float(n_chroma) / 2))
D = (numpy.remainder(((D + n_chroma2) + (10 * n_chroma)), n_chroma) - n_chroma2)
tempResult = exp(((- 0.5) * (((2 * D) / numpy.tile(binwidthbins, (n_chroma, 1))) ** 2)))
	
===================================================================	
chroma: 64	
----------------------------	

"Create a Filterbank matrix to convert STFT to chroma\n\n\n    Parameters\n    ----------\n    sr        : number > 0 [scalar]\n        audio sampling rate\n\n    n_fft     : int > 0 [scalar]\n        number of FFT bins\n\n    n_chroma  : int > 0 [scalar]\n        number of chroma bins\n\n    A440      : float > 0 [scalar]\n        Reference frequency for A440\n\n    ctroct    : float > 0 [scalar]\n\n    octwidth  : float > 0 or None [scalar]\n        `ctroct` and `octwidth` specify a dominance window -\n        a Gaussian weighting centered on `ctroct` (in octs, A0 = 27.5Hz)\n        and with a gaussian half-width of `octwidth`.\n        Set `octwidth` to `None` to use a flat weighting.\n\n    norm : float > 0 or np.inf\n        Normalization factor for each filter\n\n    base_c : bool\n        If True, the filter bank will start at 'C'.\n        If False, the filter bank will start at 'A'.\n\n    Returns\n    -------\n    wts : ndarray [shape=(n_chroma, 1 + n_fft / 2)]\n        Chroma filter matrix\n\n    See Also\n    --------\n    util.normalize\n    feature.chroma_stft\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    Examples\n    --------\n    Build a simple chroma filter bank\n\n    >>> chromafb = librosa.filters.chroma(22050, 4096)\n    array([[  1.689e-05,   3.024e-04, ...,   4.639e-17,   5.327e-17],\n           [  1.716e-05,   2.652e-04, ...,   2.674e-25,   3.176e-25],\n    ...,\n           [  1.578e-05,   3.619e-04, ...,   8.577e-06,   9.205e-06],\n           [  1.643e-05,   3.355e-04, ...,   1.474e-10,   1.636e-10]])\n\n    Use quarter-tones instead of semitones\n\n    >>> librosa.filters.chroma(22050, 4096, n_chroma=24)\n    array([[  1.194e-05,   2.138e-04, ...,   6.297e-64,   1.115e-63],\n           [  1.206e-05,   2.009e-04, ...,   1.546e-79,   2.929e-79],\n    ...,\n           [  1.162e-05,   2.372e-04, ...,   6.417e-38,   9.923e-38],\n           [  1.180e-05,   2.260e-04, ...,   4.697e-50,   7.772e-50]])\n\n\n    Equally weight all octaves\n\n    >>> librosa.filters.chroma(22050, 4096, octwidth=None)\n    array([[  3.036e-01,   2.604e-01, ...,   2.445e-16,   2.809e-16],\n           [  3.084e-01,   2.283e-01, ...,   1.409e-24,   1.675e-24],\n    ...,\n           [  2.836e-01,   3.116e-01, ...,   4.520e-05,   4.854e-05],\n           [  2.953e-01,   2.888e-01, ...,   7.768e-10,   8.629e-10]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> librosa.display.specshow(chromafb, x_axis='linear')\n    >>> plt.ylabel('Chroma filter')\n    >>> plt.title('Chroma filter bank')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n    "
wts = numpy.zeros((n_chroma, n_fft))
frequencies = numpy.linspace(0, sr, n_fft, endpoint=False)[1:]
frqbins = (n_chroma * hz_to_octs(frequencies, A440))
frqbins = numpy.concatenate(([(frqbins[0] - (1.5 * n_chroma))], frqbins))
binwidthbins = numpy.concatenate((numpy.maximum((frqbins[1:] - frqbins[:(- 1)]), 1.0), [1]))
D = np.subtract.outer(frqbins, np.arange(0, n_chroma, dtype='d')).T
n_chroma2 = numpy.round((float(n_chroma) / 2))
D = (numpy.remainder(((D + n_chroma2) + (10 * n_chroma)), n_chroma) - n_chroma2)
wts = numpy.exp(((- 0.5) * (((2 * D) / numpy.tile(binwidthbins, (n_chroma, 1))) ** 2)))
wts = util.normalize(wts, norm=norm, axis=0)
if (octwidth is not None):
    tempResult = exp(((- 0.5) * ((((frqbins / n_chroma) - ctroct) / octwidth) ** 2)))
	
===================================================================	
constant_q: 94	
----------------------------	

'Construct a constant-Q basis.\n\n    This uses the filter bank described by [1]_.\n\n    .. [1] McVicar, Matthew.\n            "A machine learning approach to automatic chord extraction."\n            Dissertation, University of Bristol. 2013.\n\n\n    Parameters\n    ----------\n    sr : number > 0 [scalar]\n        Audio sampling rate\n\n    fmin : float > 0 [scalar]\n        Minimum frequency bin. Defaults to `C1 ~= 32.70`\n\n    n_bins : int > 0 [scalar]\n        Number of frequencies.  Defaults to 7 octaves (84 bins).\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)` [scalar]\n        Tuning deviation from A440 in fractions of a bin\n\n    window : string, tuple, number, or function\n        Windowing function to apply to filters.\n\n    filter_scale : float > 0 [scalar]\n        Scale of filter windows.\n        Small values (<1) use shorter windows for higher temporal resolution.\n\n    pad_fft : boolean\n        Center-pad all filters up to the nearest integral power of 2.\n\n        By default, padding is done with zeros, but this can be overridden\n        by setting the `mode=` field in *kwargs*.\n\n    norm : {inf, -inf, 0, float > 0}\n        Type of norm to use for basis function normalization.\n        See librosa.util.normalize\n\n    kwargs : additional keyword arguments\n        Arguments to `np.pad()` when `pad==True`.\n\n    Returns\n    -------\n    filters : np.ndarray, `len(filters) == n_bins`\n        `filters[i]` is `i`\\ th time-domain CQT basis filter\n\n    lengths : np.ndarray, `len(lengths) == n_bins`\n        The (fractional) length of each filter\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    See Also\n    --------\n    constant_q_lengths\n    librosa.core.cqt\n    librosa.util.normalize\n\n\n    Examples\n    --------\n    Use a shorter window for each filter\n\n    >>> basis, lengths = librosa.filters.constant_q(22050, filter_scale=0.5)\n\n    Plot one octave of filters in time and frequency\n\n    >>> import matplotlib.pyplot as plt\n    >>> basis, lengths = librosa.filters.constant_q(22050)\n    >>> plt.figure(figsize=(10, 6))\n    >>> plt.subplot(2, 1, 1)\n    >>> notes = librosa.midi_to_note(np.arange(24, 24 + len(basis)))\n    >>> for i, (f, n) in enumerate(zip(basis, notes[:12])):\n    ...     f_scale = librosa.util.normalize(f) / 2\n    ...     plt.plot(i + f_scale.real)\n    ...     plt.plot(i + f_scale.imag, linestyle=\':\')\n    >>> plt.axis(\'tight\')\n    >>> plt.yticks(np.arange(len(notes[:12])), notes[:12])\n    >>> plt.ylabel(\'CQ filters\')\n    >>> plt.title(\'CQ filters (one octave, time domain)\')\n    >>> plt.xlabel(\'Time (samples at 22050 Hz)\')\n    >>> plt.legend([\'Real\', \'Imaginary\'], frameon=True, framealpha=0.8)\n    >>> plt.subplot(2, 1, 2)\n    >>> F = np.abs(np.fft.fftn(basis, axes=[-1]))\n    >>> # Keep only the positive frequencies\n    >>> F = F[:, :(1 + F.shape[1] // 2)]\n    >>> librosa.display.specshow(F, x_axis=\'linear\')\n    >>> plt.yticks(np.arange(len(notes))[::12], notes[::12])\n    >>> plt.ylabel(\'CQ filters\')\n    >>> plt.title(\'CQ filter magnitudes (frequency domain)\')\n    >>> plt.tight_layout()\n    '
if (fmin is None):
    fmin = note_to_hz('C1')
lengths = constant_q_lengths(sr, fmin, n_bins=n_bins, bins_per_octave=bins_per_octave, tuning=tuning, window=window, filter_scale=filter_scale)
correction = (2.0 ** (float(tuning) / bins_per_octave))
fmin = (correction * fmin)
Q = (float(filter_scale) / ((2.0 ** (1.0 / bins_per_octave)) - 1))
freqs = ((Q * sr) / lengths)
filters = []
for (ilen, freq) in zip(lengths, freqs):
    tempResult = exp((((((numpy.arange(ilen, dtype=float) * 1j) * 2) * numpy.pi) * freq) / sr))
	
===================================================================	
recurrence_matrix: 61	
----------------------------	

"Compute a recurrence matrix from a data matrix.\n\n\n    `rec[i, j]` is non-zero if (`data[:, i]`, `data[:, j]`) are\n    k-nearest-neighbors and `|i - j| >= width`\n\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A feature matrix\n\n    k : int > 0 [scalar] or None\n        the number of nearest-neighbors for each sample\n\n        Default: `k = 2 * ceil(sqrt(t - 2 * width + 1))`,\n        or `k = 2` if `t <= 2 * width + 1`\n\n    width : int >= 1 [scalar]\n        only link neighbors `(data[:, i], data[:, j])`\n        if `|i - j| >= width`\n\n    metric : str\n        Distance metric to use for nearest-neighbor calculation.\n\n        See `sklearn.neighbors.NearestNeighbors` for details.\n\n    sym : bool [scalar]\n        set `sym=True` to only link mutual nearest-neighbors\n\n    sparse : bool [scalar]\n        if False, returns a dense type (ndarray)\n        if True, returns a sparse type (scipy.sparse.csr_matrix)\n\n    mode : str, {'connectivity', 'distance', 'affinity'}\n        If 'connectivity', a binary connectivity matrix is produced.\n\n        If 'distance', then a non-zero entry contains the distance between\n        points.\n\n        If 'adjacency', then non-zero entries are mapped to\n        `exp( - distance(i, j) / bandwidth)` where `bandwidth` is\n        as specified below.\n\n    bandwidth : None or float > 0\n        If using ``mode='affinity'``, this can be used to set the\n        bandwidth on the affinity kernel.\n\n        If no value is provided, it is set automatically to the median\n        distance between furthest nearest neighbors.\n\n    axis : int\n        The axis along which to compute recurrence.\n        By default, the last index (-1) is taken.\n\n    Returns\n    -------\n    rec : np.ndarray or scipy.sparse.csr_matrix, [shape=(t, t)]\n        Recurrence matrix\n\n    See Also\n    --------\n    sklearn.neighbors.NearestNeighbors\n    scipy.spatial.distance.cdist\n    librosa.feature.stack_memory\n    recurrence_to_lag\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    Find nearest neighbors in MFCC space\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> mfcc = librosa.feature.mfcc(y=y, sr=sr)\n    >>> R = librosa.segment.recurrence_matrix(mfcc)\n\n    Or fix the number of nearest neighbors to 5\n\n    >>> R = librosa.segment.recurrence_matrix(mfcc, k=5)\n\n    Suppress neighbors within +- 7 samples\n\n    >>> R = librosa.segment.recurrence_matrix(mfcc, width=7)\n\n    Use cosine similarity instead of Euclidean distance\n\n    >>> R = librosa.segment.recurrence_matrix(mfcc, metric='cosine')\n\n    Require mutual nearest neighbors\n\n    >>> R = librosa.segment.recurrence_matrix(mfcc, sym=True)\n\n    Use an affinity matrix instead of binary connectivity\n\n    >>> R_aff = librosa.segment.recurrence_matrix(mfcc, mode='affinity')\n\n    Plot the feature and recurrence matrices\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> librosa.display.specshow(R, x_axis='time', y_axis='time')\n    >>> plt.title('Binary recurrence (symmetric)')\n    >>> plt.subplot(1, 2, 2)\n    >>> librosa.display.specshow(R_aff, x_axis='time', y_axis='time',\n    ...                          cmap='magma_r')\n    >>> plt.title('Affinity recurrence')\n    >>> plt.tight_layout()\n\n    "
data = numpy.atleast_2d(data)
data = numpy.swapaxes(data, axis, 0)
t = data.shape[0]
data = data.reshape((t, (- 1)))
if (width < 1):
    raise ParameterError('width must be at least 1')
if (mode not in ['connectivity', 'distance', 'affinity']):
    raise ParameterError("Invalid mode='{}'. Must be one of ['connectivity', 'distance', 'affinity']".format(mode))
if (k is None):
    if (t > ((2 * width) + 1)):
        k = (2 * numpy.ceil(numpy.sqrt(((t - (2 * width)) + 1))))
    else:
        k = 2
if (bandwidth is not None):
    if (bandwidth <= 0):
        raise ParameterError('Invalid bandwidth={}. Must be strictly positive.'.format(bandwidth))
k = int(k)
try:
    knn = sklearn.neighbors.NearestNeighbors(n_neighbors=min((t - 1), (k + (2 * width))), metric=metric, algorithm='auto')
except ValueError:
    knn = sklearn.neighbors.NearestNeighbors(n_neighbors=min((t - 1), (k + (2 * width))), metric=metric, algorithm='brute')
knn.fit(data)
if (mode == 'affinity'):
    kng_mode = 'distance'
else:
    kng_mode = mode
rec = knn.kneighbors_graph(mode=kng_mode).tolil()
for diag in range(((- width) + 1), width):
    rec.setdiag(0, diag)
for i in range(t):
    links = rec[i].nonzero()[1]
    idx = links[numpy.argsort(rec[(i, links)].toarray())][0]
    rec[(i, idx[k:])] = 0
if sym:
    rec = rec.minimum(rec.T)
rec = rec.tocsr()
rec.eliminate_zeros()
if (mode == 'connectivity'):
    rec = rec.astype(numpy.bool)
elif (mode == 'affinity'):
    if (bandwidth is None):
        bandwidth = numpy.median(rec.max(axis=1).data)
    tempResult = exp(((- rec.data) / bandwidth))
	
===================================================================	
phase_vocoder: 121	
----------------------------	

'Phase vocoder.  Given an STFT matrix D, speed up by a factor of `rate`\n\n    Based on the implementation provided by [1]_.\n\n    .. [1] Ellis, D. P. W. "A phase vocoder in Matlab."\n        Columbia University, 2002.\n        http://www.ee.columbia.edu/~dpwe/resources/matlab/pvoc/\n\n    Examples\n    --------\n    >>> # Play at double speed\n    >>> y, sr   = librosa.load(librosa.util.example_audio_file())\n    >>> D       = librosa.stft(y, n_fft=2048, hop_length=512)\n    >>> D_fast  = librosa.phase_vocoder(D, 2.0, hop_length=512)\n    >>> y_fast  = librosa.istft(D_fast, hop_length=512)\n\n    >>> # Or play at 1/3 speed\n    >>> y, sr   = librosa.load(librosa.util.example_audio_file())\n    >>> D       = librosa.stft(y, n_fft=2048, hop_length=512)\n    >>> D_slow  = librosa.phase_vocoder(D, 1./3, hop_length=512)\n    >>> y_slow  = librosa.istft(D_slow, hop_length=512)\n\n    Parameters\n    ----------\n    D : np.ndarray [shape=(d, t), dtype=complex]\n        STFT matrix\n\n    rate :  float > 0 [scalar]\n        Speed-up factor: `rate > 1` is faster, `rate < 1` is slower.\n\n    hop_length : int > 0 [scalar] or None\n        The number of samples between successive columns of `D`.\n\n        If None, defaults to `n_fft/4 = (D.shape[0]-1)/2`\n\n    Returns\n    -------\n    D_stretched  : np.ndarray [shape=(d, t / rate), dtype=complex]\n        time-stretched STFT\n    '
n_fft = (2 * (D.shape[0] - 1))
if (hop_length is None):
    hop_length = int((n_fft // 4))
time_steps = numpy.arange(0, D.shape[1], rate, dtype=numpy.float)
d_stretch = numpy.zeros((D.shape[0], len(time_steps)), D.dtype, order='F')
phi_advance = numpy.linspace(0, (numpy.pi * hop_length), D.shape[0])
phase_acc = numpy.angle(D[:, 0])
D = numpy.pad(D, [(0, 0), (0, 2)], mode='constant')
for (t, step) in enumerate(time_steps):
    columns = D[:, int(step):int((step + 2))]
    alpha = numpy.mod(step, 1.0)
    mag = (((1.0 - alpha) * numpy.abs(columns[:, 0])) + (alpha * numpy.abs(columns[:, 1])))
    tempResult = exp((1j * phase_acc))
	
===================================================================	
magphase: 104	
----------------------------	

'Separate a complex-valued spectrogram D into its magnitude (S)\n    and phase (P) components, so that `D = S * P`.\n\n\n    Parameters\n    ----------\n    D       : np.ndarray [shape=(d, t), dtype=complex]\n        complex-valued spectrogram\n\n\n    Returns\n    -------\n    D_mag   : np.ndarray [shape=(d, t), dtype=real]\n        magnitude of `D`\n    D_phase : np.ndarray [shape=(d, t), dtype=complex]\n        `exp(1.j * phi)` where `phi` is the phase of `D`\n\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> D = librosa.stft(y)\n    >>> magnitude, phase = librosa.magphase(D)\n    >>> magnitude\n    array([[  2.524e-03,   4.329e-02, ...,   3.217e-04,   3.520e-05],\n           [  2.645e-03,   5.152e-02, ...,   3.283e-04,   3.432e-04],\n           ...,\n           [  1.966e-05,   9.828e-06, ...,   3.164e-07,   9.370e-06],\n           [  1.966e-05,   9.830e-06, ...,   3.161e-07,   9.366e-06]], dtype=float32)\n    >>> phase\n    array([[  1.000e+00 +0.000e+00j,   1.000e+00 +0.000e+00j, ...,\n             -1.000e+00 +8.742e-08j,  -1.000e+00 +8.742e-08j],\n           [  1.000e+00 +1.615e-16j,   9.950e-01 -1.001e-01j, ...,\n              9.794e-01 +2.017e-01j,   1.492e-02 -9.999e-01j],\n           ...,\n           [  1.000e+00 -5.609e-15j,  -5.081e-04 +1.000e+00j, ...,\n             -9.549e-01 -2.970e-01j,   2.938e-01 -9.559e-01j],\n           [ -1.000e+00 +8.742e-08j,  -1.000e+00 +8.742e-08j, ...,\n             -1.000e+00 +8.742e-08j,  -1.000e+00 +8.742e-08j]], dtype=complex64)\n\n\n    Or get the phase angle (in radians)\n\n    >>> np.angle(phase)\n    array([[  0.000e+00,   0.000e+00, ...,   3.142e+00,   3.142e+00],\n           [  1.615e-16,  -1.003e-01, ...,   2.031e-01,  -1.556e+00],\n           ...,\n           [ -5.609e-15,   1.571e+00, ...,  -2.840e+00,  -1.273e+00],\n           [  3.142e+00,   3.142e+00, ...,   3.142e+00,   3.142e+00]], dtype=float32)\n\n    '
mag = numpy.abs(D)
tempResult = exp((1j * numpy.angle(D)))
	
===================================================================	
fmt: 193	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    n_fmt = int(numpy.ceil(((over_sample * (numpy.log((n - 1)) - numpy.log(t_min))) / log_base)))
elif (n_fmt < 3):
    raise ParameterError('n_fmt=={:} < 3'.format(n_fmt))
else:
    log_base = ((numpy.log((n_fmt - 1)) - numpy.log((n_fmt - 2))) / over_sample)
if (not numpy.all(numpy.isfinite(y))):
    raise ParameterError('y must be finite everywhere')
tempResult = exp(log_base)
	
===================================================================	
mel_to_hz: 127	
----------------------------	

'Convert mel bin numbers to frequencies\n\n    Examples\n    --------\n    >>> librosa.mel_to_hz(3)\n    array([ 200.])\n\n    >>> librosa.mel_to_hz([1,2,3,4,5])\n    array([  66.667,  133.333,  200.   ,  266.667,  333.333])\n\n    Parameters\n    ----------\n    mels          : np.ndarray [shape=(n,)], float\n        mel bins to convert\n    htk           : bool\n        use HTK formula instead of Slaney\n\n    Returns\n    -------\n    frequencies   : np.ndarray [shape=(n,)]\n        input mels in Hz\n\n    See Also\n    --------\n    hz_to_mel\n    '
mels = numpy.atleast_1d(mels)
if htk:
    return (700.0 * ((10.0 ** (mels / 2595.0)) - 1.0))
f_min = 0.0
f_sp = (200.0 / 3)
freqs = (f_min + (f_sp * mels))
min_log_hz = 1000.0
min_log_mel = ((min_log_hz - f_min) / f_sp)
logstep = (numpy.log(6.4) / 27.0)
log_t = (mels >= min_log_mel)
tempResult = exp((logstep * (mels[log_t] - min_log_mel)))
	
===================================================================	
Get no callers of function numpy.exp at line 477 col 16.	
===================================================================	
Get no callers of function numpy.exp at line 332 col 41.	
***************************************************	
mne_python-0.15.0: 14	
===================================================================	
minimum_phase: 294	
----------------------------	

'Convert a linear-phase FIR filter to minimum phase.\n\n    Parameters\n    ----------\n    h : array\n        Linear-phase FIR filter coefficients.\n\n    Returns\n    -------\n    h_minimum : array\n        The minimum-phase version of the filter, with length\n        ``(length(h) + 1) // 2``.\n    '
try:
    from scipy.signal import minimum_phase
except Exception:
    pass
else:
    return minimum_phase(h)
from scipy.fftpack import fft, ifft
h = numpy.asarray(h)
if numpy.iscomplexobj(h):
    raise ValueError('Complex filters not supported')
if ((h.ndim != 1) or (h.size <= 2)):
    raise ValueError('h must be 1D and at least 2 samples long')
n_half = (len(h) // 2)
if (not numpy.allclose(h[(- n_half):][::(- 1)], h[:n_half])):
    warnings.warn('h does not appear to by symmetric, conversion may fail', RuntimeWarning)
n_fft = (2 ** int(numpy.ceil(numpy.log2(((2 * (len(h) - 1)) / 0.01)))))
h_temp = numpy.abs(fft(h, n_fft))
h_temp += (1e-07 * h_temp[(h_temp > 0)].min())
numpy.log(h_temp, out=h_temp)
h_temp *= 0.5
h_temp = ifft(h_temp).real
win = numpy.zeros(n_fft)
win[0] = 1
stop = ((len(h) + 1) // 2)
win[1:stop] = 2
if (len(h) % 2):
    win[stop] = 1
h_temp *= win
tempResult = exp(fft(h_temp))
	
===================================================================	
_sph_harm: 311	
----------------------------	

"Evaluate point in specified multipolar moment.\n\n    When using, pay close attention to inputs. Spherical harmonic notation for\n    order/degree, and theta/phi are both reversed in original SSS work compared\n    to many other sources. See mathworld.wolfram.com/SphericalHarmonic.html for\n    more discussion.\n\n    Note that scipy has ``scipy.special.sph_harm``, but that function is\n    too slow on old versions (< 0.15) for heavy use.\n\n    Parameters\n    ----------\n    order : int\n        Order of spherical harmonic. (Usually) corresponds to 'm'.\n    degree : int\n        Degree of spherical harmonic. (Usually) corresponds to 'l'.\n    az : float\n        Azimuthal (longitudinal) spherical coordinate [0, 2*pi]. 0 is aligned\n        with x-axis.\n    pol : float\n        Polar (or colatitudinal) spherical coordinate [0, pi]. 0 is aligned\n        with z-axis.\n    norm : bool\n        If True, include normalization factor.\n\n    Returns\n    -------\n    base : complex float\n        The spherical harmonic value.\n    "
from scipy.special import lpmv
from .preprocessing.maxwell import _sph_harm_norm
if (numpy.abs(order) > degree):
    raise ValueError('Absolute value of order must be <= degree')
az = numpy.asarray(az)
pol = numpy.asarray(pol)
if (np.abs(az) > (2 * np.pi)).any():
    raise ValueError('Azimuth coords must lie in [-2*pi, 2*pi]')
if ((pol < 0).any() or (pol > np.pi).any()):
    raise ValueError('Polar coords must lie in [0, pi]')
tempResult = exp(((1j * order) * az))
	
===================================================================	
simu_data: 38	
----------------------------	

'Simulate an evoked dataset with 2 sources.\n\n    One source is put in each hemisphere.\n    '
(mu, sigma) = (0.1, 0.005)
tempResult = exp(((- ((times - mu) ** 2)) / (2 * (sigma ** 2))))
	
===================================================================	
simu_data: 40	
----------------------------	

'Simulate an evoked dataset with 2 sources.\n\n    One source is put in each hemisphere.\n    '
(mu, sigma) = (0.1, 0.005)
s1 = ((1 / (sigma * numpy.sqrt((2 * numpy.pi)))) * numpy.exp(((- ((times - mu) ** 2)) / (2 * (sigma ** 2)))))
(mu, sigma) = (0.075, 0.008)
tempResult = exp(((- ((times - mu) ** 2)) / (2 * (sigma ** 2))))
	
===================================================================	
_prob_kuiper: 53	
----------------------------	

'Test for statistical significance against uniform distribution.\n\n    Parameters\n    ----------\n    d : float\n        The kuiper distance value.\n    n_eff : int\n        The effective number of elements.\n    dtype : str | obj\n        The data type to be used. Defaults to double precision floats.\n\n    Returns\n    -------\n    pk_norm : float\n        The normalized Kuiper value such that 0 < ``pk_norm`` < 1.\n\n    References\n    ----------\n    [1] Stephens MA 1970. Journal of the Royal Statistical Society, ser. B,\n    vol 32, pp 115-122.\n\n    [2] Kuiper NH 1962. Proceedings of the Koninklijke Nederlands Akademie\n    van Wetenschappen, ser Vol 63 pp 38-47\n    '
n_time_slices = numpy.size(d)
n_points = 100
en = math.sqrt(n_eff)
k_lambda = (((en + 0.155) + (0.24 / en)) * d)
l2 = (k_lambda ** 2.0)
j2 = ((numpy.arange(n_points) + 1) ** 2)
j2 = j2.repeat(n_time_slices).reshape(n_points, n_time_slices)
fact = (((4.0 * j2) * l2) - 1.0)
tempResult = exp((((- 2.0) * j2) * l2))
	
===================================================================	
infomax: 62	
----------------------------	

'Run (extended) Infomax ICA decomposition on raw data.\n\n    Parameters\n    ----------\n    data : np.ndarray, shape (n_samples, n_features)\n        The whitened data to unmix.\n    weights : np.ndarray, shape (n_features, n_features)\n        The initialized unmixing matrix.\n        Defaults to None, which means the identity matrix is used.\n    l_rate : float\n        This quantity indicates the relative size of the change in weights.\n        Defaults to ``0.01 / log(n_features ** 2)``.\n\n        .. note:: Smaller learning rates will slow down the ICA procedure.\n\n    block : int\n        The block size of randomly chosen data segments.\n        Defaults to floor(sqrt(n_times / 3.)).\n    w_change : float\n        The change at which to stop iteration. Defaults to 1e-12.\n    anneal_deg : float\n        The angle (in degrees) at which the learning rate will be reduced.\n        Defaults to 60.0.\n    anneal_step : float\n        The factor by which the learning rate will be reduced once\n        ``anneal_deg`` is exceeded: ``l_rate *= anneal_step.``\n        Defaults to 0.9.\n    extended : bool\n        Whether to use the extended Infomax algorithm or not.\n        Defaults to True.\n    n_subgauss : int\n        The number of subgaussian components. Only considered for extended\n        Infomax. Defaults to 1.\n    kurt_size : int\n        The window size for kurtosis estimation. Only considered for extended\n        Infomax. Defaults to 6000.\n    ext_blocks : int\n        Only considered for extended Infomax. If positive, denotes the number\n        of blocks after which to recompute the kurtosis, which is used to\n        estimate the signs of the sources. In this case, the number of\n        sub-gaussian sources is automatically determined.\n        If negative, the number of sub-gaussian sources to be used is fixed\n        and equal to n_subgauss. In this case, the kurtosis is not estimated.\n        Defaults to 1.\n    max_iter : int\n        The maximum number of iterations. Defaults to 200.\n    random_state : int | np.random.RandomState\n        If random_state is an int, use random_state to seed the random number\n        generator. If random_state is already a np.random.RandomState instance,\n        use random_state as random number generator.\n    blowup : float\n        The maximum difference allowed between two successive estimations of\n        the unmixing matrix. Defaults to 10000.\n    blowup_fac : float\n        The factor by which the learning rate will be reduced if the difference\n        between two successive estimations of the unmixing matrix exceededs\n        ``blowup``: ``l_rate *= blowup_fac``. Defaults to 0.5.\n    n_small_angle : int | None\n        The maximum number of allowed steps in which the angle between two\n        successive estimations of the unmixing matrix is less than\n        ``anneal_deg``. If None, this parameter is not taken into account to\n        stop the iterations. Defaults to 20.\n    use_bias : bool\n        This quantity indicates if the bias should be computed.\n        Defaults to True.\n    verbose : bool, str, int, or None\n        If not None, override default verbosity level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    unmixing_matrix : np.ndarray, shape (n_features, n_features)\n        The linear unmixing operator.\n\n    References\n    ----------\n    .. [1] A. J. Bell, T. J. Sejnowski. An information-maximization approach to\n           blind separation and blind deconvolution. Neural Computation, 7(6),\n           1129-1159, 1995.\n    .. [2] T. W. Lee, M. Girolami, T. J. Sejnowski. Independent component\n           analysis using an extended infomax algorithm for mixed subgaussian\n           and supergaussian sources. Neural Computation, 11(2), 417-441, 1999.\n    '
from scipy.stats import kurtosis
rng = check_random_state(random_state)
max_weight = 100000000.0
restart_fac = 0.9
min_l_rate = 1e-10
degconst = (180.0 / numpy.pi)
extmomentum = 0.5
signsbias = 0.02
signcount_threshold = 25
signcount_step = 2
(n_samples, n_features) = data.shape
n_features_square = (n_features ** 2)
if (l_rate is None):
    l_rate = (0.01 / math.log((n_features ** 2.0)))
if (block is None):
    block = int(math.floor(math.sqrt((n_samples / 3.0))))
utils.logger.info((('computing%sInfomax ICA' % ' Extended ') if extended else ' '))
nblock = (n_samples // block)
lastt = (((nblock - 1) * block) + 1)
if (weights is None):
    weights = numpy.identity(n_features, dtype=numpy.float64)
else:
    weights = weights.T
BI = (block * numpy.identity(n_features, dtype=numpy.float64))
bias = numpy.zeros((n_features, 1), dtype=numpy.float64)
onesrow = numpy.ones((1, block), dtype=numpy.float64)
startweights = weights.copy()
oldweights = startweights.copy()
step = 0
count_small_angle = 0
wts_blowup = False
blockno = 0
signcount = 0
initial_ext_blocks = ext_blocks
if extended:
    signs = numpy.ones(n_features)
    for k in range(n_subgauss):
        signs[k] = (- 1)
    kurt_size = min(kurt_size, n_samples)
    old_kurt = numpy.zeros(n_features, dtype=numpy.float64)
    oldsigns = numpy.zeros(n_features)
(olddelta, oldchange) = (1.0, 0.0)
while (step < max_iter):
    permute = random_permutation(n_samples, rng)
    for t in range(0, lastt, block):
        u = numpy.dot(data[permute[t:(t + block)], :], weights)
        u += np.dot(bias, onesrow).T
        if extended:
            y = numpy.tanh(u)
            weights += (l_rate * numpy.dot(weights, ((BI - (signs[None, :] * numpy.dot(u.T, y))) - numpy.dot(u.T, u))))
            if use_bias:
                bias += (l_rate * numpy.reshape((numpy.sum(y, axis=0, dtype=numpy.float64) * (- 2.0)), (n_features, 1)))
        else:
            tempResult = exp((- u))
	
===================================================================	
_sss_basis_basic: 533	
----------------------------	

'Compute SSS basis using non-optimized (but more readable) algorithms.'
(int_order, ext_order) = (exp['int_order'], exp['ext_order'])
origin = exp['origin']
if (method == 'standard'):
    (rmags, cosmags, ws, bins) = _concatenate_coils(coils)
    rmags -= origin
    (rad, az, pol) = _cart_to_sph(rmags).T
    cosmags *= ws[:, numpy.newaxis]
    del rmags, ws
    out_type = numpy.float64
else:
    (rs, wcoils, ezs, bins) = _concatenate_sph_coils(coils)
    rs -= origin
    (rad, az, pol) = _cart_to_sph(rs).T
    ezs *= wcoils[:, numpy.newaxis]
    del rs, wcoils
    out_type = numpy.complex128
del origin
(n_in, n_out) = _get_n_moments([int_order, ext_order])
S_tot = numpy.empty((len(coils), (n_in + n_out)), out_type)
S_in = S_tot[:, :n_in]
S_out = S_tot[:, n_in:]
coil_scale = numpy.ones((len(coils), 1))
coil_scale[_get_mag_mask(coils)] = mag_scale
for degree in range(1, (max(int_order, ext_order) + 1)):
    for order in range((degree + 1)):
        S_in_out = list()
        grads_in_out = list()
        sph = _get_sph_harm()(order, degree, az, pol)
        sph_norm = _sph_harm_norm(order, degree)
        az_factor = (((1j * order) * sph) / numpy.sin(numpy.maximum(pol, 1e-16)))
        tempResult = exp(((1j * order) * az))
	
===================================================================	
_make_dpss: 68	
----------------------------	

'Compute DPSS tapers for the given frequency range.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sampling frequency.\n    freqs : ndarray, shape (n_freqs,)\n        The frequencies in Hz.\n    n_cycles : float | ndarray, shape (n_freqs,), defaults to 7.\n        The number of cycles globally or for each frequency.\n    time_bandwidth : float, defaults to 4.0\n        Time x Bandwidth product.\n        The number of good tapers (low-bias) is chosen automatically based on\n        this to equal floor(time_bandwidth - 1).\n        Default is 4.0, giving 3 good tapers.\n    zero_mean : bool | None, , defaults to False\n        Make sure the wavelet has a mean of zero.\n\n\n    Returns\n    -------\n    Ws : list of array\n        The wavelets time series.\n    '
Ws = list()
if (time_bandwidth < 2.0):
    raise ValueError('time_bandwidth should be >= 2.0 for good tapers')
n_taps = int(numpy.floor((time_bandwidth - 1)))
n_cycles = numpy.atleast_1d(n_cycles)
if ((n_cycles.size != 1) and (n_cycles.size != len(freqs))):
    raise ValueError('n_cycles should be fixed or defined for each frequency.')
for m in range(n_taps):
    Wm = list()
    for (k, f) in enumerate(freqs):
        if (len(n_cycles) != 1):
            this_n_cycles = n_cycles[k]
        else:
            this_n_cycles = n_cycles[0]
        t_win = (this_n_cycles / float(f))
        t = numpy.arange(0.0, t_win, (1.0 / sfreq))
        tempResult = exp(((((2.0 * 1j) * numpy.pi) * f) * (t - (t_win / 2.0))))
	
===================================================================	
morlet: 40	
----------------------------	

'Compute Morlet wavelets for the given frequency range.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sampling Frequency.\n    freqs : array\n        frequency range of interest (1 x Frequencies)\n    n_cycles: float | array of float, defaults to 7.0\n        Number of cycles. Fixed number or one per frequency.\n    sigma : float, defaults to None\n        It controls the width of the wavelet ie its temporal\n        resolution. If sigma is None the temporal resolution\n        is adapted with the frequency like for all wavelet transform.\n        The higher the frequency the shorter is the wavelet.\n        If sigma is fixed the temporal resolution is fixed\n        like for the short time Fourier transform and the number\n        of oscillations increases with the frequency.\n    zero_mean : bool, defaults to False\n        Make sure the wavelet has a mean of zero.\n\n    Returns\n    -------\n    Ws : list of array\n        The wavelets time series.\n    '
Ws = list()
n_cycles = numpy.atleast_1d(n_cycles)
if ((n_cycles.size != 1) and (n_cycles.size != len(freqs))):
    raise ValueError('n_cycles should be fixed or defined for each frequency.')
for (k, f) in enumerate(freqs):
    if (len(n_cycles) != 1):
        this_n_cycles = n_cycles[k]
    else:
        this_n_cycles = n_cycles[0]
    if (sigma is None):
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * f))
    else:
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * sigma))
    t = numpy.arange(0.0, (5.0 * sigma_t), (1.0 / sfreq))
    t = numpy.r_[((- t[::(- 1)]), t[1:])]
    tempResult = exp(((((2.0 * 1j) * numpy.pi) * f) * t))
	
===================================================================	
morlet: 41	
----------------------------	

'Compute Morlet wavelets for the given frequency range.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sampling Frequency.\n    freqs : array\n        frequency range of interest (1 x Frequencies)\n    n_cycles: float | array of float, defaults to 7.0\n        Number of cycles. Fixed number or one per frequency.\n    sigma : float, defaults to None\n        It controls the width of the wavelet ie its temporal\n        resolution. If sigma is None the temporal resolution\n        is adapted with the frequency like for all wavelet transform.\n        The higher the frequency the shorter is the wavelet.\n        If sigma is fixed the temporal resolution is fixed\n        like for the short time Fourier transform and the number\n        of oscillations increases with the frequency.\n    zero_mean : bool, defaults to False\n        Make sure the wavelet has a mean of zero.\n\n    Returns\n    -------\n    Ws : list of array\n        The wavelets time series.\n    '
Ws = list()
n_cycles = numpy.atleast_1d(n_cycles)
if ((n_cycles.size != 1) and (n_cycles.size != len(freqs))):
    raise ValueError('n_cycles should be fixed or defined for each frequency.')
for (k, f) in enumerate(freqs):
    if (len(n_cycles) != 1):
        this_n_cycles = n_cycles[k]
    else:
        this_n_cycles = n_cycles[0]
    if (sigma is None):
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * f))
    else:
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * sigma))
    t = numpy.arange(0.0, (5.0 * sigma_t), (1.0 / sfreq))
    t = numpy.r_[((- t[::(- 1)]), t[1:])]
    oscillation = numpy.exp(((((2.0 * 1j) * numpy.pi) * f) * t))
    tempResult = exp(((- (t ** 2)) / (2.0 * (sigma_t ** 2))))
	
===================================================================	
morlet: 43	
----------------------------	

'Compute Morlet wavelets for the given frequency range.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sampling Frequency.\n    freqs : array\n        frequency range of interest (1 x Frequencies)\n    n_cycles: float | array of float, defaults to 7.0\n        Number of cycles. Fixed number or one per frequency.\n    sigma : float, defaults to None\n        It controls the width of the wavelet ie its temporal\n        resolution. If sigma is None the temporal resolution\n        is adapted with the frequency like for all wavelet transform.\n        The higher the frequency the shorter is the wavelet.\n        If sigma is fixed the temporal resolution is fixed\n        like for the short time Fourier transform and the number\n        of oscillations increases with the frequency.\n    zero_mean : bool, defaults to False\n        Make sure the wavelet has a mean of zero.\n\n    Returns\n    -------\n    Ws : list of array\n        The wavelets time series.\n    '
Ws = list()
n_cycles = numpy.atleast_1d(n_cycles)
if ((n_cycles.size != 1) and (n_cycles.size != len(freqs))):
    raise ValueError('n_cycles should be fixed or defined for each frequency.')
for (k, f) in enumerate(freqs):
    if (len(n_cycles) != 1):
        this_n_cycles = n_cycles[k]
    else:
        this_n_cycles = n_cycles[0]
    if (sigma is None):
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * f))
    else:
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * sigma))
    t = numpy.arange(0.0, (5.0 * sigma_t), (1.0 / sfreq))
    t = numpy.r_[((- t[::(- 1)]), t[1:])]
    oscillation = numpy.exp(((((2.0 * 1j) * numpy.pi) * f) * t))
    gaussian_enveloppe = numpy.exp(((- (t ** 2)) / (2.0 * (sigma_t ** 2))))
    if zero_mean:
        tempResult = exp(((- 2) * (((numpy.pi * f) * sigma_t) ** 2)))
	
===================================================================	
_precompute_st_windows: 41	
----------------------------	

'Precompute stockwell gausian windows (in the freq domain).'
tw = (scipy.fftpack.fftfreq(n_samp, (1.0 / sfreq)) / n_samp)
tw = numpy.r_[(tw[:1], tw[1:][::(- 1)])]
k = width
f_range = numpy.arange(start_f, stop_f, 1)
windows = numpy.empty((len(f_range), len(tw)), dtype=numpy.complex)
for (i_f, f) in enumerate(f_range):
    if (f == 0.0):
        window = numpy.ones(len(tw))
    else:
        tempResult = exp(((((- 0.5) * (1.0 / (k ** 2.0))) * (f ** 2.0)) * (tw ** 2.0)))
	
===================================================================	
_sensor_shape: 878	
----------------------------	

'Get the sensor shape vertices.'
rrs = numpy.empty([0, 2])
tris = numpy.empty([0, 3], int)
id_ = (coil['type'] & 65535)
if (id_ in (2, 3012, 3013, 3011)):
    long_side = coil['size']
    offset = 0.0025
    rrs = numpy.array([[offset, ((- long_side) / 2.0)], [(long_side / 2.0), ((- long_side) / 2.0)], [(long_side / 2.0), (long_side / 2.0)], [offset, (long_side / 2.0)], [(- offset), ((- long_side) / 2.0)], [((- long_side) / 2.0), ((- long_side) / 2.0)], [((- long_side) / 2.0), (long_side / 2.0)], [(- offset), (long_side / 2.0)]])
    tris = numpy.concatenate((_make_tris_fan(4), (_make_tris_fan(4) + 4)), axis=0)
elif (id_ in (2000, 3022, 3023, 3024)):
    size = (0.001 if (id_ == 2000) else (coil['size'] / 2.0))
    rrs = (numpy.array([[(- 1.0), 1.0], [1.0, 1.0], [1.0, (- 1.0)], [(- 1.0), (- 1.0)]]) * size)
    tris = _make_tris_fan(4)
elif (id_ in (4001, 4003, 5002, 7002, 7003, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_MAG)):
    n_pts = 15
    tempResult = exp((((2j * numpy.pi) * numpy.arange(n_pts)) / float(n_pts)))
	
===================================================================	
_sensor_shape: 886	
----------------------------	

'Get the sensor shape vertices.'
rrs = numpy.empty([0, 2])
tris = numpy.empty([0, 3], int)
id_ = (coil['type'] & 65535)
if (id_ in (2, 3012, 3013, 3011)):
    long_side = coil['size']
    offset = 0.0025
    rrs = numpy.array([[offset, ((- long_side) / 2.0)], [(long_side / 2.0), ((- long_side) / 2.0)], [(long_side / 2.0), (long_side / 2.0)], [offset, (long_side / 2.0)], [(- offset), ((- long_side) / 2.0)], [((- long_side) / 2.0), ((- long_side) / 2.0)], [((- long_side) / 2.0), (long_side / 2.0)], [(- offset), (long_side / 2.0)]])
    tris = numpy.concatenate((_make_tris_fan(4), (_make_tris_fan(4) + 4)), axis=0)
elif (id_ in (2000, 3022, 3023, 3024)):
    size = (0.001 if (id_ == 2000) else (coil['size'] / 2.0))
    rrs = (numpy.array([[(- 1.0), 1.0], [1.0, 1.0], [1.0, (- 1.0)], [(- 1.0), (- 1.0)]]) * size)
    tris = _make_tris_fan(4)
elif (id_ in (4001, 4003, 5002, 7002, 7003, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_MAG)):
    n_pts = 15
    circle = numpy.exp((((2j * numpy.pi) * numpy.arange(n_pts)) / float(n_pts)))
    circle = numpy.concatenate(([0.0], circle))
    circle *= (coil['size'] / 2.0)
    rrs = np.array([circle.real, circle.imag]).T
    tris = _make_tris_fan((n_pts + 1))
elif (id_ in (4002, 5001, 5003, 5004, 4004, 4005, 6001, 7001, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_GRAD, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_GRAD)):
    baseline = (coil['base'] if (id_ in (5004, 4005)) else 0.0)
    n_pts = 16
    tempResult = exp((((2j * numpy.pi) * numpy.arange((- 1), n_pts)) / float((n_pts - 1))))
	
***************************************************	

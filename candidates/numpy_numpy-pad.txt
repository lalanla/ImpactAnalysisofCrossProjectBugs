astropy_astropy-1.3.0: 0	
***************************************************	
scipy_scipy-0.19.0: 0	
***************************************************	
sklearn_sklearn-0.18.0: 0	
***************************************************	
matplotlib_matplotlib-2.0.0: 0	
***************************************************	
ipython_ipython-6.1.0: 0	
***************************************************	
pandas_pandas-0.19.2: 0	
***************************************************	
dask_dask-0.7.0: 0	
***************************************************	
nengo_nengo-2.0.0: 0	
***************************************************	
sympy_sympy-1.0.0: 0	
***************************************************	
daducci_amico-dev: 0	
***************************************************	
aplpy_aplpy-1.1.1: 0	
***************************************************	
markovmodel_msmtools-1.0.2: 0	
***************************************************	
nilearn_nilearn-0.4.0: 0	
***************************************************	
poliastro_poliastro-0.8.0: 0	
***************************************************	
skimage_skimage-0.13.0: 14	
===================================================================	
corner_subpix: 124	
----------------------------	

'Determine subpixel position of corners.\n\n    A statistical test decides whether the corner is defined as the\n    intersection of two edges or a single peak. Depending on the classification\n    result, the subpixel corner location is determined based on the local\n    covariance of the grey-values. If the significance level for either\n    statistical test is not sufficient, the corner cannot be classified, and\n    the output subpixel position is set to NaN.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    corners : (N, 2) ndarray\n        Corner coordinates `(row, col)`.\n    window_size : int, optional\n        Search window size for subpixel estimation.\n    alpha : float, optional\n        Significance level for corner classification.\n\n    Returns\n    -------\n    positions : (N, 2) ndarray\n        Subpixel corner positions. NaN for "not classified" corners.\n\n    References\n    ----------\n    .. [1] http://www.ipb.uni-bonn.de/uploads/tx_ikgpublication/           foerstner87.fast.pdf\n    .. [2] http://en.wikipedia.org/wiki/Corner_detection\n\n    Examples\n    --------\n    >>> from skimage.feature import corner_harris, corner_peaks, corner_subpix\n    >>> img = np.zeros((10, 10))\n    >>> img[:5, :5] = 1\n    >>> img[5:, 5:] = 1\n    >>> img.astype(int)\n    array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n           [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n           [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n           [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n           [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n    >>> coords = corner_peaks(corner_harris(img), min_distance=2)\n    >>> coords_subpix = corner_subpix(img, coords, window_size=7)\n    >>> coords_subpix\n    array([[ 4.5,  4.5]])\n\n    '
wext = ((window_size - 1) // 2)
tempResult = pad(image, pad_width=wext, mode='constant', constant_values=0)
	
===================================================================	
match_template: 31	
----------------------------	

'Match a template to a 2-D or 3-D image using normalized correlation.\n\n    The output is an array with values between -1.0 and 1.0. The value at a\n    given position corresponds to the correlation coefficient between the image\n    and the template.\n\n    For `pad_input=True` matches correspond to the center and otherwise to the\n    top-left corner of the template. To find the best match you must search for\n    peaks in the response (output) image.\n\n    Parameters\n    ----------\n    image : (M, N[, D]) array\n        2-D or 3-D input image.\n    template : (m, n[, d]) array\n        Template to locate. It must be `(m <= M, n <= N[, d <= D])`.\n    pad_input : bool\n        If True, pad `image` so that output is the same size as the image, and\n        output values correspond to the template center. Otherwise, the output\n        is an array with shape `(M - m + 1, N - n + 1)` for an `(M, N)` image\n        and an `(m, n)` template, and matches correspond to origin\n        (top-left corner) of the template.\n    mode : see `numpy.pad`, optional\n        Padding mode.\n    constant_values : see `numpy.pad`, optional\n        Constant values used in conjunction with ``mode=\'constant\'``.\n\n    Returns\n    -------\n    output : array\n        Response image with correlation coefficients.\n\n    Notes\n    -----\n    Details on the cross-correlation are presented in [1]_. This implementation\n    uses FFT convolutions of the image and the template. Reference [2]_\n    presents similar derivations but the approximation presented in this\n    reference is not used in our implementation.\n\n    References\n    ----------\n    .. [1] J. P. Lewis, "Fast Normalized Cross-Correlation", Industrial Light\n           and Magic.\n    .. [2] Briechle and Hanebeck, "Template Matching using Fast Normalized\n           Cross Correlation", Proceedings of the SPIE (2001).\n           DOI:10.1117/12.421129\n\n    Examples\n    --------\n    >>> template = np.zeros((3, 3))\n    >>> template[1, 1] = 1\n    >>> template\n    array([[ 0.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  0.]])\n    >>> image = np.zeros((6, 6))\n    >>> image[1, 1] = 1\n    >>> image[4, 4] = -1\n    >>> image\n    array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  1.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.]])\n    >>> result = match_template(image, template)\n    >>> np.round(result, 3)\n    array([[ 1.   , -0.125,  0.   ,  0.   ],\n           [-0.125, -0.125,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.125, -1.   ]])\n    >>> result = match_template(image, template, pad_input=True)\n    >>> np.round(result, 3)\n    array([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])\n    '
assert_nD(image, (2, 3))
if (image.ndim < template.ndim):
    raise ValueError('Dimensionality of template must be less than or equal to the dimensionality of image.')
if numpy.any(numpy.less(image.shape, template.shape)):
    raise ValueError('Image must be larger than template.')
image_shape = image.shape
image = numpy.array(image, dtype=numpy.float64, copy=False)
pad_width = tuple(((width, width) for width in template.shape))
if (mode == 'constant'):
    tempResult = pad(image, pad_width=pad_width, mode=mode, constant_values=constant_values)
	
===================================================================	
match_template: 33	
----------------------------	

'Match a template to a 2-D or 3-D image using normalized correlation.\n\n    The output is an array with values between -1.0 and 1.0. The value at a\n    given position corresponds to the correlation coefficient between the image\n    and the template.\n\n    For `pad_input=True` matches correspond to the center and otherwise to the\n    top-left corner of the template. To find the best match you must search for\n    peaks in the response (output) image.\n\n    Parameters\n    ----------\n    image : (M, N[, D]) array\n        2-D or 3-D input image.\n    template : (m, n[, d]) array\n        Template to locate. It must be `(m <= M, n <= N[, d <= D])`.\n    pad_input : bool\n        If True, pad `image` so that output is the same size as the image, and\n        output values correspond to the template center. Otherwise, the output\n        is an array with shape `(M - m + 1, N - n + 1)` for an `(M, N)` image\n        and an `(m, n)` template, and matches correspond to origin\n        (top-left corner) of the template.\n    mode : see `numpy.pad`, optional\n        Padding mode.\n    constant_values : see `numpy.pad`, optional\n        Constant values used in conjunction with ``mode=\'constant\'``.\n\n    Returns\n    -------\n    output : array\n        Response image with correlation coefficients.\n\n    Notes\n    -----\n    Details on the cross-correlation are presented in [1]_. This implementation\n    uses FFT convolutions of the image and the template. Reference [2]_\n    presents similar derivations but the approximation presented in this\n    reference is not used in our implementation.\n\n    References\n    ----------\n    .. [1] J. P. Lewis, "Fast Normalized Cross-Correlation", Industrial Light\n           and Magic.\n    .. [2] Briechle and Hanebeck, "Template Matching using Fast Normalized\n           Cross Correlation", Proceedings of the SPIE (2001).\n           DOI:10.1117/12.421129\n\n    Examples\n    --------\n    >>> template = np.zeros((3, 3))\n    >>> template[1, 1] = 1\n    >>> template\n    array([[ 0.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  0.]])\n    >>> image = np.zeros((6, 6))\n    >>> image[1, 1] = 1\n    >>> image[4, 4] = -1\n    >>> image\n    array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  1.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0., -1.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.,  0.]])\n    >>> result = match_template(image, template)\n    >>> np.round(result, 3)\n    array([[ 1.   , -0.125,  0.   ,  0.   ],\n           [-0.125, -0.125,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.125, -1.   ]])\n    >>> result = match_template(image, template, pad_input=True)\n    >>> np.round(result, 3)\n    array([[-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125,  1.   , -0.125,  0.   ,  0.   ,  0.   ],\n           [-0.125, -0.125, -0.125,  0.   ,  0.   ,  0.   ],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125, -1.   ,  0.125],\n           [ 0.   ,  0.   ,  0.   ,  0.125,  0.125,  0.125]])\n    '
assert_nD(image, (2, 3))
if (image.ndim < template.ndim):
    raise ValueError('Dimensionality of template must be less than or equal to the dimensionality of image.')
if numpy.any(numpy.less(image.shape, template.shape)):
    raise ValueError('Image must be larger than template.')
image_shape = image.shape
image = numpy.array(image, dtype=numpy.float64, copy=False)
pad_width = tuple(((width, width) for width in template.shape))
if (mode == 'constant'):
    image = numpy.pad(image, pad_width=pad_width, mode=mode, constant_values=constant_values)
else:
    tempResult = pad(image, pad_width=pad_width, mode=mode)
	
===================================================================	
_mean_std: 220	
----------------------------	

'Return local mean and standard deviation of each pixel using a\n    neighborhood defined by a rectangular window with size w times w.\n    The algorithm uses integral images to speedup computation. This is\n    used by threshold_niblack and threshold_sauvola.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    w : int\n        Odd window size (e.g. 3, 5, 7, ..., 21, ...).\n\n    Returns\n    -------\n    m : 2-D array of same size of image with local mean values.\n    s : 2-D array of same size of image with local standard\n        deviation values.\n\n    References\n    ----------\n    .. [1] F. Shafait, D. Keysers, and T. M. Breuel, "Efficient\n           implementation of local adaptive thresholding techniques\n           using integral images." in Document Recognition and\n           Retrieval XV, (San Jose, USA), Jan. 2008.\n           DOI:10.1117/12.767755\n    '
if ((w == 1) or ((w % 2) == 0)):
    raise ValueError(('Window size w = %s must be odd and greater than 1.' % w))
left_pad = ((w // 2) + 1)
right_pad = (w // 2)
tempResult = pad(image.astype('float'), (left_pad, right_pad), mode='reflect')
	
===================================================================	
block_reduce: 18	
----------------------------	

'Down-sample image by applying function to local blocks.\n\n    Parameters\n    ----------\n    image : ndarray\n        N-dimensional input image.\n    block_size : array_like\n        Array containing down-sampling integer factor along each axis.\n    func : callable\n        Function object which is used to calculate the return value for each\n        local block. This function must implement an ``axis`` parameter such\n        as ``numpy.sum`` or ``numpy.min``.\n    cval : float\n        Constant padding value if image is not perfectly divisible by the\n        block size.\n\n    Returns\n    -------\n    image : ndarray\n        Down-sampled image with same number of dimensions as input image.\n\n    Examples\n    --------\n    >>> from skimage.measure import block_reduce\n    >>> image = np.arange(3*3*4).reshape(3, 3, 4)\n    >>> image # doctest: +NORMALIZE_WHITESPACE\n    array([[[ 0,  1,  2,  3],\n            [ 4,  5,  6,  7],\n            [ 8,  9, 10, 11]],\n           [[12, 13, 14, 15],\n            [16, 17, 18, 19],\n            [20, 21, 22, 23]],\n           [[24, 25, 26, 27],\n            [28, 29, 30, 31],\n            [32, 33, 34, 35]]])\n    >>> block_reduce(image, block_size=(3, 3, 1), func=np.mean)\n    array([[[ 16.,  17.,  18.,  19.]]])\n    >>> image_max1 = block_reduce(image, block_size=(1, 3, 4), func=np.max)\n    >>> image_max1 # doctest: +NORMALIZE_WHITESPACE\n    array([[[11]],\n           [[23]],\n           [[35]]])\n    >>> image_max2 = block_reduce(image, block_size=(3, 1, 4), func=np.max)\n    >>> image_max2 # doctest: +NORMALIZE_WHITESPACE\n    array([[[27],\n            [31],\n            [35]]])\n    '
if (len(block_size) != image.ndim):
    raise ValueError('`block_size` must have the same length as `image.shape`.')
pad_width = []
for i in range(len(block_size)):
    if (block_size[i] < 1):
        raise ValueError('Down-sampling factors must be >= 1. Use `skimage.transform.resize` to up-sample an image.')
    if ((image.shape[i] % block_size[i]) != 0):
        after_width = (block_size[i] - (image.shape[i] % block_size[i]))
    else:
        after_width = 0
    pad_width.append((0, after_width))
tempResult = pad(image, pad_width=pad_width, mode='constant', constant_values=cval)
	
===================================================================	
func_out: 52	
----------------------------	

pad_widths = []
padding = False
if (out is None):
    out = numpy.empty_like(image)
for axis_len in selem.shape:
    if ((axis_len % 2) == 0):
        axis_pad_width = (axis_len - 1)
        padding = True
    else:
        axis_pad_width = 0
    pad_widths.append(((axis_pad_width,) * 2))
if padding:
    tempResult = pad(image, pad_widths, mode='edge')
	
===================================================================	
watershed: 50	
----------------------------	

'Find watershed basins in `image` flooded from given `markers`.\n\n    Parameters\n    ----------\n    image: ndarray (2-D, 3-D, ...) of integers\n        Data array where the lowest value points are labeled first.\n    markers: int, or ndarray of int, same shape as `image`\n        The desired number of markers, or an array marking the basins with the\n        values to be assigned in the label matrix. Zero means not a marker.\n    connectivity: ndarray, optional\n        An array with the same number of dimensions as `image` whose\n        non-zero elements indicate neighbors for connection.\n        Following the scipy convention, default is a one-connected array of\n        the dimension of the image.\n    offset: array_like of shape image.ndim, optional\n        offset of the connectivity (one offset per dimension)\n    mask: ndarray of bools or 0s and 1s, optional\n        Array of same shape as `image`. Only points at which mask == True\n        will be labeled.\n    compactness : float, optional\n        Use compact watershed [3]_ with given compactness parameter.\n        Higher values result in more regularly-shaped watershed basins.\n    watershed_line : bool, optional\n        If watershed_line is True, a one-pixel wide line separates the regions\n        obtained by the watershed algorithm. The line has the label 0.\n\n    Returns\n    -------\n    out: ndarray\n        A labeled matrix of the same type and shape as markers\n\n    See also\n    --------\n    skimage.segmentation.random_walker: random walker segmentation\n        A segmentation algorithm based on anisotropic diffusion, usually\n        slower than the watershed but with good results on noisy data and\n        boundaries with holes.\n\n    Notes\n    -----\n    This function implements a watershed algorithm [1]_ [2]_ that apportions\n    pixels into marked basins. The algorithm uses a priority queue to hold\n    the pixels with the metric for the priority queue being pixel value, then\n    the time of entry into the queue - this settles ties in favor of the\n    closest marker.\n\n    Some ideas taken from\n    Soille, "Automated Basin Delineation from Digital Elevation Models Using\n    Mathematical Morphology", Signal Processing 20 (1990) 171-182\n\n    The most important insight in the paper is that entry time onto the queue\n    solves two problems: a pixel should be assigned to the neighbor with the\n    largest gradient or, if there is no gradient, pixels on a plateau should\n    be split between markers on opposite sides.\n\n    This implementation converts all arguments to specific, lowest common\n    denominator types, then passes these to a C algorithm.\n\n    Markers can be determined manually, or automatically using for example\n    the local minima of the gradient of the image, or the local maxima of the\n    distance function to the background for separating overlapping objects\n    (see example).\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n\n    .. [2] http://cmm.ensmp.fr/~beucher/wtshed.html\n\n    .. [3] Peer Neubert & Peter Protzel (2014). Compact Watershed and\n           Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation\n           Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181\n           https://www.tu-chemnitz.de/etit/proaut/forschung/rsrc/cws_pSLIC_ICPR.pdf\n\n    Examples\n    --------\n    The watershed algorithm is useful to separate overlapping objects.\n\n    We first generate an initial image with two overlapping circles:\n\n    >>> x, y = np.indices((80, 80))\n    >>> x1, y1, x2, y2 = 28, 28, 44, 52\n    >>> r1, r2 = 16, 20\n    >>> mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n    >>> mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n    >>> image = np.logical_or(mask_circle1, mask_circle2)\n\n    Next, we want to separate the two circles. We generate markers at the\n    maxima of the distance to the background:\n\n    >>> from scipy import ndimage as ndi\n    >>> distance = ndi.distance_transform_edt(image)\n    >>> from skimage.feature import peak_local_max\n    >>> local_maxi = peak_local_max(distance, labels=image,\n    ...                             footprint=np.ones((3, 3)),\n    ...                             indices=False)\n    >>> markers = ndi.label(local_maxi)[0]\n\n    Finally, we run the watershed on the image and markers:\n\n    >>> labels = watershed(-distance, markers, mask=image)\n\n    The algorithm works also for 3-D images, and can be used for example to\n    separate overlapping spheres.\n    '
(image, markers, mask) = _validate_inputs(image, markers, mask)
(connectivity, offset) = _validate_connectivity(image.ndim, connectivity, offset)
pad_width = [(p, p) for p in offset]
tempResult = pad(image, pad_width, mode='constant')
	
===================================================================	
watershed: 51	
----------------------------	

'Find watershed basins in `image` flooded from given `markers`.\n\n    Parameters\n    ----------\n    image: ndarray (2-D, 3-D, ...) of integers\n        Data array where the lowest value points are labeled first.\n    markers: int, or ndarray of int, same shape as `image`\n        The desired number of markers, or an array marking the basins with the\n        values to be assigned in the label matrix. Zero means not a marker.\n    connectivity: ndarray, optional\n        An array with the same number of dimensions as `image` whose\n        non-zero elements indicate neighbors for connection.\n        Following the scipy convention, default is a one-connected array of\n        the dimension of the image.\n    offset: array_like of shape image.ndim, optional\n        offset of the connectivity (one offset per dimension)\n    mask: ndarray of bools or 0s and 1s, optional\n        Array of same shape as `image`. Only points at which mask == True\n        will be labeled.\n    compactness : float, optional\n        Use compact watershed [3]_ with given compactness parameter.\n        Higher values result in more regularly-shaped watershed basins.\n    watershed_line : bool, optional\n        If watershed_line is True, a one-pixel wide line separates the regions\n        obtained by the watershed algorithm. The line has the label 0.\n\n    Returns\n    -------\n    out: ndarray\n        A labeled matrix of the same type and shape as markers\n\n    See also\n    --------\n    skimage.segmentation.random_walker: random walker segmentation\n        A segmentation algorithm based on anisotropic diffusion, usually\n        slower than the watershed but with good results on noisy data and\n        boundaries with holes.\n\n    Notes\n    -----\n    This function implements a watershed algorithm [1]_ [2]_ that apportions\n    pixels into marked basins. The algorithm uses a priority queue to hold\n    the pixels with the metric for the priority queue being pixel value, then\n    the time of entry into the queue - this settles ties in favor of the\n    closest marker.\n\n    Some ideas taken from\n    Soille, "Automated Basin Delineation from Digital Elevation Models Using\n    Mathematical Morphology", Signal Processing 20 (1990) 171-182\n\n    The most important insight in the paper is that entry time onto the queue\n    solves two problems: a pixel should be assigned to the neighbor with the\n    largest gradient or, if there is no gradient, pixels on a plateau should\n    be split between markers on opposite sides.\n\n    This implementation converts all arguments to specific, lowest common\n    denominator types, then passes these to a C algorithm.\n\n    Markers can be determined manually, or automatically using for example\n    the local minima of the gradient of the image, or the local maxima of the\n    distance function to the background for separating overlapping objects\n    (see example).\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n\n    .. [2] http://cmm.ensmp.fr/~beucher/wtshed.html\n\n    .. [3] Peer Neubert & Peter Protzel (2014). Compact Watershed and\n           Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation\n           Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181\n           https://www.tu-chemnitz.de/etit/proaut/forschung/rsrc/cws_pSLIC_ICPR.pdf\n\n    Examples\n    --------\n    The watershed algorithm is useful to separate overlapping objects.\n\n    We first generate an initial image with two overlapping circles:\n\n    >>> x, y = np.indices((80, 80))\n    >>> x1, y1, x2, y2 = 28, 28, 44, 52\n    >>> r1, r2 = 16, 20\n    >>> mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n    >>> mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n    >>> image = np.logical_or(mask_circle1, mask_circle2)\n\n    Next, we want to separate the two circles. We generate markers at the\n    maxima of the distance to the background:\n\n    >>> from scipy import ndimage as ndi\n    >>> distance = ndi.distance_transform_edt(image)\n    >>> from skimage.feature import peak_local_max\n    >>> local_maxi = peak_local_max(distance, labels=image,\n    ...                             footprint=np.ones((3, 3)),\n    ...                             indices=False)\n    >>> markers = ndi.label(local_maxi)[0]\n\n    Finally, we run the watershed on the image and markers:\n\n    >>> labels = watershed(-distance, markers, mask=image)\n\n    The algorithm works also for 3-D images, and can be used for example to\n    separate overlapping spheres.\n    '
(image, markers, mask) = _validate_inputs(image, markers, mask)
(connectivity, offset) = _validate_connectivity(image.ndim, connectivity, offset)
pad_width = [(p, p) for p in offset]
image = numpy.pad(image, pad_width, mode='constant')
tempResult = pad(mask, pad_width, mode='constant')
	
===================================================================	
watershed: 52	
----------------------------	

'Find watershed basins in `image` flooded from given `markers`.\n\n    Parameters\n    ----------\n    image: ndarray (2-D, 3-D, ...) of integers\n        Data array where the lowest value points are labeled first.\n    markers: int, or ndarray of int, same shape as `image`\n        The desired number of markers, or an array marking the basins with the\n        values to be assigned in the label matrix. Zero means not a marker.\n    connectivity: ndarray, optional\n        An array with the same number of dimensions as `image` whose\n        non-zero elements indicate neighbors for connection.\n        Following the scipy convention, default is a one-connected array of\n        the dimension of the image.\n    offset: array_like of shape image.ndim, optional\n        offset of the connectivity (one offset per dimension)\n    mask: ndarray of bools or 0s and 1s, optional\n        Array of same shape as `image`. Only points at which mask == True\n        will be labeled.\n    compactness : float, optional\n        Use compact watershed [3]_ with given compactness parameter.\n        Higher values result in more regularly-shaped watershed basins.\n    watershed_line : bool, optional\n        If watershed_line is True, a one-pixel wide line separates the regions\n        obtained by the watershed algorithm. The line has the label 0.\n\n    Returns\n    -------\n    out: ndarray\n        A labeled matrix of the same type and shape as markers\n\n    See also\n    --------\n    skimage.segmentation.random_walker: random walker segmentation\n        A segmentation algorithm based on anisotropic diffusion, usually\n        slower than the watershed but with good results on noisy data and\n        boundaries with holes.\n\n    Notes\n    -----\n    This function implements a watershed algorithm [1]_ [2]_ that apportions\n    pixels into marked basins. The algorithm uses a priority queue to hold\n    the pixels with the metric for the priority queue being pixel value, then\n    the time of entry into the queue - this settles ties in favor of the\n    closest marker.\n\n    Some ideas taken from\n    Soille, "Automated Basin Delineation from Digital Elevation Models Using\n    Mathematical Morphology", Signal Processing 20 (1990) 171-182\n\n    The most important insight in the paper is that entry time onto the queue\n    solves two problems: a pixel should be assigned to the neighbor with the\n    largest gradient or, if there is no gradient, pixels on a plateau should\n    be split between markers on opposite sides.\n\n    This implementation converts all arguments to specific, lowest common\n    denominator types, then passes these to a C algorithm.\n\n    Markers can be determined manually, or automatically using for example\n    the local minima of the gradient of the image, or the local maxima of the\n    distance function to the background for separating overlapping objects\n    (see example).\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Watershed_%28image_processing%29\n\n    .. [2] http://cmm.ensmp.fr/~beucher/wtshed.html\n\n    .. [3] Peer Neubert & Peter Protzel (2014). Compact Watershed and\n           Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation\n           Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181\n           https://www.tu-chemnitz.de/etit/proaut/forschung/rsrc/cws_pSLIC_ICPR.pdf\n\n    Examples\n    --------\n    The watershed algorithm is useful to separate overlapping objects.\n\n    We first generate an initial image with two overlapping circles:\n\n    >>> x, y = np.indices((80, 80))\n    >>> x1, y1, x2, y2 = 28, 28, 44, 52\n    >>> r1, r2 = 16, 20\n    >>> mask_circle1 = (x - x1)**2 + (y - y1)**2 < r1**2\n    >>> mask_circle2 = (x - x2)**2 + (y - y2)**2 < r2**2\n    >>> image = np.logical_or(mask_circle1, mask_circle2)\n\n    Next, we want to separate the two circles. We generate markers at the\n    maxima of the distance to the background:\n\n    >>> from scipy import ndimage as ndi\n    >>> distance = ndi.distance_transform_edt(image)\n    >>> from skimage.feature import peak_local_max\n    >>> local_maxi = peak_local_max(distance, labels=image,\n    ...                             footprint=np.ones((3, 3)),\n    ...                             indices=False)\n    >>> markers = ndi.label(local_maxi)[0]\n\n    Finally, we run the watershed on the image and markers:\n\n    >>> labels = watershed(-distance, markers, mask=image)\n\n    The algorithm works also for 3-D images, and can be used for example to\n    separate overlapping spheres.\n    '
(image, markers, mask) = _validate_inputs(image, markers, mask)
(connectivity, offset) = _validate_connectivity(image.ndim, connectivity, offset)
pad_width = [(p, p) for p in offset]
image = numpy.pad(image, pad_width, mode='constant')
mask = np.pad(mask, pad_width, mode='constant').ravel()
tempResult = pad(markers, pad_width, mode='constant')
	
===================================================================	
skeletonize_3d: 16	
----------------------------	

'Compute the skeleton of a binary image.\n\n    Thinning is used to reduce each connected component in a binary image\n    to a single-pixel wide skeleton.\n\n    Parameters\n    ----------\n    img : ndarray, 2D or 3D\n        A binary image containing the objects to be skeletonized. Zeros\n        represent background, nonzero values are foreground.\n\n    Returns\n    -------\n    skeleton : ndarray\n        The thinned image.\n\n    See also\n    --------\n    skeletonize, medial_axis\n\n    Notes\n    -----\n    The method of [Lee94]_ uses an octree data structure to examine a 3x3x3\n    neighborhood of a pixel. The algorithm proceeds by iteratively sweeping\n    over the image, and removing pixels at each iteration until the image\n    stops changing. Each iteration consists of two steps: first, a list of\n    candidates for removal is assembled; then pixels from this list are\n    rechecked sequentially, to better preserve connectivity of the image.\n\n    The algorithm this function implements is different from the algorithms\n    used by either `skeletonize` or `medial_axis`, thus for 2D images the\n    results produced by this function are generally different.\n\n    References\n    ----------\n    .. [Lee94] T.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models\n           via 3-D medial surface/axis thinning algorithms.\n           Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.\n\n    '
if ((img.ndim < 2) or (img.ndim > 3)):
    raise ValueError(('skeletonize_3d can only handle 2D or 3D images; got img.ndim = %s instead.' % img.ndim))
img = numpy.ascontiguousarray(img)
img = img_as_ubyte(img, force_copy=False)
img_o = img
if (img.ndim == 2):
    img_o = img[(numpy.newaxis, ...)]
tempResult = pad(img_o, pad_width=1, mode='constant')
	
===================================================================	
_find_boundaries_subpixel: 19	
----------------------------	

'See ``find_boundaries(..., mode=\'subpixel\')``.\n\n    Notes\n    -----\n    This function puts in an empty row and column between each *actual*\n    row and column of the image, for a corresponding shape of $2s - 1$\n    for every image dimension of size $s$. These "interstitial" rows\n    and columns are filled as ``True`` if they separate two labels in\n    `label_img`, ``False`` otherwise.\n\n    I used ``view_as_windows`` to get the neighborhood of each pixel.\n    Then I check whether there are two labels or more in that\n    neighborhood.\n    '
ndim = label_img.ndim
max_label = np.iinfo(label_img.dtype).max
label_img_expanded = numpy.zeros([((2 * s) - 1) for s in label_img.shape], label_img.dtype)
pixels = ([slice(None, None, 2)] * ndim)
label_img_expanded[pixels] = label_img
edges = numpy.ones(label_img_expanded.shape, dtype=bool)
edges[pixels] = False
label_img_expanded[edges] = max_label
tempResult = pad(label_img_expanded, 1, mode='constant', constant_values=0)
	
===================================================================	
_sinogram_circle_to_square: 66	
----------------------------	

diagonal = int(numpy.ceil((numpy.sqrt(2) * sinogram.shape[0])))
pad = (diagonal - sinogram.shape[0])
old_center = (sinogram.shape[0] // 2)
new_center = (diagonal // 2)
pad_before = (new_center - old_center)
pad_width = ((pad_before, (pad - pad_before)), (0, 0))
tempResult = pad(sinogram, pad_width, mode='constant', constant_values=0)
	
===================================================================	
iradon: 95	
----------------------------	

'\n    Inverse radon transform.\n\n    Reconstruct an image from the radon transform, using the filtered\n    back projection algorithm.\n\n    Parameters\n    ----------\n    radon_image : array_like, dtype=float\n        Image containing radon transform (sinogram). Each column of\n        the image corresponds to a projection along a different angle. The\n        tomography rotation axis should lie at the pixel index\n        ``radon_image.shape[0] // 2`` along the 0th dimension of\n        ``radon_image``.\n    theta : array_like, dtype=float, optional\n        Reconstruction angles (in degrees). Default: m angles evenly spaced\n        between 0 and 180 (if the shape of `radon_image` is (N, M)).\n    output_size : int\n        Number of rows and columns in the reconstruction.\n    filter : str, optional (default ramp)\n        Filter used in frequency domain filtering. Ramp filter used by default.\n        Filters available: ramp, shepp-logan, cosine, hamming, hann.\n        Assign None to use no filter.\n    interpolation : str, optional (default \'linear\')\n        Interpolation method used in reconstruction. Methods available:\n        \'linear\', \'nearest\', and \'cubic\' (\'cubic\' is slow).\n    circle : boolean, optional\n        Assume the reconstructed image is zero outside the inscribed circle.\n        Also changes the default output_size to match the behaviour of\n        ``radon`` called with ``circle=True``.\n        The default behavior (None) is equivalent to False.\n\n    Returns\n    -------\n    reconstructed : ndarray\n        Reconstructed image. The rotation axis will be located in the pixel\n        with indices\n        ``(reconstructed.shape[0] // 2, reconstructed.shape[1] // 2)``.\n\n    References\n    ----------\n    .. [1] AC Kak, M Slaney, "Principles of Computerized Tomographic\n           Imaging", IEEE Press 1988.\n    .. [2] B.R. Ramesh, N. Srinivasa, K. Rajgopal, "An Algorithm for Computing\n           the Discrete Radon Transform With Some Applications", Proceedings of\n           the Fourth IEEE Region 10 International Conference, TENCON \'89, 1989\n\n    Notes\n    -----\n    It applies the Fourier slice theorem to reconstruct an image by\n    multiplying the frequency domain of the filter with the FFT of the\n    projection data. This algorithm is called filtered back projection.\n\n    '
if (radon_image.ndim != 2):
    raise ValueError('The input image must be 2-D')
if (theta is None):
    (m, n) = radon_image.shape
    theta = numpy.linspace(0, 180, n, endpoint=False)
else:
    theta = numpy.asarray(theta)
if (len(theta) != radon_image.shape[1]):
    raise ValueError('The given ``theta`` does not match the number of projections in ``radon_image``.')
interpolation_types = ('linear', 'nearest', 'cubic')
if (interpolation not in interpolation_types):
    raise ValueError(('Unknown interpolation: %s' % interpolation))
if (not output_size):
    if circle:
        output_size = radon_image.shape[0]
    else:
        output_size = int(numpy.floor(numpy.sqrt(((radon_image.shape[0] ** 2) / 2.0))))
if (circle is None):
    warn('The default of `circle` in `skimage.transform.iradon` will change to `True` in version 0.15.')
    circle = False
if circle:
    radon_image = _sinogram_circle_to_square(radon_image)
th = ((numpy.pi / 180.0) * theta)
projection_size_padded = max(64, int((2 ** numpy.ceil(numpy.log2((2 * radon_image.shape[0]))))))
pad_width = ((0, (projection_size_padded - radon_image.shape[0])), (0, 0))
tempResult = pad(radon_image, pad_width, mode='constant', constant_values=0)
	
===================================================================	
radon: 43	
----------------------------	

'\n    Calculates the radon transform of an image given specified\n    projection angles.\n\n    Parameters\n    ----------\n    image : array_like, dtype=float\n        Input image. The rotation axis will be located in the pixel with\n        indices ``(image.shape[0] // 2, image.shape[1] // 2)``.\n    theta : array_like, dtype=float, optional (default np.arange(180))\n        Projection angles (in degrees).\n    circle : boolean, optional\n        Assume image is zero outside the inscribed circle, making the\n        width of each projection (the first dimension of the sinogram)\n        equal to ``min(image.shape)``.\n        The default behavior (None) is equivalent to False.\n\n    Returns\n    -------\n    radon_image : ndarray\n        Radon transform (sinogram).  The tomography rotation axis will lie\n        at the pixel index ``radon_image.shape[0] // 2`` along the 0th\n        dimension of ``radon_image``.\n\n    References\n    ----------\n    .. [1] AC Kak, M Slaney, "Principles of Computerized Tomographic\n           Imaging", IEEE Press 1988.\n    .. [2] B.R. Ramesh, N. Srinivasa, K. Rajgopal, "An Algorithm for Computing\n           the Discrete Radon Transform With Some Applications", Proceedings of\n           the Fourth IEEE Region 10 International Conference, TENCON \'89, 1989\n\n    Notes\n    -----\n    Based on code of Justin K. Romberg\n    (http://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html)\n\n    '
if (image.ndim != 2):
    raise ValueError('The input image must be 2-D')
if (theta is None):
    theta = numpy.arange(180)
if (circle is None):
    warn('The default of `circle` in `skimage.transform.radon` will change to `True` in version 0.15.')
    circle = False
if circle:
    radius = (min(image.shape) // 2)
    (c0, c1) = numpy.ogrid[0:image.shape[0], 0:image.shape[1]]
    reconstruction_circle = (((c0 - (image.shape[0] // 2)) ** 2) + ((c1 - (image.shape[1] // 2)) ** 2))
    reconstruction_circle = (reconstruction_circle <= (radius ** 2))
    if (not numpy.all((reconstruction_circle | (image == 0)))):
        warn('Radon transform: image must be zero outside the reconstruction circle')
    slices = []
    for d in (0, 1):
        if (image.shape[d] > min(image.shape)):
            excess = (image.shape[d] - min(image.shape))
            slices.append(slice(int(numpy.ceil((excess / 2))), int((numpy.ceil((excess / 2)) + min(image.shape)))))
        else:
            slices.append(slice(None))
    slices = tuple(slices)
    padded_image = image[slices]
else:
    diagonal = (numpy.sqrt(2) * max(image.shape))
    pad = [int(numpy.ceil((diagonal - s))) for s in image.shape]
    new_center = [((s + p) // 2) for (s, p) in zip(image.shape, pad)]
    old_center = [(s // 2) for s in image.shape]
    pad_before = [(nc - oc) for (oc, nc) in zip(old_center, new_center)]
    pad_width = [(pb, (p - pb)) for (pb, p) in zip(pad_before, pad)]
    tempResult = pad(image, pad_width, mode='constant', constant_values=0)
	
***************************************************	
sunpy_sunpy-0.8.0: 2	
===================================================================	
GenericMap.rotate: 498	
----------------------------	

"\n        Returns a new rotated and rescaled map.\n\n        Specify either a rotation angle or a rotation matrix, but not both. If\n        neither an angle or a rotation matrix are specified, the map will be\n        rotated by the rotation angle in the metadata.\n\n        The map will be rotated around the reference coordinate defined in the\n        meta data.\n\n        This method also updates the ``rotation_matrix`` attribute and any\n        appropriate header data so that they correctly describe the new map.\n\n        Parameters\n        ----------\n        angle : `~astropy.units.Quantity`\n            The angle (degrees) to rotate counterclockwise.\n        rmatrix : 2x2\n            Linear transformation rotation matrix.\n        order : int 0-5\n            Interpolation order to be used. When using scikit-image this\n            parameter is passed into :func:`skimage.transform.warp` (e.g., 4\n            corresponds to bi-quartic interpolation).\n            When using scipy it is passed into\n            :func:`scipy.ndimage.interpolation.affine_transform` where it\n            controls the order of the spline. Faster performance may be\n            obtained at the cost of accuracy by using lower values.\n            Default: 4\n        scale : float\n            A scale factor for the image, default is no scaling\n        recenter : bool\n            If True, position the axis of rotation at the center of the new map\n            Default: False\n        missing : float\n            The numerical value to fill any missing points after rotation.\n            Default: 0.0\n        use_scipy : bool\n            If True, forces the rotation to use\n            :func:`scipy.ndimage.interpolation.affine_transform`, otherwise it\n            uses the :func:`skimage.transform.warp`.\n            Default: False, unless scikit-image can't be imported\n\n        Returns\n        -------\n        out : `~sunpy.map.GenericMap` or subclass\n            A new Map instance containing the rotated and rescaled data of the\n            original map.\n\n        See Also\n        --------\n        sunpy.image.transform.affine_transform : The routine this method calls\n        for the rotation.\n\n        Notes\n        -----\n        This function will remove old CROTA keywords from the header.\n        This function will also convert a CDi_j matrix to a PCi_j matrix.\n\n        See :func:`sunpy.image.transform.affine_transform` for details on the\n        transformations, situations when the underlying data is modified prior\n        to rotation, and differences from IDL's rot().\n        "
if ((angle is not None) and (rmatrix is not None)):
    raise ValueError('You cannot specify both an angle and a matrix')
elif ((angle is None) and (rmatrix is None)):
    rmatrix = self.rotation_matrix
if angle:
    try:
        equivalent = angle.unit.is_equivalent(astropy.units.deg)
        if (not equivalent):
            raise astropy.units.UnitsError("Argument '{0}' to function '{1}' must be in units convertable to '{2}'.".format('angle', 'rotate', astropy.units.deg.to_string()))
    except AttributeError:
        if hasattr(angle, 'unit'):
            error_msg = "a 'unit' attribute without an 'is_equivalent' method"
        else:
            error_msg = "no 'unit' attribute"
        raise TypeError("Argument '{0}' to function '{1}' has {2}. You may want to pass in an astropy Quantity instead.".format('angle', 'rotate', error_msg))
if (order not in range(6)):
    raise ValueError('Order must be between 0 and 5')
(lon, lat) = self._get_lon_lat(self.reference_coordinate.frame)
rotation_center = astropy.units.Quantity([lon, lat])
new_meta = self.meta.copy()
if (angle is not None):
    c = numpy.cos(numpy.deg2rad(angle))
    s = numpy.sin(numpy.deg2rad(angle))
    rmatrix = numpy.matrix([[c, (- s)], [s, c]])
extent = numpy.max(numpy.abs(numpy.vstack(((self.data.shape * rmatrix), (self.data.shape * rmatrix.T)))), axis=0)
diff = np.asarray(np.ceil(((extent - self.data.shape) / 2)), dtype=int).ravel()
pad_x = int(numpy.max((diff[1], 0)))
pad_y = int(numpy.max((diff[0], 0)))
tempResult = pad(self.data, ((pad_y, pad_y), (pad_x, pad_x)), mode='constant', constant_values=(missing, missing))
	
===================================================================	
diffrot_map: 93	
----------------------------	

"\n    Function to apply solar differential rotation to a sunpy map.\n\n    Parameters\n    ----------\n    smap : `~sunpy.map`\n        Original map that we want to transform.\n    time : sunpy-compatible time\n        date/time at which the input co-ordinate will be rotated to.\n    dt : `~astropy.units.Quantity` or `datetime`\n        Desired interval between the input map and returned map.\n    pad : `bool`\n        Whether to create a padded map for submaps to don't loose data\n\n    Returns\n    -------\n    diffrot_map : `~sunpy.map`\n        A map with the result of applying solar differential rotation to the\n        input map.\n    "
if ((time is not None) and (dt is not None)):
    raise ValueError('Only a time or an interval is accepted')
elif (not (time or dt)):
    raise ValueError('Either a time or an interval (`dt=`) needs to be provided')
elif time:
    new_time = parse_time(time)
    dt = ((new_time - smap.date).total_seconds() * astropy.units.s)
else:
    new_time = (smap.date + datetime.timedelta(seconds=dt.to(u.s).value))
if (smap.mask is not None):
    smap_data = numpy.ma.array(smap.data, mask=smap.mask)
else:
    smap_data = smap.data
submap = False
if (((2 * smap.rsun_obs) > (smap.top_right_coord.Tx - smap.bottom_left_coord.Tx)) or ((2 * smap.rsun_obs) > (smap.top_right_coord.Ty - smap.bottom_left_coord.Ty))):
    submap = True
    if pad:
        deltax = deltay = 0
        for corner in product(*product([(0 * astropy.units.pix)], smap.dimensions)):
            corner_world = smap.pixel_to_world(*corner)
            corner_world_rotated = solar_rotate_coordinate(corner_world, new_time, **diffrot_kwargs)
            corner_px_rotated = smap.world_to_pixel(corner_world_rotated)
            dx = numpy.abs((corner_px_rotated.x - corner[0]))
            dy = numpy.abs((corner_px_rotated.y - corner[1]))
            deltax = (dx if (dx > deltax) else deltax)
            deltay = (dy if (dy > deltay) else deltay)
        deltax = numpy.int(numpy.ceil(deltax.value))
        deltay = numpy.int(numpy.ceil(deltay.value))
        tempResult = pad(smap.data, ((deltay, deltay), (deltax, deltax)), 'constant', constant_values=0)
	
***************************************************	
spacetelescope_synphot-0.1: 0	
***************************************************	
librosa_librosa-0.5.1: 19	
===================================================================	
_wrap: 77	
----------------------------	

'The wrapped window'
(n_min, n_max) = (int(numpy.floor(n)), int(numpy.ceil(n)))
window = get_window(window_spec, n_min)
if (len(window) < n_max):
    tempResult = pad(window, [(0, (n_max - len(window)))], mode='constant')
	
===================================================================	
onset_strength_multi: 87	
----------------------------	

"Compute a spectral flux onset strength envelope across multiple channels.\n\n    Onset strength for channel `i` at time `t` is determined by:\n\n    `mean_{f in channels[i]} max(0, S[f, t+1] - S[f, t])`\n\n\n    Parameters\n    ----------\n    y        : np.ndarray [shape=(n,)]\n        audio time-series\n\n    sr       : number > 0 [scalar]\n        sampling rate of `y`\n\n    S        : np.ndarray [shape=(d, m)]\n        pre-computed (log-power) spectrogram\n\n    lag      : int > 0\n        time lag for computing differences\n\n    max_size : int > 0\n        size (in frequency bins) of the local max filter.\n        set to `1` to disable filtering.\n\n    detrend : bool [scalar]\n        Filter the onset strength to remove the DC component\n\n    center : bool [scalar]\n        Shift the onset function by `n_fft / (2 * hop_length)` frames\n\n    feature : function\n        Function for computing time-series features, eg, scaled spectrograms.\n        By default, uses `librosa.feature.melspectrogram` with `fmax=11025.0`\n\n    aggregate : function\n        Aggregation function to use when combining onsets\n        at different frequency bins.\n\n        Default: `np.mean`\n\n    channels : list or None\n        Array of channel boundaries or slice objects.\n        If `None`, then a single channel is generated to span all bands.\n\n    kwargs : additional keyword arguments\n        Additional parameters to `feature()`, if `S` is not provided.\n\n\n    Returns\n    -------\n    onset_envelope   : np.ndarray [shape=(n_channels, m)]\n        array containing the onset strength envelope for each specified channel\n\n\n    Raises\n    ------\n    ParameterError\n        if neither `(y, sr)` nor `S` are provided\n\n\n    See Also\n    --------\n    onset_strength\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    First, load some audio and plot the spectrogram\n\n    >>> import matplotlib.pyplot as plt\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      duration=10.0)\n    >>> D = librosa.stft(y)\n    >>> plt.figure()\n    >>> plt.subplot(2, 1, 1)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n    ...                          y_axis='log')\n    >>> plt.title('Power spectrogram')\n\n    Construct a standard onset function over four sub-bands\n\n    >>> onset_subbands = librosa.onset.onset_strength_multi(y=y, sr=sr,\n    ...                                                     channels=[0, 32, 64, 96, 128])\n    >>> plt.subplot(2, 1, 2)\n    >>> librosa.display.specshow(onset_subbands, x_axis='time')\n    >>> plt.ylabel('Sub-bands')\n    >>> plt.title('Sub-band onset strength')\n\n    "
if (feature is None):
    feature = melspectrogram
    kwargs.setdefault('fmax', 11025.0)
if (aggregate is None):
    aggregate = numpy.mean
if ((lag < 1) or (not isinstance(lag, int))):
    raise ParameterError('lag must be a positive integer')
if ((max_size < 1) or (not isinstance(max_size, int))):
    raise ParameterError('max_size must be a positive integer')
if (S is None):
    S = numpy.abs(feature(y=y, sr=sr, **kwargs))
    S = core.power_to_db(S)
n_fft = kwargs.get('n_fft', 2048)
hop_length = kwargs.get('hop_length', 512)
S = numpy.atleast_2d(S)
if (max_size == 1):
    ref_spec = S
else:
    ref_spec = scipy.ndimage.maximum_filter1d(S, max_size, axis=0)
onset_env = (S[:, lag:] - ref_spec[:, :(- lag)])
onset_env = numpy.maximum(0.0, onset_env)
pad = True
if (channels is None):
    channels = [slice(None)]
else:
    pad = False
onset_env = util.sync(onset_env, channels, aggregate=aggregate, pad=pad, axis=0)
pad_width = lag
if center:
    pad_width += (n_fft // (2 * hop_length))
tempResult = pad(onset_env, ([0, 0], [int(pad_width), 0]), mode='constant')
	
===================================================================	
recurrence_to_lag: 90	
----------------------------	

"Convert a recurrence matrix into a lag matrix.\n\n        `lag[i, j] == rec[i+j, j]`\n\n    Parameters\n    ----------\n    rec : np.ndarray, or scipy.sparse.spmatrix [shape=(n, n)]\n        A (binary) recurrence matrix, as returned by `recurrence_matrix`\n\n    pad : bool\n        If False, `lag` matrix is square, which is equivalent to\n        assuming that the signal repeats itself indefinitely.\n\n        If True, `lag` is padded with `n` zeros, which eliminates\n        the assumption of repetition.\n\n    axis : int\n        The axis to keep as the `time` axis.\n        The alternate axis will be converted to lag coordinates.\n\n    Returns\n    -------\n    lag : np.ndarray\n        The recurrence matrix in (lag, time) (if `axis=1`)\n        or (time, lag) (if `axis=0`) coordinates\n\n    Raises\n    ------\n    ParameterError : if `rec` is non-square\n\n    See Also\n    --------\n    recurrence_matrix\n    lag_to_recurrence\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> mfccs = librosa.feature.mfcc(y=y, sr=sr)\n    >>> recurrence = librosa.segment.recurrence_matrix(mfccs)\n    >>> lag_pad = librosa.segment.recurrence_to_lag(recurrence, pad=True)\n    >>> lag_nopad = librosa.segment.recurrence_to_lag(recurrence, pad=False)\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> librosa.display.specshow(lag_pad, x_axis='time', y_axis='lag')\n    >>> plt.title('Lag (zero-padded)')\n    >>> plt.subplot(1, 2, 2)\n    >>> librosa.display.specshow(lag_nopad, x_axis='time')\n    >>> plt.title('Lag (no padding)')\n    >>> plt.tight_layout()\n    "
axis = numpy.abs(axis)
if ((rec.ndim != 2) or (rec.shape[0] != rec.shape[1])):
    raise ParameterError('non-square recurrence matrix shape: {}'.format(rec.shape))
sparse = scipy.sparse.issparse(rec)
roll_ax = None
if sparse:
    roll_ax = (1 - axis)
    lag_format = rec.format
    if (axis == 0):
        rec = rec.tocsc()
    elif (axis in ((- 1), 1)):
        rec = rec.tocsr()
t = rec.shape[axis]
if sparse:
    if pad:
        kron = np.asarray([[1, 0]]).swapaxes(axis, 0)
        lag = scipy.sparse.kron(kron.astype(rec.dtype), rec, format='lil')
    else:
        lag = scipy.sparse.lil_matrix(rec)
elif pad:
    padding = [(0, 0), (0, 0)]
    padding[(1 - axis)] = (0, t)
    tempResult = pad(rec, padding, mode='constant')
	
===================================================================	
zero_crossings: 138	
----------------------------	

'Find the zero-crossings of a signal `y`: indices `i` such that\n    `sign(y[i]) != sign(y[j])`.\n\n    If `y` is multi-dimensional, then zero-crossings are computed along\n    the specified `axis`.\n\n\n    Parameters\n    ----------\n    y : np.ndarray\n        The input array\n\n    threshold : float > 0 or None\n        If specified, values where `-threshold <= y <= threshold` are\n        clipped to 0.\n\n    ref_magnitude : float > 0 or callable\n        If numeric, the threshold is scaled relative to `ref_magnitude`.\n\n        If callable, the threshold is scaled relative to\n        `ref_magnitude(np.abs(y))`.\n\n    pad : boolean\n        If `True`, then `y[0]` is considered a valid zero-crossing.\n\n    zero_pos : boolean\n        If `True` then the value 0 is interpreted as having positive sign.\n\n        If `False`, then 0, -1, and +1 all have distinct signs.\n\n    axis : int\n        Axis along which to compute zero-crossings.\n\n    Returns\n    -------\n    zero_crossings : np.ndarray [shape=y.shape, dtype=boolean]\n        Indicator array of zero-crossings in `y` along the selected axis.\n\n    Notes\n    -----\n    This function caches at level 20.\n\n    Examples\n    --------\n    >>> # Generate a time-series\n    >>> y = np.sin(np.linspace(0, 4 * 2 * np.pi, 20))\n    >>> y\n    array([  0.000e+00,   9.694e-01,   4.759e-01,  -7.357e-01,\n            -8.372e-01,   3.247e-01,   9.966e-01,   1.646e-01,\n            -9.158e-01,  -6.142e-01,   6.142e-01,   9.158e-01,\n            -1.646e-01,  -9.966e-01,  -3.247e-01,   8.372e-01,\n             7.357e-01,  -4.759e-01,  -9.694e-01,  -9.797e-16])\n    >>> # Compute zero-crossings\n    >>> z = librosa.zero_crossings(y)\n    >>> z\n    array([ True, False, False,  True, False,  True, False, False,\n            True, False,  True, False,  True, False, False,  True,\n           False,  True, False,  True], dtype=bool)\n    >>> # Stack y against the zero-crossing indicator\n    >>> np.vstack([y, z]).T\n    array([[  0.000e+00,   1.000e+00],\n           [  9.694e-01,   0.000e+00],\n           [  4.759e-01,   0.000e+00],\n           [ -7.357e-01,   1.000e+00],\n           [ -8.372e-01,   0.000e+00],\n           [  3.247e-01,   1.000e+00],\n           [  9.966e-01,   0.000e+00],\n           [  1.646e-01,   0.000e+00],\n           [ -9.158e-01,   1.000e+00],\n           [ -6.142e-01,   0.000e+00],\n           [  6.142e-01,   1.000e+00],\n           [  9.158e-01,   0.000e+00],\n           [ -1.646e-01,   1.000e+00],\n           [ -9.966e-01,   0.000e+00],\n           [ -3.247e-01,   0.000e+00],\n           [  8.372e-01,   1.000e+00],\n           [  7.357e-01,   0.000e+00],\n           [ -4.759e-01,   1.000e+00],\n           [ -9.694e-01,   0.000e+00],\n           [ -9.797e-16,   1.000e+00]])\n    >>> # Find the indices of zero-crossings\n    >>> np.nonzero(z)\n    (array([ 0,  3,  5,  8, 10, 12, 15, 17, 19]),)\n    '
if (threshold is None):
    threshold = 0.0
if six.callable(ref_magnitude):
    threshold = (threshold * ref_magnitude(numpy.abs(y)))
elif (ref_magnitude is not None):
    threshold = (threshold * ref_magnitude)
if (threshold > 0):
    y = y.copy()
    y[(numpy.abs(y) <= threshold)] = 0
if zero_pos:
    y_sign = numpy.signbit(y)
else:
    y_sign = numpy.sign(y)
slice_pre = ([slice(None)] * y.ndim)
slice_pre[axis] = slice(1, None)
slice_post = ([slice(None)] * y.ndim)
slice_post[axis] = slice((- 1))
padding = ([(0, 0)] * y.ndim)
padding[axis] = (1, 0)
tempResult = pad((y_sign[slice_post] != y_sign[slice_pre]), padding, mode='constant', constant_values=pad)
	
===================================================================	
piptrack: 47	
----------------------------	

'Pitch tracking on thresholded parabolically-interpolated STFT\n\n    .. [1] https://ccrma.stanford.edu/~jos/sasp/Sinusoidal_Peak_Interpolation.html\n\n    Parameters\n    ----------\n    y: np.ndarray [shape=(n,)] or None\n        audio signal\n\n    sr : number > 0 [scalar]\n        audio sampling rate of `y`\n\n    S: np.ndarray [shape=(d, t)] or None\n        magnitude or power spectrogram\n\n    n_fft : int > 0 [scalar] or None\n        number of FFT bins to use, if `y` is provided.\n\n    hop_length : int > 0 [scalar] or None\n        number of samples to hop\n\n    threshold : float in `(0, 1)`\n        A bin in spectrum X is considered a pitch when it is greater than\n        `threshold*X.max()`\n\n    fmin : float > 0 [scalar]\n        lower frequency cutoff.\n\n    fmax : float > 0 [scalar]\n        upper frequency cutoff.\n\n    .. note::\n        One of `S` or `y` must be provided.\n\n        If `S` is not given, it is computed from `y` using\n        the default parameters of `librosa.core.stft`.\n\n    Returns\n    -------\n    pitches : np.ndarray [shape=(d, t)]\n    magnitudes : np.ndarray [shape=(d,t)]\n        Where `d` is the subset of FFT bins within `fmin` and `fmax`.\n\n        `pitches[f, t]` contains instantaneous frequency at bin\n        `f`, time `t`\n\n        `magnitudes[f, t]` contains the corresponding magnitudes.\n\n        Both `pitches` and `magnitudes` take value 0 at bins\n        of non-maximal magnitude.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n\n    '
if (hop_length is None):
    hop_length = int((n_fft // 4))
(S, n_fft) = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length)
S = numpy.abs(S)
fmin = numpy.maximum(fmin, 0)
fmax = numpy.minimum(fmax, (float(sr) / 2))
fft_freqs = time_frequency.fft_frequencies(sr=sr, n_fft=n_fft)
avg = (0.5 * (S[2:] - S[:(- 2)]))
shift = (((2 * S[1:(- 1)]) - S[2:]) - S[:(- 2)])
shift = (avg / (shift + (numpy.abs(shift) < util.tiny(shift))))
tempResult = pad(avg, ([1, 1], [0, 0]), mode='constant')
	
===================================================================	
piptrack: 48	
----------------------------	

'Pitch tracking on thresholded parabolically-interpolated STFT\n\n    .. [1] https://ccrma.stanford.edu/~jos/sasp/Sinusoidal_Peak_Interpolation.html\n\n    Parameters\n    ----------\n    y: np.ndarray [shape=(n,)] or None\n        audio signal\n\n    sr : number > 0 [scalar]\n        audio sampling rate of `y`\n\n    S: np.ndarray [shape=(d, t)] or None\n        magnitude or power spectrogram\n\n    n_fft : int > 0 [scalar] or None\n        number of FFT bins to use, if `y` is provided.\n\n    hop_length : int > 0 [scalar] or None\n        number of samples to hop\n\n    threshold : float in `(0, 1)`\n        A bin in spectrum X is considered a pitch when it is greater than\n        `threshold*X.max()`\n\n    fmin : float > 0 [scalar]\n        lower frequency cutoff.\n\n    fmax : float > 0 [scalar]\n        upper frequency cutoff.\n\n    .. note::\n        One of `S` or `y` must be provided.\n\n        If `S` is not given, it is computed from `y` using\n        the default parameters of `librosa.core.stft`.\n\n    Returns\n    -------\n    pitches : np.ndarray [shape=(d, t)]\n    magnitudes : np.ndarray [shape=(d,t)]\n        Where `d` is the subset of FFT bins within `fmin` and `fmax`.\n\n        `pitches[f, t]` contains instantaneous frequency at bin\n        `f`, time `t`\n\n        `magnitudes[f, t]` contains the corresponding magnitudes.\n\n        Both `pitches` and `magnitudes` take value 0 at bins\n        of non-maximal magnitude.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n\n    '
if (hop_length is None):
    hop_length = int((n_fft // 4))
(S, n_fft) = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length)
S = numpy.abs(S)
fmin = numpy.maximum(fmin, 0)
fmax = numpy.minimum(fmax, (float(sr) / 2))
fft_freqs = time_frequency.fft_frequencies(sr=sr, n_fft=n_fft)
avg = (0.5 * (S[2:] - S[:(- 2)]))
shift = (((2 * S[1:(- 1)]) - S[2:]) - S[:(- 2)])
shift = (avg / (shift + (numpy.abs(shift) < util.tiny(shift))))
avg = numpy.pad(avg, ([1, 1], [0, 0]), mode='constant')
tempResult = pad(shift, ([1, 1], [0, 0]), mode='constant')
	
===================================================================	
stft: 30	
----------------------------	

"Short-time Fourier transform (STFT)\n\n    Returns a complex-valued matrix D such that\n        `np.abs(D[f, t])` is the magnitude of frequency bin `f`\n        at frame `t`\n\n        `np.angle(D[f, t])` is the phase of frequency bin `f`\n        at frame `t`\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)], real-valued\n        the input signal (audio time series)\n\n    n_fft : int > 0 [scalar]\n        FFT window size\n\n    hop_length : int > 0 [scalar]\n        number audio of frames between STFT columns.\n        If unspecified, defaults `win_length / 4`.\n\n    win_length  : int <= n_fft [scalar]\n        Each frame of audio is windowed by `window()`.\n        The window will be of length `win_length` and then padded\n        with zeros to match `n_fft`.\n\n        If unspecified, defaults to ``win_length = n_fft``.\n\n    window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n        - a window specification (string, tuple, or number);\n          see `scipy.signal.get_window`\n        - a window function, such as `scipy.signal.hanning`\n        - a vector or array of length `n_fft`\n\n        .. see also:: `filters.get_window`\n\n    center      : boolean\n        - If `True`, the signal `y` is padded so that frame\n          `D[:, t]` is centered at `y[t * hop_length]`.\n        - If `False`, then `D[:, t]` begins at `y[t * hop_length]`\n\n    dtype       : numeric type\n        Complex numeric type for `D`.  Default is 64-bit complex.\n\n    mode : string\n        If `center=True`, the padding mode to use at the edges of the signal.\n        By default, STFT uses reflection padding.\n\n\n    Returns\n    -------\n    D : np.ndarray [shape=(1 + n_fft/2, t), dtype=dtype]\n        STFT matrix\n\n\n    See Also\n    --------\n    istft : Inverse STFT\n\n    ifgram : Instantaneous frequency spectrogram\n\n    np.pad : array padding\n\n    Notes\n    -----\n    This function caches at level 20.\n\n\n    Examples\n    --------\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> D = librosa.stft(y)\n    >>> D\n    array([[  2.576e-03 -0.000e+00j,   4.327e-02 -0.000e+00j, ...,\n              3.189e-04 -0.000e+00j,  -5.961e-06 -0.000e+00j],\n           [  2.441e-03 +2.884e-19j,   5.145e-02 -5.076e-03j, ...,\n             -3.885e-04 -7.253e-05j,   7.334e-05 +3.868e-04j],\n          ...,\n           [ -7.120e-06 -1.029e-19j,  -1.951e-09 -3.568e-06j, ...,\n             -4.912e-07 -1.487e-07j,   4.438e-06 -1.448e-05j],\n           [  7.136e-06 -0.000e+00j,   3.561e-06 -0.000e+00j, ...,\n             -5.144e-07 -0.000e+00j,  -1.514e-05 -0.000e+00j]], dtype=complex64)\n\n\n    Use left-aligned frames, instead of centered frames\n\n\n    >>> D_left = librosa.stft(y, center=False)\n\n\n    Use a shorter hop length\n\n\n    >>> D_short = librosa.stft(y, hop_length=64)\n\n\n    Display a spectrogram\n\n\n    >>> import matplotlib.pyplot as plt\n    >>> librosa.display.specshow(librosa.amplitude_to_db(D,\n    ...                                                  ref=np.max),\n    ...                          y_axis='log', x_axis='time')\n    >>> plt.title('Power spectrogram')\n    >>> plt.colorbar(format='%+2.0f dB')\n    >>> plt.tight_layout()\n\n    "
if (win_length is None):
    win_length = n_fft
if (hop_length is None):
    hop_length = int((win_length // 4))
fft_window = get_window(window, win_length, fftbins=True)
fft_window = util.pad_center(fft_window, n_fft)
fft_window = fft_window.reshape(((- 1), 1))
if center:
    util.valid_audio(y)
    tempResult = pad(y, int((n_fft // 2)), mode=pad_mode)
	
===================================================================	
phase_vocoder: 116	
----------------------------	

'Phase vocoder.  Given an STFT matrix D, speed up by a factor of `rate`\n\n    Based on the implementation provided by [1]_.\n\n    .. [1] Ellis, D. P. W. "A phase vocoder in Matlab."\n        Columbia University, 2002.\n        http://www.ee.columbia.edu/~dpwe/resources/matlab/pvoc/\n\n    Examples\n    --------\n    >>> # Play at double speed\n    >>> y, sr   = librosa.load(librosa.util.example_audio_file())\n    >>> D       = librosa.stft(y, n_fft=2048, hop_length=512)\n    >>> D_fast  = librosa.phase_vocoder(D, 2.0, hop_length=512)\n    >>> y_fast  = librosa.istft(D_fast, hop_length=512)\n\n    >>> # Or play at 1/3 speed\n    >>> y, sr   = librosa.load(librosa.util.example_audio_file())\n    >>> D       = librosa.stft(y, n_fft=2048, hop_length=512)\n    >>> D_slow  = librosa.phase_vocoder(D, 1./3, hop_length=512)\n    >>> y_slow  = librosa.istft(D_slow, hop_length=512)\n\n    Parameters\n    ----------\n    D : np.ndarray [shape=(d, t), dtype=complex]\n        STFT matrix\n\n    rate :  float > 0 [scalar]\n        Speed-up factor: `rate > 1` is faster, `rate < 1` is slower.\n\n    hop_length : int > 0 [scalar] or None\n        The number of samples between successive columns of `D`.\n\n        If None, defaults to `n_fft/4 = (D.shape[0]-1)/2`\n\n    Returns\n    -------\n    D_stretched  : np.ndarray [shape=(d, t / rate), dtype=complex]\n        time-stretched STFT\n    '
n_fft = (2 * (D.shape[0] - 1))
if (hop_length is None):
    hop_length = int((n_fft // 4))
time_steps = numpy.arange(0, D.shape[1], rate, dtype=numpy.float)
d_stretch = numpy.zeros((D.shape[0], len(time_steps)), D.dtype, order='F')
phi_advance = numpy.linspace(0, (numpy.pi * hop_length), D.shape[0])
phase_acc = numpy.angle(D[:, 0])
tempResult = pad(D, [(0, 0), (0, 2)], mode='constant')
	
===================================================================	
tempogram: 24	
----------------------------	

'Compute the tempogram: local autocorrelation of the onset strength envelope. [1]_\n\n    .. [1] Grosche, Peter, Meinard Mller, and Frank Kurth.\n        "Cyclic tempogram - A mid-level tempo representation for music signals."\n        ICASSP, 2010.\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)] or None\n        Audio time series.\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    onset_envelope : np.ndarray [shape=(n,)] or None\n        Optional pre-computed onset strength envelope as provided by\n        `onset.onset_strength`\n\n    hop_length : int > 0\n        number of audio samples between successive onset measurements\n\n    win_length : int > 0\n        length of the onset autocorrelation window (in frames/onset measurements)\n        The default settings (384) corresponds to `384 * hop_length / sr ~= 8.9s`.\n\n    center : bool\n        If `True`, onset autocorrelation windows are centered.\n        If `False`, windows are left-aligned.\n\n    window : string, function, number, tuple, or np.ndarray [shape=(win_length,)]\n        A window specification as in `core.stft`.\n\n    norm : {np.inf, -np.inf, 0, float > 0, None}\n        Normalization mode.  Set to `None` to disable normalization.\n\n    Returns\n    -------\n    tempogram : np.ndarray [shape=(win_length, n)]\n        Localized autocorrelation of the onset strength envelope\n\n    Raises\n    ------\n    ParameterError\n        if neither `y` nor `onset_envelope` are provided\n\n        if `win_length < 1`\n\n    See Also\n    --------\n    librosa.onset.onset_strength\n    librosa.util.normalize\n    librosa.core.stft\n\n\n    Examples\n    --------\n    >>> # Compute local onset autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> hop_length = 512\n    >>> oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n    >>> tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,\n    ...                                       hop_length=hop_length)\n    >>> # Compute global onset autocorrelation\n    >>> ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n    >>> ac_global = librosa.util.normalize(ac_global)\n    >>> # Estimate the global tempo for display purposes\n    >>> tempo = librosa.beat.tempo(onset_envelope=oenv, sr=sr,\n    ...                            hop_length=hop_length)[0]\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 8))\n    >>> plt.subplot(4, 1, 1)\n    >>> plt.plot(oenv, label=\'Onset strength\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(4, 1, 2)\n    >>> # We\'ll truncate the display to a narrower range of tempi\n    >>> librosa.display.specshow(tempogram, sr=sr, hop_length=hop_length,\n    >>>                          x_axis=\'time\', y_axis=\'tempo\')\n    >>> plt.axhline(tempo, color=\'w\', linestyle=\'--\', alpha=1,\n    ...             label=\'Estimated tempo={:g}\'.format(tempo))\n    >>> plt.legend(frameon=True, framealpha=0.75)\n    >>> plt.subplot(4, 1, 3)\n    >>> x = np.linspace(0, tempogram.shape[0] * float(hop_length) / sr,\n    ...                 num=tempogram.shape[0])\n    >>> plt.plot(x, np.mean(tempogram, axis=1), label=\'Mean local autocorrelation\')\n    >>> plt.plot(x, ac_global, \'--\', alpha=0.75, label=\'Global autocorrelation\')\n    >>> plt.xlabel(\'Lag (seconds)\')\n    >>> plt.axis(\'tight\')\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(4,1,4)\n    >>> # We can also plot on a BPM axis\n    >>> freqs = librosa.tempo_frequencies(tempogram.shape[0], hop_length=hop_length, sr=sr)\n    >>> plt.semilogx(freqs[1:], np.mean(tempogram[1:], axis=1),\n    ...              label=\'Mean local autocorrelation\', basex=2)\n    >>> plt.semilogx(freqs[1:], ac_global[1:], \'--\', alpha=0.75,\n    ...              label=\'Global autocorrelation\', basex=2)\n    >>> plt.axvline(tempo, color=\'black\', linestyle=\'--\', alpha=.8,\n    ...             label=\'Estimated tempo={:g}\'.format(tempo))\n    >>> plt.legend(frameon=True)\n    >>> plt.xlabel(\'BPM\')\n    >>> plt.axis(\'tight\')\n    >>> plt.grid()\n    >>> plt.tight_layout()\n    '
from ..onset import onset_strength
if (win_length < 1):
    raise ParameterError('win_length must be a positive integer')
ac_window = get_window(window, win_length, fftbins=True)
if (onset_envelope is None):
    if (y is None):
        raise ParameterError('Either y or onset_envelope must be provided')
    onset_envelope = onset_strength(y=y, sr=sr, hop_length=hop_length)
n = len(onset_envelope)
if center:
    tempResult = pad(onset_envelope, int((win_length // 2)), mode='linear_ramp', end_values=[0, 0])
	
===================================================================	
rmse: 115	
----------------------------	

"Compute root-mean-square (RMS) energy for each frame, either from the\n    audio samples `y` or from a spectrogram `S`.\n\n    Computing the energy from audio samples is faster as it doesn't require a\n    STFT calculation. However, using a spectrogram will give a more accurate\n    representation of energy over time because its frames can be windowed,\n    thus prefer using `S` if it's already available.\n\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)] or None\n        (optional) audio time series. Required if `S` is not input.\n\n    S : np.ndarray [shape=(d, t)] or None\n        (optional) spectrogram magnitude. Required if `y` is not input.\n\n    frame_length : int > 0 [scalar]\n        length of analysis frame (in samples) for energy calculation\n\n    hop_length : int > 0 [scalar]\n        hop length for STFT. See `librosa.core.stft` for details.\n\n    center : bool\n        If `True` and operating on time-domain input (`y`), pad the signal\n        by `frame_length//2` on either side.\n\n        If operating on spectrogram input, this has no effect.\n\n    pad_mode : str\n        Padding mode for centered analysis.  See `np.pad` for valid\n        values.\n\n    n_fft : [DEPRECATED]\n        .. warning:: This parameter name was deprecated in librosa 0.5.0\n            Use the `frame_length` parameter instead.\n            The `n_fft` parameter will be removed in librosa 0.6.0.\n\n    Returns\n    -------\n    rms : np.ndarray [shape=(1, t)]\n        RMS value for each frame\n\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> librosa.feature.rmse(y=y)\n    array([[ 0.   ,  0.056, ...,  0.   ,  0.   ]], dtype=float32)\n\n    Or from spectrogram input\n\n    >>> S, phase = librosa.magphase(librosa.stft(y))\n    >>> rms = librosa.feature.rmse(S=S)\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> plt.subplot(2, 1, 1)\n    >>> plt.semilogy(rms.T, label='RMS Energy')\n    >>> plt.xticks([])\n    >>> plt.xlim([0, rms.shape[-1]])\n    >>> plt.legend(loc='best')\n    >>> plt.subplot(2, 1, 2)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n    ...                          y_axis='log', x_axis='time')\n    >>> plt.title('log Power spectrogram')\n    >>> plt.tight_layout()\n\n    Use a STFT window of constant ones and no frame centering to get consistent\n    results with the RMS energy computed from the audio samples `y`\n\n    >>> S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]\n    >>> librosa.feature.rmse(S=S)\n\n    "
frame_length = rename_kw('n_fft', n_fft, 'frame_length', frame_length, '0.5', '0.6')
if ((y is not None) and (S is not None)):
    raise ValueError('Either `y` or `S` should be input.')
if (y is not None):
    y = to_mono(y)
    if center:
        tempResult = pad(y, int((frame_length // 2)), mode=pad_mode)
	
===================================================================	
zero_crossing_rate: 138	
----------------------------	

'Compute the zero-crossing rate of an audio time series.\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)]\n        Audio time series\n\n    frame_length : int > 0\n        Length of the frame over which to compute zero crossing rates\n\n    hop_length : int > 0\n        Number of samples to advance for each frame\n\n    center : bool\n        If `True`, frames are centered by padding the edges of `y`.\n        This is similar to the padding in `librosa.core.stft`,\n        but uses edge-value copies instead of reflection.\n\n    kwargs : additional keyword arguments\n        See `librosa.core.zero_crossings`\n\n        .. note:: By default, the `pad` parameter is set to `False`, which\n            differs from the default specified by\n            `librosa.core.zero_crossings`.\n\n    Returns\n    -------\n    zcr : np.ndarray [shape=(1, t)]\n        `zcr[0, i]` is the fraction of zero crossings in the\n        `i` th frame\n\n    See Also\n    --------\n    librosa.core.zero_crossings\n        Compute zero-crossings in a time-series\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> librosa.feature.zero_crossing_rate(y)\n    array([[ 0.134,  0.139, ...,  0.387,  0.322]])\n\n    '
util.valid_audio(y)
if center:
    tempResult = pad(y, int((frame_length // 2)), mode='edge')
	
===================================================================	
stack_memory: 48	
----------------------------	

"Short-term history embedding: vertically concatenate a data\n    vector or matrix with delayed copies of itself.\n\n    Each column `data[:, i]` is mapped to::\n\n        data[:, i] ->  [data[:, i],\n                        data[:, i - delay],\n                        ...\n                        data[:, i - (n_steps-1)*delay]]\n\n    For columns `i < (n_steps - 1) * delay` , the data will be padded.\n    By default, the data is padded with zeros, but this behavior can be\n    overridden by supplying additional keyword arguments which are passed\n    to `np.pad()`.\n\n\n    Parameters\n    ----------\n    data : np.ndarray [shape=(t,) or (d, t)]\n        Input data matrix.  If `data` is a vector (`data.ndim == 1`),\n        it will be interpreted as a row matrix and reshaped to `(1, t)`.\n\n    n_steps : int > 0 [scalar]\n        embedding dimension, the number of steps back in time to stack\n\n    delay : int != 0 [scalar]\n        the number of columns to step.\n\n        Positive values embed from the past (previous columns).\n\n        Negative values embed from the future (subsequent columns).\n\n    kwargs : additional keyword arguments\n      Additional arguments to pass to `np.pad`.\n\n    Returns\n    -------\n    data_history : np.ndarray [shape=(m * d, t)]\n        data augmented with lagged copies of itself,\n        where `m == n_steps - 1`.\n\n    Notes\n    -----\n    This function caches at level 40.\n\n\n    Examples\n    --------\n    Keep two steps (current and previous)\n\n    >>> data = np.arange(-3, 3)\n    >>> librosa.feature.stack_memory(data)\n    array([[-3, -2, -1,  0,  1,  2],\n           [ 0, -3, -2, -1,  0,  1]])\n\n    Or three steps\n\n    >>> librosa.feature.stack_memory(data, n_steps=3)\n    array([[-3, -2, -1,  0,  1,  2],\n           [ 0, -3, -2, -1,  0,  1],\n           [ 0,  0, -3, -2, -1,  0]])\n\n    Use reflection padding instead of zero-padding\n\n    >>> librosa.feature.stack_memory(data, n_steps=3, mode='reflect')\n    array([[-3, -2, -1,  0,  1,  2],\n           [-2, -3, -2, -1,  0,  1],\n           [-1, -2, -3, -2, -1,  0]])\n\n    Or pad with edge-values, and delay by 2\n\n    >>> librosa.feature.stack_memory(data, n_steps=3, delay=2, mode='edge')\n    array([[-3, -2, -1,  0,  1,  2],\n           [-3, -3, -3, -2, -1,  0],\n           [-3, -3, -3, -3, -3, -2]])\n\n    Stack time-lagged beat-synchronous chroma edge padding\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    >>> tempo, beats = librosa.beat.beat_track(y=y, sr=sr, hop_length=512)\n    >>> beats = librosa.util.fix_frames(beats, x_min=0, x_max=chroma.shape[1])\n    >>> chroma_sync = librosa.util.sync(chroma, beats)\n    >>> chroma_lag = librosa.feature.stack_memory(chroma_sync, n_steps=3,\n    ...                                           mode='edge')\n\n    Plot the result\n\n    >>> import matplotlib.pyplot as plt\n    >>> beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=512)\n    >>> librosa.display.specshow(chroma_lag, y_axis='chroma', x_axis='time',\n    ...                          x_coords=beat_times)\n    >>> plt.yticks([0, 12, 24], ['Lag=0', 'Lag=1', 'Lag=2'])\n    >>> plt.title('Time-lagged chroma')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n    "
if (n_steps < 1):
    raise ParameterError('n_steps must be a positive integer')
if (delay == 0):
    raise ParameterError('delay must be a non-zero integer')
data = numpy.atleast_2d(data)
t = data.shape[1]
kwargs.setdefault('mode', 'constant')
if (kwargs['mode'] == 'constant'):
    kwargs.setdefault('constant_values', [0])
if (delay > 0):
    padding = (int(((n_steps - 1) * delay)), 0)
else:
    padding = (0, int(((n_steps - 1) * (- delay))))
tempResult = pad(data, [(0, 0), padding], **kwargs)
	
===================================================================	
delta: 23	
----------------------------	

"Compute delta features: local estimate of the derivative\n    of the input data along the selected axis.\n\n\n    Parameters\n    ----------\n    data      : np.ndarray\n        the input data matrix (eg, spectrogram)\n\n    width     : int >= 3, odd [scalar]\n        Number of frames over which to compute the delta feature\n\n    order     : int > 0 [scalar]\n        the order of the difference operator.\n        1 for first derivative, 2 for second, etc.\n\n    axis      : int [scalar]\n        the axis along which to compute deltas.\n        Default is -1 (columns).\n\n    trim      : bool\n        set to `True` to trim the output matrix to the original size.\n\n    Returns\n    -------\n    delta_data   : np.ndarray [shape=(d, t) or (d, t + window)]\n        delta matrix of `data`.\n\n    Notes\n    -----\n    This function caches at level 40.\n\n    Examples\n    --------\n    Compute MFCC deltas, delta-deltas\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> mfcc = librosa.feature.mfcc(y=y, sr=sr)\n    >>> mfcc_delta = librosa.feature.delta(mfcc)\n    >>> mfcc_delta\n    array([[  2.929e+01,   3.090e+01, ...,   0.000e+00,   0.000e+00],\n           [  2.226e+01,   2.553e+01, ...,   3.944e-31,   3.944e-31],\n           ...,\n           [ -1.192e+00,  -6.099e-01, ...,   9.861e-32,   9.861e-32],\n           [ -5.349e-01,  -2.077e-01, ...,   1.183e-30,   1.183e-30]])\n    >>> mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n    >>> mfcc_delta2\n    array([[  1.281e+01,   1.020e+01, ...,   0.000e+00,   0.000e+00],\n           [  2.726e+00,   3.558e+00, ...,   0.000e+00,   0.000e+00],\n           ...,\n           [ -1.702e-01,  -1.509e-01, ...,   0.000e+00,   0.000e+00],\n           [ -9.021e-02,  -7.007e-02, ...,  -2.190e-47,  -2.190e-47]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.specshow(mfcc)\n    >>> plt.title('MFCC')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.specshow(mfcc_delta)\n    >>> plt.title(r'MFCC-$\\Delta$')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 3)\n    >>> librosa.display.specshow(mfcc_delta2, x_axis='time')\n    >>> plt.title(r'MFCC-$\\Delta^2$')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n\n    "
data = numpy.atleast_1d(data)
if ((width < 3) or (numpy.mod(width, 2) != 1)):
    raise ParameterError('width must be an odd integer >= 3')
if ((order <= 0) or (not isinstance(order, int))):
    raise ParameterError('order must be a positive integer')
half_length = (1 + int((width // 2)))
window = numpy.arange((half_length - 1.0), (- half_length), (- 1.0))
window /= numpy.sum((numpy.abs(window) ** 2))
padding = ([(0, 0)] * data.ndim)
width = int(width)
padding[axis] = (width, width)
tempResult = pad(data, padding, mode='edge')
	
===================================================================	
pad_center: 62	
----------------------------	

"Wrapper for np.pad to automatically center an array prior to padding.\n    This is analogous to `str.center()`\n\n    Examples\n    --------\n    >>> # Generate a vector\n    >>> data = np.ones(5)\n    >>> librosa.util.pad_center(data, 10, mode='constant')\n    array([ 0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.])\n\n    >>> # Pad a matrix along its first dimension\n    >>> data = np.ones((3, 5))\n    >>> librosa.util.pad_center(data, 7, axis=0)\n    array([[ 0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.],\n           [ 1.,  1.,  1.,  1.,  1.],\n           [ 1.,  1.,  1.,  1.,  1.],\n           [ 1.,  1.,  1.,  1.,  1.],\n           [ 0.,  0.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  0.,  0.]])\n    >>> # Or its second dimension\n    >>> librosa.util.pad_center(data, 7, axis=1)\n    array([[ 0.,  1.,  1.,  1.,  1.,  1.,  0.],\n           [ 0.,  1.,  1.,  1.,  1.,  1.,  0.],\n           [ 0.,  1.,  1.,  1.,  1.,  1.,  0.]])\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Vector to be padded and centered\n\n    size : int >= len(data) [scalar]\n        Length to pad `data`\n\n    axis : int\n        Axis along which to pad and center the data\n\n    kwargs : additional keyword arguments\n      arguments passed to `np.pad()`\n\n    Returns\n    -------\n    data_padded : np.ndarray\n        `data` centered and padded to length `size` along the\n        specified axis\n\n    Raises\n    ------\n    ParameterError\n        If `size < data.shape[axis]`\n\n    See Also\n    --------\n    numpy.pad\n    "
kwargs.setdefault('mode', 'constant')
n = data.shape[axis]
lpad = int(((size - n) // 2))
lengths = ([(0, 0)] * data.ndim)
lengths[axis] = (lpad, int(((size - n) - lpad)))
if (lpad < 0):
    raise ParameterError('Target size ({:d}) must be at least input size ({:d})'.format(size, n))
tempResult = pad(data, lengths, **kwargs)
	
===================================================================	
fix_length: 75	
----------------------------	

"Fix the length an array `data` to exactly `size`.\n\n    If `data.shape[axis] < n`, pad according to the provided kwargs.\n    By default, `data` is padded with trailing zeros.\n\n    Examples\n    --------\n    >>> y = np.arange(7)\n    >>> # Default: pad with zeros\n    >>> librosa.util.fix_length(y, 10)\n    array([0, 1, 2, 3, 4, 5, 6, 0, 0, 0])\n    >>> # Trim to a desired length\n    >>> librosa.util.fix_length(y, 5)\n    array([0, 1, 2, 3, 4])\n    >>> # Use edge-padding instead of zeros\n    >>> librosa.util.fix_length(y, 10, mode='edge')\n    array([0, 1, 2, 3, 4, 5, 6, 6, 6, 6])\n\n    Parameters\n    ----------\n    data : np.ndarray\n      array to be length-adjusted\n\n    size : int >= 0 [scalar]\n      desired length of the array\n\n    axis : int, <= data.ndim\n      axis along which to fix length\n\n    kwargs : additional keyword arguments\n        Parameters to `np.pad()`\n\n    Returns\n    -------\n    data_fixed : np.ndarray [shape=data.shape]\n        `data` either trimmed or padded to length `size`\n        along the specified axis.\n\n    See Also\n    --------\n    numpy.pad\n    "
kwargs.setdefault('mode', 'constant')
n = data.shape[axis]
if (n > size):
    slices = ([slice(None)] * data.ndim)
    slices[axis] = slice(0, size)
    return data[slices]
elif (n < size):
    lengths = ([(0, 0)] * data.ndim)
    lengths[axis] = (0, (size - n))
    tempResult = pad(data, lengths, **kwargs)
	
===================================================================	
localmax: 203	
----------------------------	

'Find local maxima in an array `x`.\n\n    Examples\n    --------\n    >>> x = np.array([1, 0, 1, 2, -1, 0, -2, 1])\n    >>> librosa.util.localmax(x)\n    array([False, False, False,  True, False,  True, False,  True], dtype=bool)\n\n    >>> # Two-dimensional example\n    >>> x = np.array([[1,0,1], [2, -1, 0], [2, 1, 3]])\n    >>> librosa.util.localmax(x, axis=0)\n    array([[False, False, False],\n           [ True, False, False],\n           [False,  True,  True]], dtype=bool)\n    >>> librosa.util.localmax(x, axis=1)\n    array([[False, False,  True],\n           [False, False,  True],\n           [False, False,  True]], dtype=bool)\n\n    Parameters\n    ----------\n    x     : np.ndarray [shape=(d1,d2,...)]\n      input vector or array\n\n    axis : int\n      axis along which to compute local maximality\n\n    Returns\n    -------\n    m     : np.ndarray [shape=x.shape, dtype=bool]\n        indicator array of local maximality along `axis`\n\n    '
paddings = ([(0, 0)] * x.ndim)
paddings[axis] = (1, 1)
tempResult = pad(x, paddings, mode='edge')
	
===================================================================	
__test_default_norm: 60	
----------------------------	

DATA = load(infile)
wts = librosa.filters.mel(DATA['sr'][(0, 0)], DATA['nfft'][(0, 0)], n_mels=DATA['nfilts'][(0, 0)], fmin=DATA['fmin'][(0, 0)], fmax=DATA['fmax'][(0, 0)], htk=DATA['htk'][(0, 0)])
tempResult = pad(wts, [(0, 0), (0, int(((DATA['nfft'][0] // 2) - 1)))], mode='constant')
	
===================================================================	
__test_with_norm: 73	
----------------------------	

DATA = load(infile)
if (DATA['norm'].shape[(- 1)] == 0):
    norm = None
else:
    norm = DATA['norm'][(0, 0)]
wts = librosa.filters.mel(DATA['sr'][(0, 0)], DATA['nfft'][(0, 0)], n_mels=DATA['nfilts'][(0, 0)], fmin=DATA['fmin'][(0, 0)], fmax=DATA['fmax'][(0, 0)], htk=DATA['htk'][(0, 0)], norm=norm)
tempResult = pad(wts, [(0, 0), (0, int(((DATA['nfft'][0] // 2) - 1)))], mode='constant')
	
===================================================================	
__test: 102	
----------------------------	

DATA = load(infile)
octwidth = DATA['octwidth'][(0, 0)]
if (octwidth == 0):
    octwidth = None
wts = librosa.filters.chroma(DATA['sr'][(0, 0)], DATA['nfft'][(0, 0)], DATA['nchroma'][(0, 0)], A440=DATA['a440'][(0, 0)], ctroct=DATA['ctroct'][(0, 0)], octwidth=octwidth, norm=2, base_c=False)
tempResult = pad(wts, [(0, 0), (0, int(((DATA['nfft'][(0, 0)] // 2) - 1)))], mode='constant')
	
***************************************************	
mne_python-0.15.0: 2	
===================================================================	
_smart_pad: 171	
----------------------------	

'Pad vector x.'
n_pad = numpy.asarray(n_pad)
assert (n_pad.shape == (2,))
if (n_pad == 0).all():
    return x
elif (n_pad < 0).any():
    raise RuntimeError('n_pad must be non-negative')
if (pad == 'reflect_limited'):
    l_z_pad = numpy.zeros(max(((n_pad[0] - len(x)) + 1), 0), dtype=x.dtype)
    r_z_pad = numpy.zeros(max(((n_pad[0] - len(x)) + 1), 0), dtype=x.dtype)
    return numpy.concatenate([l_z_pad, ((2 * x[0]) - x[n_pad[0]:0:(- 1)]), x, ((2 * x[(- 1)]) - x[(- 2):((- n_pad[1]) - 2):(- 1)]), r_z_pad])
else:
    tempResult = pad(x, (tuple(n_pad),), pad)
	
===================================================================	
_sensor_shape: 891	
----------------------------	

'Get the sensor shape vertices.'
rrs = numpy.empty([0, 2])
tris = numpy.empty([0, 3], int)
id_ = (coil['type'] & 65535)
if (id_ in (2, 3012, 3013, 3011)):
    long_side = coil['size']
    offset = 0.0025
    rrs = numpy.array([[offset, ((- long_side) / 2.0)], [(long_side / 2.0), ((- long_side) / 2.0)], [(long_side / 2.0), (long_side / 2.0)], [offset, (long_side / 2.0)], [(- offset), ((- long_side) / 2.0)], [((- long_side) / 2.0), ((- long_side) / 2.0)], [((- long_side) / 2.0), (long_side / 2.0)], [(- offset), (long_side / 2.0)]])
    tris = numpy.concatenate((_make_tris_fan(4), (_make_tris_fan(4) + 4)), axis=0)
elif (id_ in (2000, 3022, 3023, 3024)):
    size = (0.001 if (id_ == 2000) else (coil['size'] / 2.0))
    rrs = (numpy.array([[(- 1.0), 1.0], [1.0, 1.0], [1.0, (- 1.0)], [(- 1.0), (- 1.0)]]) * size)
    tris = _make_tris_fan(4)
elif (id_ in (4001, 4003, 5002, 7002, 7003, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_MAG)):
    n_pts = 15
    circle = numpy.exp((((2j * numpy.pi) * numpy.arange(n_pts)) / float(n_pts)))
    circle = numpy.concatenate(([0.0], circle))
    circle *= (coil['size'] / 2.0)
    rrs = np.array([circle.real, circle.imag]).T
    tris = _make_tris_fan((n_pts + 1))
elif (id_ in (4002, 5001, 5003, 5004, 4004, 4005, 6001, 7001, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_GRAD, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_GRAD)):
    baseline = (coil['base'] if (id_ in (5004, 4005)) else 0.0)
    n_pts = 16
    circle = numpy.exp((((2j * numpy.pi) * numpy.arange((- 1), n_pts)) / float((n_pts - 1))))
    circle[0] = 0
    circle *= (coil['size'] / 2.0)
    rrs = np.array([np.concatenate([(circle.real + (baseline / 2.0)), (circle.real - (baseline / 2.0))]), np.concatenate([circle.imag, (- circle.imag)])]).T
    tris = numpy.concatenate([_make_tris_fan((n_pts + 1)), ((_make_tris_fan((n_pts + 1)) + n_pts) + 1)])
tempResult = pad(rrs, ((0, 0), (0, 1)), mode='constant')
	
***************************************************	

astropy_astropy-1.3.0: 4	
===================================================================	
FitnessFunc.validate_input: 41	
----------------------------	

'Validate inputs to the model.\n\n        Parameters\n        ----------\n        t : array_like\n            times of observations\n        x : array_like (optional)\n            values observed at each time\n        sigma : float or array_like (optional)\n            errors in values x\n\n        Returns\n        -------\n        t, x, sigma : array_like, float or None\n            validated and perhaps modified versions of inputs\n        '
t = numpy.asarray(t, dtype=float)
if (x is not None):
    x = numpy.asarray(x)
if (sigma is not None):
    sigma = numpy.asarray(sigma)
t = numpy.array(t)
if (t.ndim != 1):
    raise ValueError('t must be a one-dimensional array')
tempResult = unique(t, return_index=True, return_inverse=True)
	
===================================================================	
add_at: 16	
----------------------------	

'Utility that computes np.add.at()\n\n    The fast version is available only in Numpy 1.8+; for older versions of\n    numpy this defaults to a slower computation.\n    '
if hasattr(numpy.ufunc, 'at'):
    return numpy.add.at(arr, ind, vals)
else:
    warnings.warn('Using slow replacement for numpy.add.at(). For ~100x faster results update to numpy 1.8+')
    arr = numpy.asarray(arr)
    (ind, vals) = numpy.broadcast_arrays(ind, vals)
    tempResult = unique(ind)
	
===================================================================	
test_regular_events: 46	
----------------------------	

rng = numpy.random.RandomState(0)
dt = 0.01
tempResult = unique(rng.randint(0, 500, 100))
	
===================================================================	
test_regular_events: 46	
----------------------------	

rng = numpy.random.RandomState(0)
dt = 0.01
tempResult = unique(rng.randint(500, 1000, 200))
	
***************************************************	
scipy_scipy-0.19.0: 43	
===================================================================	
leaders: 890	
----------------------------	

"\n    Returns the root nodes in a hierarchical clustering.\n\n    Returns the root nodes in a hierarchical clustering corresponding\n    to a cut defined by a flat cluster assignment vector ``T``. See\n    the ``fcluster`` function for more information on the format of ``T``.\n\n    For each flat cluster :math:`j` of the :math:`k` flat clusters\n    represented in the n-sized flat cluster assignment vector ``T``,\n    this function finds the lowest cluster node :math:`i` in the linkage\n    tree Z such that:\n\n      * leaf descendents belong only to flat cluster j\n        (i.e. ``T[p]==j`` for all :math:`p` in :math:`S(i)` where\n        :math:`S(i)` is the set of leaf ids of leaf nodes descendent\n        with cluster node :math:`i`)\n\n      * there does not exist a leaf that is not descendent with\n        :math:`i` that also belongs to cluster :math:`j`\n        (i.e. ``T[q]!=j`` for all :math:`q` not in :math:`S(i)`).  If\n        this condition is violated, ``T`` is not a valid cluster\n        assignment vector, and an exception will be thrown.\n\n    Parameters\n    ----------\n    Z : ndarray\n        The hierarchical clustering encoded as a matrix. See\n        `linkage` for more information.\n    T : ndarray\n        The flat cluster assignment vector.\n\n    Returns\n    -------\n    L : ndarray\n        The leader linkage node id's stored as a k-element 1-D array\n        where ``k`` is the number of flat clusters found in ``T``.\n\n        ``L[j]=i`` is the linkage cluster node id that is the\n        leader of flat cluster with id M[j].  If ``i < n``, ``i``\n        corresponds to an original observation, otherwise it\n        corresponds to a non-singleton cluster.\n\n        For example: if ``L[3]=2`` and ``M[3]=8``, the flat cluster with\n        id 8's leader is linkage node 2.\n    M : ndarray\n        The leader linkage node id's stored as a k-element 1-D array where\n        ``k`` is the number of flat clusters found in ``T``. This allows the\n        set of flat cluster ids to be any arbitrary set of ``k`` integers.\n\n    "
Z = numpy.asarray(Z, order='c')
T = numpy.asarray(T, order='c')
if ((type(T) != numpy.ndarray) or (T.dtype != 'i')):
    raise TypeError('T must be a one-dimensional numpy array of integers.')
is_valid_linkage(Z, throw=True, name='Z')
if (len(T) != (Z.shape[0] + 1)):
    raise ValueError('Mismatch: len(T)!=Z.shape[0] + 1.')
tempResult = unique(T)
	
===================================================================	
BSpline.__init__: 50	
----------------------------	

super(BSpline, self).__init__()
self.k = int(k)
self.c = numpy.asarray(c)
self.t = numpy.ascontiguousarray(t, dtype=numpy.float64)
self.extrapolate = bool(extrapolate)
n = ((self.t.shape[0] - self.k) - 1)
if (not (0 <= axis < self.c.ndim)):
    raise ValueError(('%s must be between 0 and %s' % (axis, c.ndim)))
self.axis = axis
if (axis != 0):
    self.c = numpy.rollaxis(self.c, axis)
if (k < 0):
    raise ValueError('Spline order cannot be negative.')
if (int(k) != k):
    raise ValueError('Spline order must be integer.')
if (self.t.ndim != 1):
    raise ValueError('Knot vector must be one-dimensional.')
if (n < (self.k + 1)):
    raise ValueError(('Need at least %d knots for degree %d' % (((2 * k) + 2), k)))
if (np.diff(self.t) < 0).any():
    raise ValueError('Knots must be in a non-decreasing order.')
tempResult = unique(self.t[k:(n + 1)])
	
===================================================================	
check_splev: 272	
----------------------------	

(t, c, k) = b.tck
tempResult = unique(t)
	
===================================================================	
TestPPolyCommon.test_extend: 514	
----------------------------	

numpy.random.seed(1234)
order = 3
tempResult = unique(numpy.r_[(0, (10 * numpy.random.rand(30)), 10)])
	
===================================================================	
TestPPoly.test_roots_random: 939	
----------------------------	

numpy.random.seed(1234)
num = 0
for extrapolate in (True, False):
    for order in range(0, 20):
        tempResult = unique(numpy.r_[(0, (10 * numpy.random.rand(30)), 10)])
	
===================================================================	
_select: 241	
----------------------------	

'Returns min, max, or both, plus their positions (if requested), and\n    median.'
input = numpy.asanyarray(input)
find_positions = (find_min_positions or find_max_positions)
positions = None
if find_positions:
    positions = numpy.arange(input.size).reshape(input.shape)

def single_group(vals, positions):
    result = []
    if find_min:
        result += [vals.min()]
    if find_min_positions:
        result += [positions[(vals == vals.min())][0]]
    if find_max:
        result += [vals.max()]
    if find_max_positions:
        result += [positions[(vals == vals.max())][0]]
    if find_median:
        result += [numpy.median(vals)]
    return result
if (labels is None):
    return single_group(input, positions)
(input, labels) = numpy.broadcast_arrays(input, labels)
if (index is None):
    mask = (labels > 0)
    masked_positions = None
    if find_positions:
        masked_positions = positions[mask]
    return single_group(input[mask], masked_positions)
if numpy.isscalar(index):
    mask = (labels == index)
    masked_positions = None
    if find_positions:
        masked_positions = positions[mask]
    return single_group(input[mask], masked_positions)
if ((not _safely_castable_to_int(labels.dtype)) or (labels.min() < 0) or (labels.max() > labels.size)):
    tempResult = unique(labels, return_inverse=True)
	
===================================================================	
_stats: 158	
----------------------------	

'Count, sum, and optionally compute (sum - centre)^2 of input by label\n\n    Parameters\n    ----------\n    input : array_like, n-dimensional\n        The input data to be analyzed.\n    labels : array_like (n-dimensional), optional\n        The labels of the data in `input`.  This array must be broadcast\n        compatible with `input`; typically it is the same shape as `input`.\n        If `labels` is None, all nonzero values in `input` are treated as\n        the single labeled group.\n    index : label or sequence of labels, optional\n        These are the labels of the groups for which the stats are computed.\n        If `index` is None, the stats are computed for the single group where\n        `labels` is greater than 0.\n    centered : bool, optional\n        If True, the centered sum of squares for each labeled group is\n        also returned.  Default is False.\n\n    Returns\n    -------\n    counts : int or ndarray of ints\n        The number of elements in each labeled group.\n    sums : scalar or ndarray of scalars\n        The sums of the values in each labeled group.\n    sums_c : scalar or ndarray of scalars, optional\n        The sums of mean-centered squares of the values in each labeled group.\n        This is only returned if `centered` is True.\n\n    '

def single_group(vals):
    if centered:
        vals_c = (vals - vals.mean())
        return (vals.size, vals.sum(), (vals_c * vals_c.conjugate()).sum())
    else:
        return (vals.size, vals.sum())
if (labels is None):
    return single_group(input)
(input, labels) = numpy.broadcast_arrays(input, labels)
if (index is None):
    return single_group(input[(labels > 0)])
if numpy.isscalar(index):
    return single_group(input[(labels == index)])

def _sum_centered(labels):
    means = (sums / counts)
    centered_input = (input - means[labels])
    bc = numpy.bincount(labels.ravel(), weights=(centered_input * centered_input.conjugate()).ravel())
    return bc
if ((not _safely_castable_to_int(labels.dtype)) or (labels.min() < 0) or (labels.max() > labels.size)):
    tempResult = unique(labels, return_inverse=True)
	
===================================================================	
TestDifferentialEvolutionSolver.test_select_samples: 164	
----------------------------	

limits = np.arange(12.0, dtype='float64').reshape(2, 6)
bounds = list(zip(limits[0, :], limits[1, :]))
solver = DifferentialEvolutionSolver(None, bounds, popsize=1)
candidate = 0
(r1, r2, r3, r4, r5) = solver._select_samples(candidate, 5)
tempResult = unique(numpy.array([candidate, r1, r2, r3, r4, r5]))
	
===================================================================	
max_len_seq: 16	
----------------------------	

"\n    Maximum length sequence (MLS) generator.\n\n    Parameters\n    ----------\n    nbits : int\n        Number of bits to use. Length of the resulting sequence will\n        be ``(2**nbits) - 1``. Note that generating long sequences\n        (e.g., greater than ``nbits == 16``) can take a long time.\n    state : array_like, optional\n        If array, must be of length ``nbits``, and will be cast to binary\n        (bool) representation. If None, a seed of ones will be used,\n        producing a repeatable representation. If ``state`` is all\n        zeros, an error is raised as this is invalid. Default: None.\n    length : int, optional\n        Number of samples to compute. If None, the entire length\n        ``(2**nbits) - 1`` is computed.\n    taps : array_like, optional\n        Polynomial taps to use (e.g., ``[7, 6, 1]`` for an 8-bit sequence).\n        If None, taps will be automatically selected (for up to\n        ``nbits == 32``).\n\n    Returns\n    -------\n    seq : array\n        Resulting MLS sequence of 0's and 1's.\n    state : array\n        The final state of the shift register.\n\n    Notes\n    -----\n    The algorithm for MLS generation is generically described in:\n\n        https://en.wikipedia.org/wiki/Maximum_length_sequence\n\n    The default values for taps are specifically taken from the first\n    option listed for each value of ``nbits`` in:\n\n        http://www.newwaveinstruments.com/resources/articles/\n            m_sequence_linear_feedback_shift_register_lfsr.htm\n\n    .. versionadded:: 0.15.0\n\n    Examples\n    --------\n    MLS uses binary convention:\n\n    >>> from scipy.signal import max_len_seq\n    >>> max_len_seq(4)[0]\n    array([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0], dtype=int8)\n\n    MLS has a white spectrum (except for DC):\n\n    >>> import matplotlib.pyplot as plt\n    >>> from numpy.fft import fft, ifft, fftshift, fftfreq\n    >>> seq = max_len_seq(6)[0]*2-1  # +1 and -1\n    >>> spec = fft(seq)\n    >>> N = len(seq)\n    >>> plt.plot(fftshift(fftfreq(N)), fftshift(np.abs(spec)), '.-')\n    >>> plt.margins(0.1, 0.1)\n    >>> plt.grid(True)\n    >>> plt.show()\n\n    Circular autocorrelation of MLS is an impulse:\n\n    >>> acorrcirc = ifft(spec * np.conj(spec)).real\n    >>> plt.figure()\n    >>> plt.plot(np.arange(-N/2+1, N/2+1), fftshift(acorrcirc), '.-')\n    >>> plt.margins(0.1, 0.1)\n    >>> plt.grid(True)\n    >>> plt.show()\n\n    Linear autocorrelation of MLS is approximately an impulse:\n\n    >>> acorr = np.correlate(seq, seq, 'full')\n    >>> plt.figure()\n    >>> plt.plot(np.arange(-N+1, N), acorr, '.-')\n    >>> plt.margins(0.1, 0.1)\n    >>> plt.grid(True)\n    >>> plt.show()\n\n    "
if (taps is None):
    if (nbits not in _mls_taps):
        known_taps = numpy.array(list(_mls_taps.keys()))
        raise ValueError(('nbits must be between %s and %s if taps is None' % (known_taps.min(), known_taps.max())))
    taps = numpy.array(_mls_taps[nbits], numpy.intp)
else:
    tempResult = unique(numpy.array(taps, numpy.intp))
	
===================================================================	
_cs_matrix._insert_many: 550	
----------------------------	

'Inserts new nonzero at each (i, j) with value x\n\n        Here (i,j) index major and minor respectively.\n        i, j and x must be non-empty, 1d arrays.\n        Inserts each major group (e.g. all entries per row) at a time.\n        Maintains has_sorted_indices property.\n        Modifies i, j, x in place.\n        '
order = numpy.argsort(i, kind='mergesort')
i = i.take(order, mode='clip')
j = j.take(order, mode='clip')
x = x.take(order, mode='clip')
do_sort = self.has_sorted_indices
idx_dtype = get_index_dtype((self.indices, self.indptr), maxval=(self.indptr[(- 1)] + x.size))
self.indptr = numpy.asarray(self.indptr, dtype=idx_dtype)
self.indices = numpy.asarray(self.indices, dtype=idx_dtype)
i = numpy.asarray(i, dtype=idx_dtype)
j = numpy.asarray(j, dtype=idx_dtype)
indices_parts = []
data_parts = []
tempResult = unique(i, return_index=True)
	
===================================================================	
_cs_matrix._insert_many: 559	
----------------------------	

'Inserts new nonzero at each (i, j) with value x\n\n        Here (i,j) index major and minor respectively.\n        i, j and x must be non-empty, 1d arrays.\n        Inserts each major group (e.g. all entries per row) at a time.\n        Maintains has_sorted_indices property.\n        Modifies i, j, x in place.\n        '
order = numpy.argsort(i, kind='mergesort')
i = i.take(order, mode='clip')
j = j.take(order, mode='clip')
x = x.take(order, mode='clip')
do_sort = self.has_sorted_indices
idx_dtype = get_index_dtype((self.indices, self.indptr), maxval=(self.indptr[(- 1)] + x.size))
self.indptr = numpy.asarray(self.indptr, dtype=idx_dtype)
self.indices = numpy.asarray(self.indices, dtype=idx_dtype)
i = numpy.asarray(i, dtype=idx_dtype)
j = numpy.asarray(j, dtype=idx_dtype)
indices_parts = []
data_parts = []
(ui, ui_indptr) = numpy.unique(i, return_index=True)
ui_indptr = numpy.append(ui_indptr, len(j))
new_nnzs = numpy.diff(ui_indptr)
prev = 0
for (c, (ii, js, je)) in enumerate(izip(ui, ui_indptr, ui_indptr[1:])):
    start = self.indptr[prev]
    stop = self.indptr[ii]
    indices_parts.append(self.indices[start:stop])
    data_parts.append(self.data[start:stop])
    tempResult = unique(j[js:je][::(- 1)], return_index=True)
	
===================================================================	
coo_matrix.todia: 174	
----------------------------	

from .dia import dia_matrix
self.sum_duplicates()
ks = (self.col - self.row)
tempResult = unique(ks, return_inverse=True)
	
===================================================================	
dia_matrix.__init__: 67	
----------------------------	

data._data_matrix.__init__(self)
if isspmatrix_dia(arg1):
    if copy:
        arg1 = arg1.copy()
    self.data = arg1.data
    self.offsets = arg1.offsets
    self.shape = arg1.shape
elif isspmatrix(arg1):
    if (isspmatrix_dia(arg1) and copy):
        A = arg1.copy()
    else:
        A = arg1.todia()
    self.data = A.data
    self.offsets = A.offsets
    self.shape = A.shape
elif isinstance(arg1, tuple):
    if isshape(arg1):
        self.shape = arg1
        self.data = numpy.zeros((0, 0), getdtype(dtype, default=float))
        idx_dtype = get_index_dtype(maxval=max(self.shape))
        self.offsets = numpy.zeros(0, dtype=idx_dtype)
    else:
        try:
            (data, offsets) = arg1
        except:
            raise ValueError('unrecognized form for dia_matrix constructor')
        else:
            if (shape is None):
                raise ValueError('expected a shape argument')
            self.data = numpy.atleast_2d(numpy.array(arg1[0], dtype=dtype, copy=copy))
            self.offsets = numpy.atleast_1d(numpy.array(arg1[1], dtype=get_index_dtype(maxval=max(shape)), copy=copy))
            self.shape = shape
else:
    try:
        arg1 = numpy.asarray(arg1)
    except:
        raise ValueError(('unrecognized form for %s_matrix constructor' % self.format))
    from .coo import coo_matrix
    A = coo_matrix(arg1, dtype=dtype, shape=shape).todia()
    self.data = A.data
    self.offsets = A.offsets
    self.shape = A.shape
if (dtype is not None):
    self.data = self.data.astype(dtype)
if (self.offsets.ndim != 1):
    raise ValueError('offsets array must have rank 1')
if (self.data.ndim != 2):
    raise ValueError('data array must have rank 2')
if (self.data.shape[0] != len(self.offsets)):
    raise ValueError(('number of diagonals (%d) does not match the number of offsets (%d)' % (self.data.shape[0], len(self.offsets))))
tempResult = unique(self.offsets)
	
===================================================================	
TestConvexHull.check: 384	
----------------------------	

points = DATASETS[name]
tri = scipy.spatial.qhull.Delaunay(points)
hull = scipy.spatial.qhull.ConvexHull(points)
assert_hulls_equal(points, tri.convex_hull, hull.simplices)
if (points.shape[1] == 2):
    tempResult = unique(hull.simplices)
	
===================================================================	
TestConvexHull.check: 386	
----------------------------	

points = DATASETS[name]
tri = scipy.spatial.qhull.Delaunay(points)
hull = scipy.spatial.qhull.ConvexHull(points)
assert_hulls_equal(points, tri.convex_hull, hull.simplices)
if (points.shape[1] == 2):
    assert_equal(numpy.unique(hull.simplices), numpy.sort(hull.vertices))
else:
    tempResult = unique(hull.simplices)
	
===================================================================	
sorted_unique_tuple: 16	
----------------------------	

tempResult = unique(x)
	
===================================================================	
TestDelaunay.test_coplanar: 305	
----------------------------	

points = numpy.random.rand(10, 2)
points = numpy.r_[(points, points)]
tri = scipy.spatial.qhull.Delaunay(points)
tempResult = unique(tri.simplices.ravel())
	
===================================================================	
TestDelaunay.test_coplanar: 307	
----------------------------	

points = numpy.random.rand(10, 2)
points = numpy.r_[(points, points)]
tri = scipy.spatial.qhull.Delaunay(points)
assert_((len(numpy.unique(tri.simplices.ravel())) == (len(points) // 2)))
assert_((len(tri.coplanar) == (len(points) // 2)))
tempResult = unique(tri.coplanar[:, 2])
	
===================================================================	
TestDelaunay.check: 329	
----------------------------	

(chunks, opts) = INCREMENTAL_DATASETS[name]
points = numpy.concatenate(chunks, axis=0)
obj = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
for chunk in chunks[1:]:
    obj.add_points(chunk)
obj2 = scipy.spatial.qhull.Delaunay(points)
obj3 = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
if (len(chunks) > 1):
    obj3.add_points(numpy.concatenate(chunks[1:], axis=0), restart=True)
if name.startswith('pathological'):
    tempResult = unique(obj.simplices.ravel())
	
===================================================================	
TestDelaunay.check: 330	
----------------------------	

(chunks, opts) = INCREMENTAL_DATASETS[name]
points = numpy.concatenate(chunks, axis=0)
obj = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
for chunk in chunks[1:]:
    obj.add_points(chunk)
obj2 = scipy.spatial.qhull.Delaunay(points)
obj3 = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
if (len(chunks) > 1):
    obj3.add_points(numpy.concatenate(chunks[1:], axis=0), restart=True)
if name.startswith('pathological'):
    assert_array_equal(numpy.unique(obj.simplices.ravel()), numpy.arange(points.shape[0]))
    tempResult = unique(obj2.simplices.ravel())
	
===================================================================	
TestDelaunay.test_joggle: 299	
----------------------------	

points = numpy.random.rand(10, 2)
points = numpy.r_[(points, points)]
tri = scipy.spatial.qhull.Delaunay(points, qhull_options='QJ Qbb Pp')
tempResult = unique(tri.simplices.ravel())
	
===================================================================	
TestConvexHull.test_vertices_2d: 411	
----------------------------	

numpy.random.seed(1234)
points = numpy.random.rand(30, 2)
hull = scipy.spatial.qhull.ConvexHull(points)
tempResult = unique(hull.simplices)
	
===================================================================	
factorial: 728	
----------------------------	

'\n    The factorial of a number or array of numbers.\n\n    The factorial of non-negative integer `n` is the product of all\n    positive integers less than or equal to `n`::\n\n        n! = n * (n - 1) * (n - 2) * ... * 1\n\n    Parameters\n    ----------\n    n : int or array_like of ints\n        Input values.  If ``n < 0``, the return value is 0.\n    exact : bool, optional\n        If True, calculate the answer exactly using long integer arithmetic.\n        If False, result is approximated in floating point rapidly using the\n        `gamma` function.\n        Default is False.\n\n    Returns\n    -------\n    nf : float or int or ndarray\n        Factorial of `n`, as integer or float depending on `exact`.\n\n    Notes\n    -----\n    For arrays with ``exact=True``, the factorial is computed only once, for\n    the largest input, with each other result computed in the process.\n    The output dtype is increased to ``int64`` or ``object`` if necessary.\n\n    With ``exact=False`` the factorial is approximated using the gamma\n    function:\n\n    .. math:: n! = \\Gamma(n+1)\n\n    Examples\n    --------\n    >>> from scipy.special import factorial\n    >>> arr = np.array([3, 4, 5])\n    >>> factorial(arr, exact=False)\n    array([   6.,   24.,  120.])\n    >>> factorial(arr, exact=True)\n    array([  6,  24, 120])\n    >>> factorial(5, exact=True)\n    120L\n\n    '
if exact:
    if (numpy.ndim(n) == 0):
        return (0 if (n < 0) else math.factorial(n))
    else:
        n = asarray(n)
        tempResult = unique(n)
	
===================================================================	
Arg.values: 55	
----------------------------	

'Return an array containing approximatively `n` numbers.'
n1 = max(2, int((0.3 * n)))
n2 = max(2, int((0.2 * n)))
n3 = max(8, ((n - n1) - n2))
v1 = numpy.linspace((- 1), 1, n1)
v2 = numpy.r_[(numpy.linspace((- 10), 10, max(0, (n2 - 4))), (- 9), (- 5.5), 5.5, 9)]
if ((self.a >= 0) and (self.b > 0)):
    v3 = numpy.r_[(numpy.logspace((- 30), (- 1), (2 + (n3 // 4))), numpy.logspace(5, numpy.log10(self.b), (1 + (n3 // 4))))]
    v4 = numpy.logspace(1, 5, (1 + (n3 // 2)))
elif (self.a < 0 < self.b):
    v3 = numpy.r_[(numpy.logspace((- 30), (- 1), (2 + (n3 // 8))), numpy.logspace(5, numpy.log10(self.b), (1 + (n3 // 8))), (- numpy.logspace((- 30), (- 1), (2 + (n3 // 8)))), (- numpy.logspace(5, numpy.log10((- self.a)), (1 + (n3 // 8)))))]
    v4 = numpy.r_[(numpy.logspace(1, 5, (1 + (n3 // 4))), (- numpy.logspace(1, 5, (1 + (n3 // 4)))))]
elif (self.b < 0):
    v3 = numpy.r_[((- numpy.logspace((- 30), (- 1), (2 + (n3 // 4)))), (- numpy.logspace(5, numpy.log10((- self.b)), (1 + (n3 // 4)))))]
    v4 = (- numpy.logspace(1, 5, (1 + (n3 // 2))))
else:
    v3 = []
    v4 = []
v = numpy.r_[(v1, v2, v3, v4, 0)]
if self.inclusive_a:
    v = v[(v >= self.a)]
else:
    v = v[(v > self.a)]
if self.inclusive_b:
    v = v[(v <= self.b)]
else:
    v = v[(v < self.b)]
tempResult = unique(v)
	
===================================================================	
IntArg.values: 86	
----------------------------	

v1 = Arg(self.a, self.b).values(max((1 + (n // 2)), (n - 5))).astype(int)
v2 = numpy.arange((- 5), 5)
tempResult = unique(numpy.r_[(v1, v2)])
	
===================================================================	
ProbArg.values: 30	
----------------------------	

'Return an array containing approximatively n numbers.'
m = max(1, (n // 3))
v1 = numpy.logspace((- 30), numpy.log10(0.3), m)
v2 = numpy.linspace(0.3, 0.7, (m + 1), endpoint=False)[1:]
v3 = (1 - numpy.logspace(numpy.log10(0.3), (- 15), m))
v = numpy.r_[(v1, v2, v3)]
tempResult = unique(v)
	
===================================================================	
anderson_ksamp: 419	
----------------------------	

'The Anderson-Darling test for k-samples.\n\n    The k-sample Anderson-Darling test is a modification of the\n    one-sample Anderson-Darling test. It tests the null hypothesis\n    that k-samples are drawn from the same population without having\n    to specify the distribution function of that population. The\n    critical values depend on the number of samples.\n\n    Parameters\n    ----------\n    samples : sequence of 1-D array_like\n        Array of sample data in arrays.\n    midrank : bool, optional\n        Type of Anderson-Darling test which is computed. Default\n        (True) is the midrank test applicable to continuous and\n        discrete populations. If False, the right side empirical\n        distribution is used.\n\n    Returns\n    -------\n    statistic : float\n        Normalized k-sample Anderson-Darling test statistic.\n    critical_values : array\n        The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%.\n    significance_level : float\n        An approximate significance level at which the null hypothesis for the\n        provided samples can be rejected.\n\n    Raises\n    ------\n    ValueError\n        If less than 2 samples are provided, a sample is empty, or no\n        distinct observations are in the samples.\n\n    See Also\n    --------\n    ks_2samp : 2 sample Kolmogorov-Smirnov test\n    anderson : 1 sample Anderson-Darling test\n\n    Notes\n    -----\n    [1]_ Defines three versions of the k-sample Anderson-Darling test:\n    one for continuous distributions and two for discrete\n    distributions, in which ties between samples may occur. The\n    default of this routine is to compute the version based on the\n    midrank empirical distribution function. This test is applicable\n    to continuous and discrete data. If midrank is set to False, the\n    right side empirical distribution is used for a test for discrete\n    data. According to [1]_, the two discrete test statistics differ\n    only slightly if a few collisions due to round-off errors occur in\n    the test not adjusted for ties between samples.\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample\n           Anderson-Darling Tests, Journal of the American Statistical\n           Association, Vol. 82, pp. 918-924.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> np.random.seed(314159)\n\n    The null hypothesis that the two random samples come from the same\n    distribution can be rejected at the 5% level because the returned\n    test value is greater than the critical value for 5% (1.961) but\n    not at the 2.5% level. The interpolation gives an approximate\n    significance level of 3.1%:\n\n    >>> stats.anderson_ksamp([np.random.normal(size=50),\n    ... np.random.normal(loc=0.5, size=30)])\n    (2.4615796189876105,\n      array([ 0.325,  1.226,  1.961,  2.718,  3.752]),\n      0.03134990135800783)\n\n\n    The null hypothesis cannot be rejected for three samples from an\n    identical distribution. The approximate p-value (87%) has to be\n    computed by extrapolation and may not be very accurate:\n\n    >>> stats.anderson_ksamp([np.random.normal(size=50),\n    ... np.random.normal(size=30), np.random.normal(size=20)])\n    (-0.73091722665244196,\n      array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856]),\n      0.8789283903979661)\n\n    '
k = len(samples)
if (k < 2):
    raise ValueError('anderson_ksamp needs at least two samples')
samples = list(map(numpy.asarray, samples))
Z = numpy.sort(numpy.hstack(samples))
N = Z.size
tempResult = unique(Z)
	
===================================================================	
ks_twosamp: 488	
----------------------------	

"\n    Computes the Kolmogorov-Smirnov test on two samples.\n\n    Missing values are discarded.\n\n    Parameters\n    ----------\n    data1 : array_like\n        First data set\n    data2 : array_like\n        Second data set\n    alternative : {'two-sided', 'less', 'greater'}, optional\n        Indicates the alternative hypothesis.  Default is 'two-sided'.\n\n    Returns\n    -------\n    d : float\n        Value of the Kolmogorov Smirnov test\n    p : float\n        Corresponding p-value.\n\n    "
(data1, data2) = (numpy.ma.asarray(data1), numpy.ma.asarray(data2))
(n1, n2) = (data1.count(), data2.count())
n = ((n1 * n2) / float((n1 + n2)))
mix = numpy.ma.concatenate((data1.compressed(), data2.compressed()))
mixsort = mix.argsort(kind='mergesort')
csum = np.where((mixsort < n1), (1.0 / n1), ((- 1.0) / n2)).cumsum()
tempResult = unique(mix)
	
===================================================================	
count_tied_groups: 79	
----------------------------	

'\n    Counts the number of tied values.\n\n    Parameters\n    ----------\n    x : sequence\n        Sequence of data on which to counts the ties\n    use_missing : bool, optional\n        Whether to consider missing values as tied.\n\n    Returns\n    -------\n    count_tied_groups : dict\n        Returns a dictionary (nb of ties: nb of groups).\n\n    Examples\n    --------\n    >>> from scipy.stats import mstats\n    >>> z = [0, 0, 0, 2, 2, 2, 3, 3, 4, 5, 6]\n    >>> mstats.count_tied_groups(z)\n    {2: 1, 3: 2}\n\n    In the above example, the ties were 0 (3x), 2 (3x) and 3 (2x).\n\n    >>> z = np.ma.array([0, 0, 1, 2, 2, 2, 3, 3, 4, 5, 6])\n    >>> mstats.count_tied_groups(z)\n    {2: 2, 3: 1}\n    >>> z[[1,-1]] = np.ma.masked\n    >>> mstats.count_tied_groups(z, use_missing=True)\n    {2: 2, 3: 1}\n\n    '
nmasked = ma.getmask(x).sum()
data = ma.compressed(x).copy()
(ties, counts) = find_repeats(data)
nties = {}
if len(ties):
    tempResult = unique(counts)
	
===================================================================	
itemfreq: 404	
----------------------------	

'\n    Returns a 2-D array of item frequencies.\n\n    Parameters\n    ----------\n    a : (N,) array_like\n        Input array.\n\n    Returns\n    -------\n    itemfreq : (K, 2) ndarray\n        A 2-D frequency table.  Column 1 contains sorted, unique values from\n        `a`, column 2 contains their respective counts.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> a = np.array([1, 1, 5, 0, 1, 2, 2, 0, 1, 4])\n    >>> stats.itemfreq(a)\n    array([[ 0.,  2.],\n           [ 1.,  4.],\n           [ 2.,  2.],\n           [ 4.,  1.],\n           [ 5.,  1.]])\n    >>> np.bincount(a)\n    array([2, 4, 2, 0, 1, 1])\n\n    >>> stats.itemfreq(a/10.)\n    array([[ 0. ,  2. ],\n           [ 0.1,  4. ],\n           [ 0.2,  2. ],\n           [ 0.4,  1. ],\n           [ 0.5,  1. ]])\n\n    '
tempResult = unique(a, return_inverse=True)
	
===================================================================	
mode: 100	
----------------------------	

"\n    Returns an array of the modal (most common) value in the passed array.\n\n    If there is more than one such value, only the smallest is returned.\n    The bin-count for the modal bins is also returned.\n\n    Parameters\n    ----------\n    a : array_like\n        n-dimensional array of which to find mode(s).\n    axis : int or None, optional\n        Axis along which to operate. Default is 0. If None, compute over\n        the whole array `a`.\n    nan_policy : {'propagate', 'raise', 'omit'}, optional\n        Defines how to handle when input contains nan. 'propagate' returns nan,\n        'raise' throws an error, 'omit' performs the calculations ignoring nan\n        values. Default is 'propagate'.\n\n    Returns\n    -------\n    mode : ndarray\n        Array of modal values.\n    count : ndarray\n        Array of counts for each mode.\n\n    Examples\n    --------\n    >>> a = np.array([[6, 8, 3, 0],\n    ...               [3, 2, 1, 7],\n    ...               [8, 1, 8, 4],\n    ...               [5, 3, 0, 5],\n    ...               [4, 7, 5, 9]])\n    >>> from scipy import stats\n    >>> stats.mode(a)\n    (array([[3, 1, 0, 0]]), array([[1, 1, 1, 1]]))\n\n    To get mode of whole array, specify ``axis=None``:\n\n    >>> stats.mode(a, axis=None)\n    (array([3]), array([3]))\n\n    "
(a, axis) = _chk_asarray(a, axis)
if (a.size == 0):
    return ModeResult(numpy.array([]), numpy.array([]))
(contains_nan, nan_policy) = _contains_nan(a, nan_policy)
if (contains_nan and (nan_policy == 'omit')):
    a = numpy.ma.masked_invalid(a)
    return mstats_basic.mode(a, axis)
tempResult = unique(numpy.ravel(a))
	
===================================================================	
_iqr_percentile: 621	
----------------------------	

'\n    Private wrapper that works around older versions of `numpy`.\n\n    While this function is pretty much necessary for the moment, it\n    should be removed as soon as the minimum supported numpy version\n    allows.\n    '
if (contains_nan and (NumpyVersion(numpy.__version__) < '1.10.0a')):
    msg = "Keyword nan_policy='propagate' not correctly supported for numpy versions < 1.10.x. The default behavior of `numpy.percentile` will be used."
    warnings.warn(msg, RuntimeWarning)
try:
    result = numpy.percentile(x, q, axis=axis, keepdims=keepdims, interpolation=interpolation)
except TypeError:
    if ((interpolation != 'linear') or keepdims):
        warnings.warn('Keywords interpolation and keepdims not supported for your version of numpy', RuntimeWarning)
    try:
        original_size = len(axis)
    except TypeError:
        pass
    else:
        tempResult = unique((numpy.asarray(axis) % x.ndim))
	
===================================================================	
binned_statistic_dd: 125	
----------------------------	

"\n    Compute a multidimensional binned statistic for a set of data.\n\n    This is a generalization of a histogramdd function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values within each bin.\n\n    Parameters\n    ----------\n    sample : array_like\n        Data to histogram passed as a sequence of D arrays of length N, or\n        as an (N,D) array.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is such a list, the statistic will be\n        computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : sequence or int, optional\n        The bin specification must be in one of the following forms:\n\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (nx, ny, ... = bins).\n          * The number of bins for all dimensions (nx = ny = ... = bins).\n\n    range : sequence, optional\n        A sequence of lower and upper bin edges to be used if the edges are\n        not given explicitely in `bins`. Defaults to the minimum and maximum\n        values along each dimension.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section of\n        `binned_statistic_2d`.\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : ndarray, shape(nx1, nx2, nx3,...)\n        The values of the selected statistic in each two-dimensional bin.\n    bin_edges : list of ndarrays\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    binnumber : (N,) array of ints or (D,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open in each dimension.  In\n    other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n    ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n    last bin, however, is ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n\n    .. versionadded:: 0.11.0\n\n    "
known_stats = ['mean', 'median', 'count', 'sum', 'std', 'min', 'max']
if ((not callable(statistic)) and (statistic not in known_stats)):
    raise ValueError(('invalid statistic %r' % (statistic,)))
try:
    (Dlen, Ndim) = sample.shape
except (AttributeError, ValueError):
    sample = np.atleast_2d(sample).T
    (Dlen, Ndim) = sample.shape
values = numpy.asarray(values)
input_shape = list(values.shape)
values = numpy.atleast_2d(values)
(Vdim, Vlen) = values.shape
if ((statistic != 'count') and (Vlen != Dlen)):
    raise AttributeError('The number of `values` elements must match the length of each `sample` dimension.')
nbin = numpy.empty(Ndim, int)
edges = (Ndim * [None])
dedges = (Ndim * [None])
try:
    M = len(bins)
    if (M != Ndim):
        raise AttributeError('The dimension of bins must be equal to the dimension of the sample x.')
except TypeError:
    bins = (Ndim * [bins])
if (range is None):
    smin = numpy.atleast_1d(numpy.array(sample.min(axis=0), float))
    smax = numpy.atleast_1d(numpy.array(sample.max(axis=0), float))
else:
    smin = numpy.zeros(Ndim)
    smax = numpy.zeros(Ndim)
    for i in xrange(Ndim):
        (smin[i], smax[i]) = range[i]
for i in xrange(len(smin)):
    if (smin[i] == smax[i]):
        smin[i] = (smin[i] - 0.5)
        smax[i] = (smax[i] + 0.5)
for i in xrange(Ndim):
    if numpy.isscalar(bins[i]):
        nbin[i] = (bins[i] + 2)
        edges[i] = numpy.linspace(smin[i], smax[i], (nbin[i] - 1))
    else:
        edges[i] = numpy.asarray(bins[i], float)
        nbin[i] = (len(edges[i]) + 1)
    dedges[i] = numpy.diff(edges[i])
nbin = numpy.asarray(nbin)
sampBin = {}
for i in xrange(Ndim):
    sampBin[i] = numpy.digitize(sample[:, i], edges[i])
for i in xrange(Ndim):
    decimal = (int((- numpy.log10(dedges[i].min()))) + 6)
    on_edge = numpy.where((numpy.around(sample[:, i], decimal) == numpy.around(edges[i][(- 1)], decimal)))[0]
    sampBin[i][on_edge] -= 1
ni = nbin.argsort()
binnumbers = numpy.zeros(Dlen, int)
for i in xrange(0, (Ndim - 1)):
    binnumbers += (sampBin[ni[i]] * nbin[ni[(i + 1):]].prod())
binnumbers += sampBin[ni[(- 1)]]
result = numpy.empty([Vdim, nbin.prod()], float)
if (statistic == 'mean'):
    result.fill(numpy.nan)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        result[(vv, a)] = (flatsum[a] / flatcount[a])
elif (statistic == 'std'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        flatsum2 = numpy.bincount(binnumbers, (values[vv] ** 2))
        result[(vv, a)] = numpy.sqrt(((flatsum2[a] / flatcount[a]) - ((flatsum[a] / flatcount[a]) ** 2)))
elif (statistic == 'count'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = numpy.arange(len(flatcount))
    result[:, a] = flatcount[numpy.newaxis, :]
elif (statistic == 'sum'):
    result.fill(0)
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        a = numpy.arange(len(flatsum))
        result[(vv, a)] = flatsum
elif (statistic == 'median'):
    result.fill(numpy.nan)
    tempResult = unique(binnumbers)
	
===================================================================	
binned_statistic_dd: 130	
----------------------------	

"\n    Compute a multidimensional binned statistic for a set of data.\n\n    This is a generalization of a histogramdd function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values within each bin.\n\n    Parameters\n    ----------\n    sample : array_like\n        Data to histogram passed as a sequence of D arrays of length N, or\n        as an (N,D) array.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is such a list, the statistic will be\n        computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : sequence or int, optional\n        The bin specification must be in one of the following forms:\n\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (nx, ny, ... = bins).\n          * The number of bins for all dimensions (nx = ny = ... = bins).\n\n    range : sequence, optional\n        A sequence of lower and upper bin edges to be used if the edges are\n        not given explicitely in `bins`. Defaults to the minimum and maximum\n        values along each dimension.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section of\n        `binned_statistic_2d`.\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : ndarray, shape(nx1, nx2, nx3,...)\n        The values of the selected statistic in each two-dimensional bin.\n    bin_edges : list of ndarrays\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    binnumber : (N,) array of ints or (D,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open in each dimension.  In\n    other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n    ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n    last bin, however, is ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n\n    .. versionadded:: 0.11.0\n\n    "
known_stats = ['mean', 'median', 'count', 'sum', 'std', 'min', 'max']
if ((not callable(statistic)) and (statistic not in known_stats)):
    raise ValueError(('invalid statistic %r' % (statistic,)))
try:
    (Dlen, Ndim) = sample.shape
except (AttributeError, ValueError):
    sample = np.atleast_2d(sample).T
    (Dlen, Ndim) = sample.shape
values = numpy.asarray(values)
input_shape = list(values.shape)
values = numpy.atleast_2d(values)
(Vdim, Vlen) = values.shape
if ((statistic != 'count') and (Vlen != Dlen)):
    raise AttributeError('The number of `values` elements must match the length of each `sample` dimension.')
nbin = numpy.empty(Ndim, int)
edges = (Ndim * [None])
dedges = (Ndim * [None])
try:
    M = len(bins)
    if (M != Ndim):
        raise AttributeError('The dimension of bins must be equal to the dimension of the sample x.')
except TypeError:
    bins = (Ndim * [bins])
if (range is None):
    smin = numpy.atleast_1d(numpy.array(sample.min(axis=0), float))
    smax = numpy.atleast_1d(numpy.array(sample.max(axis=0), float))
else:
    smin = numpy.zeros(Ndim)
    smax = numpy.zeros(Ndim)
    for i in xrange(Ndim):
        (smin[i], smax[i]) = range[i]
for i in xrange(len(smin)):
    if (smin[i] == smax[i]):
        smin[i] = (smin[i] - 0.5)
        smax[i] = (smax[i] + 0.5)
for i in xrange(Ndim):
    if numpy.isscalar(bins[i]):
        nbin[i] = (bins[i] + 2)
        edges[i] = numpy.linspace(smin[i], smax[i], (nbin[i] - 1))
    else:
        edges[i] = numpy.asarray(bins[i], float)
        nbin[i] = (len(edges[i]) + 1)
    dedges[i] = numpy.diff(edges[i])
nbin = numpy.asarray(nbin)
sampBin = {}
for i in xrange(Ndim):
    sampBin[i] = numpy.digitize(sample[:, i], edges[i])
for i in xrange(Ndim):
    decimal = (int((- numpy.log10(dedges[i].min()))) + 6)
    on_edge = numpy.where((numpy.around(sample[:, i], decimal) == numpy.around(edges[i][(- 1)], decimal)))[0]
    sampBin[i][on_edge] -= 1
ni = nbin.argsort()
binnumbers = numpy.zeros(Dlen, int)
for i in xrange(0, (Ndim - 1)):
    binnumbers += (sampBin[ni[i]] * nbin[ni[(i + 1):]].prod())
binnumbers += sampBin[ni[(- 1)]]
result = numpy.empty([Vdim, nbin.prod()], float)
if (statistic == 'mean'):
    result.fill(numpy.nan)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        result[(vv, a)] = (flatsum[a] / flatcount[a])
elif (statistic == 'std'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        flatsum2 = numpy.bincount(binnumbers, (values[vv] ** 2))
        result[(vv, a)] = numpy.sqrt(((flatsum2[a] / flatcount[a]) - ((flatsum[a] / flatcount[a]) ** 2)))
elif (statistic == 'count'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = numpy.arange(len(flatcount))
    result[:, a] = flatcount[numpy.newaxis, :]
elif (statistic == 'sum'):
    result.fill(0)
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        a = numpy.arange(len(flatsum))
        result[(vv, a)] = flatsum
elif (statistic == 'median'):
    result.fill(numpy.nan)
    for i in numpy.unique(binnumbers):
        for vv in xrange(Vdim):
            result[(vv, i)] = numpy.median(values[(vv, (binnumbers == i))])
elif (statistic == 'min'):
    result.fill(numpy.nan)
    tempResult = unique(binnumbers)
	
===================================================================	
binned_statistic_dd: 135	
----------------------------	

"\n    Compute a multidimensional binned statistic for a set of data.\n\n    This is a generalization of a histogramdd function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values within each bin.\n\n    Parameters\n    ----------\n    sample : array_like\n        Data to histogram passed as a sequence of D arrays of length N, or\n        as an (N,D) array.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is such a list, the statistic will be\n        computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : sequence or int, optional\n        The bin specification must be in one of the following forms:\n\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (nx, ny, ... = bins).\n          * The number of bins for all dimensions (nx = ny = ... = bins).\n\n    range : sequence, optional\n        A sequence of lower and upper bin edges to be used if the edges are\n        not given explicitely in `bins`. Defaults to the minimum and maximum\n        values along each dimension.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section of\n        `binned_statistic_2d`.\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : ndarray, shape(nx1, nx2, nx3,...)\n        The values of the selected statistic in each two-dimensional bin.\n    bin_edges : list of ndarrays\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    binnumber : (N,) array of ints or (D,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open in each dimension.  In\n    other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n    ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n    last bin, however, is ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n\n    .. versionadded:: 0.11.0\n\n    "
known_stats = ['mean', 'median', 'count', 'sum', 'std', 'min', 'max']
if ((not callable(statistic)) and (statistic not in known_stats)):
    raise ValueError(('invalid statistic %r' % (statistic,)))
try:
    (Dlen, Ndim) = sample.shape
except (AttributeError, ValueError):
    sample = np.atleast_2d(sample).T
    (Dlen, Ndim) = sample.shape
values = numpy.asarray(values)
input_shape = list(values.shape)
values = numpy.atleast_2d(values)
(Vdim, Vlen) = values.shape
if ((statistic != 'count') and (Vlen != Dlen)):
    raise AttributeError('The number of `values` elements must match the length of each `sample` dimension.')
nbin = numpy.empty(Ndim, int)
edges = (Ndim * [None])
dedges = (Ndim * [None])
try:
    M = len(bins)
    if (M != Ndim):
        raise AttributeError('The dimension of bins must be equal to the dimension of the sample x.')
except TypeError:
    bins = (Ndim * [bins])
if (range is None):
    smin = numpy.atleast_1d(numpy.array(sample.min(axis=0), float))
    smax = numpy.atleast_1d(numpy.array(sample.max(axis=0), float))
else:
    smin = numpy.zeros(Ndim)
    smax = numpy.zeros(Ndim)
    for i in xrange(Ndim):
        (smin[i], smax[i]) = range[i]
for i in xrange(len(smin)):
    if (smin[i] == smax[i]):
        smin[i] = (smin[i] - 0.5)
        smax[i] = (smax[i] + 0.5)
for i in xrange(Ndim):
    if numpy.isscalar(bins[i]):
        nbin[i] = (bins[i] + 2)
        edges[i] = numpy.linspace(smin[i], smax[i], (nbin[i] - 1))
    else:
        edges[i] = numpy.asarray(bins[i], float)
        nbin[i] = (len(edges[i]) + 1)
    dedges[i] = numpy.diff(edges[i])
nbin = numpy.asarray(nbin)
sampBin = {}
for i in xrange(Ndim):
    sampBin[i] = numpy.digitize(sample[:, i], edges[i])
for i in xrange(Ndim):
    decimal = (int((- numpy.log10(dedges[i].min()))) + 6)
    on_edge = numpy.where((numpy.around(sample[:, i], decimal) == numpy.around(edges[i][(- 1)], decimal)))[0]
    sampBin[i][on_edge] -= 1
ni = nbin.argsort()
binnumbers = numpy.zeros(Dlen, int)
for i in xrange(0, (Ndim - 1)):
    binnumbers += (sampBin[ni[i]] * nbin[ni[(i + 1):]].prod())
binnumbers += sampBin[ni[(- 1)]]
result = numpy.empty([Vdim, nbin.prod()], float)
if (statistic == 'mean'):
    result.fill(numpy.nan)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        result[(vv, a)] = (flatsum[a] / flatcount[a])
elif (statistic == 'std'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        flatsum2 = numpy.bincount(binnumbers, (values[vv] ** 2))
        result[(vv, a)] = numpy.sqrt(((flatsum2[a] / flatcount[a]) - ((flatsum[a] / flatcount[a]) ** 2)))
elif (statistic == 'count'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = numpy.arange(len(flatcount))
    result[:, a] = flatcount[numpy.newaxis, :]
elif (statistic == 'sum'):
    result.fill(0)
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        a = numpy.arange(len(flatsum))
        result[(vv, a)] = flatsum
elif (statistic == 'median'):
    result.fill(numpy.nan)
    for i in numpy.unique(binnumbers):
        for vv in xrange(Vdim):
            result[(vv, i)] = numpy.median(values[(vv, (binnumbers == i))])
elif (statistic == 'min'):
    result.fill(numpy.nan)
    for i in numpy.unique(binnumbers):
        for vv in xrange(Vdim):
            result[(vv, i)] = numpy.min(values[(vv, (binnumbers == i))])
elif (statistic == 'max'):
    result.fill(numpy.nan)
    tempResult = unique(binnumbers)
	
===================================================================	
binned_statistic_dd: 148	
----------------------------	

"\n    Compute a multidimensional binned statistic for a set of data.\n\n    This is a generalization of a histogramdd function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values within each bin.\n\n    Parameters\n    ----------\n    sample : array_like\n        Data to histogram passed as a sequence of D arrays of length N, or\n        as an (N,D) array.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is such a list, the statistic will be\n        computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : sequence or int, optional\n        The bin specification must be in one of the following forms:\n\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (nx, ny, ... = bins).\n          * The number of bins for all dimensions (nx = ny = ... = bins).\n\n    range : sequence, optional\n        A sequence of lower and upper bin edges to be used if the edges are\n        not given explicitely in `bins`. Defaults to the minimum and maximum\n        values along each dimension.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section of\n        `binned_statistic_2d`.\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : ndarray, shape(nx1, nx2, nx3,...)\n        The values of the selected statistic in each two-dimensional bin.\n    bin_edges : list of ndarrays\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    binnumber : (N,) array of ints or (D,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open in each dimension.  In\n    other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n    ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n    last bin, however, is ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n\n    .. versionadded:: 0.11.0\n\n    "
known_stats = ['mean', 'median', 'count', 'sum', 'std', 'min', 'max']
if ((not callable(statistic)) and (statistic not in known_stats)):
    raise ValueError(('invalid statistic %r' % (statistic,)))
try:
    (Dlen, Ndim) = sample.shape
except (AttributeError, ValueError):
    sample = np.atleast_2d(sample).T
    (Dlen, Ndim) = sample.shape
values = numpy.asarray(values)
input_shape = list(values.shape)
values = numpy.atleast_2d(values)
(Vdim, Vlen) = values.shape
if ((statistic != 'count') and (Vlen != Dlen)):
    raise AttributeError('The number of `values` elements must match the length of each `sample` dimension.')
nbin = numpy.empty(Ndim, int)
edges = (Ndim * [None])
dedges = (Ndim * [None])
try:
    M = len(bins)
    if (M != Ndim):
        raise AttributeError('The dimension of bins must be equal to the dimension of the sample x.')
except TypeError:
    bins = (Ndim * [bins])
if (range is None):
    smin = numpy.atleast_1d(numpy.array(sample.min(axis=0), float))
    smax = numpy.atleast_1d(numpy.array(sample.max(axis=0), float))
else:
    smin = numpy.zeros(Ndim)
    smax = numpy.zeros(Ndim)
    for i in xrange(Ndim):
        (smin[i], smax[i]) = range[i]
for i in xrange(len(smin)):
    if (smin[i] == smax[i]):
        smin[i] = (smin[i] - 0.5)
        smax[i] = (smax[i] + 0.5)
for i in xrange(Ndim):
    if numpy.isscalar(bins[i]):
        nbin[i] = (bins[i] + 2)
        edges[i] = numpy.linspace(smin[i], smax[i], (nbin[i] - 1))
    else:
        edges[i] = numpy.asarray(bins[i], float)
        nbin[i] = (len(edges[i]) + 1)
    dedges[i] = numpy.diff(edges[i])
nbin = numpy.asarray(nbin)
sampBin = {}
for i in xrange(Ndim):
    sampBin[i] = numpy.digitize(sample[:, i], edges[i])
for i in xrange(Ndim):
    decimal = (int((- numpy.log10(dedges[i].min()))) + 6)
    on_edge = numpy.where((numpy.around(sample[:, i], decimal) == numpy.around(edges[i][(- 1)], decimal)))[0]
    sampBin[i][on_edge] -= 1
ni = nbin.argsort()
binnumbers = numpy.zeros(Dlen, int)
for i in xrange(0, (Ndim - 1)):
    binnumbers += (sampBin[ni[i]] * nbin[ni[(i + 1):]].prod())
binnumbers += sampBin[ni[(- 1)]]
result = numpy.empty([Vdim, nbin.prod()], float)
if (statistic == 'mean'):
    result.fill(numpy.nan)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        result[(vv, a)] = (flatsum[a] / flatcount[a])
elif (statistic == 'std'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        flatsum2 = numpy.bincount(binnumbers, (values[vv] ** 2))
        result[(vv, a)] = numpy.sqrt(((flatsum2[a] / flatcount[a]) - ((flatsum[a] / flatcount[a]) ** 2)))
elif (statistic == 'count'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = numpy.arange(len(flatcount))
    result[:, a] = flatcount[numpy.newaxis, :]
elif (statistic == 'sum'):
    result.fill(0)
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        a = numpy.arange(len(flatsum))
        result[(vv, a)] = flatsum
elif (statistic == 'median'):
    result.fill(numpy.nan)
    for i in numpy.unique(binnumbers):
        for vv in xrange(Vdim):
            result[(vv, i)] = numpy.median(values[(vv, (binnumbers == i))])
elif (statistic == 'min'):
    result.fill(numpy.nan)
    for i in numpy.unique(binnumbers):
        for vv in xrange(Vdim):
            result[(vv, i)] = numpy.min(values[(vv, (binnumbers == i))])
elif (statistic == 'max'):
    result.fill(numpy.nan)
    for i in numpy.unique(binnumbers):
        for vv in xrange(Vdim):
            result[(vv, i)] = numpy.max(values[(vv, (binnumbers == i))])
elif callable(statistic):
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=RuntimeWarning)
        old = numpy.seterr(invalid='ignore')
        try:
            null = statistic([])
        except:
            null = numpy.nan
        numpy.seterr(**old)
    result.fill(null)
    tempResult = unique(binnumbers)
	
===================================================================	
TestBinnedStatistic.test_1d_bincode: 95	
----------------------------	

x = self.x[:20]
v = self.v[:20]
(count1, edges1, bc) = binned_statistic(x, v, 'count', bins=3)
bc2 = numpy.array([3, 2, 1, 3, 2, 3, 3, 3, 3, 1, 1, 3, 3, 1, 2, 3, 1, 1, 2, 1])
tempResult = unique(bc)
	
===================================================================	
TestBinnedStatistic.test_2d_bincode: 217	
----------------------------	

x = self.x[:20]
y = self.y[:20]
v = self.v[:20]
(count1, binx1, biny1, bc) = binned_statistic_2d(x, y, v, 'count', bins=3)
bc2 = numpy.array([17, 11, 6, 16, 11, 17, 18, 17, 17, 7, 6, 18, 16, 6, 11, 16, 6, 6, 11, 8])
tempResult = unique(bc)
	
===================================================================	
TestBinnedStatistic.test_dd_bincode: 320	
----------------------------	

X = self.X[:20]
v = self.v[:20]
(count1, edges1, bc) = binned_statistic_dd(X, v, 'count', bins=3)
bc2 = numpy.array([63, 33, 86, 83, 88, 67, 57, 33, 42, 41, 82, 83, 92, 32, 36, 91, 43, 87, 81, 81])
tempResult = unique(bc)
	
===================================================================	
test_discrete_basic: 22	
----------------------------	

for (distname, arg) in distdiscrete:
    try:
        distfn = getattr(stats, distname)
    except TypeError:
        distfn = distname
        distname = 'sample distribution'
    numpy.random.seed(9765456)
    rvs = distfn.rvs(*arg, size=2000)
    tempResult = unique(rvs)
	
===================================================================	
TestRankData.dense_rank: 120	
----------------------------	

tempResult = unique(a)
	
===================================================================	
TestMode.test_objects: 1082	
----------------------------	


class Point(object):

    def __init__(self, x):
        self.x = x

    def __eq__(self, other):
        return (self.x == other.x)

    def __ne__(self, other):
        return (self.x != other.x)

    def __lt__(self, other):
        return (self.x < other.x)

    def __hash__(self):
        return hash(self.x)
points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
arr = numpy.empty((8,), dtype=object)
arr[:] = points
assert_((len(set(points)) == 4))
tempResult = unique(arr)
	
===================================================================	
TestFOneWay.test_nist: 2706	
----------------------------	

filenames = ['SiRstv.dat', 'SmLs01.dat', 'SmLs02.dat', 'SmLs03.dat', 'AtmWtAg.dat', 'SmLs04.dat', 'SmLs05.dat', 'SmLs06.dat', 'SmLs07.dat', 'SmLs08.dat', 'SmLs09.dat']
for test_case in filenames:
    rtol = 1e-07
    fname = os.path.abspath(os.path.join(os.path.dirname(__file__), 'data/nist_anova', test_case))
    with open(fname, 'r') as f:
        content = f.read().split('\n')
    certified = [line.split() for line in content[40:48] if line.strip()]
    dataf = numpy.loadtxt(fname, skiprows=60)
    (y, x) = dataf.T
    y = y.astype(int)
    tempResult = unique(y)
	
***************************************************	
sklearn_sklearn-0.18.0: 232	
===================================================================	
LeaveOneLabelOut.__init__: 207	
----------------------------	

super(LeaveOneLabelOut, self).__init__(len(labels))
self.labels = numpy.array(labels, copy=True)
tempResult = unique(labels)
	
===================================================================	
StratifiedShuffleSplit.__init__: 345	
----------------------------	

super(StratifiedShuffleSplit, self).__init__(len(y), n_iter, test_size, train_size, random_state)
self.y = numpy.array(y)
tempResult = unique(y, return_inverse=True)
	
===================================================================	
PredefinedSplit.__init__: 385	
----------------------------	

super(PredefinedSplit, self).__init__(len(test_fold))
self.test_fold = numpy.array(test_fold, dtype=numpy.int)
self.test_fold = column_or_1d(self.test_fold)
tempResult = unique(self.test_fold)
	
===================================================================	
_shuffle: 576	
----------------------------	

'Return a shuffled copy of y eventually shuffle among same labels.'
if (labels is None):
    ind = random_state.permutation(len(y))
else:
    ind = numpy.arange(len(labels))
    tempResult = unique(labels)
	
===================================================================	
LabelShuffleSplit.__init__: 402	
----------------------------	

tempResult = unique(labels, return_inverse=True)
	
===================================================================	
_approximate_mode: 328	
----------------------------	

"Computes approximate mode of multivariate hypergeometric.\n\n    This is an approximation to the mode of the multivariate\n    hypergeometric given by class_counts and n_draws.\n    It shouldn't be off by more than one.\n\n    It is the mostly likely outcome of drawing n_draws many\n    samples from the population given by class_counts.\n\n    Parameters\n    ----------\n    class_counts : ndarray of int\n        Population per class.\n    n_draws : int\n        Number of draws (samples to draw) from the overall population.\n    rng : random state\n        Used to break ties.\n\n    Returns\n    -------\n    sampled_classes : ndarray of int\n        Number of samples drawn from each class.\n        np.sum(sampled_classes) == n_draws\n    "
continuous = ((n_draws * class_counts) / class_counts.sum())
floored = numpy.floor(continuous)
need_to_add = int((n_draws - floored.sum()))
if (need_to_add > 0):
    remainder = (continuous - floored)
    tempResult = unique(remainder)
	
===================================================================	
StratifiedKFold.__init__: 169	
----------------------------	

super(StratifiedKFold, self).__init__(len(y), n_folds, shuffle, random_state)
y = numpy.asarray(y)
n_samples = y.shape[0]
tempResult = unique(y, return_inverse=True)
	
===================================================================	
LabelKFold.__init__: 137	
----------------------------	

super(LabelKFold, self).__init__(len(labels), n_folds, shuffle=False, random_state=None)
tempResult = unique(labels, return_inverse=True)
	
===================================================================	
LeavePLabelOut.__init__: 226	
----------------------------	

super(LeavePLabelOut, self).__init__(len(labels))
self.labels = numpy.array(labels, copy=True)
tempResult = unique(labels)
	
===================================================================	
_class_means: 44	
----------------------------	

'Compute class means.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Input data.\n\n    y : array-like, shape (n_samples,) or (n_samples, n_targets)\n        Target values.\n\n    Returns\n    -------\n    means : array-like, shape (n_features,)\n        Class means.\n    '
means = []
tempResult = unique(y)
	
===================================================================	
_class_cov: 52	
----------------------------	

"Compute class covariance matrix.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Input data.\n\n    y : array-like, shape (n_samples,) or (n_samples, n_targets)\n        Target values.\n\n    priors : array-like, shape (n_classes,)\n        Class priors.\n\n    shrinkage : string or float, optional\n        Shrinkage parameter, possible values:\n          - None: no shrinkage (default).\n          - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n          - float between 0 and 1: fixed shrinkage parameter.\n\n    Returns\n    -------\n    cov : array-like, shape (n_features, n_features)\n        Class covariance matrix.\n    "
tempResult = unique(y)
	
===================================================================	
LinearDiscriminantAnalysis.fit: 135	
----------------------------	

'Fit LinearDiscriminantAnalysis model according to the given\n           training data and parameters.\n\n           .. versionchanged:: 0.17\n              Deprecated *store_covariance* have been moved to main constructor.\n\n           .. versionchanged:: 0.17\n              Deprecated *tol* have been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array, shape (n_samples,)\n            Target values.\n        '
if store_covariance:
    warnings.warn("The parameter 'store_covariance' is deprecated as of version 0.17 and will be removed in 0.19. The parameter is no longer necessary because the value is set via the estimator initialisation or set_params method.", DeprecationWarning)
    self.store_covariance = store_covariance
if tol:
    warnings.warn("The parameter 'tol' is deprecated as of version 0.17 and will be removed in 0.19. The parameter is no longer necessary because the value is set via the estimator initialisation or set_params method.", DeprecationWarning)
    self.tol = tol
(X, y) = check_X_y(X, y, ensure_min_samples=2, estimator=self)
self.classes_ = unique_labels(y)
if (self.priors is None):
    tempResult = unique(y, return_inverse=True)
	
===================================================================	
QuadraticDiscriminantAnalysis.fit: 208	
----------------------------	

'Fit the model according to the given training data and parameters.\n\n            .. versionchanged:: 0.17\n               Deprecated *store_covariance* have been moved to main constructor.\n\n            .. versionchanged:: 0.17\n               Deprecated *tol* have been moved to main constructor.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n\n        y : array, shape = [n_samples]\n            Target values (integers)\n        '
if store_covariances:
    warnings.warn("The parameter 'store_covariances' is deprecated as of version 0.17 and will be removed in 0.19. The parameter is no longer necessary because the value is set via the estimator initialisation or set_params method.", DeprecationWarning)
    self.store_covariances = store_covariances
if tol:
    warnings.warn("The parameter 'tol' is deprecated as of version 0.17 and will be removed in 0.19. The parameter is no longer necessary because the value is set via the estimator initialisation or set_params method.", DeprecationWarning)
    self.tol = tol
(X, y) = check_X_y(X, y)
check_classification_targets(y)
tempResult = unique(y, return_inverse=True)
	
===================================================================	
_translate_train_sizes: 57	
----------------------------	

"Determine absolute sizes of training subsets and validate 'train_sizes'.\n\n    Examples:\n        _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]\n        _translate_train_sizes([5, 10], 10) -> [5, 10]\n\n    Parameters\n    ----------\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Numbers of training examples that will be used to generate the\n        learning curve. If the dtype is float, it is regarded as a\n        fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].\n\n    n_max_training_samples : int\n        Maximum number of training samples (upper bound of 'train_sizes').\n\n    Returns\n    -------\n    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n        Numbers of training examples that will be used to generate the\n        learning curve. Note that the number of ticks might be less\n        than n_ticks because duplicate entries will be removed.\n    "
train_sizes_abs = numpy.asarray(train_sizes)
n_ticks = train_sizes_abs.shape[0]
n_min_required_samples = numpy.min(train_sizes_abs)
n_max_required_samples = numpy.max(train_sizes_abs)
if numpy.issubdtype(train_sizes_abs.dtype, numpy.float):
    if ((n_min_required_samples <= 0.0) or (n_max_required_samples > 1.0)):
        raise ValueError(('train_sizes has been interpreted as fractions of the maximum number of training samples and must be within (0, 1], but is within [%f, %f].' % (n_min_required_samples, n_max_required_samples)))
    train_sizes_abs = astype((train_sizes_abs * n_max_training_samples), dtype=numpy.int, copy=False)
    train_sizes_abs = numpy.clip(train_sizes_abs, 1, n_max_training_samples)
elif ((n_min_required_samples <= 0) or (n_max_required_samples > n_max_training_samples)):
    raise ValueError(('train_sizes has been interpreted as absolute numbers of training samples and must be within (0, %d], but is within [%d, %d].' % (n_max_training_samples, n_min_required_samples, n_max_required_samples)))
tempResult = unique(train_sizes_abs)
	
===================================================================	
learning_curve: 34	
----------------------------	

'Learning curve.\n\n    Determines cross-validated training and test scores for different training\n    set sizes.\n\n    A cross-validation generator splits the whole dataset k times in training\n    and test data. Subsets of the training set with varying sizes will be used\n    to train the estimator and a score for each training subset size and the\n    test set will be computed. Afterwards, the scores will be averaged over\n    all k runs for each training subset size.\n\n    Read more in the :ref:`User Guide <learning_curves>`.\n\n    Parameters\n    ----------\n    estimator : object type that implements the "fit" and "predict" methods\n        An object of that type which is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train/test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, \n        :class:`sklearn.model_selection.StratifiedKFold` is used. In all\n        other cases, :class:`sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    exploit_incremental_learning : boolean, optional, default: False\n        If the estimator supports incremental learning, this will be\n        used to speed up fitting for different training set sizes.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like \'2*n_jobs\'.\n\n    verbose : integer, optional\n        Controls the verbosity: the higher, the more messages.\n\n    Returns\n    -------\n    train_sizes_abs : array, shape = (n_unique_ticks,), dtype int\n        Numbers of training examples that has been used to generate the\n        learning curve. Note that the number of ticks might be less\n        than n_ticks because duplicate entries will be removed.\n\n    train_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on training sets.\n\n    test_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on test set.\n\n    Notes\n    -----\n    See :ref:`examples/model_selection/plot_learning_curve.py\n    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n    '
if (exploit_incremental_learning and (not hasattr(estimator, 'partial_fit'))):
    raise ValueError('An estimator must support the partial_fit interface to exploit incremental learning')
(X, y) = indexable(X, y)
cv = list(check_cv(cv, X, y, classifier=is_classifier(estimator)))
scorer = check_scoring(estimator, scoring=scoring)
if (cv[0][0].dtype == bool):
    new_cv = []
    for i in range(len(cv)):
        new_cv.append((numpy.nonzero(cv[i][0])[0], numpy.nonzero(cv[i][1])[0]))
    cv = new_cv
n_max_training_samples = len(cv[0][0])
train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)
n_unique_ticks = train_sizes_abs.shape[0]
if (verbose > 0):
    print(('[learning_curve] Training set sizes: ' + str(train_sizes_abs)))
parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
if exploit_incremental_learning:
    tempResult = unique(y)
	
===================================================================	
OneVsOneClassifier.fit: 210	
----------------------------	

'Fit underlying estimators.\n\n        Parameters\n        ----------\n        X : (sparse) array-like, shape = [n_samples, n_features]\n            Data.\n\n        y : array-like, shape = [n_samples]\n            Multi-class targets.\n\n        Returns\n        -------\n        self\n        '
(X, y) = check_X_y(X, y, accept_sparse=['csr', 'csc'])
tempResult = unique(y)
	
===================================================================	
OutputCodeClassifier.fit: 273	
----------------------------	

'Fit underlying estimators.\n\n        Parameters\n        ----------\n        X : (sparse) array-like, shape = [n_samples, n_features]\n            Data.\n\n        y : numpy array of shape [n_samples]\n            Multi-class targets.\n\n        Returns\n        -------\n        self\n        '
if (self.code_size <= 0):
    raise ValueError('code_size should be greater than 0, got {1}'.format(self.code_size))
_check_estimator(self.estimator)
random_state = check_random_state(self.random_state)
tempResult = unique(y)
	
===================================================================	
_fit_binary: 25	
----------------------------	

'Fit a single binary estimator.'
tempResult = unique(y)
	
===================================================================	
GaussianNB.fit: 49	
----------------------------	

'Fit Gaussian Naive Bayes according to X, y\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        sample_weight : array-like, shape (n_samples,), optional (default=None)\n            Weights applied to individual samples (1. for unweighted).\n\n            .. versionadded:: 0.17\n               Gaussian Naive Bayes supports fitting with *sample_weight*.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        '
(X, y) = check_X_y(X, y)
tempResult = unique(y)
	
===================================================================	
GaussianNB._partial_fit: 108	
----------------------------	

'Actual implementation of Gaussian NB fitting.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        classes : array-like, shape (n_classes,), optional (default=None)\n            List of all the classes that can possibly appear in the y vector.\n\n            Must be provided at the first call to partial_fit, can be omitted\n            in subsequent calls.\n\n        _refit: bool, optional (default=False)\n            If true, act as though this were the first time we called\n            _partial_fit (ie, throw away any past fitting and start over).\n\n        sample_weight : array-like, shape (n_samples,), optional (default=None)\n            Weights applied to individual samples (1. for unweighted).\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        '
(X, y) = check_X_y(X, y)
epsilon = (1e-09 * np.var(X, axis=0).max())
if _refit:
    self.classes_ = None
if _check_partial_fit_first_call(self, classes):
    n_features = X.shape[1]
    n_classes = len(self.classes_)
    self.theta_ = numpy.zeros((n_classes, n_features))
    self.sigma_ = numpy.zeros((n_classes, n_features))
    self.class_count_ = numpy.zeros(n_classes, dtype=numpy.float64)
    n_classes = len(self.classes_)
    if (self.priors is not None):
        priors = numpy.asarray(self.priors)
        if (len(priors) != n_classes):
            raise ValueError('Number of priors must match number of classes.')
        if (priors.sum() != 1.0):
            raise ValueError('The sum of the priors should be 1.')
        if (priors < 0).any():
            raise ValueError('Priors must be non-negative.')
        self.class_prior_ = priors
    else:
        self.class_prior_ = numpy.zeros(len(self.classes_), dtype=numpy.float64)
else:
    if (X.shape[1] != self.theta_.shape[1]):
        msg = 'Number of features %d does not match previous data %d.'
        raise ValueError((msg % (X.shape[1], self.theta_.shape[1])))
    self.sigma_[:, :] -= epsilon
classes = self.classes_
tempResult = unique(y)
	
===================================================================	
affinity_propagation: 73	
----------------------------	

'Perform Affinity Propagation Clustering of data\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n\n    S : array-like, shape (n_samples, n_samples)\n        Matrix of similarities between points\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations\n\n    damping : float, optional, default: 0.5\n        Damping factor between 0.5 and 1.\n\n    copy : boolean, optional, default: True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency\n\n    verbose : boolean, optional, default: False\n        The verbosity level\n\n    return_n_iter : bool, default False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n\n    cluster_centers_indices : array, shape (n_clusters,)\n        index of clusters centers\n\n    labels : array, shape (n_samples,)\n        cluster labels for each point\n\n    n_iter : int\n        number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    See examples/cluster/plot_affinity_propagation.py for an example.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages\n    Between Data Points", Science Feb. 2007\n    '
S = as_float_array(S, copy=copy)
n_samples = S.shape[0]
if (S.shape[0] != S.shape[1]):
    raise ValueError(('S must be a square array (shape=%s)' % repr(S.shape)))
if (preference is None):
    preference = numpy.median(S)
if ((damping < 0.5) or (damping >= 1)):
    raise ValueError('damping must be >= 0.5 and < 1')
random_state = numpy.random.RandomState(0)
S.flat[::(n_samples + 1)] = preference
A = numpy.zeros((n_samples, n_samples))
R = numpy.zeros((n_samples, n_samples))
tmp = numpy.zeros((n_samples, n_samples))
S += (((np.finfo(np.double).eps * S) + (np.finfo(np.double).tiny * 100)) * random_state.randn(n_samples, n_samples))
e = numpy.zeros((n_samples, convergence_iter))
ind = numpy.arange(n_samples)
for it in range(max_iter):
    numpy.add(A, S, tmp)
    I = numpy.argmax(tmp, axis=1)
    Y = tmp[(ind, I)]
    tmp[(ind, I)] = (- numpy.inf)
    Y2 = numpy.max(tmp, axis=1)
    numpy.subtract(S, Y[:, None], tmp)
    tmp[(ind, I)] = (S[(ind, I)] - Y2)
    tmp *= (1 - damping)
    R *= damping
    R += tmp
    numpy.maximum(R, 0, tmp)
    tmp.flat[::(n_samples + 1)] = R.flat[::(n_samples + 1)]
    tmp -= numpy.sum(tmp, axis=0)
    dA = np.diag(tmp).copy()
    tmp.clip(0, numpy.inf, tmp)
    tmp.flat[::(n_samples + 1)] = dA
    tmp *= (1 - damping)
    A *= damping
    A -= tmp
    E = ((numpy.diag(A) + numpy.diag(R)) > 0)
    e[:, (it % convergence_iter)] = E
    K = numpy.sum(E, axis=0)
    if (it >= convergence_iter):
        se = numpy.sum(e, axis=1)
        unconverged = (numpy.sum(((se == convergence_iter) + (se == 0))) != n_samples)
        if (((not unconverged) and (K > 0)) or (it == max_iter)):
            if verbose:
                print(('Converged after %d iterations.' % it))
            break
else:
    if verbose:
        print('Did not converge')
I = numpy.where((numpy.diag((A + R)) > 0))[0]
K = I.size
if (K > 0):
    c = numpy.argmax(S[:, I], axis=1)
    c[I] = numpy.arange(K)
    for k in range(K):
        ii = numpy.where((c == k))[0]
        j = numpy.argmax(numpy.sum(S[(ii[:, numpy.newaxis], ii)], axis=0))
        I[k] = ii[j]
    c = numpy.argmax(S[:, I], axis=1)
    c[I] = numpy.arange(K)
    labels = I[c]
    tempResult = unique(labels)
	
===================================================================	
AgglomerativeClustering.fit: 290	
----------------------------	

'Fit the hierarchical clustering on the data\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            The samples a.k.a. observations.\n\n        Returns\n        -------\n        self\n        '
X = check_array(X, ensure_min_samples=2, estimator=self)
memory = self.memory
if isinstance(memory, externals.six.string_types):
    memory = Memory(cachedir=memory, verbose=0)
if (self.n_clusters <= 0):
    raise ValueError(('n_clusters should be an integer greater than 0. %s was provided.' % str(self.n_clusters)))
if ((self.linkage == 'ward') and (self.affinity != 'euclidean')):
    raise ValueError(('%s was provided as affinity. Ward can only work with euclidean distances.' % (self.affinity,)))
if (self.linkage not in _TREE_BUILDERS):
    raise ValueError(('Unknown linkage type %s.Valid options are %s' % (self.linkage, _TREE_BUILDERS.keys())))
tree_builder = _TREE_BUILDERS[self.linkage]
connectivity = self.connectivity
if (self.connectivity is not None):
    if callable(self.connectivity):
        connectivity = self.connectivity(X)
    connectivity = check_array(connectivity, accept_sparse=['csr', 'coo', 'lil'])
n_samples = len(X)
compute_full_tree = self.compute_full_tree
if (self.connectivity is None):
    compute_full_tree = True
if (compute_full_tree == 'auto'):
    compute_full_tree = (self.n_clusters < max(100, (0.02 * n_samples)))
n_clusters = self.n_clusters
if compute_full_tree:
    n_clusters = None
kwargs = {}
if (self.linkage != 'ward'):
    kwargs['linkage'] = self.linkage
    kwargs['affinity'] = self.affinity
(self.children_, self.n_components_, self.n_leaves_, parents) = memory.cache(tree_builder)(X, connectivity, n_clusters=n_clusters, **kwargs)
if compute_full_tree:
    self.labels_ = _hc_cut(self.n_clusters, self.children_, self.n_leaves_)
else:
    labels = _hierarchical.hc_get_heads(parents, copy=False)
    labels = numpy.copy(labels[:n_samples])
    tempResult = unique(labels)
	
===================================================================	
AgglomerationTransform.inverse_transform: 27	
----------------------------	

'\n        Inverse the transformation.\n        Return a vector of size nb_features with the values of Xred assigned\n        to each group of features\n\n        Parameters\n        ----------\n        Xred : array-like, shape=[n_samples, n_clusters] or [n_clusters,]\n            The values to be assigned to each cluster of samples\n\n        Returns\n        -------\n        X : array, shape=[n_samples, n_features] or [n_features]\n            A vector of size n_samples with the values of Xred assigned to\n            each of the cluster of samples.\n        '
check_is_fitted(self, 'labels_')
tempResult = unique(self.labels_, return_inverse=True)
	
===================================================================	
AgglomerationTransform.transform: 20	
----------------------------	

'\n        Transform a new matrix using the built clustering\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features] or [n_features]\n            A M by N array of M observations in N dimensions or a length\n            M array of M one-dimensional observations.\n\n        Returns\n        -------\n        Y : array, shape = [n_samples, n_clusters] or [n_clusters]\n            The pooled values for each feature cluster.\n        '
check_is_fitted(self, 'labels_')
pooling_func = self.pooling_func
X = check_array(X)
nX = []
if (len(self.labels_) != X.shape[1]):
    raise ValueError('X has a different number of features than during fitting.')
tempResult = unique(self.labels_)
	
===================================================================	
test_affinity_propagation: 28	
----------------------------	

S = (- euclidean_distances(X, squared=True))
preference = (numpy.median(S) * 10)
(cluster_centers_indices, labels) = affinity_propagation(S, preference=preference)
n_clusters_ = len(cluster_centers_indices)
assert_equal(n_clusters, n_clusters_)
af = AffinityPropagation(preference=preference, affinity='precomputed')
labels_precomputed = af.fit(S).labels_
af = AffinityPropagation(preference=preference, verbose=True)
labels = af.fit(X).labels_
assert_array_equal(labels, labels_precomputed)
cluster_centers_indices = af.cluster_centers_indices_
n_clusters_ = len(cluster_centers_indices)
tempResult = unique(labels)
	
===================================================================	
test_n_clusters: 58	
----------------------------	

(X, y) = make_blobs(n_samples=100, centers=10)
brc1 = Birch(n_clusters=10)
brc1.fit(X)
assert_greater(len(brc1.subcluster_centers_), 10)
tempResult = unique(brc1.labels_)
	
===================================================================	
test_ward_agglomeration: 127	
----------------------------	

rng = numpy.random.RandomState(0)
mask = numpy.ones([10, 10], dtype=numpy.bool)
X = rng.randn(50, 100)
connectivity = grid_to_graph(*mask.shape)
agglo = FeatureAgglomeration(n_clusters=5, connectivity=connectivity)
agglo.fit(X)
tempResult = unique(agglo.labels_)
	
===================================================================	
test_ward_agglomeration: 131	
----------------------------	

rng = numpy.random.RandomState(0)
mask = numpy.ones([10, 10], dtype=numpy.bool)
X = rng.randn(50, 100)
connectivity = grid_to_graph(*mask.shape)
agglo = FeatureAgglomeration(n_clusters=5, connectivity=connectivity)
agglo.fit(X)
assert_true((numpy.size(numpy.unique(agglo.labels_)) == 5))
X_red = agglo.transform(X)
assert_true((X_red.shape[1] == 5))
X_full = agglo.inverse_transform(X_red)
tempResult = unique(X_full[0])
	
===================================================================	
test_int_float_dict: 237	
----------------------------	

rng = numpy.random.RandomState(0)
tempResult = unique(rng.randint(100, size=10).astype(numpy.intp))
	
===================================================================	
test_agglomerative_clustering: 93	
----------------------------	
rng = np.random.RandomState(0)
mask = np.ones([10, 10], dtype=np.bool)
n_samples = 100
X = rng.randn(n_samples, 50)
connectivity = grid_to_graph(*mask.shape)
for linkage in ('ward', 'complete', 'average'):
    clustering = AgglomerativeClustering(n_clusters=10, connectivity=connectivity, linkage=linkage)
    clustering.fit(X)
    try:
        tempdir = mkdtemp()
        clustering = AgglomerativeClustering(n_clusters=10, connectivity=connectivity, memory=tempdir, linkage=linkage)
        clustering.fit(X)
        labels = clustering.labels_
        tempResult = unique(labels)	
===================================================================	
test_agglomerative_clustering: 102	
----------------------------	

rng = numpy.random.RandomState(0)
mask = numpy.ones([10, 10], dtype=numpy.bool)
n_samples = 100
X = rng.randn(n_samples, 50)
connectivity = grid_to_graph(*mask.shape)
for linkage in ('ward', 'complete', 'average'):
    clustering = AgglomerativeClustering(n_clusters=10, connectivity=connectivity, linkage=linkage)
    clustering.fit(X)
    try:
        tempdir = mkdtemp()
        clustering = AgglomerativeClustering(n_clusters=10, connectivity=connectivity, memory=tempdir, linkage=linkage)
        clustering.fit(X)
        labels = clustering.labels_
        assert_true((numpy.size(numpy.unique(labels)) == 10))
    finally:
        shutil.rmtree(tempdir)
    clustering = AgglomerativeClustering(n_clusters=10, connectivity=connectivity, linkage=linkage)
    clustering.compute_full_tree = False
    clustering.fit(X)
    assert_almost_equal(normalized_mutual_info_score(clustering.labels_, labels), 1)
    clustering.connectivity = None
    clustering.fit(X)
    tempResult = unique(clustering.labels_)
	
===================================================================	
test_k_means_function: 450	
----------------------------	

old_stdout = sys.stdout
sys.stdout = StringIO()
try:
    (cluster_centers, labels, inertia) = k_means(X, n_clusters=n_clusters, verbose=True)
finally:
    sys.stdout = old_stdout
centers = cluster_centers
assert_equal(centers.shape, (n_clusters, n_features))
labels = labels
tempResult = unique(labels)
	
===================================================================	
_check_fitted_model: 108	
----------------------------	

centers = km.cluster_centers_
assert_equal(centers.shape, (n_clusters, n_features))
labels = km.labels_
tempResult = unique(labels)
	
===================================================================	
test_k_means_new_centers: 125	
----------------------------	

X = numpy.array([[0, 0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 1, 0, 0]])
labels = [0, 1, 2, 1, 1, 2]
bad_centers = numpy.array([[(+ 0), 1, 0, 0], [0.2, 0, 0.2, 0.2], [(+ 0), 0, 0, 0]])
km = KMeans(n_clusters=3, init=bad_centers, n_init=1, max_iter=10, random_state=1)
for this_X in (X, scipy.sparse.coo_matrix(X)):
    km.fit(this_X)
    this_labels = km.labels_
    tempResult = unique(this_labels, return_index=True)
	
===================================================================	
test_k_means_non_collapsed: 335	
----------------------------	

my_X = numpy.array([[1.1, 1.1], [0.9, 1.1], [1.1, 0.9], [0.9, 1.1]])
array_init = numpy.array([[1.0, 1.0], [5.0, 5.0], [(- 5.0), (- 5.0)]])
km = KMeans(init=array_init, n_clusters=3, random_state=42, n_init=1)
km.fit(my_X)
tempResult = unique(km.labels_)
	
===================================================================	
test_mean_shift: 27	
----------------------------	

bandwidth = 1.2
ms = MeanShift(bandwidth=bandwidth)
labels = ms.fit(X).labels_
tempResult = unique(labels)
	
===================================================================	
test_mean_shift: 31	
----------------------------	

bandwidth = 1.2
ms = MeanShift(bandwidth=bandwidth)
labels = ms.fit(X).labels_
labels_unique = numpy.unique(labels)
n_clusters_ = len(labels_unique)
assert_equal(n_clusters_, n_clusters)
(cluster_centers, labels) = mean_shift(X, bandwidth=bandwidth)
tempResult = unique(labels)
	
===================================================================	
_fetch_lfw_people: 121	
----------------------------	

'Perform the actual data loading for the lfw people dataset\n\n    This operation is meant to be cached by a joblib wrapper.\n    '
(person_names, file_paths) = ([], [])
for person_name in sorted(listdir(data_folder_path)):
    folder_path = join(data_folder_path, person_name)
    if (not isdir(folder_path)):
        continue
    paths = [join(folder_path, f) for f in listdir(folder_path)]
    n_pictures = len(paths)
    if (n_pictures >= min_faces_per_person):
        person_name = person_name.replace('_', ' ')
        person_names.extend(([person_name] * n_pictures))
        file_paths.extend(paths)
n_faces = len(file_paths)
if (n_faces == 0):
    raise ValueError(('min_faces_per_person=%d is too restrictive' % min_faces_per_person))
tempResult = unique(person_names)
	
===================================================================	
test_20news: 17	
----------------------------	

try:
    data = sklearn.datasets.fetch_20newsgroups(subset='all', download_if_missing=False, shuffle=False)
except IOError:
    raise SkipTest('Download 20 newsgroups to run this test')
data2cats = sklearn.datasets.fetch_20newsgroups(subset='all', categories=data.target_names[(- 1):(- 3):(- 1)], shuffle=False)
assert_equal(data2cats.target_names, data.target_names[(- 2):])
tempResult = unique(data2cats.target)
	
===================================================================	
test_load_digits_n_class_lt_10: 117	
----------------------------	

digits = load_digits(9)
assert_equal(digits.data.shape, (1617, 64))
tempResult = unique(digits.target)
	
===================================================================	
test_load_digits: 107	
----------------------------	

digits = load_digits()
assert_equal(digits.data.shape, (1797, 64))
tempResult = unique(digits.target)
	
===================================================================	
test_make_classification: 37	
----------------------------	

(X, y) = make_classification(n_samples=100, n_features=20, n_informative=5, n_redundant=1, n_repeated=1, n_classes=3, n_clusters_per_class=1, hypercube=False, shift=None, scale=None, weights=[0.1, 0.25], random_state=0)
assert_equal(X.shape, (100, 20), 'X shape mismatch')
assert_equal(y.shape, (100,), 'y shape mismatch')
tempResult = unique(y)
	
===================================================================	
test_make_classification_informative_features: 56	
----------------------------	

'Test the construction of informative features in make_classification\n\n    Also tests `n_clusters_per_class`, `n_classes`, `hypercube` and\n    fully-specified `weights`.\n    '
class_sep = 1000000.0
make = partial(make_classification, class_sep=class_sep, n_redundant=0, n_repeated=0, flip_y=0, shift=0, scale=1, shuffle=False)
for (n_informative, weights, n_clusters_per_class) in [(2, [1], 1), (2, ([(1 / 3)] * 3), 1), (2, ([(1 / 4)] * 4), 1), (2, ([(1 / 2)] * 2), 2), (2, [(3 / 4), (1 / 4)], 2), (10, ([(1 / 3)] * 3), 10)]:
    n_classes = len(weights)
    n_clusters = (n_classes * n_clusters_per_class)
    n_samples = (n_clusters * 50)
    for hypercube in (False, True):
        (X, y) = make(n_samples=n_samples, n_classes=n_classes, weights=weights, n_features=n_informative, n_informative=n_informative, n_clusters_per_class=n_clusters_per_class, hypercube=hypercube, random_state=0)
        assert_equal(X.shape, (n_samples, n_informative))
        assert_equal(y.shape, (n_samples,))
        signs = numpy.sign(X)
        signs = signs.view(dtype='|S{0}'.format(signs.strides[0]))
        tempResult = unique(signs, return_inverse=True)
	
===================================================================	
test_make_blobs: 134	
----------------------------	

cluster_stds = numpy.array([0.05, 0.2, 0.4])
cluster_centers = numpy.array([[0.0, 0.0], [1.0, 1.0], [0.0, 1.0]])
(X, y) = make_blobs(random_state=0, n_samples=50, n_features=2, centers=cluster_centers, cluster_std=cluster_stds)
assert_equal(X.shape, (50, 2), 'X shape mismatch')
assert_equal(y.shape, (50,), 'y shape mismatch')
tempResult = unique(y)
	
===================================================================	
test_make_hastie_10_2: 108	
----------------------------	

(X, y) = make_hastie_10_2(n_samples=100, random_state=0)
assert_equal(X.shape, (100, 10), 'X shape mismatch')
assert_equal(y.shape, (100,), 'y shape mismatch')
tempResult = unique(y)
	
===================================================================	
BaggingClassifier._validate_y: 248	
----------------------------	

y = column_or_1d(y, warn=True)
check_classification_targets(y)
tempResult = unique(y, return_inverse=True)
	
===================================================================	
ForestClassifier._validate_y_class_weight: 209	
----------------------------	

check_classification_targets(y)
y = numpy.copy(y)
expanded_class_weight = None
if (self.class_weight is not None):
    y_original = numpy.copy(y)
self.classes_ = []
self.n_classes_ = []
y_store_unique_indices = numpy.zeros(y.shape, dtype=numpy.int)
for k in range(self.n_outputs_):
    tempResult = unique(y[:, k], return_inverse=True)
	
===================================================================	
GradientBoostingClassifier._validate_y: 791	
----------------------------	

check_classification_targets(y)
tempResult = unique(y, return_inverse=True)
	
===================================================================	
ZeroEstimator.fit: 120	
----------------------------	

if numpy.issubdtype(y.dtype, int):
    tempResult = unique(y)
	
===================================================================	
_grid_from_X: 24	
----------------------------	

'Generate a grid of points based on the ``percentiles of ``X``.\n\n    The grid is generated by placing ``grid_resolution`` equally\n    spaced points between the ``percentiles`` of each column\n    of ``X``.\n\n    Parameters\n    ----------\n    X : ndarray\n        The data\n    percentiles : tuple of floats\n        The percentiles which are used to construct the extreme\n        values of the grid axes.\n    grid_resolution : int\n        The number of equally spaced points that are placed\n        on the grid.\n\n    Returns\n    -------\n    grid : ndarray\n        All data points on the grid; ``grid.shape[1] == X.shape[1]``\n        and ``grid.shape[0] == grid_resolution * X.shape[1]``.\n    axes : seq of ndarray\n        The axes with which the grid has been created.\n    '
if (len(percentiles) != 2):
    raise ValueError('percentile must be tuple of len 2')
if (not all(((0.0 <= x <= 1.0) for x in percentiles))):
    raise ValueError('percentile values must be in [0, 1]')
axes = []
for col in range(X.shape[1]):
    tempResult = unique(X[:, col])
	
===================================================================	
test_bootstrap_features: 119	
----------------------------	

rng = check_random_state(0)
(X_train, X_test, y_train, y_test) = train_test_split(boston.data, boston.target, random_state=rng)
ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(), max_features=1.0, bootstrap_features=False, random_state=rng).fit(X_train, y_train)
for features in ensemble.estimators_features_:
    tempResult = unique(features)
	
===================================================================	
test_bootstrap_features: 122	
----------------------------	

rng = check_random_state(0)
(X_train, X_test, y_train, y_test) = train_test_split(boston.data, boston.target, random_state=rng)
ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(), max_features=1.0, bootstrap_features=False, random_state=rng).fit(X_train, y_train)
for features in ensemble.estimators_features_:
    assert_equal(boston.data.shape[1], np.unique(features).shape[0])
ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(), max_features=1.0, bootstrap_features=True, random_state=rng).fit(X_train, y_train)
for features in ensemble.estimators_features_:
    tempResult = unique(features)
	
===================================================================	
DummyZeroEstimator.fit: 240	
----------------------------	

tempResult = unique(y)
	
===================================================================	
mdi_importance: 181	
----------------------------	

(n_samples, n_features) = X.shape
features = list(range(n_features))
features.pop(X_m)
tempResult = unique(X[:, i])
	
===================================================================	
test_partial_dependence_classifier: 26	
----------------------------	

clf = GradientBoostingClassifier(n_estimators=10, random_state=1)
clf.fit(X, y)
(pdp, axes) = partial_dependence(clf, [0], X=X, grid_resolution=5)
assert (pdp.shape == (1, 4))
assert (axes[0].shape[0] == 4)
X_ = numpy.asarray(X)
tempResult = unique(X_[:, 0])
	
===================================================================	
test_classification_toy: 57	
----------------------------	

for alg in ['SAMME', 'SAMME.R']:
    clf = AdaBoostClassifier(algorithm=alg, random_state=0)
    clf.fit(X, y_class)
    assert_array_equal(clf.predict(T), y_t_class)
    tempResult = unique(numpy.asarray(y_t_class))
	
===================================================================	
test_iris: 67	
----------------------------	

tempResult = unique(iris.target)
	
===================================================================	
_mask_edges_weights: 39	
----------------------------	

'Apply a mask to edges (weighted or not)'
inds = numpy.arange(mask.size)
inds = inds[mask.ravel()]
ind_mask = numpy.logical_and(numpy.in1d(edges[0], inds), numpy.in1d(edges[1], inds))
edges = edges[:, ind_mask]
if (weights is not None):
    weights = weights[ind_mask]
if len(edges.ravel()):
    maxval = edges.max()
else:
    maxval = 0
tempResult = unique(edges.ravel())
	
===================================================================	
test_vectorizer_inverse_transform: 454	
----------------------------	

data = ALL_FOOD_DOCS
for vectorizer in (TfidfVectorizer(), CountVectorizer()):
    transformed_data = vectorizer.fit_transform(data)
    inversed_data = vectorizer.inverse_transform(transformed_data)
    analyze = vectorizer.build_analyzer()
    for (doc, inversed_terms) in zip(data, inversed_data):
        tempResult = unique(analyze(doc))
	
===================================================================	
test_vectorizer_inverse_transform: 455	
----------------------------	

data = ALL_FOOD_DOCS
for vectorizer in (TfidfVectorizer(), CountVectorizer()):
    transformed_data = vectorizer.fit_transform(data)
    inversed_data = vectorizer.inverse_transform(transformed_data)
    analyze = vectorizer.build_analyzer()
    for (doc, inversed_terms) in zip(data, inversed_data):
        terms = numpy.sort(numpy.unique(analyze(doc)))
        tempResult = unique(inversed_terms)
	
===================================================================	
_compute_mi_cd: 42	
----------------------------	

'Compute mutual information between continuous and discrete variables.\n\n    Parameters\n    ----------\n    c : ndarray, shape (n_samples,)\n        Samples of a continuous random variable.\n\n    d : ndarray, shape (n_samples,)\n        Samples of a discrete random variable.\n\n    n_neighbors : int\n        Number of nearest neighbors to search for each point, see [1]_.\n\n    Returns\n    -------\n    mi : float\n        Estimated mutual information. If it turned out to be negative it is\n        replace by 0.\n\n    Notes\n    -----\n    True mutual information can\'t be negative. If its estimate by a numerical\n    method is negative, it means (providing the method is adequate) that the\n    mutual information is close to 0 and replacing it by 0 is a reasonable\n    strategy.\n\n    References\n    ----------\n    .. [1] B. C. Ross "Mutual Information between Discrete and Continuous\n       Data Sets". PLoS ONE 9(2), 2014.\n    '
n_samples = c.shape[0]
c = c.reshape(((- 1), 1))
radius = numpy.empty(n_samples)
label_counts = numpy.empty(n_samples)
k_all = numpy.empty(n_samples)
nn = NearestNeighbors()
tempResult = unique(d)
	
===================================================================	
f_classif: 51	
----------------------------	

'Compute the ANOVA F-value for the provided sample.\n\n    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} shape = [n_samples, n_features]\n        The set of regressors that will be tested sequentially.\n\n    y : array of shape(n_samples)\n        The data matrix.\n\n    Returns\n    -------\n    F : array, shape = [n_features,]\n        The set of F values.\n\n    pval : array, shape = [n_features,]\n        The set of p-values.\n\n    See also\n    --------\n    chi2: Chi-squared stats of non-negative features for classification tasks.\n    f_regression: F-value between label/feature for regression tasks.\n    '
(X, y) = check_X_y(X, y, ['csr', 'csc', 'coo'])
tempResult = unique(y)
	
===================================================================	
test_partial_fit: 84	
----------------------------	

est = PassiveAggressiveClassifier(random_state=0, shuffle=False)
transformer = SelectFromModel(estimator=est)
tempResult = unique(y)
	
===================================================================	
test_partial_fit: 86	
----------------------------	

est = PassiveAggressiveClassifier(random_state=0, shuffle=False)
transformer = SelectFromModel(estimator=est)
transformer.partial_fit(data, y, classes=numpy.unique(y))
old_model = transformer.estimator_
tempResult = unique(y)
	
===================================================================	
GaussianProcessClassifier.fit: 172	
----------------------------	

'Fit Gaussian process classification model\n\n        Parameters\n        ----------\n        X : array-like, shape = (n_samples, n_features)\n            Training data\n\n        y : array-like, shape = (n_samples,)\n            Target values, must be binary\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
(X, y) = check_X_y(X, y, multi_output=False)
self.base_estimator_ = _BinaryGaussianProcessClassifierLaplace(self.kernel, self.optimizer, self.n_restarts_optimizer, self.max_iter_predict, self.warm_start, self.copy_X_train, self.random_state)
tempResult = unique(y)
	
===================================================================	
LarsCV.fit: 369	
----------------------------	

'Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        '
self.fit_path = True
(X, y) = check_X_y(X, y, y_numeric=True)
X = as_float_array(X, copy=self.copy_X)
y = as_float_array(y, copy=self.copy_X)
cv = check_cv(self.cv, classifier=False)
Gram = ('auto' if self.precompute else None)
cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, (self.verbose - 1)), normalize=self.normalize, fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for (train, test) in cv.split(X, y)))
all_alphas = numpy.concatenate(list(zip(*cv_paths))[0])
tempResult = unique(all_alphas)
	
===================================================================	
_log_reg_scoring_path: 302	
----------------------------	

'Computes scores across logistic_regression_path\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape (n_samples, n_features)\n        Training data.\n\n    y : array-like, shape (n_samples,) or (n_samples, n_targets)\n        Target labels.\n\n    train : list of indices\n        The indices of the train set.\n\n    test : list of indices\n        The indices of the test set.\n\n    pos_class : int, None\n        The class with respect to which we perform a one-vs-all fit.\n        If None, then it is assumed that the given problem is binary.\n\n    Cs : list of floats | int\n        Each of the values in Cs describes the inverse of\n        regularization strength. If Cs is as an int, then a grid of Cs\n        values are chosen in a logarithmic scale between 1e-4 and 1e4.\n        If not provided, then a fixed set of values for Cs are used.\n\n    scoring : callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``. For a list of scoring functions\n        that can be used, look at :mod:`sklearn.metrics`. The\n        default scoring option used is accuracy_score.\n\n    fit_intercept : bool\n        If False, then the bias term is set to zero. Else the last\n        term of each coef_ gives us the intercept.\n\n    max_iter : int\n        Maximum number of iterations for the solver.\n\n    tol : float\n        Tolerance for stopping criteria.\n\n    class_weight : dict or \'balanced\', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The "balanced" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    verbose : int\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    solver : {\'lbfgs\', \'newton-cg\', \'liblinear\', \'sag\'}\n        Decides which solver to use.\n\n    penalty : str, \'l1\' or \'l2\'\n        Used to specify the norm used in the penalization. The \'newton-cg\',\n        \'sag\' and \'lbfgs\' solvers support only l2 penalties.\n\n    dual : bool\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    intercept_scaling : float, default 1.\n        Useful only when the solver \'liblinear\' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a "synthetic" feature with constant value equals to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes intercept_scaling * synthetic feature weight\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : str, {\'ovr\', \'multinomial\'}\n        Multiclass option can be either \'ovr\' or \'multinomial\'. If the option\n        chosen is \'ovr\', then a binary problem is fit for each label. Else\n        the loss minimised is the multinomial loss fit across\n        the entire probability distribution. Works only for the \'lbfgs\' and\n        \'newton-cg\' solver.\n\n    random_state : int seed, RandomState instance, or None (default)\n        The seed of the pseudo random number generator to use when\n        shuffling the data. Used only in solvers \'sag\' and \'liblinear\'.\n\n    max_squared_sum : float, default None\n        Maximum squared sum of X over samples. Used only in SAG solver.\n        If None, it will be computed, going through all the samples.\n        The value should be precomputed to speed up cross validation.\n\n    sample_weight : array-like, shape(n_samples,) optional\n        Array of weights that are assigned to individual samples.\n        If not provided, then each sample is given unit weight.\n\n    Returns\n    -------\n    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)\n        List of coefficients for the Logistic Regression model. If\n        fit_intercept is set to True then the second dimension will be\n        n_features + 1, where the last item represents the intercept.\n\n    Cs : ndarray\n        Grid of Cs used for cross-validation.\n\n    scores : ndarray, shape (n_cs,)\n        Scores obtained for each Cs.\n\n    n_iter : array, shape(n_cs,)\n        Actual number of iteration for each Cs.\n    '
_check_solver_option(solver, multi_class, penalty, dual)
X_train = X[train]
X_test = X[test]
y_train = y[train]
y_test = y[test]
if (sample_weight is not None):
    sample_weight = sample_weight[train]
(coefs, Cs, n_iter) = logistic_regression_path(X_train, y_train, Cs=Cs, fit_intercept=fit_intercept, solver=solver, max_iter=max_iter, class_weight=class_weight, pos_class=pos_class, multi_class=multi_class, tol=tol, verbose=verbose, dual=dual, penalty=penalty, intercept_scaling=intercept_scaling, random_state=random_state, check_input=False, max_squared_sum=max_squared_sum, sample_weight=sample_weight)
log_reg = LogisticRegression(fit_intercept=fit_intercept)
if (multi_class == 'ovr'):
    log_reg.classes_ = numpy.array([(- 1), 1])
elif (multi_class == 'multinomial'):
    tempResult = unique(y_train)
	
===================================================================	
LogisticRegressionCV.fit: 459	
----------------------------	

'Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like, shape (n_samples,) optional\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        '
_check_solver_option(self.solver, self.multi_class, self.penalty, self.dual)
if ((not isinstance(self.max_iter, numbers.Number)) or (self.max_iter < 0)):
    raise ValueError(('Maximum number of iteration must be positive; got (max_iter=%r)' % self.max_iter))
if ((not isinstance(self.tol, numbers.Number)) or (self.tol < 0)):
    raise ValueError(('Tolerance for stopping criteria must be positive; got (tol=%r)' % self.tol))
(X, y) = check_X_y(X, y, accept_sparse='csr', dtype=numpy.float64, order='C')
if (self.solver == 'sag'):
    max_squared_sum = row_norms(X, squared=True).max()
else:
    max_squared_sum = None
check_classification_targets(y)
if ((y.ndim == 2) and (y.shape[1] == 1)):
    warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning)
    y = numpy.ravel(y)
check_consistent_length(X, y)
cv = check_cv(self.cv, y, classifier=True)
folds = list(cv.split(X, y))
self._enc = LabelEncoder()
self._enc.fit(y)
tempResult = unique(y)
	
===================================================================	
LogisticRegressionCV.fit: 472	
----------------------------	

'Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like, shape (n_samples,) optional\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        '
_check_solver_option(self.solver, self.multi_class, self.penalty, self.dual)
if ((not isinstance(self.max_iter, numbers.Number)) or (self.max_iter < 0)):
    raise ValueError(('Maximum number of iteration must be positive; got (max_iter=%r)' % self.max_iter))
if ((not isinstance(self.tol, numbers.Number)) or (self.tol < 0)):
    raise ValueError(('Tolerance for stopping criteria must be positive; got (tol=%r)' % self.tol))
(X, y) = check_X_y(X, y, accept_sparse='csr', dtype=numpy.float64, order='C')
if (self.solver == 'sag'):
    max_squared_sum = row_norms(X, squared=True).max()
else:
    max_squared_sum = None
check_classification_targets(y)
if ((y.ndim == 2) and (y.shape[1] == 1)):
    warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning)
    y = numpy.ravel(y)
check_consistent_length(X, y)
cv = check_cv(self.cv, y, classifier=True)
folds = list(cv.split(X, y))
self._enc = LabelEncoder()
self._enc.fit(y)
labels = self.classes_ = numpy.unique(y)
n_classes = len(labels)
if (n_classes < 2):
    raise ValueError(('This solver needs samples of at least 2 classes in the data, but the data contains only one class: %r' % self.classes_[0]))
if (n_classes == 2):
    n_classes = 1
    labels = labels[1:]
iter_labels = labels
if (self.multi_class == 'multinomial'):
    iter_labels = [None]
if (self.class_weight and (not (isinstance(self.class_weight, dict) or (self.class_weight in ['balanced', 'auto'])))):
    raise ValueError("class_weight provided should be a dict or 'balanced'")
if (self.class_weight in ('auto', 'balanced')):
    tempResult = unique(y)
	
===================================================================	
logistic_regression_path: 180	
----------------------------	

'Compute a Logistic Regression model for a list of regularization\n    parameters.\n\n    This is an implementation that uses the result of the previous model\n    to speed up computations along the set of solutions, making it faster\n    than sequentially calling LogisticRegression for the different parameters.\n    Note that there will be no speedup with liblinear solver, since it does\n    not handle warm-starting.\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    X : array-like or sparse matrix, shape (n_samples, n_features)\n        Input data.\n\n    y : array-like, shape (n_samples,)\n        Input data, target values.\n\n    Cs : int | array-like, shape (n_cs,)\n        List of values for the regularization parameter or integer specifying\n        the number of regularization parameters that should be used. In this\n        case, the parameters will be chosen in a logarithmic scale between\n        1e-4 and 1e4.\n\n    pos_class : int, None\n        The class with respect to which we perform a one-vs-all fit.\n        If None, then it is assumed that the given problem is binary.\n\n    fit_intercept : bool\n        Whether to fit an intercept for the model. In this case the shape of\n        the returned array is (n_cs, n_features + 1).\n\n    max_iter : int\n        Maximum number of iterations for the solver.\n\n    tol : float\n        Stopping criterion. For the newton-cg and lbfgs solvers, the iteration\n        will stop when ``max{|g_i | i = 1, ..., n} <= tol``\n        where ``g_i`` is the i-th component of the gradient.\n\n    verbose : int\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    solver : {\'lbfgs\', \'newton-cg\', \'liblinear\', \'sag\'}\n        Numerical solver to use.\n\n    coef : array-like, shape (n_features,), default None\n        Initialization value for coefficients of logistic regression.\n        Useless for liblinear solver.\n\n    copy : bool, default False\n        Whether or not to produce a copy of the data. A copy is not required\n        anymore. This parameter is deprecated and will be removed in 0.19.\n\n    class_weight : dict or \'balanced\', optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The "balanced" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n    dual : bool\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    penalty : str, \'l1\' or \'l2\'\n        Used to specify the norm used in the penalization. The \'newton-cg\',\n        \'sag\' and \'lbfgs\' solvers support only l2 penalties.\n\n    intercept_scaling : float, default 1.\n        Useful only when the solver \'liblinear\' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a "synthetic" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : str, {\'ovr\', \'multinomial\'}\n        Multiclass option can be either \'ovr\' or \'multinomial\'. If the option\n        chosen is \'ovr\', then a binary problem is fit for each label. Else\n        the loss minimised is the multinomial loss fit across\n        the entire probability distribution. Works only for the \'lbfgs\' and\n        \'newton-cg\' solvers.\n\n    random_state : int seed, RandomState instance, or None (default)\n        The seed of the pseudo random number generator to use when\n        shuffling the data. Used only in solvers \'sag\' and \'liblinear\'.\n\n    check_input : bool, default True\n        If False, the input arrays X and y will not be checked.\n\n    max_squared_sum : float, default None\n        Maximum squared sum of X over samples. Used only in SAG solver.\n        If None, it will be computed, going through all the samples.\n        The value should be precomputed to speed up cross validation.\n\n    sample_weight : array-like, shape(n_samples,) optional\n        Array of weights that are assigned to individual samples.\n        If not provided, then each sample is given unit weight.\n\n    Returns\n    -------\n    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)\n        List of coefficients for the Logistic Regression model. If\n        fit_intercept is set to True then the second dimension will be\n        n_features + 1, where the last item represents the intercept.\n\n    Cs : ndarray\n        Grid of Cs used for cross-validation.\n\n    n_iter : array, shape (n_cs,)\n        Actual number of iteration for each Cs.\n\n    Notes\n    -----\n    You might get slightly different results with the solver liblinear than\n    with the others since this uses LIBLINEAR which penalizes the intercept.\n    '
if copy:
    warnings.warn("A copy is not required anymore. The 'copy' parameter is deprecated and will be removed in 0.19.", DeprecationWarning)
if isinstance(Cs, numbers.Integral):
    Cs = numpy.logspace((- 4), 4, Cs)
_check_solver_option(solver, multi_class, penalty, dual)
if (check_input or copy):
    X = check_array(X, accept_sparse='csr', dtype=numpy.float64)
    y = check_array(y, ensure_2d=False, copy=copy, dtype=None)
    check_consistent_length(X, y)
(_, n_features) = X.shape
tempResult = unique(y)
	
===================================================================	
LogisticRegression.fit: 357	
----------------------------	

'Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : array-like, shape (n_samples,) optional\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n\n            .. versionadded:: 0.17\n               *sample_weight* support to LogisticRegression.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        '
if ((not isinstance(self.C, numbers.Number)) or (self.C < 0)):
    raise ValueError(('Penalty term must be positive; got (C=%r)' % self.C))
if ((not isinstance(self.max_iter, numbers.Number)) or (self.max_iter < 0)):
    raise ValueError(('Maximum number of iteration must be positive; got (max_iter=%r)' % self.max_iter))
if ((not isinstance(self.tol, numbers.Number)) or (self.tol < 0)):
    raise ValueError(('Tolerance for stopping criteria must be positive; got (tol=%r)' % self.tol))
(X, y) = check_X_y(X, y, accept_sparse='csr', dtype=numpy.float64, order='C')
check_classification_targets(y)
tempResult = unique(y)
	
===================================================================	
BaseSGDClassifier._fit: 241	
----------------------------	

if hasattr(self, 'classes_'):
    self.classes_ = None
(X, y) = check_X_y(X, y, 'csr', dtype=numpy.float64, order='C')
(n_samples, n_features) = X.shape
tempResult = unique(y)
	
===================================================================	
check_predictions: 33	
----------------------------	

'Check that the model is able to fit the classification data'
n_samples = len(y)
tempResult = unique(y)
	
===================================================================	
_compute_class_weight_dictionary: 422	
----------------------------	

tempResult = unique(y)
	
===================================================================	
test_predict_iris: 73	
----------------------------	

(n_samples, n_features) = iris.data.shape
target = iris.target_names[iris.target]
for clf in [LogisticRegression(C=len(iris.data)), LogisticRegression(C=len(iris.data), solver='lbfgs', multi_class='multinomial'), LogisticRegression(C=len(iris.data), solver='newton-cg', multi_class='multinomial'), LogisticRegression(C=len(iris.data), solver='sag', tol=0.01, multi_class='ovr', random_state=42)]:
    clf.fit(iris.data, target)
    tempResult = unique(target)
	
===================================================================	
test_n_iter: 574	
----------------------------	

(X, y) = (iris.data, iris.target)
y_bin = y.copy()
y_bin[(y_bin == 2)] = 0
n_Cs = 4
n_cv_fold = 2
for solver in ['newton-cg', 'liblinear', 'sag', 'lbfgs']:
    tempResult = unique(y)
	
===================================================================	
test_n_iter: 578	
----------------------------	

(X, y) = (iris.data, iris.target)
y_bin = y.copy()
y_bin[(y_bin == 2)] = 0
n_Cs = 4
n_cv_fold = 2
for solver in ['newton-cg', 'liblinear', 'sag', 'lbfgs']:
    n_classes = (1 if (solver == 'liblinear') else np.unique(y).shape[0])
    clf = LogisticRegression(tol=0.01, multi_class='ovr', solver=solver, C=1.0, random_state=42, max_iter=100)
    clf.fit(X, y)
    assert_equal(clf.n_iter_.shape, (n_classes,))
    tempResult = unique(y)
	
===================================================================	
test_classifier_partial_fit: 67	
----------------------------	

tempResult = unique(y)
	
===================================================================	
test_partial_fit_weight_class_balanced: 109	
----------------------------	

clf = PassiveAggressiveClassifier(class_weight='balanced')
tempResult = unique(y)
	
===================================================================	
test_classifier_refit: 77	
----------------------------	

clf = PassiveAggressiveClassifier().fit(X, y)
tempResult = unique(y)
	
===================================================================	
_test_ridge_classifiers: 286	
----------------------------	

tempResult = unique(y_iris)
	
===================================================================	
test_multiclass_classifier_class_weight: 434	
----------------------------	

'tests multiclass with classweights for each class'
alpha = 0.1
n_samples = 20
tol = 1e-05
max_iter = 50
class_weight = {0: 0.45, 1: 0.55, 2: 0.75}
fit_intercept = True
(X, y) = make_blobs(n_samples=n_samples, centers=3, random_state=0, cluster_std=0.1)
step_size = get_step_size(X, alpha, fit_intercept, classification=True)
tempResult = unique(y)
	
===================================================================	
test_multiclass_classifier_class_weight: 440	
----------------------------	

'tests multiclass with classweights for each class'
alpha = 0.1
n_samples = 20
tol = 1e-05
max_iter = 50
class_weight = {0: 0.45, 1: 0.55, 2: 0.75}
fit_intercept = True
(X, y) = make_blobs(n_samples=n_samples, centers=3, random_state=0, cluster_std=0.1)
step_size = get_step_size(X, alpha, fit_intercept, classification=True)
classes = numpy.unique(y)
clf1 = LogisticRegression(solver='sag', C=((1.0 / alpha) / n_samples), max_iter=max_iter, tol=tol, random_state=77, fit_intercept=fit_intercept, class_weight=class_weight)
clf2 = clone(clf1)
clf1.fit(X, y)
clf2.fit(scipy.sparse.csr_matrix(X), y)
le = LabelEncoder()
tempResult = unique(y)
	
===================================================================	
test_multinomial_loss: 485	
----------------------------	

(X, y) = (iris.data, iris.target.astype(numpy.float64))
(n_samples, n_features) = X.shape
tempResult = unique(y)
	
===================================================================	
test_binary_classifier_class_weight: 404	
----------------------------	

'tests binary classifier with classweights for each class'
alpha = 0.1
n_samples = 50
n_iter = 20
tol = 1e-05
fit_intercept = True
(X, y) = make_blobs(n_samples=n_samples, centers=2, random_state=10, cluster_std=0.1)
step_size = get_step_size(X, alpha, fit_intercept, classification=True)
tempResult = unique(y)
	
===================================================================	
test_binary_classifier_class_weight: 414	
----------------------------	

'tests binary classifier with classweights for each class'
alpha = 0.1
n_samples = 50
n_iter = 20
tol = 1e-05
fit_intercept = True
(X, y) = make_blobs(n_samples=n_samples, centers=2, random_state=10, cluster_std=0.1)
step_size = get_step_size(X, alpha, fit_intercept, classification=True)
classes = numpy.unique(y)
y_tmp = numpy.ones(n_samples)
y_tmp[(y != classes[1])] = (- 1)
y = y_tmp
class_weight = {1: 0.45, (- 1): 0.55}
clf1 = LogisticRegression(solver='sag', C=((1.0 / alpha) / n_samples), max_iter=n_iter, tol=tol, random_state=77, fit_intercept=fit_intercept, class_weight=class_weight)
clf2 = clone(clf1)
clf1.fit(X, y)
clf2.fit(scipy.sparse.csr_matrix(X), y)
le = LabelEncoder()
tempResult = unique(y)
	
===================================================================	
test_sag_multiclass_computed_correctly: 344	
----------------------------	

'tests if the multiclass classifier is computed correctly'
alpha = 0.1
n_samples = 20
tol = 1e-05
max_iter = 40
fit_intercept = True
(X, y) = make_blobs(n_samples=n_samples, centers=3, random_state=0, cluster_std=0.1)
step_size = get_step_size(X, alpha, fit_intercept, classification=True)
tempResult = unique(y)
	
===================================================================	
test_sag_classifier_computed_correctly: 319	
----------------------------	

'tests if the binary classifier is computed correctly'
alpha = 0.1
n_samples = 50
n_iter = 50
tol = 1e-05
fit_intercept = True
(X, y) = make_blobs(n_samples=n_samples, centers=2, random_state=0, cluster_std=0.1)
step_size = get_step_size(X, alpha, fit_intercept, classification=True)
tempResult = unique(y)
	
===================================================================	
DenseSGDClassifierTestCase.test_partial_fit_multiclass: 496	
----------------------------	

third = (X2.shape[0] // 3)
clf = self.factory(alpha=0.01)
tempResult = unique(Y2)
	
===================================================================	
CommonTest.test_late_onset_averaging_not_reached: 162	
----------------------------	

clf1 = self.factory(average=600)
clf2 = self.factory()
for _ in range(100):
    if isinstance(clf1, SGDClassifier):
        tempResult = unique(Y)
	
===================================================================	
CommonTest.test_late_onset_averaging_not_reached: 163	
----------------------------	

clf1 = self.factory(average=600)
clf2 = self.factory()
for _ in range(100):
    if isinstance(clf1, SGDClassifier):
        clf1.partial_fit(X, Y, classes=numpy.unique(Y))
        tempResult = unique(Y)
	
===================================================================	
DenseSGDClassifierTestCase._test_partial_fit_equal_fit: 528	
----------------------------	

for (X_, Y_, T_) in ((X, Y, T), (X2, Y2, T2)):
    clf = self.factory(alpha=0.01, eta0=0.01, n_iter=2, learning_rate=lr, shuffle=False)
    clf.fit(X_, Y_)
    y_pred = clf.decision_function(T_)
    t = clf.t_
    tempResult = unique(Y_)
	
===================================================================	
DenseSGDClassifierTestCase.test_partial_fit_binary: 481	
----------------------------	

third = (X.shape[0] // 3)
clf = self.factory(alpha=0.01)
tempResult = unique(Y)
	
===================================================================	
test_underflow_or_overlow: 757	
----------------------------	

with numpy.errstate(all='raise'):
    rng = numpy.random.RandomState(0)
    n_samples = 100
    n_features = 10
    X = rng.normal(size=(n_samples, n_features))
    X[:, :2] *= 1e+300
    assert_true(np.isfinite(X).all())
    X_scaled = MinMaxScaler().fit_transform(X)
    assert_true(np.isfinite(X_scaled).all())
    ground_truth = rng.normal(size=n_features)
    y = (np.dot(X_scaled, ground_truth) > 0.0).astype(numpy.int32)
    tempResult = unique(y)
	
===================================================================	
DenseSGDClassifierTestCase.test_partial_fit_multiclass_average: 509	
----------------------------	

third = (X2.shape[0] // 3)
clf = self.factory(alpha=0.01, average=X2.shape[0])
tempResult = unique(Y2)
	
===================================================================	
DenseSGDClassifierTestCase.test_partial_fit_weight_class_balanced: 273	
----------------------------	

tempResult = unique(Y)
	
===================================================================	
DenseSGDClassifierTestCase.test_sgd_multiclass_average: 289	
----------------------------	

eta = 0.001
alpha = 0.01
clf = self.factory(loss='squared_loss', learning_rate='constant', eta0=eta, alpha=alpha, fit_intercept=True, n_iter=1, average=True, shuffle=False)
np_Y2 = numpy.array(Y2)
clf.fit(X2, np_Y2)
tempResult = unique(np_Y2)
	
===================================================================	
_set_diag: 60	
----------------------------	

'Set the diagonal of the laplacian matrix and convert it to a\n    sparse format well suited for eigenvalue decomposition\n\n    Parameters\n    ----------\n    laplacian : array or sparse matrix\n        The graph laplacian\n    value : float\n        The value of the diagonal\n    norm_laplacian : bool\n        Whether the value of the diagonal should be changed or not\n\n    Returns\n    -------\n    laplacian : array or sparse matrix\n        An array of matrix in a form that is well suited to fast\n        eigenvalue decomposition, depending on the band width of the\n        matrix.\n    '
n_nodes = laplacian.shape[0]
if (not scipy.sparse.isspmatrix(laplacian)):
    if norm_laplacian:
        laplacian.flat[::(n_nodes + 1)] = value
else:
    laplacian = laplacian.tocoo()
    if norm_laplacian:
        diag_idx = (laplacian.row == laplacian.col)
        laplacian.data[diag_idx] = value
    tempResult = unique((laplacian.row - laplacian.col))
	
===================================================================	
_check_binary_probabilistic_predictions: 417	
----------------------------	

'Check that y_true is binary and y_prob contains valid probabilities'
check_consistent_length(y_true, y_prob)
tempResult = unique(y_true)
	
===================================================================	
hinge_loss: 388	
----------------------------	

"Average hinge loss (non-regularized)\n\n    In binary class case, assuming labels in y_true are encoded with +1 and -1,\n    when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n    always negative (since the signs disagree), implying ``1 - margin`` is\n    always greater than 1.  The cumulated hinge loss is therefore an upper\n    bound of the number of mistakes made by the classifier.\n\n    In multiclass case, the function expects that either all the labels are\n    included in y_true or an optional labels argument is provided which\n    contains all the labels. The multilabel margin is calculated according\n    to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n    is an upper bound of the number of mistakes made by the classifier.\n\n    Read more in the :ref:`User Guide <hinge_loss>`.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        True target, consisting of integers of two values. The positive label\n        must be greater than the negative label.\n\n    pred_decision : array, shape = [n_samples] or [n_samples, n_classes]\n        Predicted decisions, as output by decision_function (floats).\n\n    labels : array, optional, default None\n        Contains all the labels for the problem. Used in multiclass hinge loss.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    loss : float\n\n    References\n    ----------\n    .. [1] `Wikipedia entry on the Hinge loss\n           <https://en.wikipedia.org/wiki/Hinge_loss>`_\n\n    .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n           Implementation of Multiclass Kernel-based Vector\n           Machines. Journal of Machine Learning Research 2,\n           (2001), 265-292\n\n    .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n           by Robert C. Moore, John DeNero.\n           <http://www.ttic.edu/sigml/symposium2011/papers/\n           Moore+DeNero_Regularization.pdf>`_\n\n    Examples\n    --------\n    >>> from sklearn import svm\n    >>> from sklearn.metrics import hinge_loss\n    >>> X = [[0], [1]]\n    >>> y = [-1, 1]\n    >>> est = svm.LinearSVC(random_state=0)\n    >>> est.fit(X, y)\n    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n         intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n         verbose=0)\n    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n    >>> pred_decision  # doctest: +ELLIPSIS\n    array([-2.18...,  2.36...,  0.09...])\n    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS\n    0.30...\n\n    In the multiclass case:\n\n    >>> X = np.array([[0], [1], [2], [3]])\n    >>> Y = np.array([0, 1, 2, 3])\n    >>> labels = np.array([0, 1, 2, 3])\n    >>> est = svm.LinearSVC()\n    >>> est.fit(X, Y)\n    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n         intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n         verbose=0)\n    >>> pred_decision = est.decision_function([[-1], [2], [3]])\n    >>> y_true = [0, 2, 3]\n    >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS\n    0.56...\n    "
check_consistent_length(y_true, pred_decision, sample_weight)
pred_decision = check_array(pred_decision, ensure_2d=False)
y_true = column_or_1d(y_true)
tempResult = unique(y_true)
	
===================================================================	
label_ranking_loss: 179	
----------------------------	

'Compute Ranking loss measure\n\n    Compute the average number of label pairs that are incorrectly ordered\n    given y_score weighted by the size of the label set and the number of\n    labels not in the label set.\n\n    This is similar to the error set size, but weighted by the number of\n    relevant and irrelevant labels. The best performance is achieved with\n    a ranking loss of zero.\n\n    Read more in the :ref:`User Guide <label_ranking_loss>`.\n\n    .. versionadded:: 0.17\n       A function *label_ranking_loss*\n\n    Parameters\n    ----------\n    y_true : array or sparse matrix, shape = [n_samples, n_labels]\n        True binary labels in binary indicator format.\n\n    y_score : array, shape = [n_samples, n_labels]\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by "decision_function" on some classifiers).\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    loss : float\n\n    References\n    ----------\n    .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n           Mining multi-label data. In Data mining and knowledge discovery\n           handbook (pp. 667-685). Springer US.\n\n    '
y_true = check_array(y_true, ensure_2d=False, accept_sparse='csr')
y_score = check_array(y_score, ensure_2d=False)
check_consistent_length(y_true, y_score, sample_weight)
y_type = type_of_target(y_true)
if (y_type not in ('multilabel-indicator',)):
    raise ValueError('{0} format is not supported'.format(y_type))
if (y_true.shape != y_score.shape):
    raise ValueError('y_true and y_score have different shape')
(n_samples, n_labels) = y_true.shape
y_true = csr_matrix(y_true)
loss = numpy.zeros(n_samples)
for (i, (start, stop)) in enumerate(zip(y_true.indptr, y_true.indptr[1:])):
    tempResult = unique(y_score[i], return_inverse=True)
	
===================================================================	
_binary_roc_auc_score: 54	
----------------------------	

tempResult = unique(y_true)
	
===================================================================	
_binary_clf_curve: 69	
----------------------------	

'Calculate true and false positives per binary classification threshold.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        True targets of binary classification\n\n    y_score : array, shape = [n_samples]\n        Estimated probabilities or decision function\n\n    pos_label : int, optional (default=None)\n        The label of the positive class\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    fps : array, shape = [n_thresholds]\n        A count of false positives, at index i being the number of negative\n        samples assigned a score >= thresholds[i]. The total number of\n        negative samples is equal to fps[-1] (thus true negatives are given by\n        fps[-1] - fps).\n\n    tps : array, shape = [n_thresholds <= len(np.unique(y_score))]\n        An increasing count of true positives, at index i being the number\n        of positive samples assigned a score >= thresholds[i]. The total\n        number of positive samples is equal to tps[-1] (thus false negatives\n        are given by tps[-1] - tps).\n\n    thresholds : array, shape = [n_thresholds]\n        Decreasing score values.\n    '
check_consistent_length(y_true, y_score)
y_true = column_or_1d(y_true)
y_score = column_or_1d(y_score)
assert_all_finite(y_true)
assert_all_finite(y_score)
if (sample_weight is not None):
    sample_weight = column_or_1d(sample_weight)
tempResult = unique(y_true)
	
===================================================================	
adjusted_rand_score: 49	
----------------------------	

'Rand index adjusted for chance.\n\n    The Rand Index computes a similarity measure between two clusterings\n    by considering all pairs of samples and counting pairs that are\n    assigned in the same or different clusters in the predicted and\n    true clusterings.\n\n    The raw RI score is then "adjusted for chance" into the ARI score\n    using the following scheme::\n\n        ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n\n    The adjusted Rand index is thus ensured to have a value close to\n    0.0 for random labeling independently of the number of clusters and\n    samples and exactly 1.0 when the clusterings are identical (up to\n    a permutation).\n\n    ARI is a symmetric measure::\n\n        adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n\n    Read more in the :ref:`User Guide <adjusted_rand_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        Ground truth class labels to be used as a reference\n\n    labels_pred : array, shape = [n_samples]\n        Cluster labels to evaluate\n\n    Returns\n    -------\n    ari : float\n       Similarity score between -1.0 and 1.0. Random labelings have an ARI\n       close to 0.0. 1.0 stands for perfect match.\n\n    Examples\n    --------\n\n    Perfectly maching labelings have a score of 1 even\n\n      >>> from sklearn.metrics.cluster import adjusted_rand_score\n      >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n      1.0\n      >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n      1.0\n\n    Labelings that assign all classes members to the same clusters\n    are complete be not always pure, hence penalized::\n\n      >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  # doctest: +ELLIPSIS\n      0.57...\n\n    ARI is symmetric, so labelings that have pure clusters with members\n    coming from the same classes but unnecessary splits are penalized::\n\n      >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])  # doctest: +ELLIPSIS\n      0.57...\n\n    If classes members are completely split across different clusters, the\n    assignment is totally incomplete, hence the ARI is very low::\n\n      >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n      0.0\n\n    References\n    ----------\n\n    .. [Hubert1985] `L. Hubert and P. Arabie, Comparing Partitions,\n      Journal of Classification 1985`\n      http://link.springer.com/article/10.1007%2FBF01908075\n\n    .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n\n    See also\n    --------\n    adjusted_mutual_info_score: Adjusted Mutual Information\n\n    '
(labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
n_samples = labels_true.shape[0]
tempResult = unique(labels_true)
	
===================================================================	
adjusted_rand_score: 50	
----------------------------	

'Rand index adjusted for chance.\n\n    The Rand Index computes a similarity measure between two clusterings\n    by considering all pairs of samples and counting pairs that are\n    assigned in the same or different clusters in the predicted and\n    true clusterings.\n\n    The raw RI score is then "adjusted for chance" into the ARI score\n    using the following scheme::\n\n        ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n\n    The adjusted Rand index is thus ensured to have a value close to\n    0.0 for random labeling independently of the number of clusters and\n    samples and exactly 1.0 when the clusterings are identical (up to\n    a permutation).\n\n    ARI is a symmetric measure::\n\n        adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n\n    Read more in the :ref:`User Guide <adjusted_rand_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        Ground truth class labels to be used as a reference\n\n    labels_pred : array, shape = [n_samples]\n        Cluster labels to evaluate\n\n    Returns\n    -------\n    ari : float\n       Similarity score between -1.0 and 1.0. Random labelings have an ARI\n       close to 0.0. 1.0 stands for perfect match.\n\n    Examples\n    --------\n\n    Perfectly maching labelings have a score of 1 even\n\n      >>> from sklearn.metrics.cluster import adjusted_rand_score\n      >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n      1.0\n      >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n      1.0\n\n    Labelings that assign all classes members to the same clusters\n    are complete be not always pure, hence penalized::\n\n      >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  # doctest: +ELLIPSIS\n      0.57...\n\n    ARI is symmetric, so labelings that have pure clusters with members\n    coming from the same classes but unnecessary splits are penalized::\n\n      >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])  # doctest: +ELLIPSIS\n      0.57...\n\n    If classes members are completely split across different clusters, the\n    assignment is totally incomplete, hence the ARI is very low::\n\n      >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n      0.0\n\n    References\n    ----------\n\n    .. [Hubert1985] `L. Hubert and P. Arabie, Comparing Partitions,\n      Journal of Classification 1985`\n      http://link.springer.com/article/10.1007%2FBF01908075\n\n    .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n\n    See also\n    --------\n    adjusted_mutual_info_score: Adjusted Mutual Information\n\n    '
(labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
n_samples = labels_true.shape[0]
n_classes = np.unique(labels_true).shape[0]
tempResult = unique(labels_pred)
	
===================================================================	
normalized_mutual_info_score: 133	
----------------------------	

"Normalized Mutual Information between two clusterings.\n\n    Normalized Mutual Information (NMI) is an normalization of the Mutual\n    Information (MI) score to scale the results between 0 (no mutual\n    information) and 1 (perfect correlation). In this function, mutual\n    information is normalized by ``sqrt(H(labels_true) * H(labels_pred))``\n\n    This measure is not adjusted for chance. Therefore\n    :func:`adjusted_mustual_info_score` might be preferred.\n\n    This metric is independent of the absolute values of the labels:\n    a permutation of the class or cluster label values won't change the\n    score value in any way.\n\n    This metric is furthermore symmetric: switching ``label_true`` with\n    ``label_pred`` will return the same score value. This can be useful to\n    measure the agreement of two independent label assignments strategies\n    on the same dataset when the real ground truth is not known.\n\n    Read more in the :ref:`User Guide <mutual_info_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    labels_pred : array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    Returns\n    -------\n    nmi: float\n       score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n\n    See also\n    --------\n    adjusted_rand_score: Adjusted Rand Index\n    adjusted_mutual_info_score: Adjusted Mutual Information (adjusted\n        against chance)\n\n    Examples\n    --------\n\n    Perfect labelings are both homogeneous and complete, hence have\n    score 1.0::\n\n      >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n      >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n      1.0\n      >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n      1.0\n\n    If classes members are completely split across different clusters,\n    the assignment is totally in-complete, hence the NMI is null::\n\n      >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n      0.0\n\n    "
(labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
tempResult = unique(labels_true)
	
===================================================================	
normalized_mutual_info_score: 134	
----------------------------	

"Normalized Mutual Information between two clusterings.\n\n    Normalized Mutual Information (NMI) is an normalization of the Mutual\n    Information (MI) score to scale the results between 0 (no mutual\n    information) and 1 (perfect correlation). In this function, mutual\n    information is normalized by ``sqrt(H(labels_true) * H(labels_pred))``\n\n    This measure is not adjusted for chance. Therefore\n    :func:`adjusted_mustual_info_score` might be preferred.\n\n    This metric is independent of the absolute values of the labels:\n    a permutation of the class or cluster label values won't change the\n    score value in any way.\n\n    This metric is furthermore symmetric: switching ``label_true`` with\n    ``label_pred`` will return the same score value. This can be useful to\n    measure the agreement of two independent label assignments strategies\n    on the same dataset when the real ground truth is not known.\n\n    Read more in the :ref:`User Guide <mutual_info_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    labels_pred : array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    Returns\n    -------\n    nmi: float\n       score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n\n    See also\n    --------\n    adjusted_rand_score: Adjusted Rand Index\n    adjusted_mutual_info_score: Adjusted Mutual Information (adjusted\n        against chance)\n\n    Examples\n    --------\n\n    Perfect labelings are both homogeneous and complete, hence have\n    score 1.0::\n\n      >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n      >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n      1.0\n      >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n      1.0\n\n    If classes members are completely split across different clusters,\n    the assignment is totally in-complete, hence the NMI is null::\n\n      >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n      0.0\n\n    "
(labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
classes = numpy.unique(labels_true)
tempResult = unique(labels_pred)
	
===================================================================	
adjusted_mutual_info_score: 118	
----------------------------	

"Adjusted Mutual Information between two clusterings.\n\n    Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n    Information (MI) score to account for chance. It accounts for the fact that\n    the MI is generally higher for two clusterings with a larger number of\n    clusters, regardless of whether there is actually more information shared.\n    For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n\n        AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]\n\n    This metric is independent of the absolute values of the labels:\n    a permutation of the class or cluster label values won't change the\n    score value in any way.\n\n    This metric is furthermore symmetric: switching ``label_true`` with\n    ``label_pred`` will return the same score value. This can be useful to\n    measure the agreement of two independent label assignments strategies\n    on the same dataset when the real ground truth is not known.\n\n    Be mindful that this function is an order of magnitude slower than other\n    metrics, such as the Adjusted Rand Index.\n\n    Read more in the :ref:`User Guide <mutual_info_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    labels_pred : array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    Returns\n    -------\n    ami: float(upperlimited by 1.0)\n       The AMI returns a value of 1 when the two partitions are identical\n       (ie perfectly matched). Random partitions (independent labellings) have\n       an expected AMI around 0 on average hence can be negative.\n\n    See also\n    --------\n    adjusted_rand_score: Adjusted Rand Index\n    mutual_information_score: Mutual Information (not adjusted for chance)\n\n    Examples\n    --------\n\n    Perfect labelings are both homogeneous and complete, hence have\n    score 1.0::\n\n      >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n      >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n      1.0\n      >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n      1.0\n\n    If classes members are completely split across different clusters,\n    the assignment is totally in-complete, hence the AMI is null::\n\n      >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n      0.0\n\n    References\n    ----------\n    .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n       Clusterings Comparison: Variants, Properties, Normalization and\n       Correction for Chance, JMLR\n       <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n\n    .. [2] `Wikipedia entry for the Adjusted Mutual Information\n       <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n\n    "
(labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
n_samples = labels_true.shape[0]
tempResult = unique(labels_true)
	
===================================================================	
adjusted_mutual_info_score: 119	
----------------------------	

"Adjusted Mutual Information between two clusterings.\n\n    Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n    Information (MI) score to account for chance. It accounts for the fact that\n    the MI is generally higher for two clusterings with a larger number of\n    clusters, regardless of whether there is actually more information shared.\n    For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n\n        AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]\n\n    This metric is independent of the absolute values of the labels:\n    a permutation of the class or cluster label values won't change the\n    score value in any way.\n\n    This metric is furthermore symmetric: switching ``label_true`` with\n    ``label_pred`` will return the same score value. This can be useful to\n    measure the agreement of two independent label assignments strategies\n    on the same dataset when the real ground truth is not known.\n\n    Be mindful that this function is an order of magnitude slower than other\n    metrics, such as the Adjusted Rand Index.\n\n    Read more in the :ref:`User Guide <mutual_info_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    labels_pred : array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    Returns\n    -------\n    ami: float(upperlimited by 1.0)\n       The AMI returns a value of 1 when the two partitions are identical\n       (ie perfectly matched). Random partitions (independent labellings) have\n       an expected AMI around 0 on average hence can be negative.\n\n    See also\n    --------\n    adjusted_rand_score: Adjusted Rand Index\n    mutual_information_score: Mutual Information (not adjusted for chance)\n\n    Examples\n    --------\n\n    Perfect labelings are both homogeneous and complete, hence have\n    score 1.0::\n\n      >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n      >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n      1.0\n      >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n      1.0\n\n    If classes members are completely split across different clusters,\n    the assignment is totally in-complete, hence the AMI is null::\n\n      >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n      0.0\n\n    References\n    ----------\n    .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n       Clusterings Comparison: Variants, Properties, Normalization and\n       Correction for Chance, JMLR\n       <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n\n    .. [2] `Wikipedia entry for the Adjusted Mutual Information\n       <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n\n    "
(labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
n_samples = labels_true.shape[0]
classes = numpy.unique(labels_true)
tempResult = unique(labels_pred)
	
===================================================================	
contingency_matrix: 31	
----------------------------	

'Build a contingency matrix describing the relationship between labels.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        Ground truth class labels to be used as a reference\n\n    labels_pred : array, shape = [n_samples]\n        Cluster labels to evaluate\n\n    eps : None or float, optional.\n        If a float, that value is added to all values in the contingency\n        matrix. This helps to stop NaN propagation.\n        If ``None``, nothing is adjusted.\n\n    sparse : boolean, optional.\n        If True, return a sparse CSR continency matrix. If ``eps is not None``,\n        and ``sparse is True``, will throw ValueError.\n\n        .. versionadded:: 0.18\n\n        .. versionadded:: 0.18\n\n    Returns\n    -------\n    contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]\n        Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\n        true class :math:`i` and in predicted class :math:`j`. If\n        ``eps is None``, the dtype of this array will be integer. If ``eps`` is\n        given, the dtype will be float.\n        Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.\n    '
if ((eps is not None) and sparse):
    raise ValueError("Cannot set 'eps' when sparse=True")
tempResult = unique(labels_true, return_inverse=True)
	
===================================================================	
contingency_matrix: 32	
----------------------------	

'Build a contingency matrix describing the relationship between labels.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        Ground truth class labels to be used as a reference\n\n    labels_pred : array, shape = [n_samples]\n        Cluster labels to evaluate\n\n    eps : None or float, optional.\n        If a float, that value is added to all values in the contingency\n        matrix. This helps to stop NaN propagation.\n        If ``None``, nothing is adjusted.\n\n    sparse : boolean, optional.\n        If True, return a sparse CSR continency matrix. If ``eps is not None``,\n        and ``sparse is True``, will throw ValueError.\n\n        .. versionadded:: 0.18\n\n        .. versionadded:: 0.18\n\n    Returns\n    -------\n    contingency : {array-like, sparse}, shape=[n_classes_true, n_classes_pred]\n        Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in\n        true class :math:`i` and in predicted class :math:`j`. If\n        ``eps is None``, the dtype of this array will be integer. If ``eps`` is\n        given, the dtype will be float.\n        Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``.\n    '
if ((eps is not None) and sparse):
    raise ValueError("Cannot set 'eps' when sparse=True")
(classes, class_idx) = numpy.unique(labels_true, return_inverse=True)
tempResult = unique(labels_pred, return_inverse=True)
	
===================================================================	
entropy: 158	
----------------------------	

'Calculates the entropy for a labeling.'
if (len(labels) == 0):
    return 1.0
tempResult = unique(labels, return_inverse=True)
	
===================================================================	
test_correct_labelsize: 57	
----------------------------	

dataset = sklearn.datasets.load_iris()
X = dataset.data
y = numpy.arange(X.shape[0])
tempResult = unique(y)
	
===================================================================	
test_correct_labelsize: 59	
----------------------------	

dataset = sklearn.datasets.load_iris()
X = dataset.data
y = numpy.arange(X.shape[0])
assert_raises_regexp(ValueError, ('Number of labels is %d\\. Valid values are 2 to n_samples - 1 \\(inclusive\\)' % len(numpy.unique(y))), silhouette_score, X, y)
y = numpy.zeros(X.shape[0])
tempResult = unique(y)
	
===================================================================	
test_no_averaging_labels: 442	
----------------------------	

y_true_multilabel = numpy.array([[1, 1, 0, 0], [1, 1, 0, 0]])
y_pred_multilabel = numpy.array([[0, 0, 1, 1], [0, 1, 1, 0]])
y_true_multiclass = numpy.array([0, 1, 2])
y_pred_multiclass = numpy.array([0, 2, 3])
labels = numpy.array([3, 0, 1, 2])
tempResult = unique(labels, return_inverse=True)
	
===================================================================	
_average_precision: 65	
----------------------------	

'Alternative implementation to check for correctness of\n    `average_precision_score`.'
tempResult = unique(y_true)
	
===================================================================	
_my_lrap: 520	
----------------------------	

'Simple implementation of label ranking average precision'
check_consistent_length(y_true, y_score)
y_true = check_array(y_true)
y_score = check_array(y_score)
(n_samples, n_labels) = y_true.shape
score = numpy.empty((n_samples,))
for i in range(n_samples):
    tempResult = unique(y_score[i], return_inverse=True)
	
===================================================================	
_auc: 56	
----------------------------	

'Alternative implementation to check for correctness of\n    `roc_auc_score`.'
tempResult = unique(y_true)
	
===================================================================	
test_class_weights: 22	
----------------------------	

(X, y) = make_blobs(random_state=1)
for Model in [DPGMM, VBGMM]:
    dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50)
    dpgmm.fit(X)
    tempResult = unique(dpgmm.predict(X))
	
===================================================================	
LeavePGroupsOut.get_n_splits: 259	
----------------------------	

'Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        '
if (groups is None):
    raise ValueError('The groups parameter should not be None')
tempResult = unique(groups)
	
===================================================================	
PredefinedSplit.__init__: 419	
----------------------------	

self.test_fold = numpy.array(test_fold, dtype=numpy.int)
self.test_fold = column_or_1d(self.test_fold)
tempResult = unique(self.test_fold)
	
===================================================================	
GroupKFold._iter_test_indices: 144	
----------------------------	

if (groups is None):
    raise ValueError('The groups parameter should not be None')
tempResult = unique(groups, return_inverse=True)
	
===================================================================	
_approximate_mode: 323	
----------------------------	

"Computes approximate mode of multivariate hypergeometric.\n\n    This is an approximation to the mode of the multivariate\n    hypergeometric given by class_counts and n_draws.\n    It shouldn't be off by more than one.\n\n    It is the mostly likely outcome of drawing n_draws many\n    samples from the population given by class_counts.\n\n    Parameters\n    ----------\n    class_counts : ndarray of int\n        Population per class.\n    n_draws : int\n        Number of draws (samples to draw) from the overall population.\n    rng : random state\n        Used to break ties.\n\n    Returns\n    -------\n    sampled_classes : ndarray of int\n        Number of samples drawn from each class.\n        np.sum(sampled_classes) == n_draws\n\n    Examples\n    --------\n    >>> from sklearn.model_selection._split import _approximate_mode\n    >>> _approximate_mode(class_counts=np.array([4, 2]), n_draws=3, rng=0)\n    array([2, 1])\n    >>> _approximate_mode(class_counts=np.array([5, 2]), n_draws=4, rng=0)\n    array([3, 1])\n    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n    ...                   n_draws=2, rng=0)\n    array([0, 1, 1, 0])\n    >>> _approximate_mode(class_counts=np.array([2, 2, 2, 1]),\n    ...                   n_draws=2, rng=42)\n    array([1, 1, 0, 0])\n    "
continuous = ((n_draws * class_counts) / class_counts.sum())
floored = numpy.floor(continuous)
need_to_add = int((n_draws - floored.sum()))
if (need_to_add > 0):
    remainder = (continuous - floored)
    tempResult = unique(remainder)
	
===================================================================	
StratifiedShuffleSplit._iter_indices: 343	
----------------------------	

n_samples = _num_samples(X)
(n_train, n_test) = _validate_shuffle_split(n_samples, self.test_size, self.train_size)
tempResult = unique(y, return_inverse=True)
	
===================================================================	
LeavePGroupsOut._iter_test_masks: 247	
----------------------------	

if (groups is None):
    raise ValueError('The groups parameter should not be None')
groups = numpy.array(groups, copy=True)
tempResult = unique(groups)
	
===================================================================	
LeaveOneGroupOut.get_n_splits: 235	
----------------------------	

'Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        '
if (groups is None):
    raise ValueError('The groups parameter should not be None')
tempResult = unique(groups)
	
===================================================================	
LeaveOneGroupOut._iter_test_masks: 227	
----------------------------	

if (groups is None):
    raise ValueError('The groups parameter should not be None')
groups = numpy.array(groups, copy=True)
tempResult = unique(groups)
	
===================================================================	
GroupShuffleSplit._iter_indices: 310	
----------------------------	

if (groups is None):
    raise ValueError('The groups parameter should not be None')
tempResult = unique(groups, return_inverse=True)
	
===================================================================	
StratifiedKFold._make_test_folds: 174	
----------------------------	

if self.shuffle:
    rng = check_random_state(self.random_state)
else:
    rng = self.random_state
y = numpy.asarray(y)
n_samples = y.shape[0]
tempResult = unique(y, return_inverse=True)
	
===================================================================	
_translate_train_sizes: 220	
----------------------------	

"Determine absolute sizes of training subsets and validate 'train_sizes'.\n\n    Examples:\n        _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]\n        _translate_train_sizes([5, 10], 10) -> [5, 10]\n\n    Parameters\n    ----------\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Numbers of training examples that will be used to generate the\n        learning curve. If the dtype is float, it is regarded as a\n        fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].\n\n    n_max_training_samples : int\n        Maximum number of training samples (upper bound of 'train_sizes').\n\n    Returns\n    -------\n    train_sizes_abs : array, shape (n_unique_ticks,), dtype int\n        Numbers of training examples that will be used to generate the\n        learning curve. Note that the number of ticks might be less\n        than n_ticks because duplicate entries will be removed.\n    "
train_sizes_abs = numpy.asarray(train_sizes)
n_ticks = train_sizes_abs.shape[0]
n_min_required_samples = numpy.min(train_sizes_abs)
n_max_required_samples = numpy.max(train_sizes_abs)
if numpy.issubdtype(train_sizes_abs.dtype, numpy.float):
    if ((n_min_required_samples <= 0.0) or (n_max_required_samples > 1.0)):
        raise ValueError(('train_sizes has been interpreted as fractions of the maximum number of training samples and must be within (0, 1], but is within [%f, %f].' % (n_min_required_samples, n_max_required_samples)))
    train_sizes_abs = astype((train_sizes_abs * n_max_training_samples), dtype=numpy.int, copy=False)
    train_sizes_abs = numpy.clip(train_sizes_abs, 1, n_max_training_samples)
elif ((n_min_required_samples <= 0) or (n_max_required_samples > n_max_training_samples)):
    raise ValueError(('train_sizes has been interpreted as absolute numbers of training samples and must be within (0, %d], but is within [%d, %d].' % (n_max_training_samples, n_min_required_samples, n_max_required_samples)))
tempResult = unique(train_sizes_abs)
	
===================================================================	
_shuffle: 176	
----------------------------	

'Return a shuffled copy of y eventually shuffle among same groups.'
if (groups is None):
    indices = random_state.permutation(len(y))
else:
    indices = numpy.arange(len(groups))
    tempResult = unique(groups)
	
===================================================================	
learning_curve: 197	
----------------------------	

'Learning curve.\n\n    Determines cross-validated training and test scores for different training\n    set sizes.\n\n    A cross-validation generator splits the whole dataset k times in training\n    and test data. Subsets of the training set with varying sizes will be used\n    to train the estimator and a score for each training subset size and the\n    test set will be computed. Afterwards, the scores will be averaged over\n    all k runs for each training subset size.\n\n    Read more in the :ref:`User Guide <learning_curve>`.\n\n    Parameters\n    ----------\n    estimator : object type that implements the "fit" and "predict" methods\n        An object of that type which is cloned for each validation.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross validation,\n          - integer, to specify the number of folds in a `(Stratified)KFold`,\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    scoring : string, callable or None, optional, default: None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    exploit_incremental_learning : boolean, optional, default: False\n        If the estimator supports incremental learning, this will be\n        used to speed up fitting for different training set sizes.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n\n    pre_dispatch : integer or string, optional\n        Number of predispatched jobs for parallel execution (default is\n        all). The option can reduce the allocated memory. The string can\n        be an expression like \'2*n_jobs\'.\n\n    verbose : integer, optional\n        Controls the verbosity: the higher, the more messages.\n\n    Returns\n    -------\n    train_sizes_abs : array, shape = (n_unique_ticks,), dtype int\n        Numbers of training examples that has been used to generate the\n        learning curve. Note that the number of ticks might be less\n        than n_ticks because duplicate entries will be removed.\n\n    train_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on training sets.\n\n    test_scores : array, shape (n_ticks, n_cv_folds)\n        Scores on test set.\n\n    Notes\n    -----\n    See :ref:`examples/model_selection/plot_learning_curve.py\n    <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n    '
if (exploit_incremental_learning and (not hasattr(estimator, 'partial_fit'))):
    raise ValueError('An estimator must support the partial_fit interface to exploit incremental learning')
(X, y, groups) = indexable(X, y, groups)
cv = check_cv(cv, y, classifier=is_classifier(estimator))
cv_iter = cv.split(X, y, groups)
cv_iter = list(cv_iter)
scorer = check_scoring(estimator, scoring=scoring)
n_max_training_samples = len(cv_iter[0][0])
train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)
n_unique_ticks = train_sizes_abs.shape[0]
if (verbose > 0):
    print(('[learning_curve] Training set sizes: ' + str(train_sizes_abs)))
parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
if exploit_incremental_learning:
    tempResult = unique(y)
	
===================================================================	
test_stratified_shuffle_split_iter: 364	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50))), numpy.concatenate([([i] * (100 + i)) for i in range(11)])]
for y in ys:
    sss = StratifiedShuffleSplit(6, test_size=0.33, random_state=0).split(numpy.ones(len(y)), y)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        tempResult = unique(y[train])
	
===================================================================	
test_stratified_shuffle_split_iter: 364	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50))), numpy.concatenate([([i] * (100 + i)) for i in range(11)])]
for y in ys:
    sss = StratifiedShuffleSplit(6, test_size=0.33, random_state=0).split(numpy.ones(len(y)), y)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        tempResult = unique(y[test])
	
===================================================================	
test_stratified_shuffle_split_iter: 365	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50))), numpy.concatenate([([i] * (100 + i)) for i in range(11)])]
for y in ys:
    sss = StratifiedShuffleSplit(6, test_size=0.33, random_state=0).split(numpy.ones(len(y)), y)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        assert_array_equal(numpy.unique(y[train]), numpy.unique(y[test]))
        tempResult = unique(y[train], return_inverse=True)
	
===================================================================	
test_stratified_shuffle_split_iter: 366	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50))), numpy.concatenate([([i] * (100 + i)) for i in range(11)])]
for y in ys:
    sss = StratifiedShuffleSplit(6, test_size=0.33, random_state=0).split(numpy.ones(len(y)), y)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        assert_array_equal(numpy.unique(y[train]), numpy.unique(y[test]))
        p_train = (numpy.bincount(numpy.unique(y[train], return_inverse=True)[1]) / float(len(y[train])))
        tempResult = unique(y[test], return_inverse=True)
	
===================================================================	
test_group_shuffle_split: 442	
----------------------------	

groups = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]
for l in groups:
    X = y = numpy.ones(len(l))
    n_splits = 6
    test_size = (1.0 / 3)
    slo = GroupShuffleSplit(n_splits, test_size=test_size, random_state=0)
    repr(slo)
    assert_equal(slo.get_n_splits(X, y, groups=l), n_splits)
    tempResult = unique(l)
	
===================================================================	
test_group_shuffle_split: 444	
----------------------------	

groups = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]
for l in groups:
    X = y = numpy.ones(len(l))
    n_splits = 6
    test_size = (1.0 / 3)
    slo = GroupShuffleSplit(n_splits, test_size=test_size, random_state=0)
    repr(slo)
    assert_equal(slo.get_n_splits(X, y, groups=l), n_splits)
    l_unique = numpy.unique(l)
    for (train, test) in slo.split(X, y, groups=l):
        tempResult = unique(l[train])
	
===================================================================	
test_group_shuffle_split: 445	
----------------------------	

groups = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]
for l in groups:
    X = y = numpy.ones(len(l))
    n_splits = 6
    test_size = (1.0 / 3)
    slo = GroupShuffleSplit(n_splits, test_size=test_size, random_state=0)
    repr(slo)
    assert_equal(slo.get_n_splits(X, y, groups=l), n_splits)
    l_unique = numpy.unique(l)
    for (train, test) in slo.split(X, y, groups=l):
        l_train_unique = numpy.unique(l[train])
        tempResult = unique(l[test])
	
===================================================================	
MockClassifier.fit: 78	
----------------------------	

'The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        '
self.dummy_int = dummy_int
self.dummy_str = dummy_str
self.dummy_obj = dummy_obj
if (callback is not None):
    callback(self)
if self.allow_nd:
    X = X.reshape(len(X), (- 1))
if ((X.ndim >= 3) and (not self.allow_nd)):
    raise ValueError('X cannot be d')
if (sample_weight is not None):
    assert_true((sample_weight.shape[0] == X.shape[0]), 'MockClassifier extra fit_param sample_weight.shape[0] is {0}, should be {1}'.format(sample_weight.shape[0], X.shape[0]))
if (class_prior is not None):
    tempResult = unique(y)
	
===================================================================	
MockClassifier.fit: 78	
----------------------------	

'The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        '
self.dummy_int = dummy_int
self.dummy_str = dummy_str
self.dummy_obj = dummy_obj
if (callback is not None):
    callback(self)
if self.allow_nd:
    X = X.reshape(len(X), (- 1))
if ((X.ndim >= 3) and (not self.allow_nd)):
    raise ValueError('X cannot be d')
if (sample_weight is not None):
    assert_true((sample_weight.shape[0] == X.shape[0]), 'MockClassifier extra fit_param sample_weight.shape[0] is {0}, should be {1}'.format(sample_weight.shape[0], X.shape[0]))
if (class_prior is not None):
    tempResult = unique(y)
	
===================================================================	
test_predefinedsplit_with_kfold_split: 426	
----------------------------	

folds = ((- 1) * numpy.ones(10))
kf_train = []
kf_test = []
for (i, (train_ind, test_ind)) in enumerate(KFold(5, shuffle=True).split(X)):
    kf_train.append(train_ind)
    kf_test.append(test_ind)
    folds[test_ind] = i
ps_train = []
ps_test = []
ps = PredefinedSplit(folds)
tempResult = unique(folds)
	
===================================================================	
test_stratified_shuffle_split_even: 399	
----------------------------	

n_folds = 5
n_splits = 1000

def assert_counts_are_ok(idx_counts, p):
    threshold = (0.05 / n_splits)
    bf = scipy.stats.binom(n_splits, p)
    for count in idx_counts:
        prob = bf.pmf(count)
        assert_true((prob > threshold), 'An index is not drawn with chance corresponding to even draws')
for n_samples in (6, 22):
    groups = numpy.array(((n_samples // 2) * [0, 1]))
    splits = StratifiedShuffleSplit(n_splits=n_splits, test_size=(1.0 / n_folds), random_state=0)
    train_counts = ([0] * n_samples)
    test_counts = ([0] * n_samples)
    n_splits_actual = 0
    for (train, test) in splits.split(X=numpy.ones(n_samples), y=groups):
        n_splits_actual += 1
        for (counter, ids) in [(train_counts, train), (test_counts, test)]:
            for id in ids:
                counter[id] += 1
    assert_equal(n_splits_actual, n_splits)
    (n_train, n_test) = _validate_shuffle_split(n_samples, test_size=(1.0 / n_folds), train_size=(1.0 - (1.0 / n_folds)))
    assert_equal(len(train), n_train)
    assert_equal(len(test), n_test)
    assert_equal(len(set(train).intersection(test)), 0)
    tempResult = unique(groups)
	
===================================================================	
test_group_kfold: 600	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
tempResult = unique(groups)
	
===================================================================	
test_group_kfold: 606	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
len(numpy.unique(groups))
folds = numpy.zeros(n_samples)
lkf = GroupKFold(n_splits=n_splits)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
tempResult = unique(folds)
	
===================================================================	
test_group_kfold: 608	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
len(numpy.unique(groups))
folds = numpy.zeros(n_samples)
lkf = GroupKFold(n_splits=n_splits)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
tempResult = unique(groups)
	
===================================================================	
test_group_kfold: 609	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
len(numpy.unique(groups))
folds = numpy.zeros(n_samples)
lkf = GroupKFold(n_splits=n_splits)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
for group in numpy.unique(groups):
    tempResult = unique(folds[(groups == group)])
	
===================================================================	
test_group_kfold: 614	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
len(numpy.unique(groups))
folds = numpy.zeros(n_samples)
lkf = GroupKFold(n_splits=n_splits)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
for group in numpy.unique(groups):
    assert_equal(len(numpy.unique(folds[(groups == group)])), 1)
groups = numpy.asarray(groups, dtype=object)
for (train, test) in lkf.split(X, y, groups):
    assert_equal(len(numpy.intersect1d(groups[train], groups[test])), 0)
groups = numpy.array(['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia'])
tempResult = unique(groups)
	
===================================================================	
test_group_kfold: 624	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
len(numpy.unique(groups))
folds = numpy.zeros(n_samples)
lkf = GroupKFold(n_splits=n_splits)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
for group in numpy.unique(groups):
    assert_equal(len(numpy.unique(folds[(groups == group)])), 1)
groups = numpy.asarray(groups, dtype=object)
for (train, test) in lkf.split(X, y, groups):
    assert_equal(len(numpy.intersect1d(groups[train], groups[test])), 0)
groups = numpy.array(['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia'])
n_groups = len(numpy.unique(groups))
n_samples = len(groups)
n_splits = 5
tolerance = (0.05 * n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
X = y = numpy.ones(n_samples)
folds = numpy.zeros(n_samples)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
tempResult = unique(folds)
	
===================================================================	
test_group_kfold: 628	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
len(numpy.unique(groups))
folds = numpy.zeros(n_samples)
lkf = GroupKFold(n_splits=n_splits)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
for group in numpy.unique(groups):
    assert_equal(len(numpy.unique(folds[(groups == group)])), 1)
groups = numpy.asarray(groups, dtype=object)
for (train, test) in lkf.split(X, y, groups):
    assert_equal(len(numpy.intersect1d(groups[train], groups[test])), 0)
groups = numpy.array(['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia'])
n_groups = len(numpy.unique(groups))
n_samples = len(groups)
n_splits = 5
tolerance = (0.05 * n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
X = y = numpy.ones(n_samples)
folds = numpy.zeros(n_samples)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
with warnings.catch_warnings():
    warnings.simplefilter('ignore', DeprecationWarning)
    tempResult = unique(groups)
	
===================================================================	
test_group_kfold: 629	
----------------------------	

rng = numpy.random.RandomState(0)
n_groups = 15
n_samples = 1000
n_splits = 5
X = y = numpy.ones(n_samples)
tolerance = (0.05 * n_samples)
groups = rng.randint(0, n_groups, n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
len(numpy.unique(groups))
folds = numpy.zeros(n_samples)
lkf = GroupKFold(n_splits=n_splits)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
for group in numpy.unique(groups):
    assert_equal(len(numpy.unique(folds[(groups == group)])), 1)
groups = numpy.asarray(groups, dtype=object)
for (train, test) in lkf.split(X, y, groups):
    assert_equal(len(numpy.intersect1d(groups[train], groups[test])), 0)
groups = numpy.array(['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia'])
n_groups = len(numpy.unique(groups))
n_samples = len(groups)
n_splits = 5
tolerance = (0.05 * n_samples)
ideal_n_groups_per_fold = (n_samples // n_splits)
X = y = numpy.ones(n_samples)
folds = numpy.zeros(n_samples)
for (i, (_, test)) in enumerate(lkf.split(X, y, groups)):
    folds[test] = i
assert_equal(len(folds), len(groups))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_groups_per_fold)))
with warnings.catch_warnings():
    warnings.simplefilter('ignore', DeprecationWarning)
    for group in numpy.unique(groups):
        tempResult = unique(folds[(groups == group)])
	
===================================================================	
test_cross_val_score_fit_params: 205	
----------------------------	

clf = MockClassifier()
n_samples = X.shape[0]
tempResult = unique(y)
	
===================================================================	
SupervisedIntegerMixin.fit: 366	
----------------------------	

"Fit the model using X as training data and y as target values\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, BallTree, KDTree}\n            Training data. If array or matrix, shape [n_samples, n_features],\n            or [n_samples, n_samples] if metric='precomputed'.\n\n        y : {array-like, sparse matrix}\n            Target values of shape = [n_samples] or [n_samples, n_outputs]\n\n        "
if (not isinstance(X, (KDTree, BallTree))):
    (X, y) = check_X_y(X, y, 'csr', multi_output=True)
if ((y.ndim == 1) or ((y.ndim == 2) and (y.shape[1] == 1))):
    if (y.ndim != 1):
        warnings.warn('A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().', DataConversionWarning, stacklevel=2)
    self.outputs_2d_ = False
    y = y.reshape(((- 1), 1))
else:
    self.outputs_2d_ = True
check_classification_targets(y)
self.classes_ = []
self._y = numpy.empty(y.shape, dtype=numpy.int)
for k in range(self._y.shape[1]):
    tempResult = unique(y[:, k], return_inverse=True)
	
===================================================================	
test_predict_proba_multiclass: 267	
----------------------------	

X = X_digits_multi[:10]
y = y_digits_multi[:10]
clf = MLPClassifier(hidden_layer_sizes=5)
with ignore_warnings(category=ConvergenceWarning):
    clf.fit(X, y)
y_proba = clf.predict_proba(X)
y_log_proba = clf.predict_log_proba(X)
tempResult = unique(y)
	
===================================================================	
test_partial_fit_classification: 186	
----------------------------	

for (X, y) in classification_datasets:
    X = X
    y = y
    mlp = MLPClassifier(solver='sgd', max_iter=100, random_state=1, tol=0, alpha=1e-05, learning_rate_init=0.2)
    with ignore_warnings(category=ConvergenceWarning):
        mlp.fit(X, y)
    pred1 = mlp.predict(X)
    mlp = MLPClassifier(solver='sgd', random_state=1, alpha=1e-05, learning_rate_init=0.2)
    for i in range(100):
        tempResult = unique(y)
	
===================================================================	
MultiLabelBinarizer.fit_transform: 266	
----------------------------	

'Fit the label sets binarizer and transform the given label sets\n\n        Parameters\n        ----------\n        y : iterable of iterables\n            A set of labels (any orderable and hashable object) for each\n            sample. If the `classes` parameter is set, `y` will not be\n            iterated.\n\n        Returns\n        -------\n        y_indicator : array or CSR matrix, shape (n_samples, n_classes)\n            A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n            `y[i]`, and 0 otherwise.\n        '
if (self.classes is not None):
    return self.fit(y).transform(y)
class_mapping = defaultdict(int)
class_mapping.default_factory = class_mapping.__len__
yt = self._transform(y, class_mapping)
tmp = sorted(class_mapping, key=class_mapping.get)
dtype = (numpy.int if all((isinstance(c, int) for c in tmp)) else object)
class_mapping = numpy.empty(len(tmp), dtype=dtype)
class_mapping[:] = tmp
tempResult = unique(class_mapping, return_inverse=True)
	
===================================================================	
LabelEncoder.fit_transform: 42	
----------------------------	

'Fit label encoder and return encoded labels\n\n        Parameters\n        ----------\n        y : array-like of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : array-like of shape [n_samples]\n        '
y = column_or_1d(y, warn=True)
_check_numpy_unicode_bug(y)
tempResult = unique(y, return_inverse=True)
	
===================================================================	
LabelEncoder.fit: 35	
----------------------------	

'Fit label encoder\n\n        Parameters\n        ----------\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
y = column_or_1d(y, warn=True)
_check_numpy_unicode_bug(y)
tempResult = unique(y)
	
===================================================================	
LabelEncoder.transform: 49	
----------------------------	

'Transform labels to normalized encoding.\n\n        Parameters\n        ----------\n        y : array-like of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : array-like of shape [n_samples]\n        '
check_is_fitted(self, 'classes_')
y = column_or_1d(y, warn=True)
tempResult = unique(y)
	
===================================================================	
BaseLabelPropagation.fit: 80	
----------------------------	

'Fit a semi-supervised label propagation model based\n\n        All the input data is provided matrix X (labeled and unlabeled)\n        and corresponding label matrix y with a dedicated marker value for\n        unlabeled samples.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            A {n_samples by n_samples} size matrix will be created from this\n\n        y : array_like, shape = [n_samples]\n            n_labeled_samples (unlabeled points are marked as -1)\n            All unlabeled samples will be transductively assigned labels\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
(X, y) = check_X_y(X, y)
self.X_ = X
check_classification_targets(y)
graph_matrix = self._build_graph()
tempResult = unique(y)
	
===================================================================	
BaseSVC._validate_targets: 260	
----------------------------	

y_ = column_or_1d(y, warn=True)
check_classification_targets(y)
tempResult = unique(y_, return_inverse=True)
	
===================================================================	
LinearSVC.fit: 40	
----------------------------	

'Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples]\n            Target vector relative to X\n\n        sample_weight : array-like, shape = [n_samples], optional\n            Array of weights that are assigned to individual\n            samples. If not provided,\n            then each sample is given unit weight.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        '
msg = "loss='%s' has been deprecated in favor of loss='%s' as of 0.16. Backward compatibility for the loss='%s' will be removed in %s"
if (self.loss in ('l1', 'l2')):
    old_loss = self.loss
    self.loss = {'l1': 'hinge', 'l2': 'squared_hinge'}.get(self.loss)
    warnings.warn((msg % (old_loss, self.loss, old_loss, '1.0')), DeprecationWarning)
if (self.C < 0):
    raise ValueError(('Penalty term must be positive; got (C=%r)' % self.C))
(X, y) = check_X_y(X, y, accept_sparse='csr', dtype=numpy.float64, order='C')
check_classification_targets(y)
tempResult = unique(y)
	
===================================================================	
test_auto_weight: 277	
----------------------------	

from sklearn.linear_model import LogisticRegression
from sklearn.utils import compute_class_weight
(X, y) = (iris.data[:, :2], (iris.target + 1))
unbalanced = numpy.delete(numpy.arange(y.size), numpy.where((y > 2))[0][::2])
tempResult = unique(y[unbalanced])
	
===================================================================	
test_label_kfold: 250	
----------------------------	

rng = numpy.random.RandomState(0)
n_labels = 15
n_samples = 1000
n_folds = 5
tolerance = (0.05 * n_samples)
labels = rng.randint(0, n_labels, n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
tempResult = unique(folds)
	
===================================================================	
test_label_kfold: 252	
----------------------------	

rng = numpy.random.RandomState(0)
n_labels = 15
n_samples = 1000
n_folds = 5
tolerance = (0.05 * n_samples)
labels = rng.randint(0, n_labels, n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
tempResult = unique(labels)
	
===================================================================	
test_label_kfold: 253	
----------------------------	

rng = numpy.random.RandomState(0)
n_labels = 15
n_samples = 1000
n_folds = 5
tolerance = (0.05 * n_samples)
labels = rng.randint(0, n_labels, n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
for label in numpy.unique(labels):
    tempResult = unique(folds[(labels == label)])
	
===================================================================	
test_label_kfold: 259	
----------------------------	

rng = numpy.random.RandomState(0)
n_labels = 15
n_samples = 1000
n_folds = 5
tolerance = (0.05 * n_samples)
labels = rng.randint(0, n_labels, n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
for label in numpy.unique(labels):
    assert_equal(len(numpy.unique(folds[(labels == label)])), 1)
labels = numpy.asarray(labels, dtype=object)
for (train, test) in sklearn.cross_validation.LabelKFold(labels, n_folds=n_folds):
    assert_equal(len(numpy.intersect1d(labels[train], labels[test])), 0)
labels = ['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia']
labels = numpy.asarray(labels, dtype=object)
tempResult = unique(labels)
	
===================================================================	
test_label_kfold: 266	
----------------------------	

rng = numpy.random.RandomState(0)
n_labels = 15
n_samples = 1000
n_folds = 5
tolerance = (0.05 * n_samples)
labels = rng.randint(0, n_labels, n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
for label in numpy.unique(labels):
    assert_equal(len(numpy.unique(folds[(labels == label)])), 1)
labels = numpy.asarray(labels, dtype=object)
for (train, test) in sklearn.cross_validation.LabelKFold(labels, n_folds=n_folds):
    assert_equal(len(numpy.intersect1d(labels[train], labels[test])), 0)
labels = ['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia']
labels = numpy.asarray(labels, dtype=object)
n_labels = len(numpy.unique(labels))
n_samples = len(labels)
n_folds = 5
tolerance = (0.05 * n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
tempResult = unique(folds)
	
===================================================================	
test_label_kfold: 268	
----------------------------	

rng = numpy.random.RandomState(0)
n_labels = 15
n_samples = 1000
n_folds = 5
tolerance = (0.05 * n_samples)
labels = rng.randint(0, n_labels, n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
for label in numpy.unique(labels):
    assert_equal(len(numpy.unique(folds[(labels == label)])), 1)
labels = numpy.asarray(labels, dtype=object)
for (train, test) in sklearn.cross_validation.LabelKFold(labels, n_folds=n_folds):
    assert_equal(len(numpy.intersect1d(labels[train], labels[test])), 0)
labels = ['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia']
labels = numpy.asarray(labels, dtype=object)
n_labels = len(numpy.unique(labels))
n_samples = len(labels)
n_folds = 5
tolerance = (0.05 * n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
tempResult = unique(labels)
	
===================================================================	
test_label_kfold: 269	
----------------------------	

rng = numpy.random.RandomState(0)
n_labels = 15
n_samples = 1000
n_folds = 5
tolerance = (0.05 * n_samples)
labels = rng.randint(0, n_labels, n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
for label in numpy.unique(labels):
    assert_equal(len(numpy.unique(folds[(labels == label)])), 1)
labels = numpy.asarray(labels, dtype=object)
for (train, test) in sklearn.cross_validation.LabelKFold(labels, n_folds=n_folds):
    assert_equal(len(numpy.intersect1d(labels[train], labels[test])), 0)
labels = ['Albert', 'Jean', 'Bertrand', 'Michel', 'Jean', 'Francis', 'Robert', 'Michel', 'Rachel', 'Lois', 'Michelle', 'Bernard', 'Marion', 'Laura', 'Jean', 'Rachel', 'Franck', 'John', 'Gael', 'Anna', 'Alix', 'Robert', 'Marion', 'David', 'Tony', 'Abel', 'Becky', 'Madmood', 'Cary', 'Mary', 'Alexandre', 'David', 'Francis', 'Barack', 'Abdoul', 'Rasha', 'Xi', 'Silvia']
labels = numpy.asarray(labels, dtype=object)
n_labels = len(numpy.unique(labels))
n_samples = len(labels)
n_folds = 5
tolerance = (0.05 * n_samples)
folds = cval.LabelKFold(labels, n_folds=n_folds).idxs
ideal_n_labels_per_fold = (n_samples // n_folds)
assert_equal(len(folds), len(labels))
for i in numpy.unique(folds):
    assert_greater_equal(tolerance, abs((sum((folds == i)) - ideal_n_labels_per_fold)))
for label in numpy.unique(labels):
    tempResult = unique(folds[(labels == label)])
	
===================================================================	
test_stratified_shuffle_split_even: 342	
----------------------------	

n_folds = 5
n_iter = 1000

def assert_counts_are_ok(idx_counts, p):
    threshold = (0.05 / n_splits)
    bf = scipy.stats.binom(n_splits, p)
    for count in idx_counts:
        p = bf.pmf(count)
        assert_true((p > threshold), 'An index is not drawn with chance corresponding to even draws')
for n_samples in (6, 22):
    labels = numpy.array(((n_samples // 2) * [0, 1]))
    splits = sklearn.cross_validation.StratifiedShuffleSplit(labels, n_iter=n_iter, test_size=(1.0 / n_folds), random_state=0)
    train_counts = ([0] * n_samples)
    test_counts = ([0] * n_samples)
    n_splits = 0
    for (train, test) in splits:
        n_splits += 1
        for (counter, ids) in [(train_counts, train), (test_counts, test)]:
            for id in ids:
                counter[id] += 1
    assert_equal(n_splits, n_iter)
    assert_equal(len(train), splits.n_train)
    assert_equal(len(test), splits.n_test)
    assert_equal(len(set(train).intersection(test)), 0)
    tempResult = unique(labels)
	
===================================================================	
MockClassifier.fit: 67	
----------------------------	

'The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        '
self.dummy_int = dummy_int
self.dummy_str = dummy_str
self.dummy_obj = dummy_obj
if (callback is not None):
    callback(self)
if self.allow_nd:
    X = X.reshape(len(X), (- 1))
if ((X.ndim >= 3) and (not self.allow_nd)):
    raise ValueError('X cannot be d')
if (sample_weight is not None):
    assert_true((sample_weight.shape[0] == X.shape[0]), 'MockClassifier extra fit_param sample_weight.shape[0] is {0}, should be {1}'.format(sample_weight.shape[0], X.shape[0]))
if (class_prior is not None):
    tempResult = unique(y)
	
===================================================================	
MockClassifier.fit: 67	
----------------------------	

'The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        '
self.dummy_int = dummy_int
self.dummy_str = dummy_str
self.dummy_obj = dummy_obj
if (callback is not None):
    callback(self)
if self.allow_nd:
    X = X.reshape(len(X), (- 1))
if ((X.ndim >= 3) and (not self.allow_nd)):
    raise ValueError('X cannot be d')
if (sample_weight is not None):
    assert_true((sample_weight.shape[0] == X.shape[0]), 'MockClassifier extra fit_param sample_weight.shape[0] is {0}, should be {1}'.format(sample_weight.shape[0], X.shape[0]))
if (class_prior is not None):
    tempResult = unique(y)
	
===================================================================	
test_stratified_shuffle_split_iter: 308	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50)))]
for y in ys:
    sss = sklearn.cross_validation.StratifiedShuffleSplit(y, 6, test_size=0.33, random_state=0)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        tempResult = unique(y[train])
	
===================================================================	
test_stratified_shuffle_split_iter: 308	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50)))]
for y in ys:
    sss = sklearn.cross_validation.StratifiedShuffleSplit(y, 6, test_size=0.33, random_state=0)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        tempResult = unique(y[test])
	
===================================================================	
test_stratified_shuffle_split_iter: 309	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50)))]
for y in ys:
    sss = sklearn.cross_validation.StratifiedShuffleSplit(y, 6, test_size=0.33, random_state=0)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        assert_array_equal(numpy.unique(y[train]), numpy.unique(y[test]))
        tempResult = unique(y[train], return_inverse=True)
	
===================================================================	
test_stratified_shuffle_split_iter: 310	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array(([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2] * 2)), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]), numpy.array((([(- 1)] * 800) + ([1] * 50)))]
for y in ys:
    sss = sklearn.cross_validation.StratifiedShuffleSplit(y, 6, test_size=0.33, random_state=0)
    test_size = numpy.ceil((0.33 * len(y)))
    train_size = (len(y) - test_size)
    for (train, test) in sss:
        assert_array_equal(numpy.unique(y[train]), numpy.unique(y[test]))
        p_train = (numpy.bincount(numpy.unique(y[train], return_inverse=True)[1]) / float(len(y[train])))
        tempResult = unique(y[test], return_inverse=True)
	
===================================================================	
test_label_shuffle_split: 382	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]
for y in ys:
    n_iter = 6
    test_size = (1.0 / 3)
    slo = sklearn.cross_validation.LabelShuffleSplit(y, n_iter, test_size=test_size, random_state=0)
    repr(slo)
    assert_equal(len(slo), n_iter)
    tempResult = unique(y)
	
===================================================================	
test_label_shuffle_split: 384	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]
for y in ys:
    n_iter = 6
    test_size = (1.0 / 3)
    slo = sklearn.cross_validation.LabelShuffleSplit(y, n_iter, test_size=test_size, random_state=0)
    repr(slo)
    assert_equal(len(slo), n_iter)
    y_unique = numpy.unique(y)
    for (train, test) in slo:
        tempResult = unique(y[train])
	
===================================================================	
test_label_shuffle_split: 385	
----------------------------	

ys = [numpy.array([1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]), numpy.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]), numpy.array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2]), numpy.array([1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4])]
for y in ys:
    n_iter = 6
    test_size = (1.0 / 3)
    slo = sklearn.cross_validation.LabelShuffleSplit(y, n_iter, test_size=test_size, random_state=0)
    repr(slo)
    assert_equal(len(slo), n_iter)
    y_unique = numpy.unique(y)
    for (train, test) in slo:
        y_train_unique = numpy.unique(y[train])
        tempResult = unique(y[test])
	
===================================================================	
test_cross_val_score_fit_params: 477	
----------------------------	

clf = MockClassifier()
n_samples = X.shape[0]
tempResult = unique(y)
	
===================================================================	
_check_predict_proba: 31	
----------------------------	

proba = clf.predict_proba(X)
log_proba = clf.predict_log_proba(X)
y = numpy.atleast_1d(y)
if (y.ndim == 1):
    y = numpy.reshape(y, ((- 1), 1))
n_outputs = y.shape[1]
n_samples = len(X)
if (n_outputs == 1):
    proba = [proba]
    log_proba = [log_proba]
for k in range(n_outputs):
    assert_equal(proba[k].shape[0], n_samples)
    tempResult = unique(y[:, k])
	
===================================================================	
test_ovo_decision_function: 335	
----------------------------	

n_samples = iris.data.shape[0]
ovo_clf = OneVsOneClassifier(LinearSVC(random_state=0))
ovo_clf.fit(iris.data, iris.target)
decisions = ovo_clf.decision_function(iris.data)
assert_equal(decisions.shape, (n_samples, n_classes))
assert_array_equal(decisions.argmax(axis=1), ovo_clf.predict(iris.data))
votes = numpy.zeros((n_samples, n_classes))
k = 0
for i in range(n_classes):
    for j in range((i + 1), n_classes):
        pred = ovo_clf.estimators_[k].predict(iris.data)
        votes[((pred == 0), i)] += 1
        votes[((pred == 1), j)] += 1
        k += 1
assert_array_equal(votes, numpy.round(decisions))
for class_idx in range(n_classes):
    assert_true(set(votes[:, class_idx]).issubset(set([0.0, 1.0, 2.0])))
    tempResult = unique(decisions[:, class_idx])
	
===================================================================	
test_ovo_partial_fit_predict: 298	
----------------------------	

(X, y) = shuffle(iris.data, iris.target)
ovo1 = OneVsOneClassifier(MultinomialNB())
tempResult = unique(y)
	
===================================================================	
test_ovo_partial_fit_predict: 308	
----------------------------	

(X, y) = shuffle(iris.data, iris.target)
ovo1 = OneVsOneClassifier(MultinomialNB())
ovo1.partial_fit(X[:100], y[:100], numpy.unique(y))
ovo1.partial_fit(X[100:], y[100:])
pred1 = ovo1.predict(X)
ovo2 = OneVsOneClassifier(MultinomialNB())
ovo2.fit(X, y)
pred2 = ovo2.predict(X)
assert_equal(len(ovo1.estimators_), ((n_classes * (n_classes - 1)) / 2))
assert_greater(numpy.mean((y == pred1)), 0.65)
assert_almost_equal(pred1, pred2)
ovo1 = OneVsOneClassifier(MultinomialNB())
tempResult = unique(iris.target)
	
===================================================================	
test_ovo_partial_fit_predict: 314	
----------------------------	

(X, y) = shuffle(iris.data, iris.target)
ovo1 = OneVsOneClassifier(MultinomialNB())
ovo1.partial_fit(X[:100], y[:100], numpy.unique(y))
ovo1.partial_fit(X[100:], y[100:])
pred1 = ovo1.predict(X)
ovo2 = OneVsOneClassifier(MultinomialNB())
ovo2.fit(X, y)
pred2 = ovo2.predict(X)
assert_equal(len(ovo1.estimators_), ((n_classes * (n_classes - 1)) / 2))
assert_greater(numpy.mean((y == pred1)), 0.65)
assert_almost_equal(pred1, pred2)
ovo1 = OneVsOneClassifier(MultinomialNB())
ovo1.partial_fit(iris.data[:60], iris.target[:60], numpy.unique(iris.target))
ovo1.partial_fit(iris.data[60:], iris.target[60:])
pred1 = ovo1.predict(iris.data)
ovo2 = OneVsOneClassifier(MultinomialNB())
pred2 = ovo2.fit(iris.data, iris.target).predict(iris.data)
assert_almost_equal(pred1, pred2)
tempResult = unique(iris.target)
	
===================================================================	
test_ovr_partial_fit: 61	
----------------------------	

(X, y) = shuffle(iris.data, iris.target, random_state=0)
ovr = OneVsRestClassifier(MultinomialNB())
tempResult = unique(y)
	
===================================================================	
test_ovr_partial_fit: 67	
----------------------------	

(X, y) = shuffle(iris.data, iris.target, random_state=0)
ovr = OneVsRestClassifier(MultinomialNB())
ovr.partial_fit(X[:100], y[:100], numpy.unique(y))
ovr.partial_fit(X[100:], y[100:])
pred = ovr.predict(X)
ovr2 = OneVsRestClassifier(MultinomialNB())
pred2 = ovr2.fit(X, y).predict(X)
assert_almost_equal(pred, pred2)
tempResult = unique(y)
	
===================================================================	
test_ovr_partial_fit: 70	
----------------------------	

(X, y) = shuffle(iris.data, iris.target, random_state=0)
ovr = OneVsRestClassifier(MultinomialNB())
ovr.partial_fit(X[:100], y[:100], numpy.unique(y))
ovr.partial_fit(X[100:], y[100:])
pred = ovr.predict(X)
ovr2 = OneVsRestClassifier(MultinomialNB())
pred2 = ovr2.fit(X, y).predict(X)
assert_almost_equal(pred, pred2)
assert_equal(len(ovr.estimators_), len(numpy.unique(y)))
assert_greater(numpy.mean((y == pred)), 0.65)
ovr = OneVsRestClassifier(MultinomialNB())
tempResult = unique(iris.target)
	
===================================================================	
test_ovr_partial_fit: 76	
----------------------------	

(X, y) = shuffle(iris.data, iris.target, random_state=0)
ovr = OneVsRestClassifier(MultinomialNB())
ovr.partial_fit(X[:100], y[:100], numpy.unique(y))
ovr.partial_fit(X[100:], y[100:])
pred = ovr.predict(X)
ovr2 = OneVsRestClassifier(MultinomialNB())
pred2 = ovr2.fit(X, y).predict(X)
assert_almost_equal(pred, pred2)
assert_equal(len(ovr.estimators_), len(numpy.unique(y)))
assert_greater(numpy.mean((y == pred)), 0.65)
ovr = OneVsRestClassifier(MultinomialNB())
ovr.partial_fit(iris.data[:60], iris.target[:60], numpy.unique(iris.target))
ovr.partial_fit(iris.data[60:], iris.target[60:])
pred = ovr.predict(iris.data)
ovr2 = OneVsRestClassifier(MultinomialNB())
pred2 = ovr2.fit(iris.data, iris.target).predict(iris.data)
assert_almost_equal(pred, pred2)
tempResult = unique(iris.target)
	
===================================================================	
test_ovr_always_present: 123	
----------------------------	

X = numpy.ones((10, 2))
X[:5, :] = 0
y = numpy.zeros((10, 3))
y[5:, 0] = 1
y[:, 1] = 1
y[:, 2] = 1
ovr = OneVsRestClassifier(LogisticRegression())
assert_warns(UserWarning, ovr.fit, X, y)
y_pred = ovr.predict(X)
assert_array_equal(numpy.array(y_pred), numpy.array(y))
y_pred = ovr.decision_function(X)
tempResult = unique(y_pred[:, (- 2):])
	
===================================================================	
test_ovr_ovo_regressor: 83	
----------------------------	

ovr = OneVsRestClassifier(DecisionTreeRegressor())
pred = ovr.fit(iris.data, iris.target).predict(iris.data)
assert_equal(len(ovr.estimators_), n_classes)
tempResult = unique(pred)
	
===================================================================	
test_ovr_ovo_regressor: 88	
----------------------------	

ovr = OneVsRestClassifier(DecisionTreeRegressor())
pred = ovr.fit(iris.data, iris.target).predict(iris.data)
assert_equal(len(ovr.estimators_), n_classes)
assert_array_equal(numpy.unique(pred), [0, 1, 2])
assert_greater(numpy.mean((pred == iris.target)), 0.9)
ovr = OneVsOneClassifier(DecisionTreeRegressor())
pred = ovr.fit(iris.data, iris.target).predict(iris.data)
assert_equal(len(ovr.estimators_), ((n_classes * (n_classes - 1)) / 2))
tempResult = unique(pred)
	
===================================================================	
module: 80	
----------------------------	

import numpy as np
import scipy.sparse as sp
from sklearn.utils import shuffle
from sklearn.utils.testing import assert_almost_equal
from sklearn.utils.testing import assert_raises
from sklearn.utils.testing import assert_raises_regex
from sklearn.utils.testing import assert_array_equal
from sklearn.utils.testing import assert_equal
from sklearn.exceptions import NotFittedError
from sklearn import datasets
from sklearn.base import clone
from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier
from sklearn.linear_model import Lasso
from sklearn.svm import LinearSVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier

def test_multi_target_regression():
    (X, y) = sklearn.datasets.make_regression(n_targets=3)
    (X_train, y_train) = (X[:50], y[:50])
    (X_test, y_test) = (X[50:], y[50:])
    references = numpy.zeros_like(y_test)
    for n in range(3):
        rgr = GradientBoostingRegressor(random_state=0)
        rgr.fit(X_train, y_train[:, n])
        references[:, n] = rgr.predict(X_test)
    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
    rgr.fit(X_train, y_train)
    y_pred = rgr.predict(X_test)
    assert_almost_equal(references, y_pred)

def test_multi_target_regression_one_target():
    (X, y) = sklearn.datasets.make_regression(n_targets=1)
    (X_train, y_train) = (X[:50], y[:50])
    (X_test, y_test) = (X[50:], y[50:])
    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
    assert_raises(ValueError, rgr.fit, X_train, y_train)

def test_multi_target_sparse_regression():
    (X, y) = sklearn.datasets.make_regression(n_targets=3)
    (X_train, y_train) = (X[:50], y[:50])
    (X_test, y_test) = (X[50:], y[50:])
    for sparse in [scipy.sparse.csr_matrix, scipy.sparse.csc_matrix, scipy.sparse.coo_matrix, scipy.sparse.dok_matrix, scipy.sparse.lil_matrix]:
        rgr = MultiOutputRegressor(Lasso(random_state=0))
        rgr_sparse = MultiOutputRegressor(Lasso(random_state=0))
        rgr.fit(X_train, y_train)
        rgr_sparse.fit(sparse(X_train), y_train)
        assert_almost_equal(rgr.predict(X_test), rgr_sparse.predict(sparse(X_test)))

def test_multi_target_sample_weights_api():
    X = [[1, 2, 3], [4, 5, 6]]
    y = [[3.141, 2.718], [2.718, 3.141]]
    w = [0.8, 0.6]
    rgr = MultiOutputRegressor(Lasso())
    assert_raises_regex(ValueError, 'does not support sample weights', rgr.fit, X, y, w)
    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
    rgr.fit(X, y, w)

def test_multi_target_sample_weights():
    Xw = [[1, 2, 3], [4, 5, 6]]
    yw = [[3.141, 2.718], [2.718, 3.141]]
    w = [2.0, 1.0]
    rgr_w = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
    rgr_w.fit(Xw, yw, w)
    X = [[1, 2, 3], [1, 2, 3], [4, 5, 6]]
    y = [[3.141, 2.718], [3.141, 2.718], [2.718, 3.141]]
    rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
    rgr.fit(X, y)
    X_test = [[1.5, 2.5, 3.5], [3.5, 4.5, 5.5]]
    assert_almost_equal(rgr.predict(X_test), rgr_w.predict(X_test))
iris = sklearn.datasets.load_iris()
X = iris.data
y1 = iris.target
y2 = shuffle(y1, random_state=1)
y3 = shuffle(y1, random_state=2)
y = numpy.column_stack((y1, y2, y3))
(n_samples, n_features) = X.shape
n_outputs = y.shape[1]
tempResult = unique(y1)
	
===================================================================	
test_mnnb: 118	
----------------------------	

for X in [X2, scipy.sparse.csr_matrix(X2)]:
    clf = MultinomialNB()
    assert_raises(ValueError, clf.fit, (- X), y2)
    y_pred = clf.fit(X, y2).predict(X)
    assert_array_equal(y_pred, y2)
    y_pred_proba = clf.predict_proba(X)
    y_pred_log_proba = clf.predict_log_proba(X)
    assert_array_almost_equal(numpy.log(y_pred_proba), y_pred_log_proba, 8)
    clf2 = MultinomialNB()
    tempResult = unique(y2)
	
===================================================================	
test_mnnb: 129	
----------------------------	

for X in [X2, scipy.sparse.csr_matrix(X2)]:
    clf = MultinomialNB()
    assert_raises(ValueError, clf.fit, (- X), y2)
    y_pred = clf.fit(X, y2).predict(X)
    assert_array_equal(y_pred, y2)
    y_pred_proba = clf.predict_proba(X)
    y_pred_log_proba = clf.predict_log_proba(X)
    assert_array_almost_equal(numpy.log(y_pred_proba), y_pred_log_proba, 8)
    clf2 = MultinomialNB()
    clf2.partial_fit(X[:2], y2[:2], classes=numpy.unique(y2))
    clf2.partial_fit(X[2:5], y2[2:5])
    clf2.partial_fit(X[5:], y2[5:])
    y_pred2 = clf2.predict(X)
    assert_array_equal(y_pred2, y2)
    y_pred_proba2 = clf2.predict_proba(X)
    y_pred_log_proba2 = clf2.predict_log_proba(X)
    assert_array_almost_equal(numpy.log(y_pred_proba2), y_pred_log_proba2, 8)
    assert_array_almost_equal(y_pred_proba2, y_pred_proba)
    assert_array_almost_equal(y_pred_log_proba2, y_pred_log_proba)
    clf3 = MultinomialNB()
    tempResult = unique(y2)
	
===================================================================	
test_discretenb_pickle: 176	
----------------------------	

for cls in [BernoulliNB, MultinomialNB, GaussianNB]:
    clf = cls().fit(X2, y2)
    y_pred = clf.predict(X2)
    store = BytesIO()
    pickle.dump(clf, store)
    clf = pickle.load(BytesIO(store.getvalue()))
    assert_array_equal(y_pred, clf.predict(X2))
    if (cls is not GaussianNB):
        tempResult = unique(y2)
	
===================================================================	
test_input_check_partial_fit: 191	
----------------------------	

for cls in [BernoulliNB, MultinomialNB]:
    tempResult = unique(y2)
	
===================================================================	
test_input_check_partial_fit: 194	
----------------------------	

for cls in [BernoulliNB, MultinomialNB]:
    assert_raises(ValueError, cls().partial_fit, X2, y2[:(- 1)], classes=numpy.unique(y2))
    assert_raises(ValueError, cls().partial_fit, X2, y2)
    clf = cls()
    tempResult = unique(y2)
	
===================================================================	
test_gnb_partial_fit: 157	
----------------------------	

clf = GaussianNB().fit(X, y)
tempResult = unique(y)
	
===================================================================	
test_gnb_partial_fit: 161	
----------------------------	

clf = GaussianNB().fit(X, y)
clf_pf = GaussianNB().partial_fit(X, y, numpy.unique(y))
assert_array_almost_equal(clf.theta_, clf_pf.theta_)
assert_array_almost_equal(clf.sigma_, clf_pf.sigma_)
assert_array_almost_equal(clf.class_prior_, clf_pf.class_prior_)
tempResult = unique(y)
	
===================================================================	
test_pipeline_methods_preprocessing_svm: 170	
----------------------------	

iris = load_iris()
X = iris.data
y = iris.target
n_samples = X.shape[0]
tempResult = unique(y)
	
===================================================================	
test_classes_property: 415	
----------------------------	

iris = load_iris()
X = iris.data
y = iris.target
reg = make_pipeline(SelectKBest(k=1), LinearRegression())
reg.fit(X, y)
assert_raises(AttributeError, getattr, reg, 'classes_')
clf = make_pipeline(SelectKBest(k=1), LogisticRegression(random_state=0))
assert_raises(AttributeError, getattr, clf, 'classes_')
clf.fit(X, y)
tempResult = unique(y)
	
===================================================================	
test_sparse_random_matrix: 100	
----------------------------	

n_components = 100
n_features = 500
for density in [0.3, 1.0]:
    s = (1 / density)
    A = sparse_random_matrix(n_components, n_features, density=density, random_state=0)
    A = densify(A)
    tempResult = unique(A)
	
===================================================================	
recurse: 137	
----------------------------	

if (node_id == _tree.TREE_LEAF):
    raise ValueError(('Invalid node_id %s' % _tree.TREE_LEAF))
left_child = tree.children_left[node_id]
right_child = tree.children_right[node_id]
if ((max_depth is None) or (depth <= max_depth)):
    if (left_child == _tree.TREE_LEAF):
        ranks['leaves'].append(str(node_id))
    elif (str(depth) not in ranks):
        ranks[str(depth)] = [str(node_id)]
    else:
        ranks[str(depth)].append(str(node_id))
    out_file.write(('%d [label=%s' % (node_id, node_to_str(tree, node_id, criterion))))
    if filled:
        if ('rgb' not in colors):
            colors['rgb'] = _color_brew(tree.n_classes[0])
            if (tree.n_outputs != 1):
                colors['bounds'] = (numpy.min((- tree.impurity)), numpy.max((- tree.impurity)))
            tempResult = unique(tree.value)
	
===================================================================	
BaseDecisionTree.fit: 84	
----------------------------	

"Build a decision tree from the training set (X, y).\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape = [n_samples, n_features]\n            The training input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csc_matrix``.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n            The target values (class labels in classification, real numbers in\n            regression). In the regression case, use ``dtype=np.float64`` and\n            ``order='C'`` for maximum efficiency.\n\n        sample_weight : array-like, shape = [n_samples] or None\n            Sample weights. If None, then samples are equally weighted. Splits\n            that would create child nodes with net zero or negative weight are\n            ignored while searching for a split in each node. In the case of\n            classification, splits are also ignored if they would result in any\n            single class carrying a negative weight in either child node.\n\n        check_input : boolean, (default=True)\n            Allow to bypass several input checking.\n            Don't use this parameter unless you know what you do.\n\n        X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n            The indexes of the sorted training input samples. If many tree\n            are grown on the same dataset, this allows the ordering to be\n            cached between trees. If None, the data will be sorted here.\n            Don't use this parameter unless you know what to do.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        "
random_state = check_random_state(self.random_state)
if check_input:
    X = check_array(X, dtype=DTYPE, accept_sparse='csc')
    y = check_array(y, ensure_2d=False, dtype=None)
    if issparse(X):
        X.sort_indices()
        if ((X.indices.dtype != numpy.intc) or (X.indptr.dtype != numpy.intc)):
            raise ValueError('No support for np.int64 index based sparse matrices')
(n_samples, self.n_features_) = X.shape
is_classification = isinstance(self, ClassifierMixin)
y = numpy.atleast_1d(y)
expanded_class_weight = None
if (y.ndim == 1):
    y = numpy.reshape(y, ((- 1), 1))
self.n_outputs_ = y.shape[1]
if is_classification:
    check_classification_targets(y)
    y = numpy.copy(y)
    self.classes_ = []
    self.n_classes_ = []
    if (self.class_weight is not None):
        y_original = numpy.copy(y)
    y_encoded = numpy.zeros(y.shape, dtype=numpy.int)
    for k in range(self.n_outputs_):
        tempResult = unique(y[:, k], return_inverse=True)
	
===================================================================	
compute_sample_weight: 58	
----------------------------	

'Estimate sample weights by class for unbalanced datasets.\n\n    Parameters\n    ----------\n    class_weight : dict, list of dicts, "balanced", or None, optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        The "balanced" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data:\n        ``n_samples / (n_classes * np.bincount(y))``.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n    y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n        Array of original class labels per sample.\n\n    indices : array-like, shape (n_subsample,), or None\n        Array of indices to be used in a subsample. Can be of length less than\n        n_samples in the case of a subsample, or equal to n_samples in the\n        case of a bootstrap subsample with repeated indices. If None, the\n        sample weight will be calculated over the full sample. Only "auto" is\n        supported for class_weight if this is provided.\n\n    Returns\n    -------\n    sample_weight_vect : ndarray, shape (n_samples,)\n        Array with sample weights as applied to the original y\n    '
y = numpy.atleast_1d(y)
if (y.ndim == 1):
    y = numpy.reshape(y, ((- 1), 1))
n_outputs = y.shape[1]
if isinstance(class_weight, externals.six.string_types):
    if (class_weight not in ['balanced', 'auto']):
        raise ValueError(('The only valid preset for class_weight is "balanced". Given "%s".' % class_weight))
elif ((indices is not None) and (not isinstance(class_weight, externals.six.string_types))):
    raise ValueError(('The only valid class_weight for subsampling is "balanced". Given "%s".' % class_weight))
elif (n_outputs > 1):
    if ((not hasattr(class_weight, '__iter__')) or isinstance(class_weight, dict)):
        raise ValueError('For multi-output, class_weight should be a list of dicts, or a valid string.')
    if (len(class_weight) != n_outputs):
        raise ValueError('For multi-output, number of elements in class_weight should match number of outputs.')
expanded_class_weight = []
for k in range(n_outputs):
    y_full = y[:, k]
    tempResult = unique(y_full)
	
===================================================================	
compute_sample_weight: 66	
----------------------------	

'Estimate sample weights by class for unbalanced datasets.\n\n    Parameters\n    ----------\n    class_weight : dict, list of dicts, "balanced", or None, optional\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        The "balanced" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data:\n        ``n_samples / (n_classes * np.bincount(y))``.\n\n        For multi-output, the weights of each column of y will be multiplied.\n\n    y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n        Array of original class labels per sample.\n\n    indices : array-like, shape (n_subsample,), or None\n        Array of indices to be used in a subsample. Can be of length less than\n        n_samples in the case of a subsample, or equal to n_samples in the\n        case of a bootstrap subsample with repeated indices. If None, the\n        sample weight will be calculated over the full sample. Only "auto" is\n        supported for class_weight if this is provided.\n\n    Returns\n    -------\n    sample_weight_vect : ndarray, shape (n_samples,)\n        Array with sample weights as applied to the original y\n    '
y = numpy.atleast_1d(y)
if (y.ndim == 1):
    y = numpy.reshape(y, ((- 1), 1))
n_outputs = y.shape[1]
if isinstance(class_weight, externals.six.string_types):
    if (class_weight not in ['balanced', 'auto']):
        raise ValueError(('The only valid preset for class_weight is "balanced". Given "%s".' % class_weight))
elif ((indices is not None) and (not isinstance(class_weight, externals.six.string_types))):
    raise ValueError(('The only valid class_weight for subsampling is "balanced". Given "%s".' % class_weight))
elif (n_outputs > 1):
    if ((not hasattr(class_weight, '__iter__')) or isinstance(class_weight, dict)):
        raise ValueError('For multi-output, class_weight should be a list of dicts, or a valid string.')
    if (len(class_weight) != n_outputs):
        raise ValueError('For multi-output, number of elements in class_weight should match number of outputs.')
expanded_class_weight = []
for k in range(n_outputs):
    y_full = y[:, k]
    classes_full = numpy.unique(y_full)
    classes_missing = None
    if ((class_weight in ['balanced', 'auto']) or (n_outputs == 1)):
        class_weight_k = class_weight
    else:
        class_weight_k = class_weight[k]
    if (indices is not None):
        y_subsample = y[(indices, k)]
        tempResult = unique(y_subsample)
	
===================================================================	
check_class_weight_balanced_linear_classifier: 911	
----------------------------	

'Test class weights with non-contiguous class labels.'
X = numpy.array([[(- 1.0), (- 1.0)], [(- 1.0), 0], [(- 0.8), (- 1.0)], [1.0, 1.0], [1.0, 0.0]])
y = numpy.array([1, 1, 1, (- 1), (- 1)])
with ignore_warnings(category=DeprecationWarning):
    classifier = Classifier()
if hasattr(classifier, 'n_iter'):
    classifier.set_params(n_iter=1000)
set_random_state(classifier)
classifier.set_params(class_weight='balanced')
coef_balanced = classifier.fit(X, y).coef_.copy()
n_samples = len(y)
tempResult = unique(y)
	
===================================================================	
check_estimators_partial_fit_n_features: 596	
----------------------------	
if (not hasattr(Alg, 'partial_fit')):
    return
(X, y) = make_blobs(n_samples=50, random_state=1)
X -= X.min()
with ignore_warnings(category=DeprecationWarning):
    alg = Alg()
if (not hasattr(alg, 'partial_fit')):
    return
set_testing_parameters(alg)
try:
    if isinstance(alg, ClassifierMixin):
        tempResult = unique(y)	
===================================================================	
check_classifiers_train: 679	
----------------------------	

(X_m, y_m) = make_blobs(n_samples=300, random_state=0)
(X_m, y_m) = shuffle(X_m, y_m, random_state=7)
X_m = StandardScaler().fit_transform(X_m)
y_b = y_m[(y_m != 2)]
X_b = X_m[(y_m != 2)]
for (X, y) in [(X_m, y_m), (X_b, y_b)]:
    tempResult = unique(y)
	
===================================================================	
check_classifiers_classes: 779	
----------------------------	

(X, y) = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)
(X, y) = shuffle(X, y, random_state=7)
X = StandardScaler().fit_transform(X)
X -= (X.min() - 0.1)
y_names = numpy.array(['one', 'two', 'three'])[y]
for y_names in [y_names, y_names.astype('O')]:
    if (name in ['LabelPropagation', 'LabelSpreading']):
        y_ = y
    else:
        y_ = y_names
    tempResult = unique(y_)
	
===================================================================	
check_classifiers_classes: 788	
----------------------------	

(X, y) = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)
(X, y) = shuffle(X, y, random_state=7)
X = StandardScaler().fit_transform(X)
X -= (X.min() - 0.1)
y_names = numpy.array(['one', 'two', 'three'])[y]
for y_names in [y_names, y_names.astype('O')]:
    if (name in ['LabelPropagation', 'LabelSpreading']):
        y_ = y
    else:
        y_ = y_names
    classes = numpy.unique(y_)
    with ignore_warnings(category=DeprecationWarning):
        classifier = Classifier()
    if (name == 'BernoulliNB'):
        classifier.set_params(binarize=X.mean())
    set_testing_parameters(classifier)
    set_random_state(classifier)
    classifier.fit(X, y_)
    y_pred = classifier.predict(X)
    tempResult = unique(y_)
	
===================================================================	
check_classifiers_classes: 788	
----------------------------	

(X, y) = make_blobs(n_samples=30, random_state=0, cluster_std=0.1)
(X, y) = shuffle(X, y, random_state=7)
X = StandardScaler().fit_transform(X)
X -= (X.min() - 0.1)
y_names = numpy.array(['one', 'two', 'three'])[y]
for y_names in [y_names, y_names.astype('O')]:
    if (name in ['LabelPropagation', 'LabelSpreading']):
        y_ = y
    else:
        y_ = y_names
    classes = numpy.unique(y_)
    with ignore_warnings(category=DeprecationWarning):
        classifier = Classifier()
    if (name == 'BernoulliNB'):
        classifier.set_params(binarize=X.mean())
    set_testing_parameters(classifier)
    set_random_state(classifier)
    classifier.fit(X, y_)
    y_pred = classifier.predict(X)
    tempResult = unique(y_pred)
	
===================================================================	
check_class_weight_classifiers: 870	
----------------------------	

if (name == 'NuSVC'):
    raise SkipTest
if name.endswith('NB'):
    raise SkipTest
for n_centers in [2, 3]:
    (X, y) = make_blobs(centers=n_centers, random_state=0, cluster_std=20)
    (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.5, random_state=0)
    tempResult = unique(y_train)
	
===================================================================	
weighted_mode: 175	
----------------------------	

"Returns an array of the weighted modal (most common) value in a\n\n    If there is more than one such value, only the first is returned.\n    The bin-count for the modal bins is also returned.\n\n    This is an extension of the algorithm in scipy.stats.mode.\n\n    Parameters\n    ----------\n    a : array_like\n        n-dimensional array of which to find mode(s).\n    w : array_like\n        n-dimensional array of weights for each value\n    axis : int, optional\n        Axis along which to operate. Default is 0, i.e. the first axis.\n\n    Returns\n    -------\n    vals : ndarray\n        Array of modal values.\n    score : ndarray\n        Array of weighted counts for each mode.\n\n    Examples\n    --------\n    >>> from sklearn.utils.extmath import weighted_mode\n    >>> x = [4, 1, 4, 2, 4, 2]\n    >>> weights = [1, 1, 1, 1, 1, 1]\n    >>> weighted_mode(x, weights)\n    (array([ 4.]), array([ 3.]))\n\n    The value 4 appears three times: with uniform weights, the result is\n    simply the mode of the distribution.\n\n    >>> weights = [1, 3, 0.5, 1.5, 1, 2] # deweight the 4's\n    >>> weighted_mode(x, weights)\n    (array([ 2.]), array([ 3.5]))\n\n    The value 2 has the highest score: it appears twice with weights of\n    1.5 and 2: the sum of these is 3.\n\n    See Also\n    --------\n    scipy.stats.mode\n    "
if (axis is None):
    a = numpy.ravel(a)
    w = numpy.ravel(w)
    axis = 0
else:
    a = numpy.asarray(a)
    w = numpy.asarray(w)
    axis = axis
if (a.shape != w.shape):
    w = (numpy.zeros(a.shape, dtype=w.dtype) + w)
tempResult = unique(numpy.ravel(a))
	
===================================================================	
in1d: 194	
----------------------------	

ar1 = np.asarray(ar1).ravel()
ar2 = np.asarray(ar2).ravel()
if (len(ar2) < (10 * (len(ar1) ** 0.145))):
    if invert:
        mask = numpy.ones(len(ar1), dtype=numpy.bool)
        for a in ar2:
            mask &= (ar1 != a)
    else:
        mask = numpy.zeros(len(ar1), dtype=numpy.bool)
        for a in ar2:
            mask |= (ar1 == a)
    return mask
if (not assume_unique):
    tempResult = unique(ar1, return_inverse=True)
	
===================================================================	
in1d: 195	
----------------------------	

ar1 = np.asarray(ar1).ravel()
ar2 = np.asarray(ar2).ravel()
if (len(ar2) < (10 * (len(ar1) ** 0.145))):
    if invert:
        mask = numpy.ones(len(ar1), dtype=numpy.bool)
        for a in ar2:
            mask &= (ar1 != a)
    else:
        mask = numpy.zeros(len(ar1), dtype=numpy.bool)
        for a in ar2:
            mask |= (ar1 == a)
    return mask
if (not assume_unique):
    (ar1, rev_idx) = numpy.unique(ar1, return_inverse=True)
    tempResult = unique(ar2)
	
===================================================================	
CheckingClassifier.fit: 47	
----------------------------	

assert_true((len(X) == len(y)))
if (self.check_X is not None):
    assert_true(self.check_X(X))
if (self.check_y is not None):
    assert_true(self.check_y(y))
tempResult = unique(check_array(y, ensure_2d=False, allow_nd=True))
	
===================================================================	
is_multilabel: 58	
----------------------------	

' Check if ``y`` is in a multilabel format.\n\n    Parameters\n    ----------\n    y : numpy array of shape [n_samples]\n        Target values.\n\n    Returns\n    -------\n    out : bool,\n        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.multiclass import is_multilabel\n    >>> is_multilabel([0, 1, 0, 1])\n    False\n    >>> is_multilabel([[1], [0, 2], []])\n    False\n    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n    True\n    >>> is_multilabel(np.array([[1], [0], [0]]))\n    False\n    >>> is_multilabel(np.array([[1, 0, 0]]))\n    True\n    '
if hasattr(y, '__array__'):
    y = numpy.asarray(y)
if (not (hasattr(y, 'shape') and (y.ndim == 2) and (y.shape[1] > 1))):
    return False
if issparse(y):
    if isinstance(y, (dok_matrix, lil_matrix)):
        y = y.tocsr()
    tempResult = unique(y.data)
	
===================================================================	
is_multilabel: 58	
----------------------------	

' Check if ``y`` is in a multilabel format.\n\n    Parameters\n    ----------\n    y : numpy array of shape [n_samples]\n        Target values.\n\n    Returns\n    -------\n    out : bool,\n        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.multiclass import is_multilabel\n    >>> is_multilabel([0, 1, 0, 1])\n    False\n    >>> is_multilabel([[1], [0, 2], []])\n    False\n    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n    True\n    >>> is_multilabel(np.array([[1], [0], [0]]))\n    False\n    >>> is_multilabel(np.array([[1, 0, 0]]))\n    True\n    '
if hasattr(y, '__array__'):
    y = numpy.asarray(y)
if (not (hasattr(y, 'shape') and (y.ndim == 2) and (y.shape[1] > 1))):
    return False
if issparse(y):
    if isinstance(y, (dok_matrix, lil_matrix)):
        y = y.tocsr()
    tempResult = unique(y.data)
	
===================================================================	
is_multilabel: 60	
----------------------------	

' Check if ``y`` is in a multilabel format.\n\n    Parameters\n    ----------\n    y : numpy array of shape [n_samples]\n        Target values.\n\n    Returns\n    -------\n    out : bool,\n        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.utils.multiclass import is_multilabel\n    >>> is_multilabel([0, 1, 0, 1])\n    False\n    >>> is_multilabel([[1], [0, 2], []])\n    False\n    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n    True\n    >>> is_multilabel(np.array([[1], [0], [0]]))\n    False\n    >>> is_multilabel(np.array([[1, 0, 0]]))\n    True\n    '
if hasattr(y, '__array__'):
    y = numpy.asarray(y)
if (not (hasattr(y, 'shape') and (y.ndim == 2) and (y.shape[1] > 1))):
    return False
if issparse(y):
    if isinstance(y, (dok_matrix, lil_matrix)):
        y = y.tocsr()
    return ((len(y.data) == 0) or ((np.unique(y.data).size == 1) and ((y.dtype.kind in 'biu') or _is_integral_float(numpy.unique(y.data)))))
else:
    tempResult = unique(y)
	
===================================================================	
type_of_target: 95	
----------------------------	

"Determine the type of data indicated by target `y`\n\n    Parameters\n    ----------\n    y : array-like\n\n    Returns\n    -------\n    target_type : string\n        One of:\n        * 'continuous': `y` is an array-like of floats that are not all\n          integers, and is 1d or a column vector.\n        * 'continuous-multioutput': `y` is a 2d array of floats that are\n          not all integers, and both dimensions are of size > 1.\n        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n          vector.\n        * 'multiclass': `y` contains more than two discrete values, is not a\n          sequence of sequences, and is 1d or a column vector.\n        * 'multiclass-multioutput': `y` is a 2d array that contains more\n          than two discrete values, is not a sequence of sequences, and both\n          dimensions are of size > 1.\n        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n          of two dimensions with at least two columns, and at most 2 unique\n          values.\n        * 'unknown': `y` is array-like but none of the above, such as a 3d\n          array, sequence of sequences, or an array of non-sequence objects.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> type_of_target([0.1, 0.6])\n    'continuous'\n    >>> type_of_target([1, -1, -1, 1])\n    'binary'\n    >>> type_of_target(['a', 'b', 'a'])\n    'binary'\n    >>> type_of_target([1.0, 2.0])\n    'binary'\n    >>> type_of_target([1, 0, 2])\n    'multiclass'\n    >>> type_of_target([1.0, 0.0, 3.0])\n    'multiclass'\n    >>> type_of_target(['a', 'b', 'c'])\n    'multiclass'\n    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n    'multiclass-multioutput'\n    >>> type_of_target([[1, 2]])\n    'multiclass-multioutput'\n    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n    'continuous-multioutput'\n    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n    'multilabel-indicator'\n    "
valid = ((isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__')) and (not isinstance(y, string_types)))
if (not valid):
    raise ValueError(('Expected array-like (array or non-string sequence), got %r' % y))
if is_multilabel(y):
    return 'multilabel-indicator'
try:
    y = numpy.asarray(y)
except ValueError:
    return 'unknown'
try:
    if ((not hasattr(y[0], '__array__')) and isinstance(y[0], Sequence) and (not isinstance(y[0], string_types))):
        raise ValueError('You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead.')
except IndexError:
    pass
if ((y.ndim > 2) or ((y.dtype == object) and len(y) and (not isinstance(y.flat[0], string_types)))):
    return 'unknown'
if ((y.ndim == 2) and (y.shape[1] == 0)):
    return 'unknown'
if ((y.ndim == 2) and (y.shape[1] > 1)):
    suffix = '-multioutput'
else:
    suffix = ''
if ((y.dtype.kind == 'f') and numpy.any((y != y.astype(int)))):
    return ('continuous' + suffix)
tempResult = unique(y)
	
===================================================================	
_unique_multiclass: 18	
----------------------------	

if hasattr(y, '__array__'):
    tempResult = unique(numpy.asarray(y))
	
===================================================================	
class_distribution: 130	
----------------------------	

'Compute class priors from multioutput-multiclass target data\n\n    Parameters\n    ----------\n    y : array like or sparse matrix of size (n_samples, n_outputs)\n        The labels for each example.\n\n    sample_weight : array-like of shape = (n_samples,), optional\n        Sample weights.\n\n    Returns\n    -------\n    classes : list of size n_outputs of arrays of size (n_classes,)\n        List of classes for each column.\n\n    n_classes : list of integers of size n_outputs\n        Number of classes in each column\n\n    class_prior : list of size n_outputs of arrays of size (n_classes,)\n        Class distribution of each column.\n\n    '
classes = []
n_classes = []
class_prior = []
(n_samples, n_outputs) = y.shape
if issparse(y):
    y = y.tocsc()
    y_nnz = numpy.diff(y.indptr)
    for k in range(n_outputs):
        col_nonzero = y.indices[y.indptr[k]:y.indptr[(k + 1)]]
        if (sample_weight is not None):
            nz_samp_weight = numpy.asarray(sample_weight)[col_nonzero]
            zeros_samp_weight_sum = (numpy.sum(sample_weight) - numpy.sum(nz_samp_weight))
        else:
            nz_samp_weight = None
            zeros_samp_weight_sum = (y.shape[0] - y_nnz[k])
        tempResult = unique(y.data[y.indptr[k]:y.indptr[(k + 1)]], return_inverse=True)
	
===================================================================	
class_distribution: 142	
----------------------------	

'Compute class priors from multioutput-multiclass target data\n\n    Parameters\n    ----------\n    y : array like or sparse matrix of size (n_samples, n_outputs)\n        The labels for each example.\n\n    sample_weight : array-like of shape = (n_samples,), optional\n        Sample weights.\n\n    Returns\n    -------\n    classes : list of size n_outputs of arrays of size (n_classes,)\n        List of classes for each column.\n\n    n_classes : list of integers of size n_outputs\n        Number of classes in each column\n\n    class_prior : list of size n_outputs of arrays of size (n_classes,)\n        Class distribution of each column.\n\n    '
classes = []
n_classes = []
class_prior = []
(n_samples, n_outputs) = y.shape
if issparse(y):
    y = y.tocsc()
    y_nnz = numpy.diff(y.indptr)
    for k in range(n_outputs):
        col_nonzero = y.indices[y.indptr[k]:y.indptr[(k + 1)]]
        if (sample_weight is not None):
            nz_samp_weight = numpy.asarray(sample_weight)[col_nonzero]
            zeros_samp_weight_sum = (numpy.sum(sample_weight) - numpy.sum(nz_samp_weight))
        else:
            nz_samp_weight = None
            zeros_samp_weight_sum = (y.shape[0] - y_nnz[k])
        (classes_k, y_k) = numpy.unique(y.data[y.indptr[k]:y.indptr[(k + 1)]], return_inverse=True)
        class_prior_k = bincount(y_k, weights=nz_samp_weight)
        if (0 in classes_k):
            class_prior_k[(classes_k == 0)] += zeros_samp_weight_sum
        if ((0 not in classes_k) and (y_nnz[k] < y.shape[0])):
            classes_k = numpy.insert(classes_k, 0, 0)
            class_prior_k = numpy.insert(class_prior_k, 0, zeros_samp_weight_sum)
        classes.append(classes_k)
        n_classes.append(classes_k.shape[0])
        class_prior.append((class_prior_k / class_prior_k.sum()))
else:
    for k in range(n_outputs):
        tempResult = unique(y[:, k], return_inverse=True)
	
===================================================================	
choice: 70	
----------------------------	

"\n    choice(a, size=None, replace=True, p=None)\n\n    Generates a random sample from a given 1-D array\n\n    .. versionadded:: 1.7.0\n\n    Parameters\n    -----------\n    a : 1-D array-like or int\n        If an ndarray, a random sample is generated from its elements.\n        If an int, the random sample is generated as if a was np.arange(n)\n\n    size : int or tuple of ints, optional\n        Output shape. Default is None, in which case a single value is\n        returned.\n\n    replace : boolean, optional\n        Whether the sample is with or without replacement.\n\n    p : 1-D array-like, optional\n        The probabilities associated with each entry in a.\n        If not given the sample assumes a uniform distribution over all\n        entries in a.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n\n    Returns\n    --------\n    samples : 1-D ndarray, shape (size,)\n    The generated random samples\n\n    Raises\n    -------\n    ValueError\n    If a is an int and less than zero, if a or p are not 1-dimensional,\n    if a is an array-like of size 0, if p is not a vector of\n    probabilities, if a and p have different lengths, or if\n    replace=False and the sample size is greater than the population\n    size\n\n    See Also\n    ---------\n    randint, shuffle, permutation\n\n    Examples\n    ---------\n    Generate a uniform random sample from np.arange(5) of size 3:\n\n    >>> np.random.choice(5, 3)  # doctest: +SKIP\n    array([0, 3, 4])\n    >>> #This is equivalent to np.random.randint(0,5,3)\n\n    Generate a non-uniform random sample from np.arange(5) of size 3:\n\n    >>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])  # doctest: +SKIP\n    array([3, 3, 0])\n\n    Generate a uniform random sample from np.arange(5) of size 3 without\n    replacement:\n\n    >>> np.random.choice(5, 3, replace=False)  # doctest: +SKIP\n    array([3,1,0])\n    >>> #This is equivalent to np.random.shuffle(np.arange(5))[:3]\n\n    Generate a non-uniform random sample from np.arange(5) of size\n    3 without replacement:\n\n    >>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\n    ... # doctest: +SKIP\n    array([2, 3, 0])\n\n    Any of the above can be repeated with an arbitrary array-like\n    instead of just integers. For instance:\n\n    >>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n    >>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\n    ... # doctest: +SKIP\n    array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'],\n    dtype='|S11')\n\n    "
random_state = check_random_state(random_state)
a = numpy.array(a, copy=False)
if (a.ndim == 0):
    try:
        pop_size = operator.index(a.item())
    except TypeError:
        raise ValueError('a must be 1-dimensional or an integer')
    if (pop_size <= 0):
        raise ValueError('a must be greater than 0')
elif (a.ndim != 1):
    raise ValueError('a must be 1-dimensional')
else:
    pop_size = a.shape[0]
    if (pop_size is 0):
        raise ValueError('a must be non-empty')
if (p is not None):
    p = numpy.array(p, dtype=numpy.double, ndmin=1, copy=False)
    if (p.ndim != 1):
        raise ValueError('p must be 1-dimensional')
    if (p.size != pop_size):
        raise ValueError('a and p must have same size')
    if numpy.any((p < 0)):
        raise ValueError('probabilities are not non-negative')
    if (not numpy.allclose(p.sum(), 1)):
        raise ValueError('probabilities do not sum to 1')
shape = size
if (shape is not None):
    size = numpy.prod(shape, dtype=numpy.intp)
else:
    size = 1
if replace:
    if (p is not None):
        cdf = p.cumsum()
        cdf /= cdf[(- 1)]
        uniform_samples = random_state.random_sample(shape)
        idx = cdf.searchsorted(uniform_samples, side='right')
        idx = numpy.array(idx, copy=False)
    else:
        idx = random_state.randint(0, pop_size, size=shape)
else:
    if (size > pop_size):
        raise ValueError("Cannot take a larger sample than population when 'replace=False'")
    if (p is not None):
        if (numpy.sum((p > 0)) < size):
            raise ValueError('Fewer non-zero entries in p than size')
        n_uniq = 0
        p = p.copy()
        found = numpy.zeros(shape, dtype=numpy.int)
        flat_found = found.ravel()
        while (n_uniq < size):
            x = random_state.rand((size - n_uniq))
            if (n_uniq > 0):
                p[flat_found[0:n_uniq]] = 0
            cdf = numpy.cumsum(p)
            cdf /= cdf[(- 1)]
            new = cdf.searchsorted(x, side='right')
            tempResult = unique(new, return_index=True)
	
===================================================================	
_rankdata: 10	
----------------------------	

"Assign ranks to data, dealing with ties appropriately.\n\n    Ranks begin at 1. The method argument controls how ranks are assigned\n    to equal values.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked. The array is first flattened.\n\n    method : str, optional\n        The method used to assign ranks to tied elements.\n        The options are 'max'.\n        'max': The maximum of the ranks that would have been assigned\n              to all the tied values is assigned to each value.\n\n    Returns\n    -------\n    ranks : ndarray\n        An array of length equal to the size of a, containing rank scores.\n\n    Notes\n    -----\n    We only backport the 'max' method\n\n    "
if (method != 'max'):
    raise NotImplementedError()
tempResult = unique(a, return_inverse=True)
	
===================================================================	
check_consistent_length: 81	
----------------------------	

'Check that all arrays have consistent first dimensions.\n\n    Checks whether all objects in arrays have the same shape or length.\n\n    Parameters\n    ----------\n    *arrays : list or tuple of input objects.\n        Objects that will be checked for consistent length.\n    '
lengths = [_num_samples(X) for X in arrays if (X is not None)]
tempResult = unique(lengths)
	
===================================================================	
test_compute_class_weight: 17	
----------------------------	

y = numpy.asarray([2, 2, 2, 3, 3, 4])
tempResult = unique(y)
	
===================================================================	
test_int_float_dict: 10	
----------------------------	

rng = numpy.random.RandomState(0)
tempResult = unique(rng.randint(100, size=10).astype(numpy.intp))
	
===================================================================	
check_sample_int: 39	
----------------------------	

n_population = 100
for n_samples in range((n_population + 1)):
    s = sample_without_replacement(n_population, n_samples)
    assert_equal(len(s), n_samples)
    tempResult = unique(s)
	
***************************************************	
matplotlib_matplotlib-2.0.0: 5	
===================================================================	
_mark_every_path: 108	
----------------------------	

'\n    Helper function that sorts out how to deal the input\n    `markevery` and returns the points where markers should be drawn.\n\n    Takes in the `markevery` value and the line path and returns the\n    sub-sampled path.\n    '
(codes, verts) = (tpath.codes, tpath.vertices)

def _slice_or_none(in_v, slc):
    '\n        Helper function to cope with `codes` being an\n        ndarray or `None`\n        '
    if (in_v is None):
        return None
    return in_v[slc]
if isinstance(markevery, float):
    markevery = (0.0, markevery)
elif isinstance(markevery, int):
    markevery = (0, markevery)
if isinstance(markevery, tuple):
    if (len(markevery) != 2):
        raise ValueError(('`markevery` is a tuple but its len is not 2; markevery=%s' % (markevery,)))
    (start, step) = markevery
    if isinstance(step, int):
        if (not isinstance(start, int)):
            raise ValueError(('`markevery` is a tuple with len 2 and second element is an int, but the first element is not an int; markevery=%s' % (markevery,)))
        return Path(verts[slice(start, None, step)], _slice_or_none(codes, slice(start, None, step)))
    elif isinstance(step, float):
        if (not (isinstance(start, int) or isinstance(start, float))):
            raise ValueError(('`markevery` is a tuple with len 2 and second element is a float, but the first element is not a float or an int; markevery=%s' % (markevery,)))
        disp_coords = affine.transform(tpath.vertices)
        delta = numpy.empty((len(disp_coords), 2), dtype=float)
        delta[0, :] = 0.0
        delta[1:, :] = (disp_coords[1:, :] - disp_coords[:(- 1), :])
        delta = numpy.sum((delta ** 2), axis=1)
        delta = numpy.sqrt(delta)
        delta = numpy.cumsum(delta)
        scale = ax_transform.transform(numpy.array([[0, 0], [1, 1]]))
        scale = numpy.diff(scale, axis=0)
        scale = numpy.sum((scale ** 2))
        scale = numpy.sqrt(scale)
        marker_delta = numpy.arange((start * scale), delta[(- 1)], (step * scale))
        inds = numpy.abs((delta[numpy.newaxis, :] - marker_delta[:, numpy.newaxis]))
        inds = inds.argmin(axis=1)
        tempResult = unique(inds)
	
===================================================================	
test_delaunay_duplicate_points: 47	
----------------------------	

npoints = 10
duplicate = 7
duplicate_of = 3
numpy.random.seed(23)
x = numpy.random.random(npoints)
y = numpy.random.random(npoints)
x[duplicate] = x[duplicate_of]
y[duplicate] = y[duplicate_of]
triang = matplotlib.tri.Triangulation(x, y)
tempResult = unique(triang.triangles)
	
===================================================================	
test_delaunay: 35	
----------------------------	

nx = 5
ny = 4
(x, y) = numpy.meshgrid(numpy.linspace(0.0, 1.0, nx), numpy.linspace(0.0, 1.0, ny))
x = x.ravel()
y = y.ravel()
npoints = (nx * ny)
ntriangles = ((2 * (nx - 1)) * (ny - 1))
nedges = (((((3 * nx) * ny) - (2 * nx)) - (2 * ny)) + 1)
triang = matplotlib.tri.Triangulation(x, y)
assert_array_almost_equal(triang.x, x)
assert_array_almost_equal(triang.y, y)
assert_equal(len(triang.triangles), ntriangles)
assert_equal(numpy.min(triang.triangles), 0)
assert_equal(numpy.max(triang.triangles), (npoints - 1))
assert_equal(len(triang.edges), nedges)
assert_equal(numpy.min(triang.edges), 0)
assert_equal(numpy.max(triang.edges), (npoints - 1))
neighbors = triang.neighbors
triang._neighbors = None
assert_array_equal(triang.neighbors, neighbors)
tempResult = unique(triang.triangles)
	
===================================================================	
_Sparse_Matrix_coo.compress_csc: 486	
----------------------------	

'\n        Compress rows, cols, vals / summing duplicates. Sort for csc format.\n        '
tempResult = unique((self.rows + (self.n * self.cols)), return_index=True, return_inverse=True)
	
===================================================================	
_Sparse_Matrix_coo.compress_csr: 493	
----------------------------	

'\n        Compress rows, cols, vals / summing duplicates. Sort for csr format.\n        '
tempResult = unique(((self.m * self.rows) + self.cols), return_index=True, return_inverse=True)
	
***************************************************	
ipython_ipython-6.1.0: 0	
***************************************************	
pandas_pandas-0.19.2: 22	
===================================================================	
Categorical.remove_unused_categories: 331	
----------------------------	

' Removes categories which are not used.\n\n        Parameters\n        ----------\n        inplace : boolean (default: False)\n           Whether or not to drop unused categories inplace or return a copy of\n           this categorical with unused categories dropped.\n\n        Returns\n        -------\n        cat : Categorical with unused categories dropped or None if inplace.\n\n        See also\n        --------\n        rename_categories\n        reorder_categories\n        add_categories\n        remove_categories\n        set_categories\n        '
cat = (self if inplace else self.copy())
tempResult = unique(cat._codes, return_inverse=True)
	
===================================================================	
NDFrame.describe: 1999	
----------------------------	

if (self.ndim >= 3):
    msg = 'describe is not implemented on Panel or PanelND objects.'
    raise NotImplementedError(msg)
elif ((self.ndim == 2) and (self.columns.size == 0)):
    raise ValueError('Cannot describe a DataFrame without columns')
if (percentiles is not None):
    self._check_percentile(percentiles)
    if (0.5 not in percentiles):
        percentiles.append(0.5)
    percentiles = numpy.asarray(percentiles)
else:
    percentiles = numpy.array([0.25, 0.5, 0.75])
tempResult = unique(percentiles)
	
===================================================================	
numpy_groupby: 2579	
----------------------------	

s = numpy.argsort(labels)
tempResult = unique(labels, return_inverse=True)
	
===================================================================	
_ensure_like_indices: 34	
----------------------------	

'\n    Makes sure that time and panels are conformable\n    '
n_time = len(time)
n_panel = len(panels)
tempResult = unique(panels)
	
===================================================================	
_ensure_like_indices: 35	
----------------------------	

'\n    Makes sure that time and panels are conformable\n    '
n_time = len(time)
n_panel = len(panels)
u_panels = numpy.unique(panels)
tempResult = unique(time)
	
===================================================================	
format_percentiles: 1516	
----------------------------	

"\n    Outputs rounded and formatted percentiles.\n\n    Parameters\n    ----------\n    percentiles : list-like, containing floats from interval [0,1]\n\n    Returns\n    -------\n    formatted : list of strings\n\n    Notes\n    -----\n    Rounding precision is chosen so that: (1) if any two elements of\n    ``percentiles`` differ, they remain different after rounding\n    (2) no entry is *rounded* to 0% or 100%.\n    Any non-integer is always rounded to at least 1 decimal place.\n\n    Examples\n    --------\n    Keeps all entries different after rounding:\n\n    >>> format_percentiles([0.01999, 0.02001, 0.5, 0.666666, 0.9999])\n    ['1.999%', '2.001%', '50%', '66.667%', '99.99%']\n\n    No element is rounded to 0% or 100% (unless already equal to it).\n    Duplicates are allowed:\n\n    >>> format_percentiles([0, 0.5, 0.02001, 0.5, 0.666666, 0.9999])\n    ['0%', '50%', '2.0%', '50%', '66.67%', '99.99%']\n    "
percentiles = numpy.asarray(percentiles)
with numpy.errstate(invalid='ignore'):
    if ((not is_numeric_dtype(percentiles)) or (not numpy.all((percentiles >= 0))) or (not numpy.all((percentiles <= 1)))):
        raise ValueError('percentiles should all be in the interval [0,1]')
percentiles = (100 * percentiles)
int_idx = (percentiles.astype(int) == percentiles)
if numpy.all(int_idx):
    out = percentiles.astype(int).astype(str)
    return [(i + '%') for i in out]
tempResult = unique(percentiles)
	
===================================================================	
StataReader._do_convert_missing: 887	
----------------------------	

for (i, colname) in enumerate(data):
    fmt = self.typlist[i]
    if (fmt not in self.VALID_RANGE):
        continue
    (nmin, nmax) = self.VALID_RANGE[fmt]
    series = data[colname]
    missing = numpy.logical_or((series < nmin), (series > nmax))
    if (not missing.any()):
        continue
    if convert_missing:
        missing_loc = numpy.argwhere(missing)
        tempResult = unique(series[missing], return_inverse=True)
	
===================================================================	
_intern: 163	
----------------------------	

values = numpy.asarray(values)
tempResult = unique(values)
	
===================================================================	
_bucketpanel_by: 130	
----------------------------	

xby = xby.reindex(series.index)
yby = yby.reindex(series.index)
xlabels = _bucket_labels(xby.reindex(series.index), xbins)
ylabels = _bucket_labels(yby.reindex(series.index), ybins)
labels = _uniquify(xlabels, ylabels, xbins, ybins)
mask = pandas.core.common.isnull(labels)
labels[mask] = (- 1)
tempResult = unique(labels)
	
===================================================================	
bucketcat: 97	
----------------------------	

"\n    Produce DataFrame representing quantiles of a Series\n\n    Parameters\n    ----------\n    series : Series\n    cat : Series or same-length array\n        bucket by category; mutually exclusive with 'by'\n\n    Returns\n    -------\n    DataFrame\n    "
if (not isinstance(series, Series)):
    series = Series(series, index=numpy.arange(len(series)))
cats = numpy.asarray(cats)
tempResult = unique(cats)
	
===================================================================	
_bucketpanel_cat: 151	
----------------------------	

(xlabels, xmapping) = _intern(xcat)
(ylabels, ymapping) = _intern(ycat)
shift = (10 ** numpy.ceil(numpy.log10(ylabels.max())))
labels = ((xlabels * shift) + ylabels)
sorter = labels.argsort()
sorted_labels = labels.take(sorter)
sorted_xlabels = xlabels.take(sorter)
sorted_ylabels = ylabels.take(sorter)
tempResult = unique(labels)
	
===================================================================	
test_unique_label_indices: 658	
----------------------------	

from pandas.hashtable import unique_label_indices
a = np.random.randint(1, (1 << 10), (1 << 15)).astype('i8')
left = unique_label_indices(a)
tempResult = unique(a, return_index=True)
	
===================================================================	
test_unique_label_indices: 662	
----------------------------	

from pandas.hashtable import unique_label_indices
a = np.random.randint(1, (1 << 10), (1 << 15)).astype('i8')
left = unique_label_indices(a)
right = numpy.unique(a, return_index=True)[1]
pandas.util.testing.assert_numpy_array_equal(left, right, check_dtype=False)
a[numpy.random.choice(len(a), 10)] = (- 1)
left = unique_label_indices(a)
tempResult = unique(a, return_index=True)
	
===================================================================	
TestIndexOps.test_value_counts_unique_nunique: 366	
----------------------------	

for orig in self.objs:
    o = orig.copy()
    klass = type(o)
    values = o._values
    if isinstance(values, Index):
        values.name = None
    if (isinstance(o, Index) and o.is_boolean()):
        continue
    elif isinstance(o, Index):
        expected_index = pandas.Index(o[::(- 1)])
        expected_index.name = None
        o = o.repeat(range(1, (len(o) + 1)))
        o.name = 'a'
    else:
        expected_index = pandas.Index(values[::(- 1)])
        idx = o.index.repeat(range(1, (len(o) + 1)))
        rep = numpy.repeat(values, range(1, (len(o) + 1)))
        o = klass(rep, index=idx, name='a')
    self.assertEqual(o.dtype, orig.dtype)
    expected_s = Series(range(10, 0, (- 1)), index=expected_index, dtype='int64', name='a')
    result = o.value_counts()
    pandas.util.testing.assert_series_equal(result, expected_s)
    self.assertTrue((result.index.name is None))
    self.assertEqual(result.name, 'a')
    result = o.unique()
    if isinstance(o, Index):
        self.assertTrue(isinstance(result, o.__class__))
        self.assert_index_equal(result, orig)
    elif is_datetimetz(o):
        self.assertEqual(result[0], orig[0])
        for r in result:
            self.assertIsInstance(r, pandas.Timestamp)
        pandas.util.testing.assert_numpy_array_equal(result, orig._values.asobject.values)
    else:
        pandas.util.testing.assert_numpy_array_equal(result, orig.values)
    tempResult = unique(o.values)
	
===================================================================	
TestIndexOps.test_value_counts_inferred: 443	
----------------------------	

klasses = [Index, Series]
for klass in klasses:
    s_values = ['a', 'b', 'b', 'b', 'b', 'c', 'd', 'd', 'a', 'a']
    s = klass(s_values)
    expected = Series([4, 3, 2, 1], index=['b', 'a', 'd', 'c'])
    pandas.util.testing.assert_series_equal(s.value_counts(), expected)
    if isinstance(s, Index):
        tempResult = unique(numpy.array(s_values, dtype=numpy.object_))
	
===================================================================	
TestIndexOps.test_value_counts_inferred: 446	
----------------------------	

klasses = [Index, Series]
for klass in klasses:
    s_values = ['a', 'b', 'b', 'b', 'b', 'c', 'd', 'd', 'a', 'a']
    s = klass(s_values)
    expected = Series([4, 3, 2, 1], index=['b', 'a', 'd', 'c'])
    pandas.util.testing.assert_series_equal(s.value_counts(), expected)
    if isinstance(s, Index):
        exp = Index(numpy.unique(numpy.array(s_values, dtype=numpy.object_)))
        pandas.util.testing.assert_index_equal(s.unique(), exp)
    else:
        tempResult = unique(numpy.array(s_values, dtype=numpy.object_))
	
===================================================================	
TestCategoricalAsBlock.cmp: 2582	
----------------------------	

tempResult = unique(a)
	
===================================================================	
TestCategoricalAsBlock.cmp: 2582	
----------------------------	

tempResult = unique(b)
	
===================================================================	
TestSeriesMisc.test_numpy_unique: 218	
----------------------------	

tempResult = unique(self.ts)
	
===================================================================	
TestCut.test_qcut_bounds: 127	
----------------------------	

arr = numpy.random.randn(1000)
factor = qcut(arr, 10, labels=False)
tempResult = unique(factor)
	
===================================================================	
_get_index_freq: 167	
----------------------------	

freq = getattr(data.index, 'freq', None)
if (freq is None):
    freq = getattr(data.index, 'inferred_freq', None)
    if (freq == 'B'):
        tempResult = unique(data.index.dayofweek)
	
===================================================================	
_gen_unique_rand: 969	
----------------------------	

ind = rng.rand(int(_extra_size))
tempResult = unique(numpy.floor(((ind * nrows) * ncols)))
	
***************************************************	
dask_dask-0.7.0: 4	
===================================================================	
insert: 980	
----------------------------	

if (not ((- arr.ndim) <= axis < arr.ndim)):
    raise IndexError(('axis %r is out of bounds for an array of dimension %s' % (axis, arr.ndim)))
if (axis < 0):
    axis += arr.ndim
if isinstance(obj, slice):
    obj = numpy.arange(*obj.indices(arr.shape[axis]))
obj = numpy.asarray(obj)
scalar_obj = (obj.ndim == 0)
if scalar_obj:
    obj = numpy.atleast_1d(obj)
obj = numpy.where((obj < 0), (obj + arr.shape[axis]), obj)
if (np.diff(obj) < 0).any():
    raise NotImplementedError('da.insert only implemented for monotonic ``obj`` argument')
tempResult = unique(obj)
	
===================================================================	
unique: 1037	
----------------------------	

name = ('unique-' + x.name)
dsk = dict((((name, i), (numpy.unique, key)) for (i, key) in enumerate(x._keys())))
parts = Array._get(merge(dsk, x.dask), list(dsk.keys()))
tempResult = unique(numpy.concatenate(parts))
	
===================================================================	
unique: 28	
----------------------------	

" Polymorphic unique function\n\n    >>> list(unique([1, 2, 3, 1, 2, 3]))\n    [1, 2, 3]\n\n    >>> unique(np.array([1, 2, 3, 1, 2, 3]))\n    array([1, 2, 3])\n\n    >>> unique(pd.Categorical(['Alice', 'Bob', 'Alice'], ordered=False))\n    [Alice, Bob]\n    Categories (2, object): [Alice, Bob]\n    "
if isinstance(divisions, numpy.ndarray):
    tempResult = unique(divisions)
	
===================================================================	
unique: 30	
----------------------------	

" Polymorphic unique function\n\n    >>> list(unique([1, 2, 3, 1, 2, 3]))\n    [1, 2, 3]\n\n    >>> unique(np.array([1, 2, 3, 1, 2, 3]))\n    array([1, 2, 3])\n\n    >>> unique(pd.Categorical(['Alice', 'Bob', 'Alice'], ordered=False))\n    [Alice, Bob]\n    Categories (2, object): [Alice, Bob]\n    "
if isinstance(divisions, numpy.ndarray):
    return numpy.unique(divisions)
if isinstance(divisions, pandas.Categorical):
    tempResult = unique(divisions.codes)
	
***************************************************	
nengo_nengo-2.0.0: 3	
===================================================================	
TransformParam.validate: 88	
----------------------------	

transform = numpy.asarray(transform, dtype=numpy.float64)
if (transform.ndim == 0):
    self.shape = ()
elif (transform.ndim == 1):
    self.shape = ('size_out',)
elif (transform.ndim == 2):
    self.shape = ('size_out', '*')
else:
    raise ValueError('Cannot handle transforms with dimensions > 2')
super(TransformParam, self).validate(conn, transform)
if (transform.ndim == 2):
    tempResult = unique(x)
	
===================================================================	
test_zerofilter: 361	
----------------------------	

m = nengo.Network(seed=seed)
with m:
    a = nengo.Ensemble(1, dimensions=1, neuron_type=nengo.Direct())
    nengo.Connection(a, a, synapse=0)
    b = nengo.Ensemble(3, dimensions=1, intercepts=[(- 0.9), (- 0.8), (- 0.7)], neuron_type=nengo.LIF())
    bp = nengo.Probe(b.neurons)
sim = Simulator(m)
sim.run(1.0)
tempResult = unique(sim.data[bp])
	
===================================================================	
full_transform: 26	
----------------------------	

'Compute the full transform for a connection.\n\n    Parameters\n    ----------\n    conn : Connection\n        The connection for which to compute the full transform.\n    slice_pre : boolean, optional (True)\n        Whether to compute the pre slice as part of the transform.\n    slice_post : boolean, optional (True)\n        Whether to compute the post slice as part of the transform.\n    allow_scalars : boolean, optional (True)\n        If true (default), will not make scalars into full transforms when\n        not using slicing, since these work fine in the reference builder.\n        If false, these scalars will be turned into scaled identity matrices.\n    '
transform = conn.transform
pre_slice = (conn.pre_slice if slice_pre else slice(None))
post_slice = (conn.post_slice if slice_post else slice(None))
if ((pre_slice == slice(None)) and (post_slice == slice(None))):
    if (transform.ndim == 2):
        return numpy.array(transform)
    elif ((transform.size == 1) and allow_scalars):
        return numpy.array(transform)
func_size = conn.function_info.size
size_in = ((conn.pre_obj.size_out if (func_size is None) else func_size) if slice_pre else conn.size_mid)
size_out = (conn.post_obj.size_in if slice_post else conn.size_out)
new_transform = numpy.zeros((size_out, size_in))
if (transform.ndim < 2):
    new_transform[(numpy.arange(size_out)[post_slice], numpy.arange(size_in)[pre_slice])] = transform
    return new_transform
elif (transform.ndim == 2):
    tempResult = unique(x)
	
***************************************************	
sympy_sympy-1.0.0: 0	
***************************************************	
daducci_amico-dev: 1	
===================================================================	
Scheme.load_from_table: 49	
----------------------------	

'Build the structure from an input matrix.\n\n        The first three columns represent the gradient directions.\n        Then, we accept two formats to describe each gradient:\n            - if the shape of data is Nx4, the 4^ column is the b-value;\n            - if the shape of data is Nx7, the last 4 columns are, respectively, the gradient strength, big delta, small delta and TE.\n\n        Parameters\n        ----------\n        data : numpy.ndarray\n            Matrix containing tall the values.\n        b0_thr : float\n            The threshold on the b-values to identify the b0 images (default: 0)\n        '
if (data.ndim == 1):
    data = numpy.expand_dims(data, axis=0)
self.raw = data
self.nS = self.raw.shape[0]
if (self.raw.shape[1] == 4):
    self.version = 0
    self.b = self.raw[:, 3]
elif (self.raw.shape[1] == 7):
    self.version = 1
    self.b = (((((267513000.0 * self.raw[:, 3]) * self.raw[:, 5]) ** 2) * (self.raw[:, 4] - (self.raw[:, 5] / 3.0))) * 1e-06)
else:
    raise ValueError('Unrecognized scheme format')
self.b0_thr = b0_thr
self.b0_idx = numpy.where((self.b <= b0_thr))[0]
self.b0_count = len(self.b0_idx)
self.dwi_idx = numpy.where((self.b > b0_thr))[0]
self.dwi_count = len(self.dwi_idx)
idx = numpy.where((self.raw[:, 1] < 0))[0]
self.raw[idx, 0:3] = (- self.raw[idx, 0:3])
self.shells = []
tmp = numpy.ascontiguousarray(self.raw[:, 3:])
tempResult = unique(tmp.view(([('', tmp.dtype)] * tmp.shape[1])), return_index=True)
	
***************************************************	
aplpy_aplpy-1.1.1: 4	
===================================================================	
Grid._update_norefresh: 164	
----------------------------	

if (not self._active):
    return self.ax
if (len(args) == 1):
    if (id(self.ax) != id(args[0])):
        raise Exception('ax ids should match')
lines = []
if self.x_auto_spacing:
    if self.ax.xaxis.apl_auto_tick_spacing:
        xspacing = default_spacing(self.ax, 'x', self.ax.xaxis.apl_label_form)
    else:
        xspacing = self.ax.xaxis.apl_tick_spacing
else:
    xspacing = self.x_grid_spacing
if (xspacing is None):
    warnings.warn('Could not determine x tick spacing - grid cannot be drawn')
    return
if (self._wcs.xaxis_coord_type in ['longitude', 'latitude']):
    xspacing = xspacing.todegrees()
if self.y_auto_spacing:
    if self.ax.yaxis.apl_auto_tick_spacing:
        yspacing = default_spacing(self.ax, 'y', self.ax.yaxis.apl_label_form)
    else:
        yspacing = self.ax.yaxis.apl_tick_spacing
else:
    yspacing = self.y_grid_spacing
if (yspacing is None):
    warnings.warn('Could not determine y tick spacing - grid cannot be drawn')
    return
if (self._wcs.yaxis_coord_type in ['longitude', 'latitude']):
    yspacing = yspacing.todegrees()
(grid_x_i, grid_y_i) = find_intersections(self.ax, 'x', xspacing)
if (self._wcs.xaxis_coord_type == 'longitude'):
    grid_x_i = numpy.mod(grid_x_i, 360.0)
elif (self._wcs.xaxis_coord_type == 'latitude'):
    grid_x_i = (numpy.mod((grid_x_i + 90.0), 180.0) - 90.0)
if (self._wcs.yaxis_coord_type == 'longitude'):
    grid_y_i = numpy.mod(grid_y_i, 360.0)
elif (self._wcs.yaxis_coord_type == 'latitude'):
    grid_y_i = (numpy.mod((grid_y_i + 90.0), 180.0) - 90.0)
if ((self._wcs.xaxis_coord_type == 'latitude') and (self._wcs.yaxis_coord_type == 'longitude') and (len(grid_x_i) > 0)):
    gx = grid_x_i.min()
    while True:
        gx -= xspacing
        (xpix, ypix) = wcs_util.world2pix(self._wcs, gx, 0.0)
        if (in_plot(self.ax, xpix, ypix) and (gx >= (- 90.0))):
            grid_x_i = numpy.hstack([grid_x_i, gx, gx])
            grid_y_i = numpy.hstack([grid_y_i, 0.0, 360.0])
        else:
            break
    gx = grid_x_i.max()
    while True:
        gx += xspacing
        (xpix, ypix) = wcs_util.world2pix(self._wcs, gx, 0.0)
        if (in_plot(self.ax, xpix, ypix) and (gx <= (+ 90.0))):
            grid_x_i = numpy.hstack([grid_x_i, gx, gx])
            grid_y_i = numpy.hstack([grid_y_i, 0.0, 360.0])
        else:
            break
tempResult = unique(grid_x_i)
	
===================================================================	
Grid._update_norefresh: 195	
----------------------------	

if (not self._active):
    return self.ax
if (len(args) == 1):
    if (id(self.ax) != id(args[0])):
        raise Exception('ax ids should match')
lines = []
if self.x_auto_spacing:
    if self.ax.xaxis.apl_auto_tick_spacing:
        xspacing = default_spacing(self.ax, 'x', self.ax.xaxis.apl_label_form)
    else:
        xspacing = self.ax.xaxis.apl_tick_spacing
else:
    xspacing = self.x_grid_spacing
if (xspacing is None):
    warnings.warn('Could not determine x tick spacing - grid cannot be drawn')
    return
if (self._wcs.xaxis_coord_type in ['longitude', 'latitude']):
    xspacing = xspacing.todegrees()
if self.y_auto_spacing:
    if self.ax.yaxis.apl_auto_tick_spacing:
        yspacing = default_spacing(self.ax, 'y', self.ax.yaxis.apl_label_form)
    else:
        yspacing = self.ax.yaxis.apl_tick_spacing
else:
    yspacing = self.y_grid_spacing
if (yspacing is None):
    warnings.warn('Could not determine y tick spacing - grid cannot be drawn')
    return
if (self._wcs.yaxis_coord_type in ['longitude', 'latitude']):
    yspacing = yspacing.todegrees()
(grid_x_i, grid_y_i) = find_intersections(self.ax, 'x', xspacing)
if (self._wcs.xaxis_coord_type == 'longitude'):
    grid_x_i = numpy.mod(grid_x_i, 360.0)
elif (self._wcs.xaxis_coord_type == 'latitude'):
    grid_x_i = (numpy.mod((grid_x_i + 90.0), 180.0) - 90.0)
if (self._wcs.yaxis_coord_type == 'longitude'):
    grid_y_i = numpy.mod(grid_y_i, 360.0)
elif (self._wcs.yaxis_coord_type == 'latitude'):
    grid_y_i = (numpy.mod((grid_y_i + 90.0), 180.0) - 90.0)
if ((self._wcs.xaxis_coord_type == 'latitude') and (self._wcs.yaxis_coord_type == 'longitude') and (len(grid_x_i) > 0)):
    gx = grid_x_i.min()
    while True:
        gx -= xspacing
        (xpix, ypix) = wcs_util.world2pix(self._wcs, gx, 0.0)
        if (in_plot(self.ax, xpix, ypix) and (gx >= (- 90.0))):
            grid_x_i = numpy.hstack([grid_x_i, gx, gx])
            grid_y_i = numpy.hstack([grid_y_i, 0.0, 360.0])
        else:
            break
    gx = grid_x_i.max()
    while True:
        gx += xspacing
        (xpix, ypix) = wcs_util.world2pix(self._wcs, gx, 0.0)
        if (in_plot(self.ax, xpix, ypix) and (gx <= (+ 90.0))):
            grid_x_i = numpy.hstack([grid_x_i, gx, gx])
            grid_y_i = numpy.hstack([grid_y_i, 0.0, 360.0])
        else:
            break
for gx in numpy.unique(grid_x_i):
    for line in plot_grid_x(self.ax, grid_x_i, grid_y_i, gx):
        lines.append(line)
(grid_x_i, grid_y_i) = find_intersections(self.ax, 'y', yspacing)
if (self._wcs.xaxis_coord_type == 'longitude'):
    grid_x_i = numpy.mod(grid_x_i, 360.0)
elif (self._wcs.xaxis_coord_type == 'latitude'):
    grid_x_i = (numpy.mod((grid_x_i + 90.0), 180.0) - 90.0)
if (self._wcs.yaxis_coord_type == 'longitude'):
    grid_y_i = numpy.mod(grid_y_i, 360.0)
elif (self._wcs.yaxis_coord_type == 'latitude'):
    grid_y_i = (numpy.mod((grid_y_i + 90.0), 180.0) - 90.0)
if ((self._wcs.xaxis_coord_type == 'longitude') and (self._wcs.yaxis_coord_type == 'latitude') and (len(grid_y_i) > 0)):
    gy = grid_y_i.min()
    while True:
        gy -= yspacing
        (xpix, ypix) = wcs_util.world2pix(self._wcs, 0.0, gy)
        if (in_plot(self.ax, xpix, ypix) and (gy >= (- 90.0))):
            grid_x_i = numpy.hstack([grid_x_i, 0.0, 360.0])
            grid_y_i = numpy.hstack([grid_y_i, gy, gy])
        else:
            break
    gy = grid_y_i.max()
    while True:
        gy += yspacing
        (xpix, ypix) = wcs_util.world2pix(self._wcs, 0.0, gy)
        if (in_plot(self.ax, xpix, ypix) and (gy <= (+ 90.0))):
            grid_x_i = numpy.hstack([grid_x_i, 0.0, 360.0])
            grid_y_i = numpy.hstack([grid_y_i, gy, gy])
        else:
            break
tempResult = unique(grid_y_i)
	
===================================================================	
default_spacing: 285	
----------------------------	

wcs = ax._wcs
(xmin, xmax) = ax.xaxis.get_view_interval()
(ymin, ymax) = ax.yaxis.get_view_interval()
(px, py, wx, wy) = axis_positions(wcs, coord, False, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)
keep = ((~ numpy.isnan(wx)) & (~ numpy.isnan(wy)))
if (numpy.sum(keep) == 0):
    return None
else:
    px = px[keep]
    py = py[keep]
    wx = wx[keep]
    wy = wy[keep]
coord_type = (wcs.xaxis_coord_type if (coord == 'x') else wcs.yaxis_coord_type)
if (coord == 'x'):
    tempResult = unique(wx)
	
===================================================================	
default_spacing: 300	
----------------------------	

wcs = ax._wcs
(xmin, xmax) = ax.xaxis.get_view_interval()
(ymin, ymax) = ax.yaxis.get_view_interval()
(px, py, wx, wy) = axis_positions(wcs, coord, False, xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax)
keep = ((~ numpy.isnan(wx)) & (~ numpy.isnan(wy)))
if (numpy.sum(keep) == 0):
    return None
else:
    px = px[keep]
    py = py[keep]
    wx = wx[keep]
    wy = wy[keep]
coord_type = (wcs.xaxis_coord_type if (coord == 'x') else wcs.yaxis_coord_type)
if (coord == 'x'):
    if ((len(wx) > 1) and (len(numpy.unique(wx)) == 1)):
        return None
    if (coord_type in ['longitude', 'latitude']):
        if (coord_type == 'longitude'):
            (wxmin, wxmax) = math_util.smart_range(wx)
        else:
            (wxmin, wxmax) = (min(wx), max(wx))
        if ('d.' in format):
            spacing = angle_util.smart_round_angle_decimal(((wxmax - wxmin) / 5.0), latitude=(coord_type == 'latitude'))
        else:
            spacing = angle_util.smart_round_angle_sexagesimal(((wxmax - wxmin) / 5.0), latitude=(coord_type == 'latitude'), hours=('hh' in format))
    else:
        (wxmin, wxmax) = (numpy.min(wx), numpy.max(wx))
        spacing = scalar_util.smart_round_angle_decimal(((wxmax - wxmin) / 5.0))
else:
    tempResult = unique(wy)
	
***************************************************	
markovmodel_msmtools-1.0.2: 2	
===================================================================	
forward_committor_sensitivity: 13	
----------------------------	

' \n    calculate the sensitivity matrix for index of the forward committor from A to B given transition matrix T.\n    Parameters\n    ----------\n    T : numpy.ndarray shape = (n, n)\n        Transition matrix\n    A : array like\n        List of integer state labels for set A\n    B : array like\n        List of integer state labels for set B\n    index : entry of the committor for which the sensitivity is to be computed\n        \n    Returns\n    -------\n    x : ndarray, shape=(n, n)\n        Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.\n    '
n = len(T)
set_X = numpy.arange(n)
tempResult = unique(A)
	
===================================================================	
forward_committor_sensitivity: 14	
----------------------------	

' \n    calculate the sensitivity matrix for index of the forward committor from A to B given transition matrix T.\n    Parameters\n    ----------\n    T : numpy.ndarray shape = (n, n)\n        Transition matrix\n    A : array like\n        List of integer state labels for set A\n    B : array like\n        List of integer state labels for set B\n    index : entry of the committor for which the sensitivity is to be computed\n        \n    Returns\n    -------\n    x : ndarray, shape=(n, n)\n        Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.\n    '
n = len(T)
set_X = numpy.arange(n)
set_A = numpy.unique(A)
tempResult = unique(B)
	
***************************************************	
nilearn_nilearn-0.4.0: 26	
===================================================================	
_load_mask_img: 22	
----------------------------	

'Check that a mask is valid, ie with two values including 0 and load it.\n\n    Parameters\n    ----------\n    mask_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        The mask to check\n\n    allow_empty: boolean, optional\n        Allow loading an empty mask (full of 0 values)\n\n    Returns\n    -------\n    mask: numpy.ndarray\n        boolean version of the mask\n    '
mask_img = _utils.check_niimg_3d(mask_img)
mask = mask_img.get_data()
tempResult = unique(mask)
	
===================================================================	
clean: 169	
----------------------------	

'Improve SNR on masked fMRI signals.\n\n    This function can do several things on the input signals, in\n    the following order:\n\n    - detrend\n    - standardize\n    - remove confounds\n    - low- and high-pass filter\n\n    Low-pass filtering improves specificity.\n\n    High-pass filtering should be kept small, to keep some\n    sensitivity.\n\n    Filtering is only meaningful on evenly-sampled signals.\n\n    Parameters\n    ----------\n    signals: numpy.ndarray\n        Timeseries. Must have shape (instant number, features number).\n        This array is not modified.\n\n    sessions : numpy array, optional\n        Add a session level to the cleaning process. Each session will be\n        cleaned independently. Must be a 1D array of n_samples elements.\n\n    confounds: numpy.ndarray, str or list of\n        Confounds timeseries. Shape must be\n        (instant number, confound number), or just (instant number,)\n        The number of time instants in signals and confounds must be\n        identical (i.e. signals.shape[0] == confounds.shape[0]).\n        If a string is provided, it is assumed to be the name of a csv file\n        containing signals as columns, with an optional one-line header.\n        If a list is provided, all confounds are removed from the input\n        signal, as if all were in the same array.\n\n    t_r: float\n        Repetition time, in second (sampling period).\n\n    low_pass, high_pass: float\n        Respectively low and high cutoff frequencies, in Hertz.\n\n    detrend: bool\n        If detrending should be applied on timeseries (before\n        confound removal)\n\n    standardize: bool\n        If True, returned signals are set to unit variance.\n\n    ensure_finite: bool\n        If True, the non-finite values (NANs and infs) found in the data\n        will be replaced by zeros.\n\n    Returns\n    -------\n    cleaned_signals: numpy.ndarray\n        Input signals, cleaned. Same shape as `signals`.\n\n    Notes\n    -----\n    Confounds removal is based on a projection on the orthogonal\n    of the signal space. See `Friston, K. J., A. P. Holmes,\n    K. J. Worsley, J.-P. Poline, C. D. Frith, et R. S. J. Frackowiak.\n    "Statistical Parametric Maps in Functional Imaging: A General\n    Linear Approach". Human Brain Mapping 2, no 4 (1994): 189-210.\n    <http://dx.doi.org/10.1002/hbm.460020402>`_\n\n    See Also\n    --------\n        nilearn.image.clean_img\n    '
if isinstance(low_pass, bool):
    raise TypeError("low pass must be float or None but you provided low_pass='{0}'".format(low_pass))
if isinstance(high_pass, bool):
    raise TypeError("high pass must be float or None but you provided high_pass='{0}'".format(high_pass))
if (not isinstance(confounds, (list, tuple, _basestring, numpy.ndarray, type(None)))):
    raise TypeError(('confounds keyword has an unhandled type: %s' % confounds.__class__))
if (not isinstance(ensure_finite, bool)):
    raise ValueError("'ensure_finite' must be boolean type True or False but you provided ensure_finite={0}".format(ensure_finite))
if (not isinstance(signals, numpy.ndarray)):
    signals = as_ndarray(signals)
if ensure_finite:
    signals[numpy.logical_not(numpy.isfinite(signals))] = 0
if (confounds is not None):
    if (not isinstance(confounds, (list, tuple))):
        confounds = (confounds,)
    all_confounds = []
    for confound in confounds:
        if isinstance(confound, _basestring):
            filename = confound
            confound = csv_to_array(filename)
            if numpy.isnan(confound.flat[0]):
                if (NP_VERSION >= [1, 4, 0]):
                    confound = csv_to_array(filename, skip_header=1)
                else:
                    confound = csv_to_array(filename, skiprows=1)
            if (confound.shape[0] != signals.shape[0]):
                raise ValueError('Confound signal has an incorrect length')
        elif isinstance(confound, numpy.ndarray):
            if (confound.ndim == 1):
                confound = np.atleast_2d(confound).T
            elif (confound.ndim != 2):
                raise ValueError(('confound array has an incorrect number of dimensions: %d' % confound.ndim))
            if (confound.shape[0] != signals.shape[0]):
                raise ValueError('Confound signal has an incorrect length')
        else:
            raise TypeError(('confound has an unhandled type: %s' % confound.__class__))
        all_confounds.append(confound)
    confounds = numpy.hstack(all_confounds)
    del all_confounds
if (sessions is not None):
    if (not (len(sessions) == len(signals))):
        raise ValueError(('The length of the session vector (%i) does not match the length of the signals (%i)' % (len(sessions), len(signals))))
    tempResult = unique(sessions)
	
===================================================================	
fetch_atlas_harvard_oxford: 87	
----------------------------	

'Load Harvard-Oxford parcellations from FSL.\n\n    This function downloads Harvard Oxford atlas packaged from FSL 5.0\n    and stores atlases in NILEARN_DATA folder in home directory.\n\n    This function can also load Harvard Oxford atlas from your local directory\n    specified by your FSL installed path given in `data_dir` argument.\n    See documentation for details.\n\n    Parameters\n    ----------\n    atlas_name: string\n        Name of atlas to load. Can be:\n        cort-maxprob-thr0-1mm,  cort-maxprob-thr0-2mm,\n        cort-maxprob-thr25-1mm, cort-maxprob-thr25-2mm,\n        cort-maxprob-thr50-1mm, cort-maxprob-thr50-2mm,\n        sub-maxprob-thr0-1mm,  sub-maxprob-thr0-2mm,\n        sub-maxprob-thr25-1mm, sub-maxprob-thr25-2mm,\n        sub-maxprob-thr50-1mm, sub-maxprob-thr50-2mm,\n        cort-prob-1mm, cort-prob-2mm,\n        sub-prob-1mm, sub-prob-2mm\n\n    data_dir: string, optional\n        Path of data directory where data will be stored. Optionally,\n        it can also be a FSL installation directory (which is dependent\n        on your installation).\n        Example, if FSL is installed in /usr/share/fsl/ then\n        specifying as \'/usr/share/\' can get you Harvard Oxford atlas\n        from your installed directory. Since we mimic same root directory\n        as FSL to load it easily from your installation.\n\n    symmetric_split: bool, optional, (default False).\n        If True, lateralized atlases of cort or sub with maxprob will be\n        returned. For subcortical types (sub-maxprob), we split every\n        symmetric region in left and right parts. Effectively doubles the\n        number of regions.\n        NOTE Not implemented for full probabilistic atlas (*-prob-* atlases).\n\n    Returns\n    -------\n    data: sklearn.datasets.base.Bunch\n        dictionary-like object, keys are:\n\n        - "maps": nibabel.Nifti1Image, 4D maps if a probabilistic atlas is\n          requested and 3D labels if a maximum probabilistic atlas was\n          requested.\n\n        - "labels": string list, labels of the regions in the atlas.\n    '
atlas_items = ('cort-maxprob-thr0-1mm', 'cort-maxprob-thr0-2mm', 'cort-maxprob-thr25-1mm', 'cort-maxprob-thr25-2mm', 'cort-maxprob-thr50-1mm', 'cort-maxprob-thr50-2mm', 'sub-maxprob-thr0-1mm', 'sub-maxprob-thr0-2mm', 'sub-maxprob-thr25-1mm', 'sub-maxprob-thr25-2mm', 'sub-maxprob-thr50-1mm', 'sub-maxprob-thr50-2mm', 'cort-prob-1mm', 'cort-prob-2mm', 'sub-prob-1mm', 'sub-prob-2mm')
if (atlas_name not in atlas_items):
    raise ValueError('Invalid atlas name: {0}. Please chose an atlas among:\n{1}'.format(atlas_name, '\n'.join(atlas_items)))
url = 'http://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz'
dataset_name = 'fsl'
data_dir = _get_dataset_dir(dataset_name, data_dir=data_dir, verbose=verbose)
opts = {'uncompress': True}
root = os.path.join('data', 'atlases')
if (atlas_name[0] == 'c'):
    if (('cort-maxprob' in atlas_name) and symmetric_split):
        split_name = atlas_name.split('cort')
        atlas_name = ('cortl' + split_name[1])
        label_file = 'HarvardOxford-Cortical-Lateralized.xml'
        lateralized = True
    else:
        label_file = 'HarvardOxford-Cortical.xml'
        lateralized = False
else:
    label_file = 'HarvardOxford-Subcortical.xml'
    lateralized = False
label_file = os.path.join(root, label_file)
atlas_file = os.path.join(root, 'HarvardOxford', (('HarvardOxford-' + atlas_name) + '.nii.gz'))
(atlas_img, label_file) = _fetch_files(data_dir, [(atlas_file, url, opts), (label_file, url, opts)], resume=resume, verbose=verbose)
names = {}
from xml.etree import ElementTree
names[0] = 'Background'
for label in ElementTree.parse(label_file).findall('.//label'):
    names[(int(label.get('index')) + 1)] = label.text
names = list(names.values())
if (not symmetric_split):
    return Bunch(maps=atlas_img, labels=names)
if (atlas_name in ('cort-prob-1mm', 'cort-prob-2mm', 'sub-prob-1mm', 'sub-prob-2mm')):
    raise ValueError('Region splitting not supported for probabilistic atlases')
atlas_img = check_niimg(atlas_img)
if lateralized:
    return Bunch(maps=atlas_img, labels=names)
atlas = atlas_img.get_data()
tempResult = unique(atlas)
	
===================================================================	
test_fetch_coords_dosenbach_2010: 249	
----------------------------	

bunch = nilearn.datasets.atlas.fetch_coords_dosenbach_2010()
assert_equal(len(bunch.rois), 160)
assert_equal(len(bunch.labels), 160)
tempResult = unique(bunch.networks)
	
===================================================================	
test_nifti_labels_masker_resampling: 106	
----------------------------	

shape1 = (10, 11, 12)
affine = numpy.eye(4)
shape2 = (16, 17, 18)
shape3 = (13, 14, 15)
n_regions = 9
length = 3
(fmri11_img, _) = generate_random_img(shape1, affine=affine, length=length)
(_, mask22_img) = generate_random_img(shape2, affine=affine, length=length)
labels33_img = nilearn._utils.testing.generate_labeled_regions(shape3, n_regions, affine=affine)
assert_raises(ValueError, NiftiLabelsMasker, labels33_img, resampling_target='mask')
assert_raises(ValueError, NiftiLabelsMasker, labels33_img, resampling_target='invalid')
masker = NiftiLabelsMasker(labels33_img, mask_img=mask22_img, resampling_target='labels')
masker.fit()
numpy.testing.assert_almost_equal(masker.labels_img_.affine, labels33_img.affine)
assert_equal(masker.labels_img_.shape, labels33_img.shape)
numpy.testing.assert_almost_equal(masker.mask_img_.affine, masker.labels_img_.affine)
assert_equal(masker.mask_img_.shape, masker.labels_img_.shape[:3])
transformed = masker.transform(fmri11_img)
assert_equal(transformed.shape, (length, n_regions))
fmri11_img_r = masker.inverse_transform(transformed)
numpy.testing.assert_almost_equal(fmri11_img_r.affine, masker.labels_img_.affine)
assert_equal(fmri11_img_r.shape, (masker.labels_img_.shape[:3] + (length,)))
shape1 = (10, 11, 12)
shape2 = (8, 9, 10)
shape3 = (16, 18, 20)
n_regions = 9
length = 21
(fmri11_img, _) = generate_random_img(shape1, affine=affine, length=length)
(_, mask22_img) = generate_random_img(shape2, affine=affine, length=length)
labels33_img = nilearn._utils.testing.generate_labeled_regions(shape3, n_regions, affine=affine)
masker = NiftiLabelsMasker(labels33_img, mask_img=mask22_img, resampling_target='labels')
masker.fit()
numpy.testing.assert_almost_equal(masker.labels_img_.affine, labels33_img.affine)
assert_equal(masker.labels_img_.shape, labels33_img.shape)
numpy.testing.assert_almost_equal(masker.mask_img_.affine, masker.labels_img_.affine)
assert_equal(masker.mask_img_.shape, masker.labels_img_.shape[:3])
tempResult = unique(masker.labels_img_.get_data())
	
===================================================================	
permuted_ols: 81	
----------------------------	

'Massively univariate group analysis with permuted OLS.\n\n    Tested variates are independently fitted to target variates descriptors\n    (e.g. brain imaging signal) according to a linear model solved with an\n    Ordinary Least Squares criterion.\n    Confounding variates may be included in the model.\n    Permutation testing is used to assess the significance of the relationship\n    between the tested variates and the target variates [1, 2]. A max-type\n    procedure is used to obtain family-wise corrected p-values.\n\n    The specific permutation scheme implemented here is the one of\n    Freedman & Lane [3]. Its has been demonstrated in [1] that this scheme\n    conveys more sensitivity than alternative schemes. This holds for\n    neuroimaging applications, as discussed in details in [2].\n\n    Permutations are performed on parallel computing units. Each of them\n    performs a fraction of permutations on the whole dataset. Thus, the max\n    t-score amongst data descriptors can be computed directly, which avoids\n    storing all the computed t-scores.\n\n    The variates should be given C-contiguous. target_vars are fortran-ordered\n    automatically to speed-up computations.\n\n    Parameters\n    ----------\n    tested_vars : array-like, shape=(n_samples, n_regressors)\n      Explanatory variates, fitted and tested independently from each others.\n\n    target_vars : array-like, shape=(n_samples, n_descriptors)\n      fMRI data, trying to be explained by explanatory and confounding\n      variates.\n\n    confounding_vars : array-like, shape=(n_samples, n_covars)\n      Confounding variates (covariates), fitted but not tested.\n      If None, no confounding variate is added to the model\n      (except maybe a constant column according to the value of\n      `model_intercept`)\n\n    model_intercept : bool,\n      If True, a constant column is added to the confounding variates\n      unless the tested variate is already the intercept.\n\n    n_perm : int,\n      Number of permutations to perform.\n      Permutations are costly but the more are performed, the more precision\n      one gets in the p-values estimation.\n\n    two_sided_test : boolean,\n      If True, performs an unsigned t-test. Both positive and negative\n      effects are considered; the null hypothesis is that the effect is zero.\n      If False, only positive effects are considered as relevant. The null\n      hypothesis is that the effect is zero or negative.\n\n    random_state : int or None,\n      Seed for random number generator, to have the same permutations\n      in each computing units.\n\n    n_jobs : int,\n      Number of parallel workers.\n      If 0 is provided, all CPUs are used.\n      A negative number indicates that all the CPUs except (abs(n_jobs) - 1)\n      ones will be used.\n\n    verbose: int, optional\n        verbosity level (0 means no message).\n\n    Returns\n    -------\n    pvals : array-like, shape=(n_regressors, n_descriptors)\n      Negative log10 p-values associated with the significance test of the\n      n_regressors explanatory variates against the n_descriptors target\n      variates. Family-wise corrected p-values.\n\n    score_orig_data : numpy.ndarray, shape=(n_regressors, n_descriptors)\n      t-statistic associated with the significance test of the n_regressors\n      explanatory variates against the n_descriptors target variates.\n      The ranks of the scores into the h0 distribution correspond to the\n      p-values.\n\n    h0_fmax : array-like, shape=(n_perm, )\n      Distribution of the (max) t-statistic under the null hypothesis\n      (obtained from the permutations). Array is sorted.\n\n    References\n    ----------\n    [1] Anderson, M. J. & Robinson, J. (2001).\n        Permutation tests for linear models.\n        Australian & New Zealand Journal of Statistics, 43(1), 75-88.\n    [2] Winkler, A. M. et al. (2014).\n        Permutation inference for the general linear model.\n        Neuroimage.\n    [3] Freedman, D. & Lane, D. (1983).\n        A nonstochastic interpretation of reported significance levels.\n        J. Bus. Econ. Stats., 1(4), 292-298\n\n    '
rng = check_random_state(random_state)
if (n_jobs == 0):
    raise ValueError("'n_jobs == 0' is not a valid choice. Please provide a positive number of CPUs, or -1 for all CPUs, or a negative number (-i) for 'all but (i-1)' CPUs (joblib conventions).")
elif (n_jobs < 0):
    n_jobs = max(1, ((sklearn.externals.joblib.cpu_count() - int(n_jobs)) + 1))
else:
    n_jobs = min(n_jobs, sklearn.externals.joblib.cpu_count())
if (target_vars.ndim != 2):
    raise ValueError(("'target_vars' should be a 2D array. An array with %d dimension%s was passed" % (target_vars.ndim, ('s' if (target_vars.ndim > 1) else ''))))
target_vars = numpy.asfortranarray(target_vars)
n_descriptors = target_vars.shape[1]
if (tested_vars.ndim == 1):
    tested_vars = np.atleast_2d(tested_vars).T
(n_samples, n_regressors) = tested_vars.shape
tempResult = unique(tested_vars)
	
===================================================================	
find_cut_slices: 134	
----------------------------	

' Find \'good\' cross-section slicing positions along a given axis.\n\n    Parameters\n    ----------\n    img: 3D Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        the brain map\n    direction: string, optional (default "z")\n        sectional direction; possible values are "x", "y", or "z"\n    n_cuts: int, optional (default 7)\n        number of cuts in the plot\n    spacing: \'auto\' or int, optional (default \'auto\')\n        minimum spacing between cuts (in voxels, not milimeters)\n        if \'auto\', the spacing is .5 / n_cuts * img_length\n\n    Returns\n    -------\n    cut_coords: 1D array of length n_cuts\n        the computed cut_coords\n\n    Notes\n    -----\n    This code works by iteratively locating peak activations that are\n    separated by a distance of at least \'spacing\'. If n_cuts is very\n    large and all the activated regions are covered, cuts with a spacing\n    less than \'spacing\' will be returned.\n    '
if (not (direction in 'xyz')):
    raise ValueError(("'direction' must be one of 'x', 'y', or 'z'. Got '%s'" % direction))
axis = 'xyz'.index(direction)
img = check_niimg_3d(img)
affine = img.affine
orig_data = numpy.abs(_safe_get_data(img))
this_shape = orig_data.shape[axis]
if (not isinstance(n_cuts, numbers.Number)):
    raise ValueError(('The number of cuts (n_cuts) must be an integer greater than or equal to 1. You provided a value of n_cuts=%s. ' % n_cuts))
if (n_cuts > this_shape):
    warnings.warn(('Too many cuts requested for the data: n_cuts=%i, data size=%i' % (n_cuts, this_shape)))
    return _transform_cut_coords(numpy.arange(this_shape), direction, affine)
data = orig_data.copy()
if (data.dtype.kind in ('i', 'u')):
    data = data.astype(numpy.float)
data = _smooth_array(data, affine, fwhm='fast')
epsilon = np.finfo(np.float32).eps
difference = abs((round(n_cuts) - n_cuts))
if ((round(n_cuts) < 1.0) or (difference > epsilon)):
    message = ('Image has %d slices in direction %s. Therefore, the number of cuts must be between 1 and %d. You provided n_cuts=%s ' % (this_shape, direction, this_shape, n_cuts))
    raise ValueError(message)
else:
    n_cuts = int(round(n_cuts))
if (spacing == 'auto'):
    spacing = max(int(((0.5 / n_cuts) * data.shape[axis])), 1)
slices = [slice(None, None), slice(None, None), slice(None, None)]
cut_coords = list()
for _ in range(n_cuts):
    max_along_axis = numpy.unravel_index(np.abs(data).argmax(), data.shape)[axis]
    start = max(0, (max_along_axis - spacing))
    stop = (max_along_axis + spacing)
    slices[axis] = slice(start, stop)
    data[slices] *= 0.001
    cut_coords.append(max_along_axis)
tempResult = unique(cut_coords)
	
===================================================================	
find_cut_slices: 156	
----------------------------	

' Find \'good\' cross-section slicing positions along a given axis.\n\n    Parameters\n    ----------\n    img: 3D Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        the brain map\n    direction: string, optional (default "z")\n        sectional direction; possible values are "x", "y", or "z"\n    n_cuts: int, optional (default 7)\n        number of cuts in the plot\n    spacing: \'auto\' or int, optional (default \'auto\')\n        minimum spacing between cuts (in voxels, not milimeters)\n        if \'auto\', the spacing is .5 / n_cuts * img_length\n\n    Returns\n    -------\n    cut_coords: 1D array of length n_cuts\n        the computed cut_coords\n\n    Notes\n    -----\n    This code works by iteratively locating peak activations that are\n    separated by a distance of at least \'spacing\'. If n_cuts is very\n    large and all the activated regions are covered, cuts with a spacing\n    less than \'spacing\' will be returned.\n    '
if (not (direction in 'xyz')):
    raise ValueError(("'direction' must be one of 'x', 'y', or 'z'. Got '%s'" % direction))
axis = 'xyz'.index(direction)
img = check_niimg_3d(img)
affine = img.affine
orig_data = numpy.abs(_safe_get_data(img))
this_shape = orig_data.shape[axis]
if (not isinstance(n_cuts, numbers.Number)):
    raise ValueError(('The number of cuts (n_cuts) must be an integer greater than or equal to 1. You provided a value of n_cuts=%s. ' % n_cuts))
if (n_cuts > this_shape):
    warnings.warn(('Too many cuts requested for the data: n_cuts=%i, data size=%i' % (n_cuts, this_shape)))
    return _transform_cut_coords(numpy.arange(this_shape), direction, affine)
data = orig_data.copy()
if (data.dtype.kind in ('i', 'u')):
    data = data.astype(numpy.float)
data = _smooth_array(data, affine, fwhm='fast')
epsilon = np.finfo(np.float32).eps
difference = abs((round(n_cuts) - n_cuts))
if ((round(n_cuts) < 1.0) or (difference > epsilon)):
    message = ('Image has %d slices in direction %s. Therefore, the number of cuts must be between 1 and %d. You provided n_cuts=%s ' % (this_shape, direction, this_shape, n_cuts))
    raise ValueError(message)
else:
    n_cuts = int(round(n_cuts))
if (spacing == 'auto'):
    spacing = max(int(((0.5 / n_cuts) * data.shape[axis])), 1)
slices = [slice(None, None), slice(None, None), slice(None, None)]
cut_coords = list()
for _ in range(n_cuts):
    max_along_axis = numpy.unravel_index(np.abs(data).argmax(), data.shape)[axis]
    start = max(0, (max_along_axis - spacing))
    stop = (max_along_axis + spacing)
    slices[axis] = slice(start, stop)
    data[slices] *= 0.001
    cut_coords.append(max_along_axis)
cut_coords = np.unique(cut_coords).tolist()
while (len(cut_coords) < n_cuts):
    slice_below = (min(cut_coords) - 2)
    slice_above = (max(cut_coords) + 2)
    candidates = [slice_above]
    if (len(cut_coords) > 1):
        middle_idx = numpy.argmax(numpy.diff(cut_coords))
        slice_middle = int((0.5 * (cut_coords[middle_idx] + cut_coords[(middle_idx + 1)])))
        if (not (slice_middle in cut_coords)):
            candidates.append(slice_middle)
    if (slice_below >= 0):
        candidates.append(slice_below)
    best_weight = (- 10)
    for candidate in candidates:
        if (candidate >= this_shape):
            this_weight = 0
        else:
            this_weight = numpy.sum(numpy.rollaxis(orig_data, axis)[candidate])
        if (this_weight > best_weight):
            best_candidate = candidate
            best_weight = this_weight
    cut_coords.append(best_candidate)
    tempResult = unique(cut_coords)
	
===================================================================	
_remove_small_regions: 37	
----------------------------	

'Remove small regions in volume from input_data of specified min_size.\n\n    min_size should be specified in mm^3 (region size in volume).\n\n    Parameters\n    ----------\n    input_data : numpy.ndarray\n        Values inside the regions defined by labels contained in input_data\n        are summed together to get the size and compare with given min_size.\n        For example, see scipy.ndimage.label\n\n    index : numpy.ndarray\n        A sequence of label numbers of the regions to be measured corresponding\n        to input_data. For example, sequence can be generated using\n        np.arange(n_labels + 1)\n\n    affine : numpy.ndarray\n        Affine of input_data is used to convert size in voxels to size in\n        volume of region in mm^3.\n\n    min_size : float in mm^3\n        Size of regions in input_data which falls below the specified min_size\n        of volume in mm^3 will be discarded.\n\n    Returns\n    -------\n    out : numpy.ndarray\n        Data returned will have regions removed specified by min_size\n        Otherwise, if criterion is not met then same input data will be\n        returned.\n    '
tempResult = unique(input_data, return_inverse=True)
	
===================================================================	
_remove_small_regions: 45	
----------------------------	

'Remove small regions in volume from input_data of specified min_size.\n\n    min_size should be specified in mm^3 (region size in volume).\n\n    Parameters\n    ----------\n    input_data : numpy.ndarray\n        Values inside the regions defined by labels contained in input_data\n        are summed together to get the size and compare with given min_size.\n        For example, see scipy.ndimage.label\n\n    index : numpy.ndarray\n        A sequence of label numbers of the regions to be measured corresponding\n        to input_data. For example, sequence can be generated using\n        np.arange(n_labels + 1)\n\n    affine : numpy.ndarray\n        Affine of input_data is used to convert size in voxels to size in\n        volume of region in mm^3.\n\n    min_size : float in mm^3\n        Size of regions in input_data which falls below the specified min_size\n        of volume in mm^3 will be discarded.\n\n    Returns\n    -------\n    out : numpy.ndarray\n        Data returned will have regions removed specified by min_size\n        Otherwise, if criterion is not met then same input data will be\n        returned.\n    '
(_, region_indices) = numpy.unique(input_data, return_inverse=True)
region_sizes = numpy.bincount(region_indices)
size_in_vox = (min_size / numpy.abs(numpy.linalg.det(affine[:3, :3])))
labels_kept = (region_sizes > size_in_vox)
if (not numpy.all(labels_kept)):
    rejected_labels_mask = np.in1d(input_data, np.where(np.logical_not(labels_kept))[0]).reshape(input_data.shape)
    input_data = input_data.copy()
    input_data[rejected_labels_mask] = 0
    tempResult = unique(input_data)
	
===================================================================	
connected_label_regions: 128	
----------------------------	

" Extract connected regions from a brain atlas image defined by labels\n    (integers).\n\n    For each label in an parcellations, separates out connected\n    components and assigns to each separated region a unique label.\n\n    Parameters\n    ----------\n\n    labels_img : Nifti-like image\n        A 3D image which contains regions denoted as labels. Each region\n        is assigned with integers.\n\n    min_size : float, in mm^3 optional (default None)\n        Minimum region size in volume required to keep after extraction.\n        Removes small or spurious regions.\n\n    connect_diag : bool (default True)\n        If 'connect_diag' is True, two voxels are considered in the same region\n        if they are connected along the diagonal (26-connectivity). If it is\n        False, two voxels are considered connected only if they are within the\n        same x, y, or z direction.\n\n    labels : 1D numpy array or list of str, (default None), optional\n        Each string in a list or array denote the name of the brain atlas\n        regions given in labels_img input. If provided, same names will be\n        re-assigned corresponding to each connected component based extraction\n        of regions relabelling. The total number of names should match with the\n        number of labels assigned in the image.\n\n        NOTE: The order of the names given in labels should be appropriately\n        matched with the unique labels (integers) assigned to each region\n        given in labels_img (also excluding 'Background' label).\n\n    Returns\n    -------\n    new_labels_img : Nifti-like image\n        A new image comprising of regions extracted on an input labels_img.\n\n    new_labels : list, optional\n        If labels are provided, new labels assigned to region extracted will\n        be returned. Otherwise, only new labels image will be returned.\n\n    See Also\n    --------\n    nilearn.datasets.fetch_atlas_harvard_oxford : For an example of atlas with\n        labels.\n\n    nilearn.regions.RegionExtractor : A class can be used for region extraction\n        on continuous type atlas images.\n\n    nilearn.regions.connected_regions : A function used for region extraction\n        on continuous type atlas images.\n\n    "
labels_img = check_niimg_3d(labels_img)
labels_data = _safe_get_data(labels_img, ensure_finite=True)
affine = labels_img.affine
tempResult = unique(labels_data)
	
===================================================================	
signals_to_img_labels: 57	
----------------------------	

'Create image from region signals defined as labels.\n\n    The same region signal is used for each voxel of the corresponding 3D\n    volume.\n\n    labels_img, mask_img must have the same shapes and affines.\n\n    Parameters\n    ----------\n    signals: numpy.ndarray\n        2D array with shape: (scan number, number of regions in labels_img)\n\n    labels_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        Region definitions using labels.\n\n    mask_img: Niimg-like object, optional\n        Boolean array giving voxels to process. integer arrays also accepted,\n        In this array, zero means False, non-zero means True.\n\n    background_label: number\n        label to use for "no region".\n\n    order: str\n        ordering of output array ("C" or "F"). Defaults to "F".\n\n    Returns\n    -------\n    img: nibabel.Nifti1Image\n        Reconstructed image. dtype is that of "signals", affine and shape are\n        those of labels_img.\n\n    See also\n    --------\n    nilearn.regions.img_to_signals_labels\n    nilearn.regions.signals_to_img_maps\n    '
labels_img = _utils.check_niimg_3d(labels_img)
signals = numpy.asarray(signals)
target_affine = labels_img.affine
target_shape = labels_img.shape[:3]
if (mask_img is not None):
    mask_img = _utils.check_niimg_3d(mask_img)
    if (mask_img.shape != target_shape):
        raise ValueError('mask_img and labels_img shapes must be identical.')
    if (abs((mask_img.affine - target_affine)).max() > 1e-09):
        raise ValueError('mask_img and labels_img affines must be identical')
labels_data = _safe_get_data(labels_img, ensure_finite=True)
tempResult = unique(labels_data)
	
===================================================================	
img_to_signals_labels: 27	
----------------------------	

'Extract region signals from image.\n\n    This function is applicable to regions defined by labels.\n\n    labels, imgs and mask shapes and affines must fit. This function\n    performs no resampling.\n\n    Parameters\n    ----------\n    imgs: 4D Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        input images.\n\n    labels_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        regions definition as labels. By default, the label zero is used to\n        denote an absence of region. Use background_label to change it.\n\n    mask_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        Mask to apply to labels before extracting signals. Every point\n        outside the mask is considered as background (i.e. no region).\n\n    background_label: number\n        number representing background in labels_img.\n\n    order: str\n        ordering of output array ("C" or "F"). Defaults to "F".\n\n    Returns\n    -------\n    signals: numpy.ndarray\n        Signals extracted from each region. One output signal is the mean\n        of all input signals in a given region. If some regions are entirely\n        outside the mask, the corresponding signal is zero.\n        Shape is: (scan number, number of regions)\n\n    labels: list or tuple\n        corresponding labels for each signal. signal[:, n] was extracted from\n        the region with label labels[n].\n\n    See also\n    --------\n    nilearn.regions.signals_to_img_labels\n    nilearn.regions.img_to_signals_maps\n    '
labels_img = _utils.check_niimg_3d(labels_img)
imgs = _utils.check_niimg_4d(imgs)
target_affine = imgs.affine
target_shape = imgs.shape[:3]
if (labels_img.shape != target_shape):
    raise ValueError('labels_img and imgs shapes must be identical.')
if (abs((labels_img.affine - target_affine)).max() > 1e-09):
    raise ValueError('labels_img and imgs affines must be identical')
if (mask_img is not None):
    mask_img = _utils.check_niimg_3d(mask_img)
    if (mask_img.shape != target_shape):
        raise ValueError('mask_img and imgs shapes must be identical.')
    if (abs((mask_img.affine - target_affine)).max() > 1e-09):
        raise ValueError('mask_img and imgs affines must be identical')
labels_data = _safe_get_data(labels_img, ensure_finite=True)
tempResult = unique(labels_data)
	
===================================================================	
img_to_signals_labels: 38	
----------------------------	

'Extract region signals from image.\n\n    This function is applicable to regions defined by labels.\n\n    labels, imgs and mask shapes and affines must fit. This function\n    performs no resampling.\n\n    Parameters\n    ----------\n    imgs: 4D Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        input images.\n\n    labels_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        regions definition as labels. By default, the label zero is used to\n        denote an absence of region. Use background_label to change it.\n\n    mask_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        Mask to apply to labels before extracting signals. Every point\n        outside the mask is considered as background (i.e. no region).\n\n    background_label: number\n        number representing background in labels_img.\n\n    order: str\n        ordering of output array ("C" or "F"). Defaults to "F".\n\n    Returns\n    -------\n    signals: numpy.ndarray\n        Signals extracted from each region. One output signal is the mean\n        of all input signals in a given region. If some regions are entirely\n        outside the mask, the corresponding signal is zero.\n        Shape is: (scan number, number of regions)\n\n    labels: list or tuple\n        corresponding labels for each signal. signal[:, n] was extracted from\n        the region with label labels[n].\n\n    See also\n    --------\n    nilearn.regions.signals_to_img_labels\n    nilearn.regions.img_to_signals_maps\n    '
labels_img = _utils.check_niimg_3d(labels_img)
imgs = _utils.check_niimg_4d(imgs)
target_affine = imgs.affine
target_shape = imgs.shape[:3]
if (labels_img.shape != target_shape):
    raise ValueError('labels_img and imgs shapes must be identical.')
if (abs((labels_img.affine - target_affine)).max() > 1e-09):
    raise ValueError('labels_img and imgs affines must be identical')
if (mask_img is not None):
    mask_img = _utils.check_niimg_3d(mask_img)
    if (mask_img.shape != target_shape):
        raise ValueError('mask_img and imgs shapes must be identical.')
    if (abs((mask_img.affine - target_affine)).max() > 1e-09):
        raise ValueError('mask_img and imgs affines must be identical')
labels_data = _safe_get_data(labels_img, ensure_finite=True)
labels = list(numpy.unique(labels_data))
if (background_label in labels):
    labels.remove(background_label)
if (mask_img is not None):
    mask_data = _safe_get_data(mask_img, ensure_finite=True)
    labels_data = labels_data.copy()
    labels_data[numpy.logical_not(mask_data)] = background_label
data = _safe_get_data(imgs)
signals = numpy.ndarray((data.shape[(- 1)], len(labels)), order=order)
for (n, img) in enumerate(numpy.rollaxis(data, (- 1))):
    signals[n] = numpy.asarray(scipy.ndimage.measurements.mean(img, labels=labels_data, index=labels))
tempResult = unique(labels_data)
	
===================================================================	
test_connected_label_regions: 141	
----------------------------	

shape = (13, 11, 12)
affine = numpy.eye(4)
n_regions = 9
labels_img = nilearn._utils.testing.generate_labeled_regions(shape, affine=affine, n_regions=n_regions)
labels_data = labels_img.get_data()
tempResult = unique(labels_data)
	
===================================================================	
test_connected_label_regions: 144	
----------------------------	

shape = (13, 11, 12)
affine = numpy.eye(4)
n_regions = 9
labels_img = nilearn._utils.testing.generate_labeled_regions(shape, affine=affine, n_regions=n_regions)
labels_data = labels_img.get_data()
n_labels_wo_reg_ext = len(numpy.unique(labels_data))
extracted_regions_on_labels_img = connected_label_regions(labels_img)
extracted_regions_labels_data = extracted_regions_on_labels_img.get_data()
tempResult = unique(extracted_regions_labels_data)
	
===================================================================	
test_connected_label_regions: 148	
----------------------------	

shape = (13, 11, 12)
affine = numpy.eye(4)
n_regions = 9
labels_img = nilearn._utils.testing.generate_labeled_regions(shape, affine=affine, n_regions=n_regions)
labels_data = labels_img.get_data()
n_labels_wo_reg_ext = len(numpy.unique(labels_data))
extracted_regions_on_labels_img = connected_label_regions(labels_img)
extracted_regions_labels_data = extracted_regions_on_labels_img.get_data()
n_labels_wo_min = len(numpy.unique(extracted_regions_labels_data))
assert_true((n_labels_wo_reg_ext < n_labels_wo_min))
extracted_regions_with_min = connected_label_regions(labels_img, min_size=100)
extracted_regions_with_min_data = extracted_regions_with_min.get_data()
tempResult = unique(extracted_regions_with_min_data)
	
===================================================================	
test_connected_label_regions: 152	
----------------------------	

shape = (13, 11, 12)
affine = numpy.eye(4)
n_regions = 9
labels_img = nilearn._utils.testing.generate_labeled_regions(shape, affine=affine, n_regions=n_regions)
labels_data = labels_img.get_data()
n_labels_wo_reg_ext = len(numpy.unique(labels_data))
extracted_regions_on_labels_img = connected_label_regions(labels_img)
extracted_regions_labels_data = extracted_regions_on_labels_img.get_data()
n_labels_wo_min = len(numpy.unique(extracted_regions_labels_data))
assert_true((n_labels_wo_reg_ext < n_labels_wo_min))
extracted_regions_with_min = connected_label_regions(labels_img, min_size=100)
extracted_regions_with_min_data = extracted_regions_with_min.get_data()
n_labels_with_min = len(numpy.unique(extracted_regions_with_min_data))
assert_true((n_labels_wo_min > n_labels_with_min))
ext_reg_without_connect_diag = connected_label_regions(labels_img, connect_diag=False)
data_wo_connect_diag = ext_reg_without_connect_diag.get_data()
tempResult = unique(data_wo_connect_diag)
	
===================================================================	
test_connected_label_regions: 155	
----------------------------	

shape = (13, 11, 12)
affine = numpy.eye(4)
n_regions = 9
labels_img = nilearn._utils.testing.generate_labeled_regions(shape, affine=affine, n_regions=n_regions)
labels_data = labels_img.get_data()
n_labels_wo_reg_ext = len(numpy.unique(labels_data))
extracted_regions_on_labels_img = connected_label_regions(labels_img)
extracted_regions_labels_data = extracted_regions_on_labels_img.get_data()
n_labels_wo_min = len(numpy.unique(extracted_regions_labels_data))
assert_true((n_labels_wo_reg_ext < n_labels_wo_min))
extracted_regions_with_min = connected_label_regions(labels_img, min_size=100)
extracted_regions_with_min_data = extracted_regions_with_min.get_data()
n_labels_with_min = len(numpy.unique(extracted_regions_with_min_data))
assert_true((n_labels_wo_min > n_labels_with_min))
ext_reg_without_connect_diag = connected_label_regions(labels_img, connect_diag=False)
data_wo_connect_diag = ext_reg_without_connect_diag.get_data()
n_labels_wo_connect_diag = len(numpy.unique(data_wo_connect_diag))
assert_true((n_labels_wo_connect_diag > n_labels_wo_reg_ext))
extract_reg_min_size_large = connected_label_regions(labels_img, min_size=500)
tempResult = unique(extract_reg_min_size_large.get_data())
	
===================================================================	
test_connected_label_regions: 164	
----------------------------	

shape = (13, 11, 12)
affine = numpy.eye(4)
n_regions = 9
labels_img = nilearn._utils.testing.generate_labeled_regions(shape, affine=affine, n_regions=n_regions)
labels_data = labels_img.get_data()
n_labels_wo_reg_ext = len(numpy.unique(labels_data))
extracted_regions_on_labels_img = connected_label_regions(labels_img)
extracted_regions_labels_data = extracted_regions_on_labels_img.get_data()
n_labels_wo_min = len(numpy.unique(extracted_regions_labels_data))
assert_true((n_labels_wo_reg_ext < n_labels_wo_min))
extracted_regions_with_min = connected_label_regions(labels_img, min_size=100)
extracted_regions_with_min_data = extracted_regions_with_min.get_data()
n_labels_with_min = len(numpy.unique(extracted_regions_with_min_data))
assert_true((n_labels_wo_min > n_labels_with_min))
ext_reg_without_connect_diag = connected_label_regions(labels_img, connect_diag=False)
data_wo_connect_diag = ext_reg_without_connect_diag.get_data()
n_labels_wo_connect_diag = len(numpy.unique(data_wo_connect_diag))
assert_true((n_labels_wo_connect_diag > n_labels_wo_reg_ext))
extract_reg_min_size_large = connected_label_regions(labels_img, min_size=500)
assert_true((numpy.unique(extract_reg_min_size_large.get_data()) == 0))
labels = ['region_a', 'region_b', 'region_c', 'region_d', 'region_e', 'region_f', 'region_g', 'region_h', 'region_i']
(extracted_reg, new_labels) = connected_label_regions(labels_img, min_size=100, labels=labels)
assert_not_equal(new_labels, '')
assert_true((len(new_labels) <= len(labels)))
labels = numpy.asarray(labels)
(extracted_reg2, new_labels2) = connected_label_regions(labels_img, labels=labels)
assert_not_equal(new_labels, '')
assert_true((len(new_labels2) >= len(labels)))
tempResult = unique(numpy.asarray(labels_img.get_data()))
	
===================================================================	
test_generate_labeled_regions: 38	
----------------------------	

'Minimal testing of generate_labeled_regions'
shape = (3, 4, 5)
n_regions = 10
regions = generate_labeled_regions(shape, n_regions)
assert_true((regions.shape == shape))
tempResult = unique(regions.get_data())
	
===================================================================	
_mask_edges_weights: 76	
----------------------------	

'\n    Remove edges of the graph connected to masked nodes, as well as\n    corresponding weights of the edges.\n    '
mask0 = numpy.hstack((mask[:, :, :(- 1)].ravel(), mask[:, :(- 1)].ravel(), mask[:(- 1)].ravel()))
mask1 = numpy.hstack((mask[:, :, 1:].ravel(), mask[:, 1:].ravel(), mask[1:].ravel()))
ind_mask = numpy.logical_and(mask0, mask1)
(edges, weights) = (edges[:, ind_mask], weights[ind_mask])
max_node_index = edges.max()
tempResult = unique(edges.ravel())
	
===================================================================	
_random_walker: 113	
----------------------------	

"Random walker algorithm for segmentation from markers.\n\n    Parameters\n    ----------\n    data : array_like\n        Image to be segmented in phases. Data spacing is assumed isotropic unless\n        the `spacing` keyword argument is used.\n    labels : array of ints, of same shape as `data` without channels dimension\n        Array of seed markers labeled with different positive integers\n        for different phases. Zero-labeled pixels are unlabeled pixels.\n        Negative labels correspond to inactive pixels that are not taken\n        into account (they are removed from the graph). If labels are not\n        consecutive integers, the labels array will be transformed so that\n        labels are consecutive.\n    beta : float\n        Penalization coefficient for the random walker motion\n        (the greater `beta`, the more difficult the diffusion).\n    tol : float\n        tolerance to achieve when solving the linear system, in\n        cg' mode.\n    copy : bool\n        If copy is False, the `labels` array will be overwritten with\n        the result of the segmentation. Use copy=False if you want to\n        save on memory.\n    spacing : iterable of floats\n        Spacing between voxels in each spatial dimension. If `None`, then\n        the spacing between pixels/voxels in each dimension is assumed 1.\n\n    Returns\n    -------\n    output : ndarray\n        * An array of ints of same shape as `data`, in which each pixel has\n          been labeled according to the marker that reached the pixel first\n          by anisotropic diffusion.\n\n    Notes\n    -----\n    The `spacing` argument is specifically for anisotropic datasets, where\n    data points are spaced differently in one or more spatial dimensions.\n    Anisotropic data is commonly encountered in medical imaging.\n\n    The algorithm was first proposed in *Random walks for image\n    segmentation*, Leo Grady, IEEE Trans Pattern Anal Mach Intell.\n    2006 Nov;28(11):1768-83.\n\n    The algorithm solves the diffusion equation at infinite times for\n    sources placed on markers of each phase in turn. A pixel is labeled with\n    the phase that has the greatest probability to diffuse first to the pixel.\n\n    The diffusion equation is solved by minimizing x.T L x for each phase,\n    where L is the Laplacian of the weighted graph of the image, and x is\n    the probability that a marker of the given phase arrives first at a pixel\n    by diffusion (x=1 on markers of the phase, x=0 on the other markers, and\n    the other coefficients are looked for). Each pixel is attributed the label\n    for which it has a maximal value of x. The Laplacian L of the image\n    is defined as:\n\n       - L_ii = d_i, the number of neighbors of pixel i (the degree of i)\n       - L_ij = -w_ij if i and j are adjacent pixels\n\n    The weight w_ij is a decreasing function of the norm of the local gradient.\n    This ensures that diffusion is easier between pixels of similar values.\n\n    When the Laplacian is decomposed into blocks of marked and unmarked\n    pixels::\n\n        L = M B.T\n            B A\n\n    with first indices corresponding to marked pixels, and then to unmarked\n    pixels, minimizing x.T L x for one phase amount to solving::\n\n        A x = - B x_m\n\n    where x_m = 1 on markers of the given phase, and 0 on other markers.\n    This linear system is solved in the algorithm using a direct method for\n    small images, and an iterative method for larger images.\n\n    "
if (labels != 0).all():
    warnings.warn('Random walker only segments unlabeled areas, where labels == 0. No zero valued areas in labels were found. Returning provided labels.')
    out_labels = labels
    return out_labels
multichannel = False
if (not multichannel):
    if ((data.ndim < 2) or (data.ndim > 3)):
        raise ValueError('For non-multichannel input, data must be of dimension 2 or 3.')
    dims = data.shape
    data = numpy.atleast_3d(as_float_array(data))[(..., numpy.newaxis)]
if (spacing is None):
    spacing = numpy.asarray(((1.0,) * 3))
elif (len(spacing) == len(dims)):
    if (len(spacing) == 2):
        spacing = numpy.r_[(spacing, 1.0)]
    else:
        spacing = numpy.asarray(spacing)
else:
    raise ValueError('Input argument `spacing` incorrect, should be an iterable with one number per spatial dimension.')
if copy:
    labels = numpy.copy(labels)
tempResult = unique(labels)
	
===================================================================	
_random_walker: 116	
----------------------------	

"Random walker algorithm for segmentation from markers.\n\n    Parameters\n    ----------\n    data : array_like\n        Image to be segmented in phases. Data spacing is assumed isotropic unless\n        the `spacing` keyword argument is used.\n    labels : array of ints, of same shape as `data` without channels dimension\n        Array of seed markers labeled with different positive integers\n        for different phases. Zero-labeled pixels are unlabeled pixels.\n        Negative labels correspond to inactive pixels that are not taken\n        into account (they are removed from the graph). If labels are not\n        consecutive integers, the labels array will be transformed so that\n        labels are consecutive.\n    beta : float\n        Penalization coefficient for the random walker motion\n        (the greater `beta`, the more difficult the diffusion).\n    tol : float\n        tolerance to achieve when solving the linear system, in\n        cg' mode.\n    copy : bool\n        If copy is False, the `labels` array will be overwritten with\n        the result of the segmentation. Use copy=False if you want to\n        save on memory.\n    spacing : iterable of floats\n        Spacing between voxels in each spatial dimension. If `None`, then\n        the spacing between pixels/voxels in each dimension is assumed 1.\n\n    Returns\n    -------\n    output : ndarray\n        * An array of ints of same shape as `data`, in which each pixel has\n          been labeled according to the marker that reached the pixel first\n          by anisotropic diffusion.\n\n    Notes\n    -----\n    The `spacing` argument is specifically for anisotropic datasets, where\n    data points are spaced differently in one or more spatial dimensions.\n    Anisotropic data is commonly encountered in medical imaging.\n\n    The algorithm was first proposed in *Random walks for image\n    segmentation*, Leo Grady, IEEE Trans Pattern Anal Mach Intell.\n    2006 Nov;28(11):1768-83.\n\n    The algorithm solves the diffusion equation at infinite times for\n    sources placed on markers of each phase in turn. A pixel is labeled with\n    the phase that has the greatest probability to diffuse first to the pixel.\n\n    The diffusion equation is solved by minimizing x.T L x for each phase,\n    where L is the Laplacian of the weighted graph of the image, and x is\n    the probability that a marker of the given phase arrives first at a pixel\n    by diffusion (x=1 on markers of the phase, x=0 on the other markers, and\n    the other coefficients are looked for). Each pixel is attributed the label\n    for which it has a maximal value of x. The Laplacian L of the image\n    is defined as:\n\n       - L_ii = d_i, the number of neighbors of pixel i (the degree of i)\n       - L_ij = -w_ij if i and j are adjacent pixels\n\n    The weight w_ij is a decreasing function of the norm of the local gradient.\n    This ensures that diffusion is easier between pixels of similar values.\n\n    When the Laplacian is decomposed into blocks of marked and unmarked\n    pixels::\n\n        L = M B.T\n            B A\n\n    with first indices corresponding to marked pixels, and then to unmarked\n    pixels, minimizing x.T L x for one phase amount to solving::\n\n        A x = - B x_m\n\n    where x_m = 1 on markers of the given phase, and 0 on other markers.\n    This linear system is solved in the algorithm using a direct method for\n    small images, and an iterative method for larger images.\n\n    "
if (labels != 0).all():
    warnings.warn('Random walker only segments unlabeled areas, where labels == 0. No zero valued areas in labels were found. Returning provided labels.')
    out_labels = labels
    return out_labels
multichannel = False
if (not multichannel):
    if ((data.ndim < 2) or (data.ndim > 3)):
        raise ValueError('For non-multichannel input, data must be of dimension 2 or 3.')
    dims = data.shape
    data = numpy.atleast_3d(as_float_array(data))[(..., numpy.newaxis)]
if (spacing is None):
    spacing = numpy.asarray(((1.0,) * 3))
elif (len(spacing) == len(dims)):
    if (len(spacing) == 2):
        spacing = numpy.r_[(spacing, 1.0)]
    else:
        spacing = numpy.asarray(spacing)
else:
    raise ValueError('Input argument `spacing` incorrect, should be an iterable with one number per spatial dimension.')
if copy:
    labels = numpy.copy(labels)
label_values = numpy.unique(labels)
if numpy.any((numpy.diff(label_values) != 1)):
    mask = (labels >= 0)
    tempResult = unique(labels[mask])
	
===================================================================	
generate_labeled_regions_large: 219	
----------------------------	

'Similar to generate_labeled_regions, but suitable for a large number of\n    regions.\n\n    See generate_labeled_regions for details.\n    '
if (rand_gen is None):
    rand_gen = numpy.random.RandomState(0)
data = rand_gen.randint((n_regions + 1), size=shape)
tempResult = unique(data)
	
===================================================================	
check_consistent_length: 97	
----------------------------	

'Check that all arrays have consistent first dimensions.\n    Checks whether all objects in arrays have the same shape or length.\n    Parameters\n    ----------\n    *arrays : list or tuple of input objects.\n        Objects that will be checked for consistent length.\n    '
tempResult = unique([_num_samples(X) for X in arrays if (X is not None)])
	
***************************************************	
poliastro_poliastro-0.8.0: 0	
***************************************************	
skimage_skimage-0.13.0: 26	
===================================================================	
_match_label_with_color: 25	
----------------------------	

'Return `unique_labels` and `color_cycle` for label array and color list.\n\n    Colors are cycled for normal labels, but the background color should only\n    be used for the background.\n    '
if (bg_color is None):
    bg_color = (0, 0, 0)
bg_color = _rgb_vector([bg_color])
tempResult = unique(label, return_inverse=True)
	
===================================================================	
_label2rgb_avg: 83	
----------------------------	

'Visualise each segment in `label_field` with its mean color in `image`.\n\n    Parameters\n    ----------\n    label_field : array of int\n        A segmentation of an image.\n    image : array, shape ``label_field.shape + (3,)``\n        A color image of the same spatial shape as `label_field`.\n    bg_label : int, optional\n        A value in `label_field` to be treated as background.\n    bg_color : 3-tuple of int, optional\n        The color for the background label\n\n    Returns\n    -------\n    out : array, same shape and type as `image`\n        The output visualization.\n    '
out = numpy.zeros_like(image)
tempResult = unique(label_field)
	
===================================================================	
peak_local_max: 25	
----------------------------	

'Find peaks in an image as coordinate list or boolean mask.\n\n    Peaks are the local maxima in a region of `2 * min_distance + 1`\n    (i.e. peaks are separated by at least `min_distance`).\n\n    If peaks are flat (i.e. multiple adjacent pixels have identical\n    intensities), the coordinates of all such pixels are returned.\n\n    If both `threshold_abs` and `threshold_rel` are provided, the maximum\n    of the two is chosen as the minimum intensity threshold of peaks.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    min_distance : int, optional\n        Minimum number of pixels separating peaks in a region of `2 *\n        min_distance + 1` (i.e. peaks are separated by at least\n        `min_distance`).\n        To find the maximum number of peaks, use `min_distance=1`.\n    threshold_abs : float, optional\n        Minimum intensity of peaks. By default, the absolute threshold is\n        the minimum intensity of the image.\n    threshold_rel : float, optional\n        Minimum intensity of peaks, calculated as `max(image) * threshold_rel`.\n    exclude_border : int, optional\n        If nonzero, `exclude_border` excludes peaks from\n        within `exclude_border`-pixels of the border of the image.\n    indices : bool, optional\n        If True, the output will be an array representing peak\n        coordinates.  If False, the output will be a boolean array shaped as\n        `image.shape` with peaks present at True elements.\n    num_peaks : int, optional\n        Maximum number of peaks. When the number of peaks exceeds `num_peaks`,\n        return `num_peaks` peaks based on highest peak intensity.\n    footprint : ndarray of bools, optional\n        If provided, `footprint == 1` represents the local region within which\n        to search for peaks at every point in `image`.  Overrides\n        `min_distance` (also for `exclude_border`).\n    labels : ndarray of ints, optional\n        If provided, each unique region `labels == value` represents a unique\n        region to search for peaks. Zero is reserved for background.\n    num_peaks_per_label : int, optional\n        Maximum number of peaks for each label.\n\n    Returns\n    -------\n    output : ndarray or ndarray of bools\n\n        * If `indices = True`  : (row, column, ...) coordinates of peaks.\n        * If `indices = False` : Boolean array shaped like `image`, with peaks\n          represented by True values.\n\n    Notes\n    -----\n    The peak local maximum function returns the coordinates of local peaks\n    (maxima) in an image. A maximum filter is used for finding local maxima.\n    This operation dilates the original image. After comparison of the dilated\n    and original image, this function returns the coordinates or a mask of the\n    peaks where the dilated image equals the original image.\n\n    Examples\n    --------\n    >>> img1 = np.zeros((7, 7))\n    >>> img1[3, 4] = 1\n    >>> img1[3, 2] = 1.5\n    >>> img1\n    array([[ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  1.5,  0. ,  1. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ]])\n\n    >>> peak_local_max(img1, min_distance=1)\n    array([[3, 4],\n           [3, 2]])\n\n    >>> peak_local_max(img1, min_distance=2)\n    array([[3, 2]])\n\n    >>> img2 = np.zeros((20, 20, 20))\n    >>> img2[10, 10, 10] = 1\n    >>> peak_local_max(img2, exclude_border=0)\n    array([[10, 10, 10]])\n\n    '
if (type(exclude_border) == bool):
    exclude_border = (min_distance if exclude_border else 0)
out = numpy.zeros_like(image, dtype=numpy.bool)
if (labels is not None):
    tempResult = unique(labels)
	
===================================================================	
peak_local_max: 30	
----------------------------	

'Find peaks in an image as coordinate list or boolean mask.\n\n    Peaks are the local maxima in a region of `2 * min_distance + 1`\n    (i.e. peaks are separated by at least `min_distance`).\n\n    If peaks are flat (i.e. multiple adjacent pixels have identical\n    intensities), the coordinates of all such pixels are returned.\n\n    If both `threshold_abs` and `threshold_rel` are provided, the maximum\n    of the two is chosen as the minimum intensity threshold of peaks.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image.\n    min_distance : int, optional\n        Minimum number of pixels separating peaks in a region of `2 *\n        min_distance + 1` (i.e. peaks are separated by at least\n        `min_distance`).\n        To find the maximum number of peaks, use `min_distance=1`.\n    threshold_abs : float, optional\n        Minimum intensity of peaks. By default, the absolute threshold is\n        the minimum intensity of the image.\n    threshold_rel : float, optional\n        Minimum intensity of peaks, calculated as `max(image) * threshold_rel`.\n    exclude_border : int, optional\n        If nonzero, `exclude_border` excludes peaks from\n        within `exclude_border`-pixels of the border of the image.\n    indices : bool, optional\n        If True, the output will be an array representing peak\n        coordinates.  If False, the output will be a boolean array shaped as\n        `image.shape` with peaks present at True elements.\n    num_peaks : int, optional\n        Maximum number of peaks. When the number of peaks exceeds `num_peaks`,\n        return `num_peaks` peaks based on highest peak intensity.\n    footprint : ndarray of bools, optional\n        If provided, `footprint == 1` represents the local region within which\n        to search for peaks at every point in `image`.  Overrides\n        `min_distance` (also for `exclude_border`).\n    labels : ndarray of ints, optional\n        If provided, each unique region `labels == value` represents a unique\n        region to search for peaks. Zero is reserved for background.\n    num_peaks_per_label : int, optional\n        Maximum number of peaks for each label.\n\n    Returns\n    -------\n    output : ndarray or ndarray of bools\n\n        * If `indices = True`  : (row, column, ...) coordinates of peaks.\n        * If `indices = False` : Boolean array shaped like `image`, with peaks\n          represented by True values.\n\n    Notes\n    -----\n    The peak local maximum function returns the coordinates of local peaks\n    (maxima) in an image. A maximum filter is used for finding local maxima.\n    This operation dilates the original image. After comparison of the dilated\n    and original image, this function returns the coordinates or a mask of the\n    peaks where the dilated image equals the original image.\n\n    Examples\n    --------\n    >>> img1 = np.zeros((7, 7))\n    >>> img1[3, 4] = 1\n    >>> img1[3, 2] = 1.5\n    >>> img1\n    array([[ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  1.5,  0. ,  1. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n           [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ]])\n\n    >>> peak_local_max(img1, min_distance=1)\n    array([[3, 4],\n           [3, 2]])\n\n    >>> peak_local_max(img1, min_distance=2)\n    array([[3, 2]])\n\n    >>> img2 = np.zeros((20, 20, 20))\n    >>> img2[10, 10, 10] = 1\n    >>> peak_local_max(img2, exclude_border=0)\n    array([[10, 10, 10]])\n\n    '
if (type(exclude_border) == bool):
    exclude_border = (min_distance if exclude_border else 0)
out = numpy.zeros_like(image, dtype=numpy.bool)
if (labels is not None):
    label_values = numpy.unique(labels)
    if numpy.any((numpy.diff(label_values) != 1)):
        mask = (labels >= 1)
        labels[mask] = (1 + rank_order(labels[mask])[0].astype(labels.dtype))
    labels = labels.astype(numpy.int32)
    tempResult = unique(labels)
	
===================================================================	
convex_hull_image: 29	
----------------------------	

'Compute the convex hull image of a binary image.\n\n    The convex hull is the set of pixels included in the smallest convex\n    polygon that surround all white pixels in the input image.\n\n    Parameters\n    ----------\n    image : (M, N) array\n        Binary input image. This array is cast to bool before processing.\n\n    Returns\n    -------\n    hull : (M, N) array of bool\n        Binary image with pixels in convex hull set to True.\n\n    References\n    ----------\n    .. [1] http://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-algorithm-notes/\n\n    '
if (image.ndim > 2):
    raise ValueError('Input must be a 2D image')
if (Delaunay is None):
    raise ImportError('Could not import scipy.spatial.Delaunay, only available in scipy >= 0.9.')
coords = possible_hull(image.astype(numpy.uint8))
N = len(coords)
coords_corners = numpy.empty(((N * 4), 2))
for (i, (x_offset, y_offset)) in enumerate(zip((0, 0, (- 0.5), 0.5), ((- 0.5), 0.5, 0, 0))):
    coords_corners[(i * N):((i + 1) * N)] = (coords + [x_offset, y_offset])
coords = unique_rows(coords_corners)
offset = coords.mean(axis=0)
coords -= offset
chull = Delaunay(coords).convex_hull
tempResult = unique(chull)
	
===================================================================	
_find_min_diff: 62	
----------------------------	

'\n    Find the minimal difference of grey levels inside the image.\n    '
tempResult = unique(img.flatten())
	
===================================================================	
_find_boundaries_subpixel: 23	
----------------------------	

'See ``find_boundaries(..., mode=\'subpixel\')``.\n\n    Notes\n    -----\n    This function puts in an empty row and column between each *actual*\n    row and column of the image, for a corresponding shape of $2s - 1$\n    for every image dimension of size $s$. These "interstitial" rows\n    and columns are filled as ``True`` if they separate two labels in\n    `label_img`, ``False`` otherwise.\n\n    I used ``view_as_windows`` to get the neighborhood of each pixel.\n    Then I check whether there are two labels or more in that\n    neighborhood.\n    '
ndim = label_img.ndim
max_label = np.iinfo(label_img.dtype).max
label_img_expanded = numpy.zeros([((2 * s) - 1) for s in label_img.shape], label_img.dtype)
pixels = ([slice(None, None, 2)] * ndim)
label_img_expanded[pixels] = label_img
edges = numpy.ones(label_img_expanded.shape, dtype=bool)
edges[pixels] = False
label_img_expanded[edges] = max_label
windows = view_as_windows(numpy.pad(label_img_expanded, 1, mode='constant', constant_values=0), ((3,) * ndim))
boundaries = numpy.zeros_like(edges)
for index in numpy.ndindex(label_img_expanded.shape):
    if edges[index]:
        tempResult = unique(windows[index].ravel())
	
===================================================================	
random_walker: 129	
----------------------------	

'Random walker algorithm for segmentation from markers.\n\n    Random walker algorithm is implemented for gray-level or multichannel\n    images.\n\n    Parameters\n    ----------\n    data : array_like\n        Image to be segmented in phases. Gray-level `data` can be two- or\n        three-dimensional; multichannel data can be three- or four-\n        dimensional (multichannel=True) with the highest dimension denoting\n        channels. Data spacing is assumed isotropic unless the `spacing`\n        keyword argument is used.\n    labels : array of ints, of same shape as `data` without channels dimension\n        Array of seed markers labeled with different positive integers\n        for different phases. Zero-labeled pixels are unlabeled pixels.\n        Negative labels correspond to inactive pixels that are not taken\n        into account (they are removed from the graph). If labels are not\n        consecutive integers, the labels array will be transformed so that\n        labels are consecutive. In the multichannel case, `labels` should have\n        the same shape as a single channel of `data`, i.e. without the final\n        dimension denoting channels.\n    beta : float\n        Penalization coefficient for the random walker motion\n        (the greater `beta`, the more difficult the diffusion).\n    mode : string, available options {\'cg_mg\', \'cg\', \'bf\'}\n        Mode for solving the linear system in the random walker algorithm.\n        If no preference given, automatically attempt to use the fastest\n        option available (\'cg_mg\' from pyamg >> \'cg\' with UMFPACK > \'bf\').\n\n        - \'bf\' (brute force): an LU factorization of the Laplacian is\n          computed. This is fast for small images (<1024x1024), but very slow\n          and memory-intensive for large images (e.g., 3-D volumes).\n        - \'cg\' (conjugate gradient): the linear system is solved iteratively\n          using the Conjugate Gradient method from scipy.sparse.linalg. This is\n          less memory-consuming than the brute force method for large images,\n          but it is quite slow.\n        - \'cg_mg\' (conjugate gradient with multigrid preconditioner): a\n          preconditioner is computed using a multigrid solver, then the\n          solution is computed with the Conjugate Gradient method.  This mode\n          requires that the pyamg module (http://pyamg.org/) is\n          installed. For images of size > 512x512, this is the recommended\n          (fastest) mode.\n\n    tol : float\n        tolerance to achieve when solving the linear system, in\n        cg\' and \'cg_mg\' modes.\n    copy : bool\n        If copy is False, the `labels` array will be overwritten with\n        the result of the segmentation. Use copy=False if you want to\n        save on memory.\n    multichannel : bool, default False\n        If True, input data is parsed as multichannel data (see \'data\' above\n        for proper input format in this case)\n    return_full_prob : bool, default False\n        If True, the probability that a pixel belongs to each of the labels\n        will be returned, instead of only the most likely label.\n    spacing : iterable of floats\n        Spacing between voxels in each spatial dimension. If `None`, then\n        the spacing between pixels/voxels in each dimension is assumed 1.\n\n    Returns\n    -------\n    output : ndarray\n        * If `return_full_prob` is False, array of ints of same shape as\n          `data`, in which each pixel has been labeled according to the marker\n          that reached the pixel first by anisotropic diffusion.\n        * If `return_full_prob` is True, array of floats of shape\n          `(nlabels, data.shape)`. `output[label_nb, i, j]` is the probability\n          that label `label_nb` reaches the pixel `(i, j)` first.\n\n    See also\n    --------\n    skimage.morphology.watershed: watershed segmentation\n        A segmentation algorithm based on mathematical morphology\n        and "flooding" of regions from markers.\n\n    Notes\n    -----\n    Multichannel inputs are scaled with all channel data combined. Ensure all\n    channels are separately normalized prior to running this algorithm.\n\n    The `spacing` argument is specifically for anisotropic datasets, where\n    data points are spaced differently in one or more spatial dimensions.\n    Anisotropic data is commonly encountered in medical imaging.\n\n    The algorithm was first proposed in *Random walks for image\n    segmentation*, Leo Grady, IEEE Trans Pattern Anal Mach Intell.\n    2006 Nov;28(11):1768-83.\n\n    The algorithm solves the diffusion equation at infinite times for\n    sources placed on markers of each phase in turn. A pixel is labeled with\n    the phase that has the greatest probability to diffuse first to the pixel.\n\n    The diffusion equation is solved by minimizing x.T L x for each phase,\n    where L is the Laplacian of the weighted graph of the image, and x is\n    the probability that a marker of the given phase arrives first at a pixel\n    by diffusion (x=1 on markers of the phase, x=0 on the other markers, and\n    the other coefficients are looked for). Each pixel is attributed the label\n    for which it has a maximal value of x. The Laplacian L of the image\n    is defined as:\n\n       - L_ii = d_i, the number of neighbors of pixel i (the degree of i)\n       - L_ij = -w_ij if i and j are adjacent pixels\n\n    The weight w_ij is a decreasing function of the norm of the local gradient.\n    This ensures that diffusion is easier between pixels of similar values.\n\n    When the Laplacian is decomposed into blocks of marked and unmarked\n    pixels::\n\n        L = M B.T\n            B A\n\n    with first indices corresponding to marked pixels, and then to unmarked\n    pixels, minimizing x.T L x for one phase amount to solving::\n\n        A x = - B x_m\n\n    where x_m = 1 on markers of the given phase, and 0 on other markers.\n    This linear system is solved in the algorithm using a direct method for\n    small images, and an iterative method for larger images.\n\n    Examples\n    --------\n    >>> np.random.seed(0)\n    >>> a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)\n    >>> a[5:8, 5:8] += 1\n    >>> b = np.zeros_like(a)\n    >>> b[3, 3] = 1  # Marker for first phase\n    >>> b[6, 6] = 2  # Marker for second phase\n    >>> random_walker(a, b)\n    array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)\n\n    '
if (mode is None):
    if amg_loaded:
        mode = 'cg_mg'
    elif (UmfpackContext is not None):
        mode = 'cg'
    else:
        mode = 'bf'
elif (mode not in ('cg_mg', 'cg', 'bf')):
    raise ValueError("{mode} is not a valid mode. Valid modes are 'cg_mg', 'cg' and 'bf'".format(mode=mode))
if ((UmfpackContext is None) and (mode == 'cg')):
    warn('"cg" mode will be used, but it may be slower than "bf" because SciPy was built without UMFPACK. Consider rebuilding SciPy with UMFPACK; this will greatly accelerate the conjugate gradient ("cg") solver. You may also install pyamg and run the random_walker function in "cg_mg" mode (see docstring).')
if (labels != 0).all():
    warn('Random walker only segments unlabeled areas, where labels == 0. No zero valued areas in labels were found. Returning provided labels.')
    if return_full_prob:
        tempResult = unique(labels)
	
===================================================================	
random_walker: 160	
----------------------------	

'Random walker algorithm for segmentation from markers.\n\n    Random walker algorithm is implemented for gray-level or multichannel\n    images.\n\n    Parameters\n    ----------\n    data : array_like\n        Image to be segmented in phases. Gray-level `data` can be two- or\n        three-dimensional; multichannel data can be three- or four-\n        dimensional (multichannel=True) with the highest dimension denoting\n        channels. Data spacing is assumed isotropic unless the `spacing`\n        keyword argument is used.\n    labels : array of ints, of same shape as `data` without channels dimension\n        Array of seed markers labeled with different positive integers\n        for different phases. Zero-labeled pixels are unlabeled pixels.\n        Negative labels correspond to inactive pixels that are not taken\n        into account (they are removed from the graph). If labels are not\n        consecutive integers, the labels array will be transformed so that\n        labels are consecutive. In the multichannel case, `labels` should have\n        the same shape as a single channel of `data`, i.e. without the final\n        dimension denoting channels.\n    beta : float\n        Penalization coefficient for the random walker motion\n        (the greater `beta`, the more difficult the diffusion).\n    mode : string, available options {\'cg_mg\', \'cg\', \'bf\'}\n        Mode for solving the linear system in the random walker algorithm.\n        If no preference given, automatically attempt to use the fastest\n        option available (\'cg_mg\' from pyamg >> \'cg\' with UMFPACK > \'bf\').\n\n        - \'bf\' (brute force): an LU factorization of the Laplacian is\n          computed. This is fast for small images (<1024x1024), but very slow\n          and memory-intensive for large images (e.g., 3-D volumes).\n        - \'cg\' (conjugate gradient): the linear system is solved iteratively\n          using the Conjugate Gradient method from scipy.sparse.linalg. This is\n          less memory-consuming than the brute force method for large images,\n          but it is quite slow.\n        - \'cg_mg\' (conjugate gradient with multigrid preconditioner): a\n          preconditioner is computed using a multigrid solver, then the\n          solution is computed with the Conjugate Gradient method.  This mode\n          requires that the pyamg module (http://pyamg.org/) is\n          installed. For images of size > 512x512, this is the recommended\n          (fastest) mode.\n\n    tol : float\n        tolerance to achieve when solving the linear system, in\n        cg\' and \'cg_mg\' modes.\n    copy : bool\n        If copy is False, the `labels` array will be overwritten with\n        the result of the segmentation. Use copy=False if you want to\n        save on memory.\n    multichannel : bool, default False\n        If True, input data is parsed as multichannel data (see \'data\' above\n        for proper input format in this case)\n    return_full_prob : bool, default False\n        If True, the probability that a pixel belongs to each of the labels\n        will be returned, instead of only the most likely label.\n    spacing : iterable of floats\n        Spacing between voxels in each spatial dimension. If `None`, then\n        the spacing between pixels/voxels in each dimension is assumed 1.\n\n    Returns\n    -------\n    output : ndarray\n        * If `return_full_prob` is False, array of ints of same shape as\n          `data`, in which each pixel has been labeled according to the marker\n          that reached the pixel first by anisotropic diffusion.\n        * If `return_full_prob` is True, array of floats of shape\n          `(nlabels, data.shape)`. `output[label_nb, i, j]` is the probability\n          that label `label_nb` reaches the pixel `(i, j)` first.\n\n    See also\n    --------\n    skimage.morphology.watershed: watershed segmentation\n        A segmentation algorithm based on mathematical morphology\n        and "flooding" of regions from markers.\n\n    Notes\n    -----\n    Multichannel inputs are scaled with all channel data combined. Ensure all\n    channels are separately normalized prior to running this algorithm.\n\n    The `spacing` argument is specifically for anisotropic datasets, where\n    data points are spaced differently in one or more spatial dimensions.\n    Anisotropic data is commonly encountered in medical imaging.\n\n    The algorithm was first proposed in *Random walks for image\n    segmentation*, Leo Grady, IEEE Trans Pattern Anal Mach Intell.\n    2006 Nov;28(11):1768-83.\n\n    The algorithm solves the diffusion equation at infinite times for\n    sources placed on markers of each phase in turn. A pixel is labeled with\n    the phase that has the greatest probability to diffuse first to the pixel.\n\n    The diffusion equation is solved by minimizing x.T L x for each phase,\n    where L is the Laplacian of the weighted graph of the image, and x is\n    the probability that a marker of the given phase arrives first at a pixel\n    by diffusion (x=1 on markers of the phase, x=0 on the other markers, and\n    the other coefficients are looked for). Each pixel is attributed the label\n    for which it has a maximal value of x. The Laplacian L of the image\n    is defined as:\n\n       - L_ii = d_i, the number of neighbors of pixel i (the degree of i)\n       - L_ij = -w_ij if i and j are adjacent pixels\n\n    The weight w_ij is a decreasing function of the norm of the local gradient.\n    This ensures that diffusion is easier between pixels of similar values.\n\n    When the Laplacian is decomposed into blocks of marked and unmarked\n    pixels::\n\n        L = M B.T\n            B A\n\n    with first indices corresponding to marked pixels, and then to unmarked\n    pixels, minimizing x.T L x for one phase amount to solving::\n\n        A x = - B x_m\n\n    where x_m = 1 on markers of the given phase, and 0 on other markers.\n    This linear system is solved in the algorithm using a direct method for\n    small images, and an iterative method for larger images.\n\n    Examples\n    --------\n    >>> np.random.seed(0)\n    >>> a = np.zeros((10, 10)) + 0.2 * np.random.rand(10, 10)\n    >>> a[5:8, 5:8] += 1\n    >>> b = np.zeros_like(a)\n    >>> b[3, 3] = 1  # Marker for first phase\n    >>> b[6, 6] = 2  # Marker for second phase\n    >>> random_walker(a, b)\n    array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n           [1, 1, 1, 1, 1, 2, 2, 2, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)\n\n    '
if (mode is None):
    if amg_loaded:
        mode = 'cg_mg'
    elif (UmfpackContext is not None):
        mode = 'cg'
    else:
        mode = 'bf'
elif (mode not in ('cg_mg', 'cg', 'bf')):
    raise ValueError("{mode} is not a valid mode. Valid modes are 'cg_mg', 'cg' and 'bf'".format(mode=mode))
if ((UmfpackContext is None) and (mode == 'cg')):
    warn('"cg" mode will be used, but it may be slower than "bf" because SciPy was built without UMFPACK. Consider rebuilding SciPy with UMFPACK; this will greatly accelerate the conjugate gradient ("cg") solver. You may also install pyamg and run the random_walker function in "cg_mg" mode (see docstring).')
if (labels != 0).all():
    warn('Random walker only segments unlabeled areas, where labels == 0. No zero valued areas in labels were found. Returning provided labels.')
    if return_full_prob:
        unique_labels = numpy.unique(labels)
        unique_labels = unique_labels[(unique_labels > 0)]
        out_labels = numpy.empty((labels.shape + (len(unique_labels),)), dtype=numpy.bool)
        for (n, i) in enumerate(unique_labels):
            out_labels[(..., n)] = (labels == i)
    else:
        out_labels = labels
    return out_labels
if (not multichannel):
    if ((data.ndim < 2) or (data.ndim > 3)):
        raise ValueError('For non-multichannel input, data must be of dimension 2 or 3.')
    dims = data.shape
    data = numpy.atleast_3d(img_as_float(data))[(..., numpy.newaxis)]
else:
    if (data.ndim < 3):
        raise ValueError('For multichannel input, data must have 3 or 4 dimensions.')
    dims = data[(..., 0)].shape
    data = img_as_float(data)
    if (data.ndim == 3):
        data = data[:, :, numpy.newaxis, :]
if (spacing is None):
    spacing = numpy.asarray(((1.0,) * 3))
elif (len(spacing) == len(dims)):
    if (len(spacing) == 2):
        spacing = numpy.r_[(spacing, 1.0)]
    else:
        spacing = numpy.asarray(spacing)
else:
    raise ValueError('Input argument `spacing` incorrect, should be an iterable with one number per spatial dimension.')
if copy:
    labels = numpy.copy(labels)
tempResult = unique(labels)
	
===================================================================	
_mask_edges_weights: 99	
----------------------------	

'\n    Remove edges of the graph connected to masked nodes, as well as\n    corresponding weights of the edges.\n    '
mask0 = numpy.hstack((mask[:, :, :(- 1)].ravel(), mask[:, :(- 1)].ravel(), mask[:(- 1)].ravel()))
mask1 = numpy.hstack((mask[:, :, 1:].ravel(), mask[:, 1:].ravel(), mask[1:].ravel()))
ind_mask = numpy.logical_and(mask0, mask1)
(edges, weights) = (edges[:, ind_mask], weights[ind_mask])
max_node_index = edges.max()
tempResult = unique(edges.ravel())
	
===================================================================	
clear_border: 23	
----------------------------	

'Clear objects connected to the label image border.\n\n    Parameters\n    ----------\n    labels : (M[, N[, ..., P]]) array of int or bool\n        Imaging data labels.\n    buffer_size : int, optional\n        The width of the border examined.  By default, only objects\n        that touch the outside of the image are removed.\n    bgval : float or int, optional\n        Cleared objects are set to this value.\n    in_place : bool, optional\n        Whether or not to manipulate the labels array in-place.\n\n    Returns\n    -------\n    out : (M[, N[, ..., P]]) array\n        Imaging data labels with cleared borders\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.segmentation import clear_border\n    >>> labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0],\n    ...                    [0, 0, 0, 0, 1, 0, 0, 0, 0],\n    ...                    [1, 0, 0, 1, 0, 1, 0, 0, 0],\n    ...                    [0, 0, 1, 1, 1, 1, 1, 0, 0],\n    ...                    [0, 1, 1, 1, 1, 1, 1, 1, 0],\n    ...                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n    >>> clear_border(labels)\n    array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 0, 1, 0, 1, 0, 0, 0],\n           [0, 0, 1, 1, 1, 1, 1, 0, 0],\n           [0, 1, 1, 1, 1, 1, 1, 1, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n    '
image = labels
if any(((buffer_size >= s) for s in image.shape)):
    raise ValueError('buffer size may not be greater than image size')
borders = numpy.zeros_like(image, dtype=numpy.bool_)
ext = (buffer_size + 1)
slstart = slice(ext)
slend = slice((- ext), None)
slices = [slice(s) for s in image.shape]
for d in range(image.ndim):
    slicedim = list(slices)
    slicedim[d] = slstart
    borders[slicedim] = True
    slicedim[d] = slend
    borders[slicedim] = True
labels = label(image, background=0)
number = (numpy.max(labels) + 1)
tempResult = unique(labels[borders])
	
===================================================================	
relabel_sequential: 27	
----------------------------	

'Relabel arbitrary labels to {`offset`, ... `offset` + number_of_labels}.\n\n    This function also returns the forward map (mapping the original labels to\n    the reduced labels) and the inverse map (mapping the reduced labels back\n    to the original ones).\n\n    Parameters\n    ----------\n    label_field : numpy array of int, arbitrary shape\n        An array of labels.\n    offset : int, optional\n        The return labels will start at `offset`, which should be\n        strictly positive.\n\n    Returns\n    -------\n    relabeled : numpy array of int, same shape as `label_field`\n        The input label field with labels mapped to\n        {offset, ..., number_of_labels + offset - 1}.\n    forward_map : numpy array of int, shape ``(label_field.max() + 1,)``\n        The map from the original label space to the returned label\n        space. Can be used to re-apply the same mapping. See examples\n        for usage.\n    inverse_map : 1D numpy array of int, of length offset + number of labels\n        The map from the new label space to the original space. This\n        can be used to reconstruct the original label field from the\n        relabeled one.\n\n    Notes\n    -----\n    The label 0 is assumed to denote the background and is never remapped.\n\n    The forward map can be extremely big for some inputs, since its\n    length is given by the maximum of the label field. However, in most\n    situations, ``label_field.max()`` is much smaller than\n    ``label_field.size``, and in these cases the forward map is\n    guaranteed to be smaller than either the input or output images.\n\n    Examples\n    --------\n    >>> from skimage.segmentation import relabel_sequential\n    >>> label_field = np.array([1, 1, 5, 5, 8, 99, 42])\n    >>> relab, fw, inv = relabel_sequential(label_field)\n    >>> relab\n    array([1, 1, 2, 2, 3, 5, 4])\n    >>> fw\n    array([0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 5])\n    >>> inv\n    array([ 0,  1,  5,  8, 42, 99])\n    >>> (fw[label_field] == relab).all()\n    True\n    >>> (inv[relab] == label_field).all()\n    True\n    >>> relab, fw, inv = relabel_sequential(label_field, offset=5)\n    >>> relab\n    array([5, 5, 6, 6, 7, 9, 8])\n    '
m = label_field.max()
if (not numpy.issubdtype(label_field.dtype, numpy.int)):
    new_type = numpy.min_scalar_type(int(m))
    label_field = label_field.astype(new_type)
    m = m.astype(new_type)
tempResult = unique(label_field)
	
===================================================================	
test_color: 52	
----------------------------	

img = numpy.zeros((20, 21, 3))
img[:10, :10, 0] = 1
img[10:, :10, 1] = 1
img[10:, 10:, 2] = 1
seg = felzenszwalb(img, sigma=0)
tempResult = unique(seg)
	
===================================================================	
test_merging: 61	
----------------------------	

img = numpy.array([[0, 0.3], [0.7, 1]])
seg = felzenszwalb(img, scale=0, sigma=0, min_size=2)
tempResult = unique(seg)
	
===================================================================	
test_grey: 15	
----------------------------	

img = numpy.zeros((20, 21))
img[:10, 10:] = 0.2
img[10:, :10] = 0.4
img[10:, 10:] = 0.6
seg = felzenszwalb(img, sigma=0)
tempResult = unique(seg)
	
===================================================================	
test_color: 32	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21, 3))
img[:10, :10, 0] = 1
img[10:, :10, 1] = 1
img[10:, 10:, 2] = 1
img += (0.01 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = quickshift(img, random_seed=0, max_dist=30, kernel_size=10, sigma=0)
tempResult = unique(seg)
	
===================================================================	
test_color: 38	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21, 3))
img[:10, :10, 0] = 1
img[10:, :10, 1] = 1
img[10:, 10:, 2] = 1
img += (0.01 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = quickshift(img, random_seed=0, max_dist=30, kernel_size=10, sigma=0)
assert_equal(len(numpy.unique(seg)), 4)
assert_array_equal(seg[:10, :10], 1)
assert_array_equal(seg[10:, :10], 2)
assert_array_equal(seg[:10, 10:], 0)
assert_array_equal(seg[10:, 10:], 3)
seg2 = quickshift(img, kernel_size=1, max_dist=2, random_seed=0, convert2lab=False, sigma=0)
tempResult = unique(seg2)
	
===================================================================	
test_grey: 17	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21))
img[:10, 10:] = 0.2
img[10:, :10] = 0.4
img[10:, 10:] = 0.6
img += (0.1 * rnd.normal(size=img.shape))
seg = quickshift(img, kernel_size=2, max_dist=3, random_seed=0, convert2lab=False, sigma=0)
tempResult = unique(seg)
	
===================================================================	
test_color_2d: 19	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21, 3))
img[:10, :10, 0] = 1
img[10:, :10, 1] = 1
img[10:, 10:, 2] = 1
img += (0.01 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = slic(img, n_segments=4, sigma=0, enforce_connectivity=False)
tempResult = unique(seg)
	
===================================================================	
test_gray_3d: 94	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21, 22))
slices = []
for dim_size in img.shape:
    midpoint = (dim_size // 2)
    slices.append((slice(None, midpoint), slice(midpoint, None)))
slices = list(itertools.product(*slices))
shades = numpy.arange(0, 1.000001, (1.0 / 7))
for (s, sh) in zip(slices, shades):
    img[s] = sh
img += (0.001 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = slic(img, sigma=0, n_segments=8, compactness=1, multichannel=False, convert2lab=False)
tempResult = unique(seg)
	
===================================================================	
test_slic_zero: 142	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21, 3))
img[:10, :10, 0] = 1
img[10:, :10, 1] = 1
img[10:, 10:, 2] = 1
img += (0.01 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = slic(img, n_segments=4, sigma=0, slic_zero=True)
tempResult = unique(seg)
	
===================================================================	
test_gray_2d: 53	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21))
img[:10, :10] = 0.33
img[10:, :10] = 0.67
img[10:, 10:] = 1.0
img += (0.0033 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = slic(img, sigma=0, n_segments=4, compactness=1, multichannel=False, convert2lab=False)
tempResult = unique(seg)
	
===================================================================	
test_color_3d: 75	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21, 22, 3))
slices = []
for dim_size in img.shape[:(- 1)]:
    midpoint = (dim_size // 2)
    slices.append((slice(None, midpoint), slice(midpoint, None)))
slices = list(itertools.product(*slices))
colors = list(itertools.product(*(([0, 1],) * 3)))
for (s, c) in zip(slices, colors):
    img[s] = c
img += (0.01 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = slic(img, sigma=0, n_segments=8)
tempResult = unique(seg)
	
===================================================================	
test_multichannel_2d: 36	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 20, 8))
img[:10, :10, 0:2] = 1
img[:10, 10:, 2:4] = 1
img[10:, :10, 4:6] = 1
img[10:, 10:, 6:8] = 1
img += (0.01 * rnd.normal(size=img.shape))
img = numpy.clip(img, 0, 1, out=img)
seg = slic(img, n_segments=4, enforce_connectivity=False)
tempResult = unique(seg)
	
===================================================================	
random_noise: 32	
----------------------------	

"\n    Function to add random noise of various types to a floating-point image.\n\n    Parameters\n    ----------\n    image : ndarray\n        Input image data. Will be converted to float.\n    mode : str\n        One of the following strings, selecting the type of noise to add:\n\n        - 'gaussian'  Gaussian-distributed additive noise.\n        - 'localvar'  Gaussian-distributed additive noise, with specified\n                      local variance at each point of `image`\n        - 'poisson'   Poisson-distributed noise generated from the data.\n        - 'salt'      Replaces random pixels with 1.\n        - 'pepper'    Replaces random pixels with 0 (for unsigned images) or\n                      -1 (for signed images).\n        - 's&p'       Replaces random pixels with either 1 or `low_val`, where\n                      `low_val` is 0 for unsigned images or -1 for signed\n                      images.\n        - 'speckle'   Multiplicative noise using out = image + n*image, where\n                      n is uniform noise with specified mean & variance.\n    seed : int\n        If provided, this will set the random seed before generating noise,\n        for valid pseudo-random comparisons.\n    clip : bool\n        If True (default), the output will be clipped after noise applied\n        for modes `'speckle'`, `'poisson'`, and `'gaussian'`. This is\n        needed to maintain the proper image data range. If False, clipping\n        is not applied, and the output may extend beyond the range [-1, 1].\n    mean : float\n        Mean of random distribution. Used in 'gaussian' and 'speckle'.\n        Default : 0.\n    var : float\n        Variance of random distribution. Used in 'gaussian' and 'speckle'.\n        Note: variance = (standard deviation) ** 2. Default : 0.01\n    local_vars : ndarray\n        Array of positive floats, same shape as `image`, defining the local\n        variance at every image point. Used in 'localvar'.\n    amount : float\n        Proportion of image pixels to replace with noise on range [0, 1].\n        Used in 'salt', 'pepper', and 'salt & pepper'. Default : 0.05\n    salt_vs_pepper : float\n        Proportion of salt vs. pepper noise for 's&p' on range [0, 1].\n        Higher values represent more salt. Default : 0.5 (equal amounts)\n\n    Returns\n    -------\n    out : ndarray\n        Output floating-point image data on range [0, 1] or [-1, 1] if the\n        input `image` was unsigned or signed, respectively.\n\n    Notes\n    -----\n    Speckle, Poisson, Localvar, and Gaussian noise may generate noise outside\n    the valid image range. The default is to clip (not alias) these values,\n    but they may be preserved by setting `clip=False`. Note that in this case\n    the output may contain values outside the ranges [0, 1] or [-1, 1].\n    Use this option with care.\n\n    Because of the prevalence of exclusively positive floating-point images in\n    intermediate calculations, it is not possible to intuit if an input is\n    signed based on dtype alone. Instead, negative values are explicity\n    searched for. Only if found does this function assume signed input.\n    Unexpected results only occur in rare, poorly exposes cases (e.g. if all\n    values are above 50 percent gray in a signed `image`). In this event,\n    manually scaling the input to the positive domain will solve the problem.\n\n    The Poisson distribution is only defined for positive integers. To apply\n    this noise type, the number of unique values in the image is found and\n    the next round power of two is used to scale up the floating-point result,\n    after which it is scaled back down to the floating-point image range.\n\n    To generate Poisson noise against a signed image, the signed image is\n    temporarily converted to an unsigned image in the floating point domain,\n    Poisson noise is generated, then it is returned to the original range.\n\n    "
mode = mode.lower()
if (image.min() < 0):
    low_clip = (- 1.0)
else:
    low_clip = 0.0
image = img_as_float(image)
if (seed is not None):
    numpy.random.seed(seed=seed)
allowedtypes = {'gaussian': 'gaussian_values', 'localvar': 'localvar_values', 'poisson': 'poisson_values', 'salt': 'sp_values', 'pepper': 'sp_values', 's&p': 's&p_values', 'speckle': 'gaussian_values'}
kwdefaults = {'mean': 0.0, 'var': 0.01, 'amount': 0.05, 'salt_vs_pepper': 0.5, 'local_vars': (numpy.zeros_like(image) + 0.01)}
allowedkwargs = {'gaussian_values': ['mean', 'var'], 'localvar_values': ['local_vars'], 'sp_values': ['amount'], 's&p_values': ['amount', 'salt_vs_pepper'], 'poisson_values': []}
for key in kwargs:
    if (key not in allowedkwargs[allowedtypes[mode]]):
        raise ValueError(('%s keyword not in allowed keywords %s' % (key, allowedkwargs[allowedtypes[mode]])))
for kw in allowedkwargs[allowedtypes[mode]]:
    kwargs.setdefault(kw, kwdefaults[kw])
if (mode == 'gaussian'):
    noise = numpy.random.normal(kwargs['mean'], (kwargs['var'] ** 0.5), image.shape)
    out = (image + noise)
elif (mode == 'localvar'):
    if (kwargs['local_vars'] <= 0).any():
        raise ValueError('All values of `local_vars` must be > 0.')
    out = (image + numpy.random.normal(0, (kwargs['local_vars'] ** 0.5)))
elif (mode == 'poisson'):
    tempResult = unique(image)
	
===================================================================	
unique_rows: 10	
----------------------------	

'Remove repeated rows from a 2D array.\n\n    In particular, if given an array of coordinates of shape\n    (Npoints, Ndim), it will remove repeated points.\n\n    Parameters\n    ----------\n    ar : 2-D ndarray\n        The input array.\n\n    Returns\n    -------\n    ar_out : 2-D ndarray\n        A copy of the input array with repeated rows removed.\n\n    Raises\n    ------\n    ValueError : if `ar` is not two-dimensional.\n\n    Notes\n    -----\n    The function will generate a copy of `ar` if it is not\n    C-contiguous, which will negatively affect performance for large\n    input arrays.\n\n    Examples\n    --------\n    >>> ar = np.array([[1, 0, 1],\n    ...                [0, 1, 0],\n    ...                [1, 0, 1]], np.uint8)\n    >>> unique_rows(ar)\n    array([[0, 1, 0],\n           [1, 0, 1]], dtype=uint8)\n    '
if (ar.ndim != 2):
    raise ValueError(('unique_rows() only makes sense for 2D arrays, got %dd' % ar.ndim))
ar = numpy.ascontiguousarray(ar)
ar_row_view = ar.view(('|S%d' % (ar.itemsize * ar.shape[1])))
tempResult = unique(ar_row_view, return_index=True)
	
***************************************************	
sunpy_sunpy-0.8.0: 0	
***************************************************	
spacetelescope_synphot-0.1: 0	
***************************************************	
librosa_librosa-0.5.1: 3	
===================================================================	
fmt: 200	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    n_fmt = int(numpy.ceil(((over_sample * (numpy.log((n - 1)) - numpy.log(t_min))) / log_base)))
elif (n_fmt < 3):
    raise ParameterError('n_fmt=={:} < 3'.format(n_fmt))
else:
    log_base = ((numpy.log((n_fmt - 1)) - numpy.log((n_fmt - 2))) / over_sample)
if (not numpy.all(numpy.isfinite(y))):
    raise ParameterError('y must be finite everywhere')
base = numpy.exp(log_base)
x = numpy.linspace(0, 1, num=n, endpoint=False)
f_interp = scipy.interpolate.interp1d(x, y, kind=kind, axis=axis)
n_over = int(numpy.ceil(over_sample))
x_exp = numpy.logspace(((numpy.log(t_min) - numpy.log(n)) / log_base), 0, num=(n_fmt + n_over), endpoint=False, base=base)[:(- n_over)]
if ((x_exp[0] < t_min) or (x_exp[(- 1)] > (float((n - 1.0)) / n))):
    x_exp = numpy.clip(x_exp, (float(t_min) / n), x[(- 1)])
tempResult = unique(x_exp)
	
===================================================================	
fix_frames: 96	
----------------------------	

'Fix a list of frames to lie within [x_min, x_max]\n\n    Examples\n    --------\n    >>> # Generate a list of frame indices\n    >>> frames = np.arange(0, 1000.0, 50)\n    >>> frames\n    array([   0.,   50.,  100.,  150.,  200.,  250.,  300.,  350.,\n            400.,  450.,  500.,  550.,  600.,  650.,  700.,  750.,\n            800.,  850.,  900.,  950.])\n    >>> # Clip to span at most 250\n    >>> librosa.util.fix_frames(frames, x_max=250)\n    array([  0,  50, 100, 150, 200, 250])\n    >>> # Or pad to span up to 2500\n    >>> librosa.util.fix_frames(frames, x_max=2500)\n    array([   0,   50,  100,  150,  200,  250,  300,  350,  400,\n            450,  500,  550,  600,  650,  700,  750,  800,  850,\n            900,  950, 2500])\n    >>> librosa.util.fix_frames(frames, x_max=2500, pad=False)\n    array([  0,  50, 100, 150, 200, 250, 300, 350, 400, 450, 500,\n           550, 600, 650, 700, 750, 800, 850, 900, 950])\n\n    >>> # Or starting away from zero\n    >>> frames = np.arange(200, 500, 33)\n    >>> frames\n    array([200, 233, 266, 299, 332, 365, 398, 431, 464, 497])\n    >>> librosa.util.fix_frames(frames)\n    array([  0, 200, 233, 266, 299, 332, 365, 398, 431, 464, 497])\n    >>> librosa.util.fix_frames(frames, x_max=500)\n    array([  0, 200, 233, 266, 299, 332, 365, 398, 431, 464, 497,\n           500])\n\n\n    Parameters\n    ----------\n    frames : np.ndarray [shape=(n_frames,)]\n        List of non-negative frame indices\n\n    x_min : int >= 0 or None\n        Minimum allowed frame index\n\n    x_max : int >= 0 or None\n        Maximum allowed frame index\n\n    pad : boolean\n        If `True`, then `frames` is expanded to span the full range\n        `[x_min, x_max]`\n\n    Returns\n    -------\n    fixed_frames : np.ndarray [shape=(n_fixed_frames,), dtype=int]\n        Fixed frame indices, flattened and sorted\n\n    Raises\n    ------\n    ParameterError\n        If `frames` contains negative values\n    '
frames = numpy.asarray(frames)
if numpy.any((frames < 0)):
    raise ParameterError('Negative frame index detected')
if (pad and ((x_min is not None) or (x_max is not None))):
    frames = numpy.clip(frames, x_min, x_max)
if pad:
    pad_data = []
    if (x_min is not None):
        pad_data.append(x_min)
    if (x_max is not None):
        pad_data.append(x_max)
    frames = numpy.concatenate((pad_data, frames))
if (x_min is not None):
    frames = frames[(frames >= x_min)]
if (x_max is not None):
    frames = frames[(frames <= x_max)]
tempResult = unique(frames)
	
===================================================================	
__test11111111111111: 510	
----------------------------	

slices = librosa.util.index_to_slice(idx, idx_min=idx_min, idx_max=idx_max, step=step, pad=pad)
if pad:
    if (idx_min is not None):
        eq_(slices[0].start, idx_min)
        if (idx.min() != idx_min):
            slices = slices[1:]
    if (idx_max is not None):
        eq_(slices[(- 1)].stop, idx_max)
        if (idx.max() != idx_max):
            slices = slices[:(- 1)]
if (idx_min is not None):
    idx = idx[(idx >= idx_min)]
if (idx_max is not None):
    idx = idx[(idx <= idx_max)]
tempResult = unique(idx)
	
***************************************************	
mne_python-0.15.0: 80	
===================================================================	
EpochsArray.__init__: 882	
----------------------------	

dtype = (numpy.complex128 if numpy.any(numpy.iscomplex(data)) else numpy.float64)
data = numpy.asanyarray(data, dtype=dtype)
if (data.ndim != 3):
    raise ValueError('Data must be a 3D array of shape (n_epochs, n_channels, n_samples)')
if (len(info['ch_names']) != data.shape[1]):
    raise ValueError('Info and data must have same number of channels.')
if (events is None):
    n_epochs = len(data)
    events = numpy.c_[(numpy.arange(n_epochs), numpy.zeros(n_epochs, int), numpy.ones(n_epochs, int))]
if (data.shape[0] != len(events)):
    raise ValueError('The number of epochs and the number of eventsmust match')
info = info.copy()
tmax = (((data.shape[2] - 1) / info['sfreq']) + tmin)
if (event_id is None):
    tempResult = unique(events[:, 2])
	
===================================================================	
average_movements: 1299	
----------------------------	

'Average data using Maxwell filtering, transforming using head positions.\n\n    Parameters\n    ----------\n    epochs : instance of Epochs\n        The epochs to operate on.\n    head_pos : array | tuple | None\n        The array should be of shape ``(N, 10)``, holding the position\n        parameters as returned by e.g. `read_head_pos`. For backward\n        compatibility, this can also be a tuple of ``(trans, rot t)``\n        as returned by `head_pos_to_trans_rot_t`.\n    orig_sfreq : float | None\n        The original sample frequency of the data (that matches the\n        event sample numbers in ``epochs.events``). Can be ``None``\n        if data have not been decimated or resampled.\n    picks : array-like of int | None\n        If None only MEG, EEG, SEEG, ECoG, and fNIRS channels are kept\n        otherwise the channels indices in picks are kept.\n    origin : array-like, shape (3,) | str\n        Origin of internal and external multipolar moment space in head\n        coords and in meters. The default is ``\'auto\'``, which means\n        a head-digitization-based origin fit.\n    weight_all : bool\n        If True, all channels are weighted by the SSS basis weights.\n        If False, only MEG channels are weighted, other channels\n        receive uniform weight per epoch.\n    int_order : int\n        Order of internal component of spherical expansion.\n    ext_order : int\n        Order of external component of spherical expansion.\n    regularize : str | None\n        Basis regularization type, must be "in" or None.\n        See :func:`mne.preprocessing.maxwell_filter` for details.\n        Regularization is chosen based only on the destination position.\n    destination : str | array-like, shape (3,) | None\n        The destination location for the head. Can be ``None``, which\n        will not change the head position, or a string path to a FIF file\n        containing a MEG device<->head transformation, or a 3-element array\n        giving the coordinates to translate to (with no rotations).\n        For example, ``destination=(0, 0, 0.04)`` would translate the bases\n        as ``--trans default`` would in MaxFilter (i.e., to the default\n        head location).\n\n        .. versionadded:: 0.12\n\n    ignore_ref : bool\n        If True, do not include reference channels in compensation. This\n        option should be True for KIT files, since Maxwell filtering\n        with reference channels is not currently supported.\n    return_mapping : bool\n        If True, return the mapping matrix.\n    mag_scale : float | str\n        The magenetometer scale-factor used to bring the magnetometers\n        to approximately the same order of magnitude as the gradiometers\n        (default 100.), as they have different units (T vs T/m).\n        Can be ``\'auto\'`` to use the reciprocal of the physical distance\n        between the gradiometer pickup loops (e.g., 0.0168 m yields\n        59.5 for VectorView).\n\n        .. versionadded:: 0.13\n\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    evoked : instance of Evoked\n        The averaged epochs.\n\n    See Also\n    --------\n    mne.preprocessing.maxwell_filter\n    mne.chpi.read_head_pos\n\n    Notes\n    -----\n    The Maxwell filtering version of this algorithm is described in [1]_,\n    in section V.B "Virtual signals and movement correction", equations\n    40-44. For additional validation, see [2]_.\n\n    Regularization has not been added because in testing it appears to\n    decrease dipole localization accuracy relative to using all components.\n    Fine calibration and cross-talk cancellation, however, could be added\n    to this algorithm based on user demand.\n\n    .. versionadded:: 0.11\n\n    References\n    ----------\n    .. [1] Taulu S. and Kajola M. "Presentation of electromagnetic\n           multichannel data: The signal space separation method,"\n           Journal of Applied Physics, vol. 97, pp. 124905 1-10, 2005.\n\n    .. [2] Wehner DT, Hmlinen MS, Mody M, Ahlfors SP. "Head movements\n           of children in MEG: Quantification, effects on source\n           estimation, and compensation. NeuroImage 40:541550, 2008.\n    '
from .preprocessing.maxwell import _trans_sss_basis, _reset_meg_bads, _check_usable, _col_norm_pinv, _get_n_moments, _get_mf_picks, _prep_mf_coils, _check_destination, _remove_meg_projs, _get_coil_scale
if (head_pos is None):
    raise TypeError('head_pos must be provided and cannot be None')
from .chpi import head_pos_to_trans_rot_t
if (not isinstance(epochs, BaseEpochs)):
    raise TypeError(('epochs must be an instance of Epochs, not %s' % (type(epochs),)))
orig_sfreq = (epochs.info['sfreq'] if (orig_sfreq is None) else orig_sfreq)
orig_sfreq = float(orig_sfreq)
if isinstance(head_pos, numpy.ndarray):
    head_pos = head_pos_to_trans_rot_t(head_pos)
(trn, rot, t) = head_pos
del head_pos
_check_usable(epochs)
origin = _check_origin(origin, epochs.info, 'head')
recon_trans = _check_destination(destination, epochs.info, True)
utils.logger.info(('Aligning and averaging up to %s epochs' % len(epochs.events)))
tempResult = unique(epochs.events[:, 0])
	
===================================================================	
BaseEpochs.__init__: 96	
----------------------------	

self.verbose = verbose
if (on_missing not in ['error', 'warning', 'ignore']):
    raise ValueError(('on_missing must be one of: error, warning, ignore. Got: %s' % on_missing))
if (event_id is None):
    tempResult = unique(events[:, 2])
	
===================================================================	
BaseEpochs.__init__: 135	
----------------------------	

self.verbose = verbose
if (on_missing not in ['error', 'warning', 'ignore']):
    raise ValueError(('on_missing must be one of: error, warning, ignore. Got: %s' % on_missing))
if (event_id is None):
    event_id = list(numpy.unique(events[:, 2]))
if isinstance(event_id, dict):
    for key in event_id.keys():
        if (not isinstance(key, string_types)):
            raise TypeError(('Event names must be of type str, got %s (%s)' % (key, type(key))))
    event_id = dict(((key, _ensure_int(val, ('event_id[%s]' % key))) for (key, val) in event_id.items()))
elif isinstance(event_id, list):
    event_id = [_ensure_int(v, ('event_id[%s]' % vi)) for (vi, v) in enumerate(event_id)]
    event_id = dict(zip((str(i) for i in event_id), event_id))
else:
    event_id = _ensure_int(event_id, 'event_id')
    event_id = {str(event_id): event_id}
self.event_id = event_id
del event_id
if (events is not None):
    if (events.dtype.kind not in ['i', 'u']):
        raise ValueError('events must be an array of type int')
    if ((events.ndim != 2) or (events.shape[1] != 3)):
        raise ValueError('events must be 2D with 3 columns')
    for (key, val) in self.event_id.items():
        if (val not in events[:, 2]):
            msg = ('No matching events found for %s (event id %i)' % (key, val))
            if (on_missing == 'error'):
                raise ValueError(msg)
            elif (on_missing == 'warning'):
                warn(msg)
            else:
                pass
    values = list(self.event_id.values())
    selected = numpy.in1d(events[:, 2], values)
    if (selection is None):
        self.selection = numpy.where(selected)[0]
    else:
        self.selection = selection
    if (drop_log is None):
        self.drop_log = [(list() if (k in self.selection) else ['IGNORED']) for k in range(len(events))]
    else:
        self.drop_log = drop_log
    events = events[selected]
    tempResult = unique(events[:, 0])
	
===================================================================	
_read_one_epoch_file: 1073	
----------------------------	

'Read a single FIF file.'
with f as fid:
    (info, meas) = read_meas_info(fid, tree, clean_bads=True)
    (events, mappings) = _read_events_fif(fid, tree)
    processed = dir_tree_find(meas, io.constants.FIFF.FIFFB_PROCESSED_DATA)
    if (len(processed) == 0):
        raise ValueError('Could not find processed data')
    epochs_node = dir_tree_find(tree, io.constants.FIFF.FIFFB_MNE_EPOCHS)
    if (len(epochs_node) == 0):
        epochs_node = dir_tree_find(tree, io.constants.FIFF.FIFFB_MNE_EPOCHS)
        if (len(epochs_node) == 0):
            epochs_node = dir_tree_find(tree, 122)
            if (len(epochs_node) == 0):
                raise ValueError('Could not find epochs data')
    my_epochs = epochs_node[0]
    data = None
    data_tag = None
    (bmin, bmax) = (None, None)
    baseline = None
    selection = None
    drop_log = None
    for k in range(my_epochs['nent']):
        kind = my_epochs['directory'][k].kind
        pos = my_epochs['directory'][k].pos
        if (kind == io.constants.FIFF.FIFF_FIRST_SAMPLE):
            tag = read_tag(fid, pos)
            first = int(tag.data)
        elif (kind == io.constants.FIFF.FIFF_LAST_SAMPLE):
            tag = read_tag(fid, pos)
            last = int(tag.data)
        elif (kind == io.constants.FIFF.FIFF_EPOCH):
            fid.seek(pos, 0)
            data_tag = read_tag_info(fid)
            data_tag.pos = pos
        elif (kind in [io.constants.FIFF.FIFF_MNE_BASELINE_MIN, 304]):
            tag = read_tag(fid, pos)
            bmin = float(tag.data)
        elif (kind in [io.constants.FIFF.FIFF_MNE_BASELINE_MAX, 305]):
            tag = read_tag(fid, pos)
            bmax = float(tag.data)
        elif (kind == io.constants.FIFF.FIFFB_MNE_EPOCHS_SELECTION):
            tag = read_tag(fid, pos)
            selection = numpy.array(tag.data)
        elif (kind == io.constants.FIFF.FIFFB_MNE_EPOCHS_DROP_LOG):
            tag = read_tag(fid, pos)
            drop_log = json.loads(tag.data)
    if ((bmin is not None) or (bmax is not None)):
        baseline = (bmin, bmax)
    n_samp = ((last - first) + 1)
    utils.logger.info('    Found the data of interest:')
    utils.logger.info(('        t = %10.2f ... %10.2f ms' % (((1000 * first) / info['sfreq']), ((1000 * last) / info['sfreq']))))
    if (info['comps'] is not None):
        utils.logger.info(('        %d CTF compensation matrices available' % len(info['comps'])))
    if (data_tag is None):
        raise ValueError('Epochs data not found')
    epoch_shape = (len(info['ch_names']), n_samp)
    expected = (len(events) * numpy.prod(epoch_shape))
    if (((data_tag.size // 4) - 4) != expected):
        raise ValueError(('Incorrect number of samples (%d instead of %d)' % ((data_tag.size // 4), expected)))
    cals = numpy.array([[(info['chs'][k]['cal'] * info['chs'][k].get('scale', 1.0))] for k in range(info['nchan'])], numpy.float64)
    if preload:
        data = read_tag(fid, data_tag.pos).data.astype(numpy.float64)
        data *= cals[numpy.newaxis, :, :]
    tmin = (first / info['sfreq'])
    tmax = (last / info['sfreq'])
    tempResult = unique(events[:, 2])
	
===================================================================	
EpochsFIF.__init__: 1120	
----------------------------	

check_fname(fname, 'epochs', ('-epo.fif', '-epo.fif.gz'))
fnames = [fname]
ep_list = list()
raw = list()
for fname in fnames:
    utils.logger.info(('Reading %s ...' % fname))
    (fid, tree, _) = fiff_open(fname)
    next_fname = _get_next_fname(fid, fname, tree)
    (info, data, data_tag, events, event_id, tmin, tmax, baseline, selection, drop_log, epoch_shape, cals) = _read_one_epoch_file(fid, tree, preload)
    epoch = BaseEpochs(info, data, events, event_id, tmin, tmax, baseline, on_missing='ignore', selection=selection, drop_log=drop_log, proj=False, verbose=False)
    ep_list.append(epoch)
    if (not preload):
        raw.append(_RawContainer(fiff_open(fname)[0], data_tag, events[:, 0].copy(), epoch_shape, cals))
    if (next_fname is not None):
        fnames.append(next_fname)
(info, data, events, event_id, tmin, tmax, baseline, selection, drop_log, _) = _concatenate_epochs(ep_list, with_data=preload)
tempResult = unique(events[:, 0])
	
===================================================================	
_find_unique_events: 260	
----------------------------	

'Uniquify events (ie remove duplicated rows.'
e = np.ascontiguousarray(events).view(numpy.dtype((numpy.void, (events.dtype.itemsize * events.shape[1]))))
tempResult = unique(e, return_index=True)
	
===================================================================	
_find_events: 254	
----------------------------	

'Help find events.'
assert (data.shape[0] == 1)
if (min_samples > 0):
    merge = int((min_samples // 1))
    if (merge == min_samples):
        merge -= 1
else:
    merge = 0
data = data.astype(numpy.int)
if uint_cast:
    data = data.astype(np.uint16).astype(numpy.int)
if (data.min() < 0):
    warn('Trigger channel contains negative values, using absolute value. If data were acquired on a Neuromag system with STI016 active, consider using uint_cast=True to work around an acquisition bug')
    data = numpy.abs(data)
events = _find_stim_steps(data, first_samp, pad_stop=0, merge=merge)
events = _mask_trigs(events, mask, mask_type)
if (consecutive == 'increasing'):
    onsets = (events[:, 2] > events[:, 1])
    offsets = numpy.logical_and(numpy.logical_or(onsets, (events[:, 2] == 0)), (events[:, 1] > 0))
elif consecutive:
    onsets = (events[:, 2] > 0)
    offsets = (events[:, 1] > 0)
else:
    onsets = (events[:, 1] == 0)
    offsets = (events[:, 2] == 0)
onset_idx = numpy.where(onsets)[0]
offset_idx = numpy.where(offsets)[0]
if ((len(onset_idx) == 0) or (len(offset_idx) == 0)):
    return numpy.empty((0, 3), dtype='int32')
if (onset_idx[0] > offset_idx[0]):
    utils.logger.info('Removing orphaned offset at the beginning of the file.')
    offset_idx = numpy.delete(offset_idx, 0)
if (onset_idx[(- 1)] > offset_idx[(- 1)]):
    utils.logger.info('Removing orphaned onset at the end of the file.')
    onset_idx = numpy.delete(onset_idx, (- 1))
if (output == 'onset'):
    events = events[onset_idx]
elif (output == 'step'):
    idx = numpy.union1d(onset_idx, offset_idx)
    events = events[idx]
elif (output == 'offset'):
    event_id = events[(onset_idx, 2)]
    events = events[offset_idx]
    events[:, 1] = events[:, 2]
    events[:, 2] = event_id
    events[:, 0] -= 1
else:
    raise ValueError(('Invalid output parameter %r' % output))
utils.logger.info(('%s events found' % len(events)))
tempResult = unique(events[:, 2])
	
===================================================================	
merge_events: 322	
----------------------------	

"Merge a set of events.\n\n    Parameters\n    ----------\n    events : array, shape (n_events_in, 3)\n        Events.\n    ids : array of int\n        The ids of events to merge.\n    new_id : int\n        The new id.\n    replace_events : bool\n        If True (default), old event ids are replaced. Otherwise,\n        new events will be added to the old event list.\n\n    Returns\n    -------\n    new_events: array, shape (n_events_out, 3)\n        The new events\n\n    Examples\n    --------\n    Here is quick example of the behavior::\n\n        >>> events = [[134, 0, 1], [341, 0, 2], [502, 0, 3]]\n        >>> merge_events(events, [1, 2], 12, replace_events=True)\n        array([[134,   0,  12],\n               [341,   0,  12],\n               [502,   0,   3]])\n        >>> merge_events(events, [1, 2], 12, replace_events=False)\n        array([[134,   0,   1],\n               [134,   0,  12],\n               [341,   0,   2],\n               [341,   0,  12],\n               [502,   0,   3]])\n\n    Notes\n    -----\n    Rather than merging events you can use hierarchical event_id\n    in Epochs. For example, here::\n\n        >>> event_id = {'auditory/left': 1, 'auditory/right': 2}\n\n    And the condition 'auditory' would correspond to either 1 or 2.\n    "
events = numpy.asarray(events)
events_out = events.copy()
idx_touched = []
for col in [1, 2]:
    for i in ids:
        mask = (events[:, col] == i)
        events_out[(mask, col)] = new_id
        idx_touched.append(numpy.where(mask)[0])
if (not replace_events):
    tempResult = unique(numpy.concatenate(idx_touched))
	
===================================================================	
_mt_spectrum_remove: 544	
----------------------------	

'Use MT-spectrum to remove line frequencies.\n\n    Based on Chronux. If line_freqs is specified, all freqs within notch_width\n    of each line_freq is set to zero.\n    '
n_tapers = len(window_fun)
tapers_odd = numpy.arange(0, n_tapers, 2)
tapers_even = numpy.arange(1, n_tapers, 2)
tapers_use = window_fun[tapers_odd]
H0 = numpy.sum(tapers_use, axis=1)
H0_sq = sum_squared(H0)
rads = ((2 * numpy.pi) * (numpy.arange(x.size) / float(sfreq)))
(x_p, freqs) = _mt_spectra(x[numpy.newaxis, :], window_fun, sfreq)
x_p_H0 = numpy.sum((x_p[:, tapers_odd, :] * H0[numpy.newaxis, :, numpy.newaxis]), axis=1)
A = (x_p_H0 / H0_sq)
if (line_freqs is None):
    x_hat = (A * H0[:, numpy.newaxis])
    num = (((n_tapers - 1) * (A * A.conj()).real) * H0_sq)
    den = (numpy.sum((numpy.abs((x_p[:, tapers_odd, :] - x_hat)) ** 2), 1) + numpy.sum((numpy.abs(x_p[:, tapers_even, :]) ** 2), 1))
    den[(den == 0)] = numpy.inf
    f_stat = (num / den)
    indices = numpy.where((f_stat > threshold))[1]
    rm_freqs = freqs[indices]
else:
    tempResult = unique([numpy.argmin(numpy.abs((freqs - lf))) for lf in line_freqs])
	
===================================================================	
_mt_spectrum_remove: 548	
----------------------------	

'Use MT-spectrum to remove line frequencies.\n\n    Based on Chronux. If line_freqs is specified, all freqs within notch_width\n    of each line_freq is set to zero.\n    '
n_tapers = len(window_fun)
tapers_odd = numpy.arange(0, n_tapers, 2)
tapers_even = numpy.arange(1, n_tapers, 2)
tapers_use = window_fun[tapers_odd]
H0 = numpy.sum(tapers_use, axis=1)
H0_sq = sum_squared(H0)
rads = ((2 * numpy.pi) * (numpy.arange(x.size) / float(sfreq)))
(x_p, freqs) = _mt_spectra(x[numpy.newaxis, :], window_fun, sfreq)
x_p_H0 = numpy.sum((x_p[:, tapers_odd, :] * H0[numpy.newaxis, :, numpy.newaxis]), axis=1)
A = (x_p_H0 / H0_sq)
if (line_freqs is None):
    x_hat = (A * H0[:, numpy.newaxis])
    num = (((n_tapers - 1) * (A * A.conj()).real) * H0_sq)
    den = (numpy.sum((numpy.abs((x_p[:, tapers_odd, :] - x_hat)) ** 2), 1) + numpy.sum((numpy.abs(x_p[:, tapers_even, :]) ** 2), 1))
    den[(den == 0)] = numpy.inf
    f_stat = (num / den)
    indices = numpy.where((f_stat > threshold))[1]
    rm_freqs = freqs[indices]
else:
    indices_1 = numpy.unique([numpy.argmin(numpy.abs((freqs - lf))) for lf in line_freqs])
    notch_widths /= 2.0
    indices_2 = [numpy.logical_and((freqs > (lf - nw)), (freqs < (lf + nw))) for (lf, nw) in zip(line_freqs, notch_widths)]
    indices_2 = numpy.where(numpy.any(numpy.array(indices_2), axis=0))[0]
    tempResult = unique(numpy.r_[(indices_1, indices_2)])
	
===================================================================	
Label.get_tris: 300	
----------------------------	

"Get the source space's triangles inside the label.\n\n        Parameters\n        ----------\n        tris : ndarray of int, shape (n_tris, 3)\n            The set of triangles corresponding to the vertices in a\n            source space.\n        vertices : ndarray of int, shape (n_vertices,) | None\n            The set of vertices to compare the label to. If None, equals to\n            ``np.arange(10242)``. Defaults to None.\n\n        Returns\n        -------\n        label_tris : ndarray of int, shape (n_tris, 3)\n            The subset of tris used by the label\n        "
vertices_ = self.get_vertices_used(vertices)
selection = numpy.all(np.in1d(tris, vertices_).reshape(tris.shape), axis=1)
label_tris = tris[selection]
tempResult = unique(label_tris)
	
===================================================================	
Label.get_tris: 304	
----------------------------	

"Get the source space's triangles inside the label.\n\n        Parameters\n        ----------\n        tris : ndarray of int, shape (n_tris, 3)\n            The set of triangles corresponding to the vertices in a\n            source space.\n        vertices : ndarray of int, shape (n_vertices,) | None\n            The set of vertices to compare the label to. If None, equals to\n            ``np.arange(10242)``. Defaults to None.\n\n        Returns\n        -------\n        label_tris : ndarray of int, shape (n_tris, 3)\n            The subset of tris used by the label\n        "
vertices_ = self.get_vertices_used(vertices)
selection = numpy.all(np.in1d(tris, vertices_).reshape(tris.shape), axis=1)
label_tris = tris[selection]
if (len(numpy.unique(label_tris)) < len(vertices_)):
    utils.logger.info('Surprising label structure. Trying to repair triangles.')
    dropped_vertices = numpy.setdiff1d(vertices_, label_tris)
    n_dropped = len(dropped_vertices)
    tempResult = unique(label_tris)
	
===================================================================	
Label.get_tris: 307	
----------------------------	

"Get the source space's triangles inside the label.\n\n        Parameters\n        ----------\n        tris : ndarray of int, shape (n_tris, 3)\n            The set of triangles corresponding to the vertices in a\n            source space.\n        vertices : ndarray of int, shape (n_vertices,) | None\n            The set of vertices to compare the label to. If None, equals to\n            ``np.arange(10242)``. Defaults to None.\n\n        Returns\n        -------\n        label_tris : ndarray of int, shape (n_tris, 3)\n            The subset of tris used by the label\n        "
vertices_ = self.get_vertices_used(vertices)
selection = numpy.all(np.in1d(tris, vertices_).reshape(tris.shape), axis=1)
label_tris = tris[selection]
if (len(numpy.unique(label_tris)) < len(vertices_)):
    utils.logger.info('Surprising label structure. Trying to repair triangles.')
    dropped_vertices = numpy.setdiff1d(vertices_, label_tris)
    n_dropped = len(dropped_vertices)
    assert (n_dropped == (len(vertices_) - len(numpy.unique(label_tris))))
    add_tris = (dropped_vertices + np.zeros((len(dropped_vertices), 3), dtype=int).T)
    label_tris = numpy.r_[(label_tris, add_tris.T)]
    tempResult = unique(label_tris)
	
===================================================================	
_spatio_temporal_src_connectivity_ico: 1125	
----------------------------	

if (src[0]['use_tris'] is None):
    raise RuntimeError('The source space does not appear to be an ico surface. Connectivity cannot be extracted from non-ico source spaces.')
tempResult = unique(s['use_tris'])
	
===================================================================	
spatio_temporal_tris_connectivity: 1172	
----------------------------	

'Compute connectivity from triangles and time instants.\n\n    Parameters\n    ----------\n    tris : array\n        N x 3 array defining triangles.\n    n_times : int\n        Number of time points\n    remap_vertices : bool\n        Reassign vertex indices based on unique values. Useful\n        to process a subset of triangles. Defaults to False.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    connectivity : sparse COO matrix\n        The connectivity matrix describing the spatio-temporal\n        graph structure. If N is the number of vertices in the\n        source space, the N first nodes in the graph are the\n        vertices are time 1, the nodes from 2 to 2N are the vertices\n        during time 2, etc.\n    '
if remap_vertices:
    utils.logger.info('Reassigning vertex indices.')
    tempResult = unique(tris)
	
===================================================================	
get_volume_labels_from_aseg: 1345	
----------------------------	

'Return a list of names and colors of segmented volumes.\n\n    Parameters\n    ----------\n    mgz_fname : str\n        Filename to read. Typically aseg.mgz or some variant in the freesurfer\n        pipeline.\n    return_colors : bool\n        If True returns also the labels colors\n\n    Returns\n    -------\n    label_names : list of str\n        The names of segmented volumes included in this mgz file.\n    label_colors : list of str\n        The RGB colors of the labels included in this mgz file.\n\n    Notes\n    -----\n    .. versionadded:: 0.9.0\n    '
import nibabel as nib
mgz_data = nib.load(mgz_fname).get_data()
lut = _get_lut()
tempResult = unique(mgz_data)
	
===================================================================	
get_volume_labels_from_aseg: 1346	
----------------------------	

'Return a list of names and colors of segmented volumes.\n\n    Parameters\n    ----------\n    mgz_fname : str\n        Filename to read. Typically aseg.mgz or some variant in the freesurfer\n        pipeline.\n    return_colors : bool\n        If True returns also the labels colors\n\n    Returns\n    -------\n    label_names : list of str\n        The names of segmented volumes included in this mgz file.\n    label_colors : list of str\n        The RGB colors of the labels included in this mgz file.\n\n    Notes\n    -----\n    .. versionadded:: 0.9.0\n    '
import nibabel as nib
mgz_data = nib.load(mgz_fname).get_data()
lut = _get_lut()
label_names = [lut[(lut['id'] == ii)]['name'][0].decode('utf-8') for ii in numpy.unique(mgz_data)]
tempResult = unique(mgz_data)
	
===================================================================	
mesh_edges: 630	
----------------------------	

'Return sparse matrix with edges as an adjacency matrix.\n\n    Parameters\n    ----------\n    tris : array of shape [n_triangles x 3]\n        The triangles.\n\n    Returns\n    -------\n    edges : sparse matrix\n        The adjacency matrix.\n    '
tempResult = unique(tris)
	
===================================================================	
rename_channels: 334	
----------------------------	

"Rename channels.\n\n    .. warning::  The channel names must have at most 15 characters\n\n    Parameters\n    ----------\n    info : dict\n        Measurement info.\n    mapping : dict | callable\n        a dictionary mapping the old channel to a new channel name\n        e.g. {'EEG061' : 'EEG161'}. Can also be a callable function\n        that takes and returns a string (new in version 0.10.0).\n    "
info._check_consistency()
bads = list(info['bads'])
ch_names = list(info['ch_names'])
if isinstance(mapping, dict):
    orig_names = sorted(list(mapping.keys()))
    missing = [(orig_name not in ch_names) for orig_name in orig_names]
    if any(missing):
        raise ValueError(('Channel name(s) in mapping missing from info: %s' % numpy.array(orig_names)[numpy.array(missing)]))
    new_names = [(ch_names.index(ch_name), new_name) for (ch_name, new_name) in mapping.items()]
elif callable(mapping):
    new_names = [(ci, mapping(ch_name)) for (ci, ch_name) in enumerate(ch_names)]
else:
    raise ValueError(('mapping must be callable or dict, not %s' % (type(mapping),)))
if any(((not isinstance(new_name[1], string_types)) for new_name in new_names)):
    raise ValueError('New channel mapping must only be to strings')
bad_new_names = [name for (_, name) in new_names if (len(name) > 15)]
if len(bad_new_names):
    raise ValueError(('Channel names cannot be longer than 15 characters. These channel names are not valid : %s' % new_names))
for (c_ind, new_name) in new_names:
    for (bi, bad) in enumerate(bads):
        if (bad == ch_names[c_ind]):
            bads[bi] = new_name
    ch_names[c_ind] = new_name
tempResult = unique(ch_names)
	
===================================================================	
_compute_ch_connectivity: 440	
----------------------------	

"Compute channel connectivity matrix using Delaunay triangulations.\n\n    Parameters\n    ----------\n    info : instance of mne.measuerment_info.Info\n        The measurement info.\n    ch_type : str\n        The channel type for computing the connectivity matrix. Currently\n        supports 'mag', 'grad' and 'eeg'.\n\n    Returns\n    -------\n    ch_connectivity : scipy.sparse matrix, shape (n_channels, n_channels)\n        The connectivity matrix.\n    ch_names : list\n        The list of channel names present in connectivity matrix.\n    "
from scipy.spatial import Delaunay
from .. import spatial_tris_connectivity
from ..channels.layout import _auto_topomap_coords, _pair_grad_sensors
tempResult = unique([ch['coil_type'] for ch in info['chs']])
	
===================================================================	
spectral_connectivity: 422	
----------------------------	

'Compute frequency- and time-frequency-domain connectivity measures.\n\n    The connectivity method(s) are specified using the "method" parameter.\n    All methods are based on estimates of the cross- and power spectral\n    densities (CSD/PSD) Sxy and Sxx, Syy.\n\n    The spectral densities can be estimated using a multitaper method with\n    digital prolate spheroidal sequence (DPSS) windows, a discrete Fourier\n    transform with Hanning windows, or a continuous wavelet transform using\n    Morlet wavelets. The spectral estimation mode is specified using the\n    "mode" parameter.\n\n    By default, the connectivity between all signals is computed (only\n    connections corresponding to the lower-triangular part of the\n    connectivity matrix). If one is only interested in the connectivity\n    between some signals, the "indices" parameter can be used. For example,\n    to compute the connectivity between the signal with index 0 and signals\n    "2, 3, 4" (a total of 3 connections) one can use the following::\n\n        indices = (np.array([0, 0, 0]),    # row indices\n                   np.array([2, 3, 4]))    # col indices\n\n        con_flat = spectral_connectivity(data, method=\'coh\',\n                                         indices=indices, ...)\n\n    In this case con_flat.shape = (3, n_freqs). The connectivity scores are\n    in the same order as defined indices.\n\n    **Supported Connectivity Measures**\n\n    The connectivity method(s) is specified using the "method" parameter. The\n    following methods are supported (note: ``E[]`` denotes average over\n    epochs). Multiple measures can be computed at once by using a list/tuple,\n    e.g., ``[\'coh\', \'pli\']`` to compute coherence and PLI.\n\n        \'coh\' : Coherence given by::\n\n                     | E[Sxy] |\n            C = ---------------------\n                sqrt(E[Sxx] * E[Syy])\n\n        \'cohy\' : Coherency given by::\n\n                       E[Sxy]\n            C = ---------------------\n                sqrt(E[Sxx] * E[Syy])\n\n        \'imcoh\' : Imaginary coherence [1]_ given by::\n\n                      Im(E[Sxy])\n            C = ----------------------\n                sqrt(E[Sxx] * E[Syy])\n\n        \'plv\' : Phase-Locking Value (PLV) [2]_ given by::\n\n            PLV = |E[Sxy/|Sxy|]|\n\n        \'ppc\' : Pairwise Phase Consistency (PPC), an unbiased estimator\n        of squared PLV [3]_.\n\n        \'pli\' : Phase Lag Index (PLI) [4]_ given by::\n\n            PLI = |E[sign(Im(Sxy))]|\n\n        \'pli2_unbiased\' : Unbiased estimator of squared PLI [5]_.\n\n        \'wpli\' : Weighted Phase Lag Index (WPLI) [5]_ given by::\n\n                      |E[Im(Sxy)]|\n            WPLI = ------------------\n                      E[|Im(Sxy)|]\n\n        \'wpli2_debiased\' : Debiased estimator of squared WPLI [5]_.\n\n\n    Parameters\n    ----------\n    data : array-like, shape=(n_epochs, n_signals, n_times) | Epochs\n        The data from which to compute connectivity. Note that it is also\n        possible to combine multiple signals by providing a list of tuples,\n        e.g., data = [(arr_0, stc_0), (arr_1, stc_1), (arr_2, stc_2)],\n        corresponds to 3 epochs, and arr_* could be an array with the same\n        number of time points as stc_*. The array-like object can also\n        be a list/generator of array, shape =(n_signals, n_times),\n        or a list/generator of SourceEstimate or VolSourceEstimate objects.\n    method : string | list of string\n        Connectivity measure(s) to compute.\n    indices : tuple of arrays | None\n        Two arrays with indices of connections for which to compute\n        connectivity. If None, all connections are computed.\n    sfreq : float\n        The sampling frequency.\n    mode : str\n        Spectrum estimation mode can be either: \'multitaper\', \'fourier\', or\n        \'cwt_morlet\'.\n    fmin : float | tuple of floats\n        The lower frequency of interest. Multiple bands are defined using\n        a tuple, e.g., (8., 20.) for two bands with 8Hz and 20Hz lower freq.\n        If None the frequency corresponding to an epoch length of 5 cycles\n        is used.\n    fmax : float | tuple of floats\n        The upper frequency of interest. Multiple bands are dedined using\n        a tuple, e.g. (13., 30.) for two band with 13Hz and 30Hz upper freq.\n    fskip : int\n        Omit every "(fskip + 1)-th" frequency bin to decimate in frequency\n        domain.\n    faverage : boolean\n        Average connectivity scores for each frequency band. If True,\n        the output freqs will be a list with arrays of the frequencies\n        that were averaged.\n    tmin : float | None\n        Time to start connectivity estimation. Note: when "data" is an array,\n        the first sample is assumed to be at time 0. For other types\n        (Epochs, etc.), the time information contained in the object is used\n        to compute the time indices.\n    tmax : float | None\n        Time to end connectivity estimation. Note: when "data" is an array,\n        the first sample is assumed to be at time 0. For other types\n        (Epochs, etc.), the time information contained in the object is used\n        to compute the time indices.\n    mt_bandwidth : float | None\n        The bandwidth of the multitaper windowing function in Hz.\n        Only used in \'multitaper\' mode.\n    mt_adaptive : bool\n        Use adaptive weights to combine the tapered spectra into PSD.\n        Only used in \'multitaper\' mode.\n    mt_low_bias : bool\n        Only use tapers with more than 90% spectral concentration within\n        bandwidth. Only used in \'multitaper\' mode.\n    cwt_freqs : array\n        Array of frequencies of interest. Only used in \'cwt_morlet\' mode.\n    cwt_n_cycles: float | array of float\n        Number of cycles. Fixed number or one per frequency. Only used in\n        \'cwt_morlet\' mode.\n    block_size : int\n        How many connections to compute at once (higher numbers are faster\n        but require more memory).\n    n_jobs : int\n        How many epochs to process in parallel.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    con : array | list of arrays\n        Computed connectivity measure(s). The shape of each array is either\n        (n_signals, n_signals, n_freqs) mode: \'multitaper\' or \'fourier\'\n        (n_signals, n_signals, n_freqs, n_times) mode: \'cwt_morlet\'\n        when "indices" is None, or\n        (n_con, n_freqs) mode: \'multitaper\' or \'fourier\'\n        (n_con, n_freqs, n_times) mode: \'cwt_morlet\'\n        when "indices" is specified and "n_con = len(indices[0])".\n    freqs : array\n        Frequency points at which the connectivity was computed.\n    times : array\n        Time points for which the connectivity was computed.\n    n_epochs : int\n        Number of epochs used for computation.\n    n_tapers : int\n        The number of DPSS tapers used. Only defined in \'multitaper\' mode.\n        Otherwise None is returned.\n\n    References\n    ----------\n    .. [1] Nolte et al. "Identifying true brain interaction from EEG data using\n           the imaginary part of coherency" Clinical neurophysiology, vol. 115,\n           no. 10, pp. 2292-2307, Oct. 2004.\n    .. [2] Lachaux et al. "Measuring phase synchrony in brain signals" Human\n           brain mapping, vol. 8, no. 4, pp. 194-208, Jan. 1999.\n    .. [3] Vinck et al. "The pairwise phase consistency: a bias-free measure of\n           rhythmic neuronal synchronization" NeuroImage, vol. 51, no. 1,\n           pp. 112-122, May 2010.\n    .. [4] Stam et al. "Phase lag index: assessment of functional connectivity\n           from multi channel EEG and MEG with diminished bias from common\n           sources" Human brain mapping, vol. 28, no. 11, pp. 1178-1193,\n           Nov. 2007.\n    .. [5] Vinck et al. "An improved index of phase-synchronization for\n           electro-physiological data in the presence of volume-conduction,\n           noise and sample-size bias" NeuroImage, vol. 55, no. 4,\n           pp. 1548-1565, Apr. 2011.\n    '
cwt_freqs = _freqs_dep(cwt_freqs, cwt_frequencies, 'cwt_')
if (n_jobs != 1):
    (parallel, my_epoch_spectral_connectivity, _) = parallel_func(_epoch_spectral_connectivity, n_jobs, verbose=verbose)
if (fmin is None):
    fmin = (- numpy.inf)
fmin = np.asarray((fmin,)).ravel()
fmax = np.asarray((fmax,)).ravel()
if (len(fmin) != len(fmax)):
    raise ValueError('fmin and fmax must have the same length')
if numpy.any((fmin > fmax)):
    raise ValueError('fmax must be larger than fmin')
n_bands = len(fmin)
if (not isinstance(method, (list, tuple))):
    method = [method]
(con_method_types, n_methods, accumulate_psd, n_comp_args) = _check_estimators(method=method, mode=mode)
if isinstance(data, BaseEpochs):
    times_in = data.times
    sfreq = data.info['sfreq']
epoch_idx = 0
utils.logger.info('Connectivity computation...')
for epoch_block in _get_n_epochs(data, n_jobs):
    if (epoch_idx == 0):
        (n_cons, times, n_times, times_in, n_times_in, tmin_idx, tmax_idx, n_freqs, freq_mask, freqs, freqs_bands, freq_idx_bands, n_signals, indices_use) = _prepare_connectivity(epoch_block=epoch_block, tmin=tmin, tmax=tmax, fmin=fmin, fmax=fmax, sfreq=sfreq, indices=indices, mode=mode, fskip=fskip, n_bands=n_bands, cwt_freqs=cwt_freqs, faverage=faverage)
        (spectral_params, mt_adaptive, n_times_spectrum, n_tapers) = _assemble_spectral_params(mode=mode, n_times=n_times, mt_adaptive=mt_adaptive, mt_bandwidth=mt_bandwidth, sfreq=sfreq, mt_low_bias=mt_low_bias, cwt_n_cycles=cwt_n_cycles, cwt_freqs=cwt_freqs, freqs=freqs, freq_mask=freq_mask)
        tempResult = unique(numpy.r_[(indices_use[0], indices_use[1])])
	
===================================================================	
CSP.fit: 54	
----------------------------	

'Estimate the CSP decomposition on epochs.\n\n        Parameters\n        ----------\n        X : ndarray, shape (n_epochs, n_channels, n_times)\n            The data on which to estimate the CSP.\n        y : array, shape (n_epochs,)\n            The class for each epoch.\n\n        Returns\n        -------\n        self : instance of CSP\n            Returns the modified instance.\n        '
if (not isinstance(X, numpy.ndarray)):
    raise ValueError(('X should be of type ndarray (got %s).' % type(X)))
self._check_Xy(X, y)
n_channels = X.shape[1]
tempResult = unique(y)
	
===================================================================	
SPoC.fit: 202	
----------------------------	

'Estimate the SPoC decomposition on epochs.\n\n        Parameters\n        ----------\n        X : ndarray, shape (n_epochs, n_channels, n_times)\n            The data on which to estimate the SPoC.\n        y : array, shape (n_epochs,)\n            The class for each epoch.\n\n        Returns\n        -------\n        self : instance of SPoC\n            Returns the modified instance.\n        '
if (not isinstance(X, numpy.ndarray)):
    raise ValueError(('X should be of type ndarray (got %s).' % type(X)))
self._check_Xy(X, y)
tempResult = unique(y)
	
===================================================================	
EMS.fit: 21	
----------------------------	

'Fit the spatial filters.\n\n        .. note : EMS is fitted on data normalized by channel type before the\n                  fitting of the spatial filters.\n\n        Parameters\n        ----------\n        X : array, shape (n_epochs, n_channels, n_times)\n            The training data.\n        y : array of int, shape (n_epochs)\n            The target classes.\n\n        Returns\n        -------\n        self : returns and instance of self.\n        '
tempResult = unique(y)
	
===================================================================	
_predict_slices: 215	
----------------------------	

"Aux function of GeneralizationAcrossTime.\n\n    Run classifiers predictions loop across time samples.\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_epochs, n_features, n_times)\n        To-be-fitted data.\n    estimators : list of array-like, shape (n_times, n_folds)\n        List of array of scikit-learn classifiers fitted in cross-validation.\n    cv_splits : list of tuples\n        List of tuples of train and test array generated from cv.\n    train_times : list\n        List of list of slices selecting data from X from which is prediction\n        is generated.\n    predict_method : str\n        Specifies prediction method for the estimator.\n    predict_mode : {'cross-validation', 'mean-prediction'}\n        Indicates how predictions are achieved with regards to the cross-\n        validation procedure:\n            'cross-validation' : estimates a single prediction per sample based\n                on the unique independent classifier fitted in the cross-\n                validation.\n            'mean-prediction' : estimates k predictions per sample, based on\n                each of the k-fold cross-validation classifiers, and average\n                these predictions into a single estimate per sample.\n        Default: 'cross-validation'\n    n_orig_epochs : int\n        Original number of predicted epochs before slice definition. Note\n        that the number of epochs may have been cropped if the cross validation\n        is not deterministic (e.g. with ShuffleSplit, we may only predict a\n        subset of epochs).\n    test_epochs : list of slices\n        List of slices to select the tested epoched in the cv.\n    "
(n_epochs, _, n_times) = X.shape
n_train = len(estimators)
n_test = [len(test_t_idxs) for test_t_idxs in train_times]
y_pred = None
for (train_t_idx, (estimator_cv, test_t_idxs)) in enumerate(zip(estimators, train_times)):
    start = numpy.arange(n_times)
    contiguous_start = numpy.array_equal([sl[0] for sl in test_t_idxs], start)
    tempResult = unique([len(sl) for sl in test_t_idxs])
	
===================================================================	
_DecodingTime.__repr__: 26	
----------------------------	

s = ''
if ('start' in self):
    s += ('start: %0.3f (s)' % self['start'])
if ('stop' in self):
    s += (', stop: %0.3f (s)' % self['stop'])
if ('step' in self):
    s += (', step: %0.3f (s)' % self['step'])
if ('length' in self):
    s += (', length: %0.3f (s)' % self['length'])
if ('slices' in self):
    depth = [len(ii) for ii in self['slices']]
    tempResult = unique(depth)
	
===================================================================	
_fit_slices: 299	
----------------------------	

'Aux function of GeneralizationAcrossTime.\n\n    Fit each classifier.\n\n    Parameters\n    ----------\n    clf : scikit-learn classifier\n        The classifier object.\n    x_chunk : ndarray, shape (n_epochs, n_features, n_times)\n        To-be-fitted data.\n    y : list | array, shape (n_epochs,)\n        To-be-fitted model.\n    slices : list | array, shape (n_training_slice,)\n        List of training slices, indicating time sample relative to X\n    cv_splits : list of tuples\n        List of (train, test) tuples generated from cv.split()\n\n    Returns\n    -------\n    estimators : list of lists of estimators\n        List of fitted scikit-learn classifiers corresponding to each training\n        slice.\n    '
from sklearn.base import clone
n_epochs = len(x_chunk)
estimators = list()
tempResult = unique([val for sl in slices for val in sl])
	
===================================================================	
_GeneralizationAcrossTime.fit: 77	
----------------------------	

'Train a classifier on each specified time slice.\n\n        .. note::\n            This function sets the ``picks_``, ``ch_names``, ``cv_``,\n            ``y_train``, ``train_times_`` and ``estimators_`` attributes.\n\n        Parameters\n        ----------\n        epochs : instance of Epochs\n            The epochs.\n        y : list or ndarray of int, shape (n_samples,) or None, optional\n            To-be-fitted model values. If None, y = epochs.events[:, 2].\n\n        Returns\n        -------\n        self : GeneralizationAcrossTime\n            Returns fitted GeneralizationAcrossTime object.\n\n        Notes\n        -----\n        If X and y are not C-ordered and contiguous arrays of np.float64 and\n        X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n\n        If X is a dense array, then the other methods will not support sparse\n        matrices as input.\n        '
from sklearn.base import clone
for att in ['picks_', 'ch_names', 'y_train_', 'cv_', 'train_times_', 'estimators_', 'test_times_', 'y_pred_', 'y_true_', 'scores_', 'scorer_']:
    if hasattr(self, att):
        delattr(self, att)
n_jobs = self.n_jobs
(X, y, self.picks_) = _check_epochs_input(epochs, y, self.picks)
self.ch_names = [epochs.ch_names[p] for p in self.picks_]
(self.cv_, self._cv_splits) = _set_cv(self.cv, self.clf, X=X, y=y)
self.y_train_ = y
self.train_times_ = _sliding_window(epochs.times, self.train_times, epochs.info['sfreq'])
(parallel, p_func, n_jobs) = parallel_func(_fit_slices, n_jobs)
n_chunks = min(len(self.train_times_['slices']), n_jobs)
time_chunks = numpy.array_split(self.train_times_['slices'], n_chunks)
tempResult = unique(numpy.concatenate(time_chunk))
	
===================================================================	
_GeneralizationAcrossTime.score: 188	
----------------------------	

"Score Epochs.\n\n        Estimate scores across trials by comparing the prediction estimated for\n        each trial to its true value.\n\n        Calls ``predict()`` if it has not been already.\n\n        .. note::\n            The function updates the ``scorer_``, ``scores_``, and\n            ``y_true_`` attributes.\n\n        .. note::\n            If ``predict_mode`` is 'mean-prediction', ``score_mode`` is\n            automatically set to 'mean-sample-wise'.\n\n        Parameters\n        ----------\n        epochs : instance of Epochs | None, optional\n            The epochs. Can be similar to fitted epochs or not.\n            If None, it needs to rely on the predictions ``y_pred_``\n            generated with ``predict()``.\n        y : list | ndarray, shape (n_epochs,) | None, optional\n            True values to be compared with the predictions ``y_pred_``\n            generated with ``predict()`` via ``scorer_``.\n            If None and ``predict_mode``=='cross-validation' y = ``y_train_``.\n\n        Returns\n        -------\n        scores : list of lists of float\n            The scores estimated by ``scorer_`` at each training time and each\n            testing time (e.g. mean accuracy of ``predict(X)``). Note that the\n            number of testing times per training time need not be regular;\n            else, np.shape(scores) = (n_train_time, n_test_time). If\n            ``score_mode`` is 'fold-wise', np.shape(scores) = (n_train_time,\n            n_test_time, n_folds).\n        "
import sklearn.metrics
from sklearn.metrics import accuracy_score, mean_squared_error
if (epochs is not None):
    self.predict(epochs)
elif (not hasattr(self, 'y_pred_')):
    raise RuntimeError('Please predict() epochs first or pass epochs to score()')
if (self.score_mode not in ('fold-wise', 'mean-fold-wise', 'mean-sample-wise')):
    raise ValueError(("score_mode must be 'fold-wise', 'mean-fold-wise' or 'mean-sample-wise'. Got %s instead'" % self.score_mode))
score_mode = self.score_mode
if ((self.predict_mode == 'mean-prediction') and (self.score_mode != 'mean-sample-wise')):
    warn(("score_mode changed from %s set to 'mean-sample-wise' because predict_mode is 'mean-prediction'." % self.score_mode))
    score_mode = 'mean-sample-wise'
self.scorer_ = self.scorer
if (self.scorer_ is None):
    if (self.predict_method == 'predict'):
        if is_classifier(self.clf):
            self.scorer_ = accuracy_score
        elif is_regressor(self.clf):
            self.scorer_ = mean_squared_error
elif isinstance(self.scorer_, str):
    if hasattr(sklearn.metrics, ('%s_score' % self.scorer_)):
        self.scorer_ = getattr(sklearn.metrics, ('%s_score' % self.scorer_))
    else:
        raise KeyError("{0} scorer Doesn't appear to be valid a scikit-learn scorer.".format(self.scorer_))
if (not self.scorer_):
    raise ValueError(('Could not find a scoring metric for clf=%s  and predict_method=%s. Manually define scorer.' % (self.clf, self.predict_method)))
if (y is None):
    if (self.predict_mode == 'cross-validation'):
        y = self.y_train_
    elif (epochs is not None):
        y = epochs.events[:, 2]
    else:
        raise RuntimeError('y is undefined because predict_mode="mean-prediction" and epochs are missing. You need to explicitly specify y.')
    tempResult = unique(y)
	
===================================================================	
_GeneralizationAcrossTime.score: 188	
----------------------------	

"Score Epochs.\n\n        Estimate scores across trials by comparing the prediction estimated for\n        each trial to its true value.\n\n        Calls ``predict()`` if it has not been already.\n\n        .. note::\n            The function updates the ``scorer_``, ``scores_``, and\n            ``y_true_`` attributes.\n\n        .. note::\n            If ``predict_mode`` is 'mean-prediction', ``score_mode`` is\n            automatically set to 'mean-sample-wise'.\n\n        Parameters\n        ----------\n        epochs : instance of Epochs | None, optional\n            The epochs. Can be similar to fitted epochs or not.\n            If None, it needs to rely on the predictions ``y_pred_``\n            generated with ``predict()``.\n        y : list | ndarray, shape (n_epochs,) | None, optional\n            True values to be compared with the predictions ``y_pred_``\n            generated with ``predict()`` via ``scorer_``.\n            If None and ``predict_mode``=='cross-validation' y = ``y_train_``.\n\n        Returns\n        -------\n        scores : list of lists of float\n            The scores estimated by ``scorer_`` at each training time and each\n            testing time (e.g. mean accuracy of ``predict(X)``). Note that the\n            number of testing times per training time need not be regular;\n            else, np.shape(scores) = (n_train_time, n_test_time). If\n            ``score_mode`` is 'fold-wise', np.shape(scores) = (n_train_time,\n            n_test_time, n_folds).\n        "
import sklearn.metrics
from sklearn.metrics import accuracy_score, mean_squared_error
if (epochs is not None):
    self.predict(epochs)
elif (not hasattr(self, 'y_pred_')):
    raise RuntimeError('Please predict() epochs first or pass epochs to score()')
if (self.score_mode not in ('fold-wise', 'mean-fold-wise', 'mean-sample-wise')):
    raise ValueError(("score_mode must be 'fold-wise', 'mean-fold-wise' or 'mean-sample-wise'. Got %s instead'" % self.score_mode))
score_mode = self.score_mode
if ((self.predict_mode == 'mean-prediction') and (self.score_mode != 'mean-sample-wise')):
    warn(("score_mode changed from %s set to 'mean-sample-wise' because predict_mode is 'mean-prediction'." % self.score_mode))
    score_mode = 'mean-sample-wise'
self.scorer_ = self.scorer
if (self.scorer_ is None):
    if (self.predict_method == 'predict'):
        if is_classifier(self.clf):
            self.scorer_ = accuracy_score
        elif is_regressor(self.clf):
            self.scorer_ = mean_squared_error
elif isinstance(self.scorer_, str):
    if hasattr(sklearn.metrics, ('%s_score' % self.scorer_)):
        self.scorer_ = getattr(sklearn.metrics, ('%s_score' % self.scorer_))
    else:
        raise KeyError("{0} scorer Doesn't appear to be valid a scikit-learn scorer.".format(self.scorer_))
if (not self.scorer_):
    raise ValueError(('Could not find a scoring metric for clf=%s  and predict_method=%s. Manually define scorer.' % (self.clf, self.predict_method)))
if (y is None):
    if (self.predict_mode == 'cross-validation'):
        y = self.y_train_
    elif (epochs is not None):
        y = epochs.events[:, 2]
    else:
        raise RuntimeError('y is undefined because predict_mode="mean-prediction" and epochs are missing. You need to explicitly specify y.')
    tempResult = unique(self.y_train_)
	
===================================================================	
restrict_forward_to_label: 763	
----------------------------	

'Restrict forward operator to labels.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    labels : label object | list\n        Label object or list of label objects.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_stc\n    '
message = 'labels must be instance of Label or a list of Label.'
vertices = [numpy.array([], int), numpy.array([], int)]
if (not isinstance(labels, list)):
    labels = [labels]
for label in labels:
    if (not isinstance(label, Label)):
        raise TypeError((message + (' Instead received %s' % type(label))))
    i = (0 if (label.hemi == 'lh') else 1)
    vertices[i] = numpy.append(vertices[i], label.vertices)
tempResult = unique(vert_hemi)
	
===================================================================	
make_forward_dipole: 326	
----------------------------	

'Convert dipole object to source estimate and calculate forward operator.\n\n    The instance of Dipole is converted to a discrete source space,\n    which is then combined with a BEM or a sphere model and\n    the sensor information in info to form a forward operator.\n\n    The source estimate object (with the forward operator) can be projected to\n    sensor-space using :func:`mne.simulation.simulate_evoked`.\n\n    .. note:: If the (unique) time points of the dipole object are unevenly\n              spaced, the first output will be a list of single-timepoint\n              source estimates.\n\n    Parameters\n    ----------\n    dipole : instance of Dipole\n        Dipole object containing position, orientation and amplitude of\n        one or more dipoles. Multiple simultaneous dipoles may be defined by\n        assigning them identical times.\n    bem : str | dict\n        The BEM filename (str) or a loaded sphere model (dict).\n    info : instance of Info\n        The measurement information dictionary. It is sensor-information etc.,\n        e.g., from a real data file.\n    trans : str | None\n        The head<->MRI transform filename. Must be provided unless BEM\n        is a sphere model.\n    n_jobs : int\n        Number of jobs to run in parallel (used in making forward solution).\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    fwd : instance of Forward\n        The forward solution corresponding to the source estimate(s).\n    stc : instance of VolSourceEstimate | list of VolSourceEstimate\n        The dipoles converted to a discrete set of points and associated\n        time courses. If the time points of the dipole are unevenly spaced,\n        a list of single-timepoint source estimates are returned.\n\n    See Also\n    --------\n    mne.simulation.simulate_evoked\n\n    Notes\n    -----\n    .. versionadded:: 0.12.0\n    '
times = dipole.times.copy()
pos = dipole.pos.copy()
amplitude = dipole.amplitude.copy()
ori = dipole.ori.copy()
sources = dict(rr=pos, nn=ori)
sp = _make_discrete_source_space(sources, coord_frame='head')
src = SourceSpaces([sp])
fwd = make_forward_solution(info, trans, src, bem, n_jobs=n_jobs, verbose=verbose)
convert_forward_solution(fwd, surf_ori=False, force_fixed=True, copy=False, use_cps=False, verbose=None)
if (fwd['src'][0]['nuse'] != len(pos)):
    inuse = fwd['src'][0]['inuse'].astype(numpy.bool)
    head = 'The following dipoles are outside the inner skull boundary'
    msg = ((((len(head) * '#') + '\n') + head) + '\n')
    for (t, pos) in zip(times[numpy.logical_not(inuse)], pos[numpy.logical_not(inuse)]):
        msg += '    t={:.0f} ms, pos=({:.0f}, {:.0f}, {:.0f}) mm\n'.format((t * 1000.0), (pos[0] * 1000.0), (pos[1] * 1000.0), (pos[2] * 1000.0))
    msg += (len(head) * '#')
    utils.logger.error(msg)
    raise ValueError('One or more dipoles outside the inner skull.')
tempResult = unique(times)
	
===================================================================	
_make_dipoles_sparse: 137	
----------------------------	

times = (tmin + (tstep * numpy.arange(X.shape[1])))
if (not active_is_idx):
    active_idx = numpy.where(active_set)[0]
else:
    active_idx = active_set
n_dip_per_pos = (1 if is_fixed_orient(forward) else 3)
if (n_dip_per_pos > 1):
    tempResult = unique((active_idx // n_dip_per_pos))
	
===================================================================	
_make_sparse_stc: 110	
----------------------------	

if (not is_fixed_orient(forward)):
    utils.logger.info('combining the current components...')
    X = combine_xyz(X)
if (not active_is_idx):
    active_idx = numpy.where(active_set)[0]
else:
    active_idx = active_set
n_dip_per_pos = (1 if is_fixed_orient(forward) else 3)
if (n_dip_per_pos > 1):
    tempResult = unique((active_idx // n_dip_per_pos))
	
===================================================================	
gamma_map: 116	
----------------------------	

'Hierarchical Bayes (Gamma-MAP) sparse source localization method.\n\n    Models each source time course using a zero-mean Gaussian prior with an\n    unknown variance (gamma) parameter. During estimation, most gammas are\n    driven to zero, resulting in a sparse source estimate, as in\n    [1]_ and [2]_.\n\n    For fixed-orientation forward operators, a separate gamma is used for each\n    source time course, while for free-orientation forward operators, the same\n    gamma is used for the three source time courses at each source space point\n    (separate gammas can be used in this case by using xyz_same_gamma=False).\n\n    Parameters\n    ----------\n    evoked : instance of Evoked\n        Evoked data to invert.\n    forward : dict\n        Forward operator.\n    noise_cov : instance of Covariance\n        Noise covariance to compute whitener.\n    alpha : float\n        Regularization parameter (noise variance).\n    loose : float in [0, 1] | \'auto\'\n        Value that weights the source variances of the dipole components\n        that are parallel (tangential) to the cortical surface. If loose\n        is 0 then the solution is computed with fixed orientation.\n        If loose is 1, it corresponds to free orientations.\n        The default value (\'auto\') is set to 0.2 for surface-oriented source\n        space and set to 1.0 for volumic or discrete source space.\n    depth: None | float in [0, 1]\n        Depth weighting coefficients. If None, no depth weighting is performed.\n    xyz_same_gamma : bool\n        Use same gamma for xyz current components at each source space point.\n        Recommended for free-orientation forward solutions.\n    maxit : int\n        Maximum number of iterations.\n    tol : float\n        Tolerance parameter for convergence.\n    update_mode : int\n        Update mode, 1: MacKay update (default), 2: Modified MacKay update.\n    gammas : array, shape=(n_sources,)\n        Initial values for posterior variances (gammas). If None, a\n        variance of 1.0 is used.\n    pca : bool\n        If True the rank of the data is reduced to the true dimension.\n    return_residual : bool\n        If True, the residual is returned as an Evoked instance.\n    return_as_dipoles : bool\n        If True, the sources are returned as a list of Dipole instances.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    stc : instance of SourceEstimate\n        Source time courses.\n    residual : instance of Evoked\n        The residual a.k.a. data not explained by the sources.\n        Only returned if return_residual is True.\n\n    References\n    ----------\n    .. [1] Wipf et al. Analysis of Empirical Bayesian Methods for\n           Neuroelectromagnetic Source Localization, Advances in Neural\n           Information Process. Systems (2007)\n\n    .. [2] D. Wipf, S. Nagarajan\n           "A unified Bayesian framework for MEG/EEG source imaging",\n           Neuroimage, Volume 44, Number 3, pp. 947-966, Feb. 2009.\n           DOI: 10.1016/j.neuroimage.2008.02.059\n    '
_check_reference(evoked)
(loose, forward) = _check_loose_forward(loose, forward)
if ((loose == 0.0) and (not is_fixed_orient(forward))):
    forward = convert_forward_solution(forward, surf_ori=True, force_fixed=True, copy=True, use_cps=True)
if (is_fixed_orient(forward) or (not xyz_same_gamma)):
    group_size = 1
else:
    group_size = 3
(gain, gain_info, whitener, source_weighting, mask) = _prepare_gain(forward, evoked.info, noise_cov, pca, depth, loose, None, None)
sel = [evoked.ch_names.index(name) for name in gain_info['ch_names']]
M = evoked.data[sel]
utils.logger.info('Whitening data matrix.')
M = numpy.dot(whitener, M)
(X, active_set) = _gamma_map_opt(M, gain, alpha, maxit=maxit, tol=tol, update_mode=update_mode, gammas=gammas, group_size=group_size, verbose=verbose)
if (len(active_set) == 0):
    raise Exception('No active dipoles found. alpha is too big.')
M_estimated = numpy.dot(gain[:, active_set], X)
n_dip_per_pos = (1 if is_fixed_orient(forward) else 3)
X = _reapply_source_weighting(X, source_weighting, active_set, n_dip_per_pos)
if return_residual:
    residual = _compute_residual(forward, evoked, X, active_set, gain_info)
if ((group_size == 1) and (not is_fixed_orient(forward))):
    tempResult = unique((active_set // 3))
	
===================================================================	
_check_update_montage: 1063	
----------------------------	

'Help eeg readers to add montage.'
if (montage is not None):
    if (not isinstance(montage, (string_types, Montage))):
        err = ('Montage must be str, None, or instance of Montage. %s was provided' % type(montage))
        raise TypeError(err)
    if (montage is not None):
        if isinstance(montage, string_types):
            montage = read_montage(montage, path=path)
        _set_montage(info, montage, update_ch_names=update_ch_names)
        missing_positions = []
        exclude = (constants.FIFF.FIFFV_EOG_CH, constants.FIFF.FIFFV_MISC_CH, constants.FIFF.FIFFV_STIM_CH)
        for ch in info['chs']:
            if (not (ch['kind'] in exclude)):
                tempResult = unique(ch['loc'])
	
===================================================================	
Info._check_consistency: 96	
----------------------------	

'Do some self-consistency checks and datatype tweaks.'
missing = [bad for bad in self['bads'] if (bad not in self['ch_names'])]
if (len(missing) > 0):
    raise RuntimeError(('bad channel(s) %s marked do not exist in info' % (missing,)))
chs = [ch['ch_name'] for ch in self['chs']]
if ((len(self['ch_names']) != len(chs)) or any(((ch_1 != ch_2) for (ch_1, ch_2) in zip(self['ch_names'], chs))) or (self['nchan'] != len(chs))):
    raise RuntimeError('info channel name inconsistency detected, please notify mne-python developers')
for key in ('sfreq', 'highpass', 'lowpass'):
    if (self.get(key) is not None):
        self[key] = float(self[key])
self._check_ch_name_length()
tempResult = unique(self['ch_names'], return_index=True)
	
===================================================================	
pick_channels: 60	
----------------------------	

'Pick channels by names.\n\n    Returns the indices of the good channels in ch_names.\n\n    Parameters\n    ----------\n    ch_names : list of string\n        List of channels.\n    include : list of string\n        List of channels to include (if empty include all available).\n\n        .. note:: This is to be treated as a set. The order of this list\n           is not used or maintained in ``sel``.\n\n    exclude : list of string\n        List of channels to exclude (if empty do not exclude any channel).\n        Defaults to [].\n\n    See Also\n    --------\n    pick_channels_regexp, pick_types\n\n    Returns\n    -------\n    sel : array of int\n        Indices of good channels.\n    '
tempResult = unique(ch_names)
	
===================================================================	
_make_projector: 248	
----------------------------	

'Subselect projs based on ch_names and bads.\n\n    Use inplace=True mode to modify ``projs`` inplace so that no\n    warning will be raised next time projectors are constructed with\n    the given inputs. If inplace=True, no meaningful data are returned.\n    '
nchan = len(ch_names)
if (nchan == 0):
    raise ValueError('No channel names specified')
default_return = (numpy.eye(nchan, nchan), 0, [])
if (projs is None):
    return default_return
nvec = 0
nproj = 0
for p in projs:
    if ((not p['active']) or include_active):
        nproj += 1
        nvec += p['data']['nrow']
if (nproj == 0):
    return default_return
vecs = numpy.zeros((nchan, nvec))
nvec = 0
nonzero = 0
bads = set(bads)
for (k, p) in enumerate(projs):
    if ((not p['active']) or include_active):
        tempResult = unique(p['data']['col_names'])
	
===================================================================	
RawEDF._read_segment_file: 61	
----------------------------	

'Read a chunk of raw data.'
from scipy.interpolate import interp1d
if (mult is not None):
    raise NotImplementedError('mult is not supported yet')
exclude = self._raw_extras[fi]['exclude']
sel = numpy.arange(self.info['nchan'])[idx]
n_samps = self._raw_extras[fi]['n_samps']
buf_len = int(self._raw_extras[fi]['max_samp'])
sfreq = self.info['sfreq']
dtype = self._raw_extras[fi]['dtype_np']
dtype_byte = self._raw_extras[fi]['dtype_byte']
data_offset = self._raw_extras[fi]['data_offset']
stim_channel = self._raw_extras[fi]['stim_channel']
tal_channels = self._raw_extras[fi]['tal_channel']
annot = self._raw_extras[fi]['annot']
annotmap = self._raw_extras[fi]['annotmap']
subtype = self._raw_extras[fi]['subtype']
stim_data = self._raw_extras[fi].get('stim_data', None)
if (numpy.size(dtype_byte) > 1):
    tempResult = unique(dtype_byte)
	
===================================================================	
test_io_egi_mff: 37	
----------------------------	

'Test importing EGI MFF simple binary files'
egi_fname_mff = os.path.join(data_path(), 'EGI', 'test_egi.mff')
raw = read_raw_egi(egi_fname_mff, include=None)
assert_true(('RawMff' in repr(raw)))
include = ['DIN1', 'DIN2', 'DIN3', 'DIN4', 'DIN5', 'DIN7']
raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname_mff, include=include, channel_naming='EEG %03d')
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if ('EEG' in c)]
assert_equal(len(eeg_chan), 129)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 129)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 8)
tempResult = unique(events[:, 1])
	
===================================================================	
test_io_egi_mff: 38	
----------------------------	

'Test importing EGI MFF simple binary files'
egi_fname_mff = os.path.join(data_path(), 'EGI', 'test_egi.mff')
raw = read_raw_egi(egi_fname_mff, include=None)
assert_true(('RawMff' in repr(raw)))
include = ['DIN1', 'DIN2', 'DIN3', 'DIN4', 'DIN5', 'DIN7']
raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname_mff, include=include, channel_naming='EEG %03d')
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if ('EEG' in c)]
assert_equal(len(eeg_chan), 129)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 129)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 8)
assert_equal(numpy.unique(events[:, 1])[0], 0)
tempResult = unique(events[:, 0])
	
===================================================================	
test_io_egi_mff: 39	
----------------------------	

'Test importing EGI MFF simple binary files'
egi_fname_mff = os.path.join(data_path(), 'EGI', 'test_egi.mff')
raw = read_raw_egi(egi_fname_mff, include=None)
assert_true(('RawMff' in repr(raw)))
include = ['DIN1', 'DIN2', 'DIN3', 'DIN4', 'DIN5', 'DIN7']
raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname_mff, include=include, channel_naming='EEG %03d')
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if ('EEG' in c)]
assert_equal(len(eeg_chan), 129)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 129)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 8)
assert_equal(numpy.unique(events[:, 1])[0], 0)
assert_true((numpy.unique(events[:, 0])[0] != 0))
tempResult = unique(events[:, 2])
	
===================================================================	
test_io_egi: 75	
----------------------------	

'Test importing EGI simple binary files.'
with open(egi_txt_fname) as fid:
    data = numpy.loadtxt(fid)
t = data[0]
data = data[1:]
data *= 1e-06
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    raw = read_raw_egi(egi_fname, include=None)
    assert_true(('RawEGI' in repr(raw)))
    assert_equal(len(w), 1)
    assert_true((w[0].category == RuntimeWarning))
    msg = 'Did not find any event code with more than one event.'
    assert_true((msg in ('%s' % w[0].message)))
(data_read, t_read) = raw[:256]
assert_allclose(t_read, t)
assert_allclose(data_read, data, atol=1e-10)
include = ['TRSP', 'XXX1']
with warnings.catch_warnings(record=True):
    raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname, include=include)
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if c.startswith('E')]
assert_equal(len(eeg_chan), 256)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 256)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 2)
tempResult = unique(events[:, 1])
	
===================================================================	
test_io_egi: 76	
----------------------------	

'Test importing EGI simple binary files.'
with open(egi_txt_fname) as fid:
    data = numpy.loadtxt(fid)
t = data[0]
data = data[1:]
data *= 1e-06
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    raw = read_raw_egi(egi_fname, include=None)
    assert_true(('RawEGI' in repr(raw)))
    assert_equal(len(w), 1)
    assert_true((w[0].category == RuntimeWarning))
    msg = 'Did not find any event code with more than one event.'
    assert_true((msg in ('%s' % w[0].message)))
(data_read, t_read) = raw[:256]
assert_allclose(t_read, t)
assert_allclose(data_read, data, atol=1e-10)
include = ['TRSP', 'XXX1']
with warnings.catch_warnings(record=True):
    raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname, include=include)
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if c.startswith('E')]
assert_equal(len(eeg_chan), 256)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 256)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 2)
assert_equal(numpy.unique(events[:, 1])[0], 0)
tempResult = unique(events[:, 0])
	
===================================================================	
test_io_egi: 77	
----------------------------	

'Test importing EGI simple binary files.'
with open(egi_txt_fname) as fid:
    data = numpy.loadtxt(fid)
t = data[0]
data = data[1:]
data *= 1e-06
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    raw = read_raw_egi(egi_fname, include=None)
    assert_true(('RawEGI' in repr(raw)))
    assert_equal(len(w), 1)
    assert_true((w[0].category == RuntimeWarning))
    msg = 'Did not find any event code with more than one event.'
    assert_true((msg in ('%s' % w[0].message)))
(data_read, t_read) = raw[:256]
assert_allclose(t_read, t)
assert_allclose(data_read, data, atol=1e-10)
include = ['TRSP', 'XXX1']
with warnings.catch_warnings(record=True):
    raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname, include=include)
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if c.startswith('E')]
assert_equal(len(eeg_chan), 256)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 256)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 2)
assert_equal(numpy.unique(events[:, 1])[0], 0)
assert_true((numpy.unique(events[:, 0])[0] != 0))
tempResult = unique(events[:, 2])
	
===================================================================	
test_io_egi: 82	
----------------------------	

'Test importing EGI simple binary files.'
with open(egi_txt_fname) as fid:
    data = numpy.loadtxt(fid)
t = data[0]
data = data[1:]
data *= 1e-06
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    raw = read_raw_egi(egi_fname, include=None)
    assert_true(('RawEGI' in repr(raw)))
    assert_equal(len(w), 1)
    assert_true((w[0].category == RuntimeWarning))
    msg = 'Did not find any event code with more than one event.'
    assert_true((msg in ('%s' % w[0].message)))
(data_read, t_read) = raw[:256]
assert_allclose(t_read, t)
assert_allclose(data_read, data, atol=1e-10)
include = ['TRSP', 'XXX1']
with warnings.catch_warnings(record=True):
    raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname, include=include)
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if c.startswith('E')]
assert_equal(len(eeg_chan), 256)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 256)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 2)
assert_equal(numpy.unique(events[:, 1])[0], 0)
assert_true((numpy.unique(events[:, 0])[0] != 0))
assert_true((numpy.unique(events[:, 2])[0] != 0))
triggers = numpy.array([[0, 1, 1, 0], [0, 0, 1, 0]])
triggers = numpy.array([[0, 1, 0, 0], [0, 0, 1, 0]])
events_ids = [12, 24]
new_trigger = _combine_triggers(triggers, events_ids)
tempResult = unique(new_trigger)
	
===================================================================	
test_io_egi: 82	
----------------------------	

'Test importing EGI simple binary files.'
with open(egi_txt_fname) as fid:
    data = numpy.loadtxt(fid)
t = data[0]
data = data[1:]
data *= 1e-06
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    raw = read_raw_egi(egi_fname, include=None)
    assert_true(('RawEGI' in repr(raw)))
    assert_equal(len(w), 1)
    assert_true((w[0].category == RuntimeWarning))
    msg = 'Did not find any event code with more than one event.'
    assert_true((msg in ('%s' % w[0].message)))
(data_read, t_read) = raw[:256]
assert_allclose(t_read, t)
assert_allclose(data_read, data, atol=1e-10)
include = ['TRSP', 'XXX1']
with warnings.catch_warnings(record=True):
    raw = _test_raw_reader(read_raw_egi, input_fname=egi_fname, include=include)
assert_equal(('eeg' in raw), True)
eeg_chan = [c for c in raw.ch_names if c.startswith('E')]
assert_equal(len(eeg_chan), 256)
picks = pick_types(raw.info, eeg=True)
assert_equal(len(picks), 256)
assert_equal(('STI 014' in raw.ch_names), True)
events = find_events(raw, stim_channel='STI 014')
assert_equal(len(events), 2)
assert_equal(numpy.unique(events[:, 1])[0], 0)
assert_true((numpy.unique(events[:, 0])[0] != 0))
assert_true((numpy.unique(events[:, 2])[0] != 0))
triggers = numpy.array([[0, 1, 1, 0], [0, 0, 1, 0]])
triggers = numpy.array([[0, 1, 0, 0], [0, 0, 1, 0]])
events_ids = [12, 24]
new_trigger = _combine_triggers(triggers, events_ids)
tempResult = unique([0, 12, 24])
	
===================================================================	
EpochsKIT.__init__: 175	
----------------------------	

if isinstance(events, string_types):
    events = read_events(events)
if isinstance(mrk, list):
    mrk = [(read_mrk(marker) if isinstance(marker, string_types) else marker) for marker in mrk]
    mrk = numpy.mean(mrk, axis=0)
if ((mrk is not None) and (elp is not None) and (hsp is not None)):
    (dig_points, dev_head_t) = _set_dig_kit(mrk, elp, hsp)
    self.info['dig'] = dig_points
    self.info['dev_head_t'] = dev_head_t
elif ((mrk is not None) or (elp is not None) or (hsp is not None)):
    err = 'mrk, elp and hsp need to be provided as a group (all or none)'
    raise ValueError(err)
utils.logger.info(('Extracting KIT Parameters from %s...' % input_fname))
input_fname = os.path.abspath(input_fname)
(self.info, kit_info) = get_kit_info(input_fname, allow_unknown_format)
kit_info.update(filename=input_fname)
self._raw_extras = [kit_info]
self._filenames = []
if (len(events) != self._raw_extras[0]['n_epochs']):
    raise ValueError('Event list does not match number of epochs.')
if (self._raw_extras[0]['acq_type'] == constants.KIT.EPOCHS):
    self._raw_extras[0]['data_length'] = constants.KIT.INT
    self._raw_extras[0]['dtype'] = 'h'
else:
    raise TypeError('SQD file contains raw data, not epochs or average. Wrong reader.')
if (event_id is None):
    tempResult = unique(events[:, 2])
	
===================================================================	
test_maxfilter_io: 21	
----------------------------	

'Test maxfilter io.'
info = read_info(raw_fname)
mf = info['proc_history'][1]['max_info']
assert_true(mf['sss_info']['frame'], mne.io.constants.FIFF.FIFFV_COORD_HEAD)
assert_true((5 <= mf['sss_info']['in_order'] <= 11))
assert_true((mf['sss_info']['out_order'] <= 5))
assert_true((mf['sss_info']['nchan'] > len(mf['sss_info']['components'])))
assert_equal(info['ch_names'][:mf['sss_info']['nchan']], mf['sss_ctc']['proj_items_chs'])
assert_equal(mf['sss_ctc']['decoupler'].shape, (mf['sss_info']['nchan'], mf['sss_info']['nchan']))
tempResult = unique(numpy.diag(mf['sss_ctc']['decoupler'].toarray()))
	
===================================================================	
ICA.find_bads_eog: 548	
----------------------------	

'Detect EOG related components using correlation.\n\n        Detection is based on Pearson correlation between the\n        filtered data and the filtered EOG channel.\n        Thresholding is based on adaptive z-scoring. The above threshold\n        components will be masked and the z-score will be recomputed\n        until no supra-threshold component remains.\n\n        Parameters\n        ----------\n        inst : instance of Raw, Epochs or Evoked\n            Object to compute sources from.\n        ch_name : str\n            The name of the channel to use for EOG peak detection.\n            The argument is mandatory if the dataset contains no EOG\n            channels.\n        threshold : int | float\n            The value above which a feature is classified as outlier.\n        start : int | float | None\n            First sample to include. If float, data will be interpreted as\n            time in seconds. If None, data will be used from the first sample.\n        stop : int | float | None\n            Last sample to not include. If float, data will be interpreted as\n            time in seconds. If None, data will be used to the last sample.\n        l_freq : float\n            Low pass frequency.\n        h_freq : float\n            High pass frequency.\n        reject_by_annotation : bool\n            If True, data annotated as bad will be omitted. Defaults to True.\n\n            .. versionadded:: 0.14.0\n\n        verbose : bool, str, int, or None\n            If not None, override default verbose level (see\n            :func:`mne.verbose` and :ref:`Logging documentation <tut_logging>`\n            for more). Defaults to self.verbose.\n\n        Returns\n        -------\n        eog_idx : list of int\n            The indices of EOG related components, sorted by score.\n        scores : np.ndarray of float, shape (``n_components_``) | list of array\n            The correlation scores.\n\n        See Also\n        --------\n        find_bads_ecg\n        '
if (verbose is None):
    verbose = self.verbose
eog_inds = _get_eog_channel_index(ch_name, inst)
if (len(eog_inds) > 2):
    eog_inds = eog_inds[:1]
    utils.logger.info(('Using EOG channel %s' % inst.ch_names[eog_inds[0]]))
(scores, eog_idx) = ([], [])
eog_chs = [inst.ch_names[k] for k in eog_inds]
targets = [self._check_target(k, inst, start, stop, reject_by_annotation) for k in eog_chs]
for (ii, (eog_ch, target)) in enumerate(zip(eog_chs, targets)):
    scores += [self.score_sources(inst, target=target, score_func='pearsonr', start=start, stop=stop, l_freq=l_freq, h_freq=h_freq, verbose=verbose, reject_by_annotation=reject_by_annotation)]
    this_idx = find_outliers(scores[(- 1)], threshold=threshold)
    eog_idx += [this_idx]
    self.labels_[(('eog/%i/' % ii) + eog_ch)] = list(this_idx)
scores_ = numpy.concatenate([scores[ii][inds] for (ii, inds) in enumerate(eog_idx)])
eog_idx_ = numpy.concatenate(eog_idx)[np.abs(scores_).argsort()[::(- 1)]]
tempResult = unique(eog_idx_)
	
===================================================================	
ICA._pick_sources: 633	
----------------------------	

'Aux function.'
if (exclude is None):
    exclude = self.exclude
else:
    exclude = list(set((self.exclude + list(exclude))))
_n_pca_comp = self._check_n_pca_components(self.n_pca_components)
if (not (self.n_components_ <= _n_pca_comp <= self.max_pca_components)):
    raise ValueError('n_pca_components must be >= n_components and <= max_pca_components.')
n_components = self.n_components_
utils.logger.info(('Transforming to ICA space (%i components)' % n_components))
if (self.pca_mean_ is not None):
    data -= self.pca_mean_[:, None]
sel_keep = numpy.arange(n_components)
if (include not in (None, [])):
    tempResult = unique(include)
	
===================================================================	
_check_pos: 356	
----------------------------	

'Check for a valid pos array and transform it to a more usable form.'
if (pos is None):
    return [None, numpy.array([(- 1)])]
if (not head_frame):
    raise ValueError('positions can only be used if coord_frame="head"')
if (not st_fixed):
    warn('st_fixed=False is untested, use with caution!')
if (not isinstance(pos, numpy.ndarray)):
    raise TypeError('pos must be an ndarray')
if ((pos.ndim != 2) or (pos.shape[1] != 10)):
    raise ValueError('pos must be an array of shape (N, 10)')
t = pos[:, 0]
t_off = (raw.first_samp / raw.info['sfreq'])
tempResult = unique(t)
	
===================================================================	
_XdawnTransformer.fit: 103	
----------------------------	

'Fit Xdawn spatial filters.\n\n        Parameters\n        ----------\n        X : array, shape (n_epochs, n_channels, n_samples)\n            The target data.\n        y : array, shape (n_epochs,) | None\n            The target labels. If None, Xdawn fit on the average evoked.\n\n        Returns\n        -------\n        self : Xdawn instance\n            The Xdawn instance.\n        '
(X, y) = self._check_Xy(X, y)
tempResult = unique(y)
	
===================================================================	
Xdawn._pick_sources: 264	
----------------------------	

'Aux method.'
utils.logger.info('Transforming to Xdawn space')
sources = numpy.dot(self.filters_[eid].T, data)
if (include not in (None, list())):
    mask = numpy.ones(len(sources), dtype=numpy.bool)
    tempResult = unique(include)
	
===================================================================	
Xdawn._pick_sources: 268	
----------------------------	

'Aux method.'
utils.logger.info('Transforming to Xdawn space')
sources = numpy.dot(self.filters_[eid].T, data)
if (include not in (None, list())):
    mask = numpy.ones(len(sources), dtype=numpy.bool)
    mask[numpy.unique(include)] = False
    sources[mask] = 0.0
    utils.logger.info(('Zeroing out %i Xdawn components' % mask.sum()))
elif (exclude not in (None, list())):
    tempResult = unique(exclude)
	
===================================================================	
_least_square_evoked: 41	
----------------------------	

"Least square estimation of evoked response from epochs data.\n\n    Parameters\n    ----------\n    epochs_data : array, shape (n_channels, n_times)\n        The epochs data to estimate evoked.\n    events : array, shape (n_events, 3)\n        The events typically returned by the read_events function.\n        If some events don't match the events of interest as specified\n        by event_id, they will be ignored.\n    tmin : float\n        Start time before event.\n    sfreq : float\n        Sampling frequency.\n\n    Returns\n    -------\n    evokeds : array, shape (n_class, n_components, n_times)\n        An concatenated array of evoked data for each event type.\n    toeplitz : array, shape (n_class * n_components, n_channels)\n        An concatenated array of toeplitz matrix for each event type.\n    "
(n_epochs, n_channels, n_times) = epochs_data.shape
tmax = (tmin + (n_times / float(sfreq)))
events = events.copy()
events[:, 0] -= (events[(0, 0)] + int((tmin * sfreq)))
raw = _construct_signal_from_epochs(epochs_data, events, sfreq, tmin)
(n_min, n_max) = (int((tmin * sfreq)), int((tmax * sfreq)))
window = (n_max - n_min)
n_samples = raw.shape[1]
toeplitz = list()
tempResult = unique(events[:, 2])
	
===================================================================	
_fit_xdawn: 58	
----------------------------	

"Fit filters and coefs using Xdawn Algorithm.\n\n    Xdawn is a spatial filtering method designed to improve the signal\n    to signal + noise ratio (SSNR) of the event related responses. Xdawn was\n    originally designed for P300 evoked potential by enhancing the target\n    response with respect to the non-target response. This implementation is a\n    generalization to any type of event related response.\n\n    Parameters\n    ----------\n    epochs_data : array, shape (n_epochs, n_channels, n_times)\n        The epochs data.\n    y : array, shape (n_epochs)\n        The epochs class.\n    n_components : int (default 2)\n        The number of components to decompose the signals signals.\n    reg : float | str | None (default None)\n        If not None, allow regularization for covariance estimation\n        if float, shrinkage covariance is used (0 <= shrinkage <= 1).\n        if str, optimal shrinkage using Ledoit-Wolf Shrinkage ('ledoit_wolf')\n        or Oracle Approximating Shrinkage ('oas').\n    signal_cov : None | Covariance | array, shape (n_channels, n_channels)\n        The signal covariance used for whitening of the data.\n        if None, the covariance is estimated from the epochs signal.\n    events : array, shape (n_epochs, 3)\n        The epochs events, used to correct for epochs overlap.\n    tmin : float\n        Epochs starting time. Only used if events is passed to correct for\n        epochs overlap.\n    sfreq : float\n        Sampling frequency.  Only used if events is passed to correct for\n        epochs overlap.\n\n    Returns\n    -------\n    filters : array, shape (n_channels, n_channels)\n        The Xdawn components used to decompose the data for each event type.\n    patterns : array, shape (n_channels, n_channels)\n        The Xdawn patterns used to restore the signals for each event type.\n    evokeds : array, shape (n_class, n_components, n_times)\n        The independent evoked responses per condition.\n    "
(n_epochs, n_channels, n_times) = epochs_data.shape
tempResult = unique(y)
	
===================================================================	
test_ctps: 50	
----------------------------	

' Test basic ctps functionality\n    '
for (ii, (n_trials, j_extent, pk_max)) in iter_test_ctps:
    data = get_data(n_trials, j_extent)
    (ks_dyn, pk_dyn, phase_trial) = ctps(data)
    data2 = _compute_normalized_phase(data)
    (ks_dyn2, pk_dyn2, phase_trial2) = ctps(data2, is_raw=False)
    for (a, b) in zip([ks_dyn, pk_dyn, phase_trial], [ks_dyn2, pk_dyn2, data2]):
        assert_array_equal(a, b)
        assert_true((a.min() >= 0))
        assert_true((a.max() <= 1))
        assert_true((b.min() >= 0))
        assert_true((b.max() <= 1))
    assert_true(((pk_dyn.min() > 0.0) or (pk_dyn.max() < 1.0)))
    assert_true((phase_trial.shape == data.shape))
    assert_true((pk_dyn.shape == data.shape[1:]))
    assert_true((pk_dyn[0].max() == 1.0))
    tempResult = unique(pk_dyn[0])
	
===================================================================	
simulate_raw: 77	
----------------------------	

'Simulate raw data.\n\n    Head movements can optionally be simulated using the ``head_pos``\n    parameter.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        The raw template to use for simulation. The ``info``, ``times``,\n        and potentially ``first_samp`` properties will be used.\n    stc : instance of SourceEstimate\n        The source estimate to use to simulate data. Must have the same\n        sample rate as the raw data.\n    trans : dict | str | None\n        Either a transformation filename (usually made using mne_analyze)\n        or an info dict (usually opened using read_trans()).\n        If string, an ending of `.fif` or `.fif.gz` will be assumed to\n        be in FIF format, any other ending will be assumed to be a text\n        file with a 4x4 transformation matrix (like the `--trans` MNE-C\n        option). If trans is None, an identity transform will be used.\n    src : str | instance of SourceSpaces\n        Source space corresponding to the stc. If string, should be a source\n        space filename. Can also be an instance of loaded or generated\n        SourceSpaces.\n    bem : str | dict\n        BEM solution  corresponding to the stc. If string, should be a BEM\n        solution filename (e.g., "sample-5120-5120-5120-bem-sol.fif").\n    cov : instance of Covariance | str | None\n        The sensor covariance matrix used to generate noise. If None,\n        no noise will be added. If \'simple\', a basic (diagonal) ad-hoc\n        noise covariance will be used. If a string, then the covariance\n        will be loaded.\n    blink : bool\n        If true, add simulated blink artifacts. See Notes for details.\n    ecg : bool\n        If true, add simulated ECG artifacts. See Notes for details.\n    chpi : bool\n        If true, simulate continuous head position indicator information.\n        Valid cHPI information must encoded in ``raw.info[\'hpi_meas\']``\n        to use this option.\n    head_pos : None | str | dict | tuple | array\n        Name of the position estimates file. Should be in the format of\n        the files produced by maxfilter. If dict, keys should\n        be the time points and entries should be 4x4 ``dev_head_t``\n        matrices. If None, the original head position (from\n        ``info[\'dev_head_t\']``) will be used. If tuple, should have the\n        same format as data returned by `head_pos_to_trans_rot_t`.\n        If array, should be of the form returned by\n        :func:`mne.chpi.read_head_pos`.\n    mindist : float\n        Minimum distance between sources and the inner skull boundary\n        to use during forward calculation.\n    interp : str\n        Either \'hann\', \'cos2\', \'linear\', or \'zero\', the type of\n        forward-solution interpolation to use between forward solutions\n        at different head positions.\n    iir_filter : None | array\n        IIR filter coefficients (denominator) e.g. [1, -1, 0.2].\n    n_jobs : int\n        Number of jobs to use.\n    random_state : None | int | np.random.RandomState\n        The random generator state used for blink, ECG, and sensor\n        noise randomization.\n    use_cps : None | bool (default None)\n        Whether to use cortical patch statistics to define normal\n        orientations. Only used when surf_ori and/or force_fixed are True.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    raw : instance of Raw\n        The simulated raw file.\n\n    See Also\n    --------\n    mne.chpi.read_head_pos\n    simulate_evoked\n    simulate_stc\n    simulate_sparse_stc\n\n    Notes\n    -----\n    Events coded with the position number (starting at 1) will be stored\n    in the trigger channel (if available) at times corresponding to t=0\n    in the ``stc``.\n\n    The resulting SNR will be determined by the structure of the noise\n    covariance, the amplitudes of ``stc``, and the head position(s) provided.\n\n    The blink and ECG artifacts are generated by 1) placing impulses at\n    random times of activation, and 2) convolving with activation kernel\n    functions. In both cases, the scale-factors of the activation functions\n    (and for the resulting EOG and ECG channel traces) were chosen based on\n    visual inspection to yield amplitudes generally consistent with those\n    seen in experimental data. Noisy versions of the blink and ECG\n    activations will be stored in the first EOG and ECG channel in the\n    raw file, respectively, if they exist.\n\n    For blink artifacts:\n\n        1. Random activation times are drawn from an inhomogeneous poisson\n           process whose blink rate oscillates between 4.5 blinks/minute\n           and 17 blinks/minute based on the low (reading) and high (resting)\n           blink rates from [1]_.\n        2. The activation kernel is a 250 ms Hanning window.\n        3. Two activated dipoles are located in the z=0 plane (in head\n           coordinates) at 30 degrees away from the y axis (nasion).\n        4. Activations affect MEG and EEG channels.\n\n    For ECG artifacts:\n\n        1. Random inter-beat intervals are drawn from a uniform distribution\n           of times corresponding to 40 and 80 beats per minute.\n        2. The activation function is the sum of three Hanning windows with\n           varying durations and scales to make a more complex waveform.\n        3. The activated dipole is located one (estimated) head radius to\n           the left (-x) of head center and three head radii below (+z)\n           head center; this dipole is oriented in the +x direction.\n        4. Activations only affect MEG channels.\n\n    .. versionadded:: 0.10.0\n\n    References\n    ----------\n    .. [1] Bentivoglio et al. "Analysis of blink rate patterns in normal\n           subjects" Movement Disorders, 1997 Nov;12(6):1028-34.\n    '
if (not isinstance(raw, BaseRaw)):
    raise TypeError('raw should be an instance of Raw')
(times, info, first_samp) = (raw.times, raw.info, raw.first_samp)
raw_verbose = raw.verbose
if (not isinstance(stc, _BaseSourceEstimate)):
    raise TypeError('stc must be a SourceEstimate')
if (not numpy.allclose(info['sfreq'], (1.0 / stc.tstep))):
    raise ValueError('stc and info must have same sample rate')
if (len(stc.times) <= 2):
    raise ValueError('stc must have at least three time points')
stim = (False if (len(pick_types(info, meg=False, stim=True)) == 0) else True)
n_jobs = check_n_jobs(n_jobs)
rng = check_random_state(random_state)
interper = _Interp2(interp)
if (head_pos is None):
    head_pos = dict()
if isinstance(head_pos, string_types):
    head_pos = read_head_pos(head_pos)
if isinstance(head_pos, numpy.ndarray):
    head_pos = head_pos_to_trans_rot_t(head_pos)
if isinstance(head_pos, tuple):
    (transs, rots, ts) = head_pos
    ts -= (first_samp / info['sfreq'])
    dev_head_ts = [numpy.r_[(numpy.c_[(r, t[:, numpy.newaxis])], [[0, 0, 0, 1]])] for (r, t) in zip(rots, transs)]
    del transs, rots
elif isinstance(head_pos, dict):
    ts = numpy.array(list(head_pos.keys()), float)
    ts.sort()
    dev_head_ts = [head_pos[float(tt)] for tt in ts]
else:
    raise TypeError(('unknown head_pos type %s' % type(head_pos)))
bad = (ts < 0)
if bad.any():
    raise RuntimeError(('All position times must be >= 0, found %s/%s< 0' % (bad.sum(), len(bad))))
bad = (ts > times[(- 1)])
if bad.any():
    raise RuntimeError(('All position times must be <= t_end (%0.1f sec), found %s/%s bad values (is this a split file?)' % (times[(- 1)], bad.sum(), len(bad))))
if ((len(ts) == 0) or (ts[0] > 0)):
    ts = numpy.r_[([0.0], ts)]
    dev_head_ts.insert(0, info['dev_head_t']['trans'])
dev_head_ts = [{'trans': d, 'to': info['dev_head_t']['to'], 'from': info['dev_head_t']['from']} for d in dev_head_ts]
offsets = raw.time_as_index(ts)
offsets = numpy.concatenate([offsets, [len(times)]])
assert (offsets[(- 2)] != offsets[(- 1)])
tempResult = unique(offsets)
	
===================================================================	
simulate_stc: 103	
----------------------------	

'Simulate sources time courses from waveforms and labels.\n\n    This function generates a source estimate with extended sources by\n    filling the labels with the waveforms given in stc_data.\n\n    Parameters\n    ----------\n    src : instance of SourceSpaces\n        The source space\n    labels : list of Labels\n        The labels\n    stc_data : array (shape: len(labels) x n_times)\n        The waveforms\n    tmin : float\n        The beginning of the timeseries\n    tstep : float\n        The time step (1 / sampling frequency)\n    value_fun : function | None\n        Function to apply to the label values to obtain the waveform\n        scaling for each vertex in the label. If None (default), uniform\n        scaling is used.\n\n    Returns\n    -------\n    stc : SourceEstimate\n        The generated source time courses.\n\n    See Also\n    --------\n    simulate_raw\n    simulate_evoked\n    simulate_sparse_stc\n    '
if (len(labels) != len(stc_data)):
    raise ValueError('labels and stc_data must have the same length')
vertno = [[], []]
stc_data_extended = [[], []]
hemi_to_ind = {'lh': 0, 'rh': 1}
for (i, label) in enumerate(labels):
    hemi_ind = hemi_to_ind[label.hemi]
    src_sel = numpy.intersect1d(src[hemi_ind]['vertno'], label.vertices)
    if (value_fun is not None):
        idx_sel = numpy.searchsorted(label.vertices, src_sel)
        values_sel = numpy.array([value_fun(v) for v in label.values[idx_sel]])
        data = numpy.outer(values_sel, stc_data[i])
    else:
        data = numpy.tile(stc_data[i], (len(src_sel), 1))
    vertno[hemi_ind].append(src_sel)
    stc_data_extended[hemi_ind].append(numpy.atleast_2d(data))
for idx in (0, 1):
    if (len(vertno[idx]) > 1):
        vertno[idx] = numpy.concatenate(vertno[idx])
    elif (len(vertno[idx]) == 1):
        vertno[idx] = vertno[idx][0]
vertno = [numpy.array(v) for v in vertno]
for (v, hemi) in zip(vertno, ('left', 'right')):
    tempResult = unique(v)
	
===================================================================	
_get_clusters_st_1step: 56	
----------------------------	

'Directly calculate connectivity.\n\n    This uses knowledge that time points are\n    only connected to adjacent neighbors for data organized as time x space.\n\n    This algorithm time increases linearly with the number of time points,\n    compared to with the square for the standard (graph) algorithm.\n\n    This algorithm creates clusters for each time point using a method more\n    efficient than the standard graph method (but otherwise equivalent), then\n    combines these clusters across time points in a reasonable way.\n    '
n_src = len(neighbors)
n_times = len(keepers)
enum_offset = 1
check = numpy.zeros((n_times, n_src), dtype=int)
clusters = list()
for (ii, k) in enumerate(keepers):
    c = _get_clusters_spatial(k, neighbors)
    for (ci, cl) in enumerate(c):
        check[(ii, cl)] = (ci + enum_offset)
    enum_offset += len(c)
    c = [(cl + (ii * n_src)) for cl in c]
    clusters += c
for (check1, check2, k) in zip(check[:(- 1)], check[1:], keepers[:(- 1)]):
    inds = k[((check2[k] - check1[k]) > 0)]
    check1_d = check1[inds]
    n = check2[inds]
    tempResult = unique(n)
	
===================================================================	
_get_clusters_st_1step: 60	
----------------------------	

'Directly calculate connectivity.\n\n    This uses knowledge that time points are\n    only connected to adjacent neighbors for data organized as time x space.\n\n    This algorithm time increases linearly with the number of time points,\n    compared to with the square for the standard (graph) algorithm.\n\n    This algorithm creates clusters for each time point using a method more\n    efficient than the standard graph method (but otherwise equivalent), then\n    combines these clusters across time points in a reasonable way.\n    '
n_src = len(neighbors)
n_times = len(keepers)
enum_offset = 1
check = numpy.zeros((n_times, n_src), dtype=int)
clusters = list()
for (ii, k) in enumerate(keepers):
    c = _get_clusters_spatial(k, neighbors)
    for (ci, cl) in enumerate(c):
        check[(ii, cl)] = (ci + enum_offset)
    enum_offset += len(c)
    c = [(cl + (ii * n_src)) for cl in c]
    clusters += c
for (check1, check2, k) in zip(check[:(- 1)], check[1:], keepers[:(- 1)]):
    inds = k[((check2[k] - check1[k]) > 0)]
    check1_d = check1[inds]
    n = check2[inds]
    nexts = numpy.unique(n)
    for num in nexts:
        prevs = check1_d[(n == num)]
        base = numpy.min(prevs)
        tempResult = unique(prevs[(prevs != base)])
	
===================================================================	
_clean_rerp_input: 168	
----------------------------	

'Remove empty and contaminated points from data & predictor matrices.'
tempResult = unique(X.nonzero()[0])
	
===================================================================	
test_permutation_large_n_samples: 69	
----------------------------	

'Test that non-replacement works with large N.'
X = (np.random.RandomState(0).randn(72, 1) + 1)
for n_samples in (11, 72):
    tails = ((0, 1) if (n_samples <= 20) else (0,))
    for tail in tails:
        H0 = permutation_cluster_1samp_test(X[:n_samples], threshold=0.0001, tail=tail)[(- 1)]
        assert (H0.shape == (1024,))
        tempResult = unique(H0)
	
===================================================================	
test_array_epochs: 1410	
----------------------------	

'Test creating epochs from array.'
import matplotlib.pyplot as plt
tempdir = _TempDir()
data = rng.random_sample((10, 20, 300))
sfreq = 1000.0
ch_names = [('EEG %03d' % (i + 1)) for i in range(20)]
types = (['eeg'] * 20)
info = create_info(ch_names, sfreq, types)
events = numpy.c_[(numpy.arange(1, 600, 60), numpy.zeros(10, int), ([1, 2] * 5))]
event_id = {'a': 1, 'b': 2}
epochs = EpochsArray(data, info, events, tmin, event_id)
assert_true(str(epochs).startswith('<EpochsArray'))
assert_raises(ValueError, EpochsArray, data[:(- 1)], info, events, tmin, event_id)
assert_raises(ValueError, EpochsArray, data, info, events, tmin, dict(a=1))
temp_fname = os.path.join(tempdir, 'test-epo.fif')
epochs.save(temp_fname)
epochs2 = read_epochs(temp_fname)
data2 = epochs2.get_data()
assert_allclose(data, data2)
assert_allclose(epochs.times, epochs2.times)
assert_equal(epochs.event_id, epochs2.event_id)
assert_array_equal(epochs.events, epochs2.events)
epochs[0].plot()
matplotlib.pyplot.close('all')
tempResult = unique(epochs['a'].events[:, 2])
	
===================================================================	
test_concatenate_epochs: 1448	
----------------------------	

'Test concatenate epochs.'
(raw, events, picks) = _get_data()
epochs = Epochs(raw=raw, events=events, event_id=event_id, tmin=tmin, tmax=tmax, picks=picks)
epochs2 = epochs.copy()
epochs_list = [epochs, epochs2]
epochs_conc = concatenate_epochs(epochs_list)
tempResult = unique(epochs_conc.events[:, 0])
	
===================================================================	
test_label_addition: 148	
----------------------------	

'Test label addition.'
pos = np.random.RandomState(0).rand(10, 3)
values = (numpy.arange(10.0) / 10)
idx0 = list(range(7))
idx1 = list(range(7, 10))
idx2 = list(range(5, 10))
l0 = Label(idx0, pos[idx0], values[idx0], 'lh', color='red')
l1 = Label(idx1, pos[idx1], values[idx1], 'lh')
l2 = Label(idx2, pos[idx2], values[idx2], 'lh', color=(0, 1, 0, 0.5))
assert_equal(len(l0), len(idx0))
l_good = l0.copy()
l_good.subject = 'sample'
l_bad = l1.copy()
l_bad.subject = 'foo'
assert_raises(ValueError, l_good.__add__, l_bad)
assert_raises(TypeError, l_good.__add__, 'foo')
assert_raises(ValueError, l_good.__sub__, l_bad)
assert_raises(TypeError, l_good.__sub__, 'foo')
l01 = (l0 + l1)
assert_equal(len(l01), (len(l0) + len(l1)))
assert_array_equal(l01.values[:len(l0)], l0.values)
assert_equal(l01.color, l0.color)
assert_labels_equal((l01 - l0), l1, comment=False, color=False)
assert_labels_equal((l01 - l1), l0, comment=False, color=False)
l = (l0 + l2)
i0 = numpy.where((l0.vertices == 6))[0][0]
i2 = numpy.where((l2.vertices == 6))[0][0]
i = numpy.where((l.vertices == 6))[0][0]
assert_equal(l.values[i], (l0.values[i0] + l2.values[i2]))
assert_equal(l.values[0], l0.values[0])
tempResult = unique(l.vertices)
	
===================================================================	
test_label_addition: 148	
----------------------------	

'Test label addition.'
pos = np.random.RandomState(0).rand(10, 3)
values = (numpy.arange(10.0) / 10)
idx0 = list(range(7))
idx1 = list(range(7, 10))
idx2 = list(range(5, 10))
l0 = Label(idx0, pos[idx0], values[idx0], 'lh', color='red')
l1 = Label(idx1, pos[idx1], values[idx1], 'lh')
l2 = Label(idx2, pos[idx2], values[idx2], 'lh', color=(0, 1, 0, 0.5))
assert_equal(len(l0), len(idx0))
l_good = l0.copy()
l_good.subject = 'sample'
l_bad = l1.copy()
l_bad.subject = 'foo'
assert_raises(ValueError, l_good.__add__, l_bad)
assert_raises(TypeError, l_good.__add__, 'foo')
assert_raises(ValueError, l_good.__sub__, l_bad)
assert_raises(TypeError, l_good.__sub__, 'foo')
l01 = (l0 + l1)
assert_equal(len(l01), (len(l0) + len(l1)))
assert_array_equal(l01.values[:len(l0)], l0.values)
assert_equal(l01.color, l0.color)
assert_labels_equal((l01 - l0), l1, comment=False, color=False)
assert_labels_equal((l01 - l1), l0, comment=False, color=False)
l = (l0 + l2)
i0 = numpy.where((l0.vertices == 6))[0][0]
i2 = numpy.where((l2.vertices == 6))[0][0]
i = numpy.where((l.vertices == 6))[0][0]
assert_equal(l.values[i], (l0.values[i0] + l2.values[i2]))
assert_equal(l.values[0], l0.values[0])
tempResult = unique((idx0 + idx2))
	
===================================================================	
test_spatio_temporal_tris_connectivity: 530	
----------------------------	

'Test spatio-temporal connectivity from triangles.'
tris = numpy.array([[0, 1, 2], [3, 4, 5]])
connectivity = spatio_temporal_tris_connectivity(tris, 2)
x = [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]
components = mne.stats.cluster_level._get_components(numpy.array(x), connectivity)
old_fmt = [0, 0, (- 2), (- 2), (- 2), (- 2), 0, (- 2), (- 2), (- 2), (- 2), 1]
new_fmt = numpy.array(old_fmt)
tempResult = unique(new_fmt[(new_fmt >= 0)])
	
===================================================================	
circular_layout: 27	
----------------------------	

'Create layout arranging nodes on a circle.\n\n    Parameters\n    ----------\n    node_names : list of str\n        Node names.\n    node_order : list of str\n        List with node names defining the order in which the nodes are\n        arranged. Must have the elements as node_names but the order can be\n        different. The nodes are arranged clockwise starting at "start_pos"\n        degrees.\n    start_pos : float\n        Angle in degrees that defines where the first node is plotted.\n    start_between : bool\n        If True, the layout starts with the position between the nodes. This is\n        the same as adding "180. / len(node_names)" to start_pos.\n    group_boundaries : None | array-like\n        List of of boundaries between groups at which point a "group_sep" will\n        be inserted. E.g. "[0, len(node_names) / 2]" will create two groups.\n    group_sep : float\n        Group separation angle in degrees. See "group_boundaries".\n\n    Returns\n    -------\n    node_angles : array, shape=(len(node_names,))\n        Node angles in degrees.\n    '
n_nodes = len(node_names)
if (len(node_order) != n_nodes):
    raise ValueError('node_order has to be the same length as node_names')
if (group_boundaries is not None):
    boundaries = numpy.array(group_boundaries, dtype=numpy.int)
    if (numpy.any((boundaries >= n_nodes)) or numpy.any((boundaries < 0))):
        raise ValueError('"group_boundaries" has to be between 0 and n_nodes - 1.')
    if ((len(boundaries) > 1) and numpy.any((numpy.diff(boundaries) <= 0))):
        raise ValueError('"group_boundaries" must have non-decreasing values.')
    n_group_sep = len(group_boundaries)
else:
    n_group_sep = 0
    boundaries = None
node_order = [node_order.index(name) for name in node_names]
node_order = numpy.array(node_order)
tempResult = unique(node_order)
	
===================================================================	
_plot_gat_time: 79	
----------------------------	

'Plot a unique score 1d array.'
tempResult = unique([len(t) for t in gat.test_times_['times']])
	
===================================================================	
_get_chance_level: 108	
----------------------------	

'Get the chance level.'
if (scorer.__name__ == 'accuracy_score'):
    tempResult = unique(y_train)
	
===================================================================	
plot_ica_scores: 228	
----------------------------	

"Plot scores related to detected components.\n\n    Use this function to asses how well your score describes outlier\n    sources and how well you were detecting them.\n\n    Parameters\n    ----------\n    ica : instance of mne.preprocessing.ICA\n        The ICA object.\n    scores : array_like of float, shape (n ica components) | list of arrays\n        Scores based on arbitrary metric to characterize ICA components.\n    exclude : array_like of int\n        The components marked for exclusion. If None (default), ICA.exclude\n        will be used.\n    labels : str | list | 'ecg' | 'eog' | None\n        The labels to consider for the axes tests. Defaults to None.\n        If list, should match the outer shape of `scores`.\n        If 'ecg' or 'eog', the ``labels_`` attributes will be looked up.\n        Note that '/' is used internally for sublabels specifying ECG and\n        EOG channels.\n    axhline : float\n        Draw horizontal line to e.g. visualize rejection threshold.\n    title : str\n        The figure title.\n    figsize : tuple of int | None\n        The figure size. If None it gets set automatically.\n    show : bool\n        Show figure if True.\n\n    Returns\n    -------\n    fig : instance of matplotlib.pyplot.Figure\n        The figure object\n    "
import matplotlib.pyplot as plt
my_range = numpy.arange(ica.n_components_)
if (exclude is None):
    exclude = ica.exclude
tempResult = unique(exclude)
	
===================================================================	
plot_events: 234	
----------------------------	

"Plot events to get a visual display of the paradigm.\n\n    Parameters\n    ----------\n    events : array, shape (n_events, 3)\n        The events.\n    sfreq : float | None\n        The sample frequency. If None, data will be displayed in samples (not\n        seconds).\n    first_samp : int\n        The index of the first sample. Typically the raw.first_samp\n        attribute. It is needed for recordings on a Neuromag\n        system as the events are defined relative to the system\n        start and not to the beginning of the recording.\n    color : dict | None\n        Dictionary of event_id value and its associated color. If None,\n        colors are automatically drawn from a default list (cycled through if\n        number of events longer than list of default colors).\n    event_id : dict | None\n        Dictionary of event label (e.g. 'aud_l') and its associated\n        event_id value. Label used to plot a legend. If None, no legend is\n        drawn.\n    axes : instance of matplotlib.axes.AxesSubplot\n       The subplot handle.\n    equal_spacing : bool\n        Use equal spacing between events in y-axis.\n    show : bool\n        Show figure if True.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The figure object containing the plot.\n\n    Notes\n    -----\n    .. versionadded:: 0.9.0\n    "
if (sfreq is None):
    sfreq = 1.0
    xlabel = 'samples'
else:
    xlabel = 'Time (s)'
events = numpy.asarray(events)
tempResult = unique(events[:, 2])
	
===================================================================	
_imshow_tfr: 181	
----------------------------	

'Show time-frequency map as two-dimensional image.'
from matplotlib import pyplot as plt, ticker
from matplotlib.widgets import RectangleSelector
if (yscale not in ['auto', 'linear', 'log']):
    raise ValueError("yscale should be either 'auto', 'linear', or 'log', got {}".format(yscale))
(cmap, interactive_cmap) = cmap
times = numpy.linspace(tmin, tmax, num=tfr[ch_idx].shape[1])
if ((yscale == 'log') and (not (freq[0] > 0))):
    raise ValueError('Using log scale for frequency axis requires all your frequencies to be positive (you cannot include the DC component (0 Hz) in the TFR).')
if ((len(freq) < 2) or (freq[0] == 0)):
    yscale = 'linear'
elif (yscale != 'linear'):
    ratio = (freq[1:] / freq[:(- 1)])
if (yscale == 'auto'):
    if ((freq[0] > 0) and numpy.allclose(ratio, ratio[0])):
        yscale = 'log'
    else:
        yscale = 'linear'
time_diff = ((numpy.diff(times) / 2.0) if (len(times) > 1) else [0.0005])
time_lims = numpy.concatenate([[(times[0] - time_diff[0])], (times[:(- 1)] + time_diff), [(times[(- 1)] + time_diff[(- 1)])]])
if (yscale == 'linear'):
    freq_diff = ((numpy.diff(freq) / 2.0) if (len(freq) > 1) else [0.5])
    freq_lims = numpy.concatenate([[(freq[0] - freq_diff[0])], (freq[:(- 1)] + freq_diff), [(freq[(- 1)] + freq_diff[(- 1)])]])
else:
    log_freqs = numpy.concatenate([[(freq[0] / ratio[0])], freq, [(freq[(- 1)] * ratio[0])]])
    freq_lims = numpy.sqrt((log_freqs[:(- 1)] * log_freqs[1:]))
(time_mesh, freq_mesh) = numpy.meshgrid(time_lims, freq_lims)
img = ax.pcolormesh(time_mesh, freq_mesh, tfr[ch_idx], cmap=cmap, vmin=vmin, vmax=vmax)
ax.set_xlim(time_lims[0], time_lims[(- 1)])
if (ylim is None):
    ylim = (freq_lims[0], freq_lims[(- 1)])
ax.set_ylim(ylim)
if (yscale == 'log'):
    ax.set_yscale('log')
    ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())
ax.yaxis.set_minor_formatter(matplotlib.ticker.NullFormatter())
ax.yaxis.set_minor_locator(matplotlib.ticker.NullLocator())
tempResult = unique(np.linspace(0, (len(freq) - 1), 12).round().astype('int'))
	
===================================================================	
_prepare_topo_plot: 35	
----------------------------	

'Prepare topo plot.'
info = copy.deepcopy((inst if isinstance(inst, Info) else inst.info))
if ((layout is None) and (ch_type is not 'eeg')):
    from ..channels import find_layout
    layout = find_layout(info)
elif (layout == 'auto'):
    layout = None
clean_ch_names = _clean_names(info['ch_names'])
for (ii, this_ch) in enumerate(info['chs']):
    this_ch['ch_name'] = clean_ch_names[ii]
info._update_redundant()
info._check_consistency()
tempResult = unique([ch['coil_type'] for ch in info['chs']])
	
===================================================================	
plot_sparse_source_estimates: 1233	
----------------------------	

'Plot source estimates obtained with sparse solver.\n\n    Active dipoles are represented in a "Glass" brain.\n    If the same source is active in multiple source estimates it is\n    displayed with a sphere otherwise with a cone in 3D.\n\n    Parameters\n    ----------\n    src : dict\n        The source space.\n    stcs : instance of SourceEstimate or list of instances of SourceEstimate\n        The source estimates (up to 3).\n    colors : list\n        List of colors\n    linewidth : int\n        Line width in 2D plot.\n    fontsize : int\n        Font size.\n    bgcolor : tuple of length 3\n        Background color in 3D.\n    opacity : float in [0, 1]\n        Opacity of brain mesh.\n    brain_color : tuple of length 3\n        Brain color.\n    show : bool\n        Show figures if True.\n    high_resolution : bool\n        If True, plot on the original (non-downsampled) cortical mesh.\n    fig_name :\n        Mayavi figure name.\n    fig_number :\n        Matplotlib figure number.\n    labels : ndarray or list of ndarrays\n        Labels to show sources in clusters. Sources with the same\n        label and the waveforms within each cluster are presented in\n        the same color. labels should be a list of ndarrays when\n        stcs is a list ie. one label for each stc.\n    modes : list\n        Should be a list, with each entry being ``\'cone\'`` or ``\'sphere\'``\n        to specify how the dipoles should be shown.\n    scale_factors : list\n        List of floating point scale factors for the markers.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n    **kwargs : kwargs\n        Keyword arguments to pass to mlab.triangular_mesh.\n\n    Returns\n    -------\n    surface : instance of mlab Surface\n        The triangular mesh surface.\n    '
mlab = _import_mlab()
import matplotlib.pyplot as plt
from matplotlib.colors import ColorConverter
known_modes = ['cone', 'sphere']
if ((not isinstance(modes, (list, tuple))) or (not all(((mode in known_modes) for mode in modes)))):
    raise ValueError('mode must be a list containing only "cone" or "sphere"')
if (not isinstance(stcs, list)):
    stcs = [stcs]
if ((labels is not None) and (not isinstance(labels, list))):
    labels = [labels]
if (colors is None):
    colors = COLORS
linestyles = ['-', '--', ':']
lh_points = src[0]['rr']
rh_points = src[1]['rr']
points = numpy.r_[(lh_points, rh_points)]
lh_normals = src[0]['nn']
rh_normals = src[1]['nn']
normals = numpy.r_[(lh_normals, rh_normals)]
if high_resolution:
    use_lh_faces = src[0]['tris']
    use_rh_faces = src[1]['tris']
else:
    use_lh_faces = src[0]['use_tris']
    use_rh_faces = src[1]['use_tris']
use_faces = numpy.r_[(use_lh_faces, (lh_points.shape[0] + use_rh_faces))]
points *= 170
vertnos = [numpy.r_[(stc.lh_vertno, (lh_points.shape[0] + stc.rh_vertno))] for stc in stcs]
tempResult = unique(np.concatenate(vertnos).ravel())
	
===================================================================	
plot_sparse_source_estimates: 1247	
----------------------------	

'Plot source estimates obtained with sparse solver.\n\n    Active dipoles are represented in a "Glass" brain.\n    If the same source is active in multiple source estimates it is\n    displayed with a sphere otherwise with a cone in 3D.\n\n    Parameters\n    ----------\n    src : dict\n        The source space.\n    stcs : instance of SourceEstimate or list of instances of SourceEstimate\n        The source estimates (up to 3).\n    colors : list\n        List of colors\n    linewidth : int\n        Line width in 2D plot.\n    fontsize : int\n        Font size.\n    bgcolor : tuple of length 3\n        Background color in 3D.\n    opacity : float in [0, 1]\n        Opacity of brain mesh.\n    brain_color : tuple of length 3\n        Brain color.\n    show : bool\n        Show figures if True.\n    high_resolution : bool\n        If True, plot on the original (non-downsampled) cortical mesh.\n    fig_name :\n        Mayavi figure name.\n    fig_number :\n        Matplotlib figure number.\n    labels : ndarray or list of ndarrays\n        Labels to show sources in clusters. Sources with the same\n        label and the waveforms within each cluster are presented in\n        the same color. labels should be a list of ndarrays when\n        stcs is a list ie. one label for each stc.\n    modes : list\n        Should be a list, with each entry being ``\'cone\'`` or ``\'sphere\'``\n        to specify how the dipoles should be shown.\n    scale_factors : list\n        List of floating point scale factors for the markers.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n    **kwargs : kwargs\n        Keyword arguments to pass to mlab.triangular_mesh.\n\n    Returns\n    -------\n    surface : instance of mlab Surface\n        The triangular mesh surface.\n    '
mlab = _import_mlab()
import matplotlib.pyplot as plt
from matplotlib.colors import ColorConverter
known_modes = ['cone', 'sphere']
if ((not isinstance(modes, (list, tuple))) or (not all(((mode in known_modes) for mode in modes)))):
    raise ValueError('mode must be a list containing only "cone" or "sphere"')
if (not isinstance(stcs, list)):
    stcs = [stcs]
if ((labels is not None) and (not isinstance(labels, list))):
    labels = [labels]
if (colors is None):
    colors = COLORS
linestyles = ['-', '--', ':']
lh_points = src[0]['rr']
rh_points = src[1]['rr']
points = numpy.r_[(lh_points, rh_points)]
lh_normals = src[0]['nn']
rh_normals = src[1]['nn']
normals = numpy.r_[(lh_normals, rh_normals)]
if high_resolution:
    use_lh_faces = src[0]['tris']
    use_rh_faces = src[1]['tris']
else:
    use_lh_faces = src[0]['use_tris']
    use_rh_faces = src[1]['use_tris']
use_faces = numpy.r_[(use_lh_faces, (lh_points.shape[0] + use_rh_faces))]
points *= 170
vertnos = [numpy.r_[(stc.lh_vertno, (lh_points.shape[0] + stc.rh_vertno))] for stc in stcs]
unique_vertnos = numpy.unique(np.concatenate(vertnos).ravel())
color_converter = ColorConverter()
f = mlab.figure(figure=fig_name, bgcolor=bgcolor, size=(600, 600))
mlab.clf()
_toggle_mlab_render(f, False)
with warnings.catch_warnings(record=True):
    surface = mlab.triangular_mesh(points[:, 0], points[:, 1], points[:, 2], use_faces, color=brain_color, opacity=opacity, **kwargs)
surface.actor.property.backface_culling = True
fig = matplotlib.pyplot.figure(fig_number)
fig.clf()
ax = fig.add_subplot(111)
colors = cycle(colors)
utils.logger.info(('Total number of active sources: %d' % len(unique_vertnos)))
if (labels is not None):
    tempResult = unique(np.concatenate(labels).ravel())
	
***************************************************	

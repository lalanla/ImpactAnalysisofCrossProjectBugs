astropy_astropy-1.3.0: 35	
===================================================================	
Voigt1D: 320	
----------------------------	

'\n    One dimensional model for the Voigt profile.\n\n    Parameters\n    ----------\n    x_0 : float\n        Position of the peak\n    amplitude_L : float\n        The Lorentzian amplitude\n    fwhm_L : float\n        The Lorentzian full width at half maximum\n    fwhm_G : float\n        The Gaussian full width at half maximum\n\n    See Also\n    --------\n    Gaussian1D, Lorentz1D\n\n    Notes\n    -----\n    Algorithm for the computation taken from\n    McLean, A. B., Mitchell, C. E. J. & Swanston, D. M. Implementation of an\n    efficient analytical approximation to the Voigt function for photoemission\n    lineshape analysis. Journal of Electron Spectroscopy and Related Phenomena\n    69, 125-132 (1994)\n\n    Examples\n    --------\n    .. plot::\n        :include-source:\n\n        import numpy as np\n        from astropy.modeling.models import Voigt1D\n        import matplotlib.pyplot as plt\n\n        plt.figure()\n        x = np.arange(0, 10, 0.01)\n        v1 = Voigt1D(x_0=5, amplitude_L=10, fwhm_L=0.5, fwhm_G=0.9)\n        plt.plot(x, v1(x))\n        plt.show()\n    '
x_0 = Parameter(default=0)
amplitude_L = Parameter(default=1)
fwhm_L = Parameter(default=(2 / numpy.pi))
tempResult = log(2)
	
===================================================================	
Moffat1D.fit_deriv: 651	
----------------------------	

'One dimensional Moffat model derivative with respect to parameters'
d_A = ((1 + (((x - x_0) ** 2) / (gamma ** 2))) ** (- alpha))
d_x_0 = (((((- amplitude) * alpha) * d_A) * (((- 2) * x) + (2 * x_0))) / ((gamma ** 2) * (d_A ** alpha)))
d_gamma = (((((2 * amplitude) * alpha) * d_A) * ((x - x_0) ** 2)) / ((gamma ** 3) * (d_A ** alpha)))
tempResult = log((1 + (((x - x_0) ** 2) / (gamma ** 2))))
	
===================================================================	
Voigt1D.fit_deriv: 337	
----------------------------	

(A, B, C, D) = cls._abcd
tempResult = log(2)
	
===================================================================	
Moffat2D.fit_deriv: 675	
----------------------------	

'Two dimensional Moffat model derivative with respect to parameters'
rr_gg = ((((x - x_0) ** 2) + ((y - y_0) ** 2)) / (gamma ** 2))
d_A = ((1 + rr_gg) ** (- alpha))
d_x_0 = (((((- amplitude) * alpha) * d_A) * (((- 2) * x) + (2 * x_0))) / ((gamma ** 2) * (1 + rr_gg)))
d_y_0 = (((((- amplitude) * alpha) * d_A) * (((- 2) * y) + (2 * y_0))) / ((gamma ** 2) * (1 + rr_gg)))
tempResult = log((1 + rr_gg))
	
===================================================================	
Voigt1D.evaluate: 326	
----------------------------	

(A, B, C, D) = cls._abcd
tempResult = log(2)
	
===================================================================	
LogParabola1D.fit_deriv: 98	
----------------------------	

'One dimensional log parabola derivative with respect to parameters'
xx = (x / x_0)
tempResult = log(xx)
	
===================================================================	
LogParabola1D.evaluate: 91	
----------------------------	

'One dimensional log parabola model function'
xx = (x / x_0)
tempResult = log(xx)
	
===================================================================	
ExponentialCutoffPowerLaw1D.fit_deriv: 76	
----------------------------	

'One dimensional exponential cutoff power law derivative with respect to parameters'
xx = (x / x_0)
xc = (x / x_cutoff)
d_amplitude = ((xx ** (- alpha)) * numpy.exp((- xc)))
d_x_0 = (((alpha * amplitude) * d_amplitude) / x_0)
tempResult = log(xx)
	
===================================================================	
PowerLaw1D.fit_deriv: 27	
----------------------------	

'One dimensional power law derivative with respect to parameters'
xx = (x / x_0)
d_amplitude = (xx ** (- alpha))
d_x_0 = (((amplitude * alpha) * d_amplitude) / x_0)
tempResult = log(xx)
	
===================================================================	
BrokenPowerLaw1D.fit_deriv: 51	
----------------------------	

'One dimensional broken power law derivative with respect to parameters'
alpha = numpy.where((x < x_break), alpha_1, alpha_2)
xx = (x / x_break)
d_amplitude = (xx ** (- alpha))
d_x_break = (((amplitude * alpha) * d_amplitude) / x_break)
tempResult = log(xx)
	
===================================================================	
FitnessFunc.p0_prior: 74	
----------------------------	

'\n        Empirical prior, parametrized by the false alarm probability ``p0``\n        See  eq. 21 in Scargle (2012)\n\n        Note that there was an error in this equation in the original Scargle\n        paper (the "log" was missing). The following corrected form is taken\n        from http://arxiv.org/abs/1304.2818\n        '
tempResult = log(((73.53 * self.p0) * (N ** (- 0.478))))
	
===================================================================	
RegularEvents.fitness: 174	
----------------------------	

M_k = (T_k / self.dt)
N_over_M = (N_k / M_k)
eps = 1e-08
if numpy.any((N_over_M > (1 + eps))):
    warnings.warn('regular events: N/M > 1.  Is the time step correct?', AstropyUserWarning)
one_m_NM = (1 - N_over_M)
N_over_M[(N_over_M <= 0)] = 1
one_m_NM[(one_m_NM <= 0)] = 1
tempResult = log(N_over_M)
	
===================================================================	
RegularEvents.fitness: 174	
----------------------------	

M_k = (T_k / self.dt)
N_over_M = (N_k / M_k)
eps = 1e-08
if numpy.any((N_over_M > (1 + eps))):
    warnings.warn('regular events: N/M > 1.  Is the time step correct?', AstropyUserWarning)
one_m_NM = (1 - N_over_M)
N_over_M[(N_over_M <= 0)] = 1
one_m_NM[(one_m_NM <= 0)] = 1
tempResult = log(one_m_NM)
	
===================================================================	
Events.fitness: 144	
----------------------------	

tempResult = log(N_k)
	
===================================================================	
Events.fitness: 144	
----------------------------	

tempResult = log(T_k)
	
===================================================================	
FitnessFunc.compute_ncp_prior: 85	
----------------------------	

'\n        If ``ncp_prior`` is not explicitly defined, compute it from ``gamma``\n        or ``p0``.\n        '
if (self.ncp_prior is not None):
    return self.ncp_prior
elif (self.gamma is not None):
    tempResult = log(self.gamma)
	
===================================================================	
module: 10	
----------------------------	

'\nThis module contains simple statistical algorithms that are straightforwardly\nimplemented as a single python function (or family of functions).\n\nThis module should generally not be used directly.  Everything in `__all__` is\nimported into `astropy.stats`, and hence that package should be used for\naccess.\n'
from __future__ import absolute_import, division, print_function, unicode_literals
import math
import numpy as np
from ..extern.six.moves import range
__all__ = ['binom_conf_interval', 'binned_binom_proportion', 'poisson_conf_interval', 'median_absolute_deviation', 'mad_std', 'biweight_location', 'biweight_midvariance', 'signal_to_noise_oir_ccd', 'bootstrap', 'gaussian_fwhm_to_sigma', 'gaussian_sigma_to_fwhm']
__doctest_skip__ = ['binned_binom_proportion']
__doctest_requires__ = {'binom_conf_interval': ['scipy.special'], 'poisson_conf_interval': ['scipy.special', 'scipy.optimize', 'scipy.integrate']}
tempResult = log(2.0)
	
===================================================================	
_KnuthF.eval: 102	
----------------------------	

'Evaluate the Knuth function\n\n        Parameters\n        ----------\n        dx : float\n            Width of bins\n\n        Returns\n        -------\n        F : float\n            evaluation of the negative Knuth likelihood function:\n            smaller values indicate a better fit.\n        '
M = int(M)
if (M <= 0):
    return numpy.inf
bins = self.bins(M)
(nk, bins) = numpy.histogram(self.data, bins)
tempResult = log(M)
	
===================================================================	
akaike_info_criterion_lsq: 26	
----------------------------	

'\n    Computes the Akaike Information Criterion assuming that the observations\n    are Gaussian distributed.\n\n    In this case, AIC is given as\n\n    .. math::\n\n        \\mathrm{AIC} = n\\ln\\left(\\dfrac{\\mathrm{SSR}}{n}\\right) + 2k\n\n    In case that the sample size is not "large enough", a correction is\n    applied, i.e.\n\n    .. math::\n\n        \\mathrm{AIC} = n\\ln\\left(\\dfrac{\\mathrm{SSR}}{n}\\right) + 2k +\n                       \\dfrac{2k(k+1)}{n-k-1}\n\n\n    in which :math:`n` is the sample size, :math:`k` is the number of free\n    parameters and :math:`\\mathrm{SSR}` stands for the sum of squared residuals\n    between model and data.\n\n    This is applicable, for instance, when the parameters of a model are\n    estimated using the least squares statistic.\n\n    Parameters\n    ----------\n    ssr : float\n        Sum of squared residuals (SSR) between model and data.\n    n_params : int\n        Number of free parameters of the model, i.e.,  the dimension of the\n        parameter space.\n    n_samples : int\n        Number of observations.\n\n    Returns\n    -------\n    aic : float\n        Akaike Information Criterion.\n\n    Examples\n    --------\n    This example is based on Astropy Modeling webpage, Compound models\n    section.\n\n    >>> import numpy as np\n    >>> from astropy.modeling import models, fitting\n    >>> from astropy.stats.info_theory import akaike_info_criterion_lsq\n    >>> np.random.seed(42)\n    >>> # Generate fake data\n    >>> g1 = models.Gaussian1D(.1, 0, 0.2) # changed this to noise level\n    >>> g2 = models.Gaussian1D(.1, 0.3, 0.2) # and added another Gaussian\n    >>> g3 = models.Gaussian1D(2.5, 0.5, 0.1)\n    >>> x = np.linspace(-1, 1, 200)\n    >>> y = g1(x) + g2(x) + g3(x) + np.random.normal(0., 0.2, x.shape)\n    >>> # Fit with three Gaussians\n    >>> g3_init = (models.Gaussian1D(.1, 0, 0.1)\n    ...            + models.Gaussian1D(.1, 0.2, 0.15)\n    ...            + models.Gaussian1D(2., .4, 0.1))\n    >>> fitter = fitting.LevMarLSQFitter()\n    >>> g3_fit = fitter(g3_init, x, y)\n    >>> # Fit with two Gaussians\n    >>> g2_init = (models.Gaussian1D(.1, 0, 0.1) +\n    ...            models.Gaussian1D(2, 0.5, 0.1))\n    >>> g2_fit = fitter(g2_init, x, y)\n    >>> # Fit with only one Gaussian\n    >>> g1_init = models.Gaussian1D(amplitude=2., mean=0.3, stddev=.5)\n    >>> g1_fit = fitter(g1_init, x, y)\n    >>> # Compute the mean squared errors\n    >>> ssr_g3 = np.sum((g3_fit(x) - y)**2.0)\n    >>> ssr_g2 = np.sum((g2_fit(x) - y)**2.0)\n    >>> ssr_g1 = np.sum((g1_fit(x) - y)**2.0)\n    >>> akaike_info_criterion_lsq(ssr_g3, 9, x.shape[0]) # doctest: +FLOAT_CMP\n    -656.32589850659224\n    >>> akaike_info_criterion_lsq(ssr_g2, 6, x.shape[0]) # doctest: +FLOAT_CMP\n    -662.83834510232043\n    >>> akaike_info_criterion_lsq(ssr_g1, 3, x.shape[0]) # doctest: +FLOAT_CMP\n    -647.47312032659499\n\n    Hence, from the AIC values, we would prefer to choose the model g2_fit.\n    However, we can considerably support the model g3_fit, since the\n    difference in AIC is about 6.5. We should reject the model g1_fit.\n\n    References\n    ----------\n    .. [1] Akaike Information Criteria\n       <http://avesbiodiv.mncn.csic.es/estadistica/ejemploaic.pdf>\n    .. [2] Hu, S. Akaike Information Criterion.\n       <http://www4.ncsu.edu/~shu3/Presentation/AIC.pdf>\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\n       <http://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\n    '
tempResult = log((ssr / n_samples))
	
===================================================================	
bayesian_info_criterion_lsq: 14	
----------------------------	

'\n    Computes the Bayesian Information Criterion (BIC) assuming that the\n    observations come from a Gaussian distribution.\n\n    In this case, BIC is given as\n\n    .. math::\n\n        \\mathrm{BIC} = n\\ln\\left(\\dfrac{\\mathrm{SSR}}{n}\\right) + k\\ln(n)\n\n    in which :math:`n` is the sample size, :math:`k` is the number of free\n    parameters and :math:`\\mathrm{SSR}` stands for the sum of squared residuals\n    between model and data.\n\n    This is applicable, for instance, when the parameters of a model are\n    estimated using the least squares statistic. See [1]_ and [2]_.\n\n    Parameters\n    ----------\n    ssr : float\n        Sum of squared residuals (SSR) between model and data.\n    n_params : int\n        Number of free parameters of the model, i.e., dimension of the\n        parameter space.\n    n_samples : int\n        Number of observations.\n\n    Returns\n    -------\n    bic : float\n\n    Examples\n    --------\n    Consider the simple 1-D fitting example presented in the Astropy\n    modeling webpage [3]_. There, two models (Box and Gaussian) were fitted to\n    a source flux using the least squares statistic. However, the fittings\n    themselves do not tell much about which model better represents this\n    hypothetical source. Therefore, we are going to apply to BIC in order to\n    decide in favor of a model.\n\n    >>> import numpy as np\n    >>> from astropy.modeling import models, fitting\n    >>> from astropy.stats.info_theory import bayesian_info_criterion_lsq\n    >>> # Generate fake data\n    >>> np.random.seed(0)\n    >>> x = np.linspace(-5., 5., 200)\n    >>> y = 3 * np.exp(-0.5 * (x - 1.3)**2 / 0.8**2)\n    >>> y += np.random.normal(0., 0.2, x.shape)\n    >>> # Fit the data using a Box model\n    >>> t_init = models.Trapezoid1D(amplitude=1., x_0=0., width=1., slope=0.5)\n    >>> fit_t = fitting.LevMarLSQFitter()\n    >>> t = fit_t(t_init, x, y)\n    >>> # Fit the data using a Gaussian\n    >>> g_init = models.Gaussian1D(amplitude=1., mean=0, stddev=1.)\n    >>> fit_g = fitting.LevMarLSQFitter()\n    >>> g = fit_g(g_init, x, y)\n    >>> # Compute the mean squared errors\n    >>> ssr_t = np.sum((t(x) - y)*(t(x) - y))\n    >>> ssr_g = np.sum((g(x) - y)*(g(x) - y))\n    >>> # Compute the bics\n    >>> bic_t = bayesian_info_criterion_lsq(ssr_t, 4, x.shape[0])\n    >>> bic_g = bayesian_info_criterion_lsq(ssr_g, 3, x.shape[0])\n    >>> bic_t - bic_g # doctest: +FLOAT_CMP\n    30.644474706065466\n\n    Hence, there is a very strong evidence that the Gaussian model has a\n    significantly better representation of the data than the Box model. This\n    is, obviously, expected since the true model is Gaussian.\n\n    References\n    ----------\n    .. [1] Wikipedia. Bayesian Information Criterion.\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\n    .. [2] Origin Lab. Comparing Two Fitting Functions.\n       <http://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\n    .. [3] Astropy Models and Fitting\n        <http://docs.astropy.org/en/stable/modeling>\n    '
tempResult = log((ssr / n_samples))
	
===================================================================	
bayesian_info_criterion: 10	
----------------------------	

' Computes the Bayesian Information Criterion (BIC) given the log of the\n    likelihood function evaluated at the estimated (or analytically derived)\n    parameters, the number of parameters, and the number of samples.\n\n    The BIC is usually applied to decide whether increasing the number of free\n    parameters (hence, increasing the model complexity) yields significantly\n    better fittings. The decision is in favor of the model with the lowest\n    BIC.\n\n    BIC is given as\n\n    .. math::\n\n        \\mathrm{BIC} = k \\ln(n) - 2L,\n\n    in which :math:`n` is the sample size, :math:`k` is the number of free\n    parameters, and :math:`L` is the log likelihood function of the model\n    evaluated at the maximum likelihood estimate (i. e., the parameters for\n    which L is maximized).\n\n    When comparing two models define\n    :math:`\\Delta \\mathrm{BIC} = \\mathrm{BIC}_h - \\mathrm{BIC}_l`, in which\n    :math:`\\mathrm{BIC}_h` is the higher BIC, and :math:`\\mathrm{BIC}_l` is\n    the lower BIC. The higher is :math:`\\Delta \\mathrm{BIC}` the stronger is\n    the evidence against the model with higher BIC.\n\n    The general rule of thumb is:\n\n    :math:`0 < \\Delta\\mathrm{BIC} \\leq 2`: weak evidence that model low is\n    better\n\n    :math:`2 < \\Delta\\mathrm{BIC} \\leq 6`: moderate evidence that model low is\n    better\n\n    :math:`6 < \\Delta\\mathrm{BIC} \\leq 10`: strong evidence that model low is\n    better\n\n    :math:`\\Delta\\mathrm{BIC} > 10`: very strong evidence that model low is\n    better\n\n    For a detailed explanation, see [1]_ - [5]_.\n\n    Parameters\n    ----------\n    log_likelihood : float\n        Logarithm of the likelihood function of the model evaluated at the\n        point of maxima (with respect to the parameter space).\n    n_params : int\n        Number of free parameters of the model, i.e., dimension of the\n        parameter space.\n    n_samples : int\n        Number of observations.\n\n    Returns\n    -------\n    bic : float\n        Bayesian Information Criterion.\n\n    Examples\n    --------\n    The following example was originally presented in [1]_. Consider a\n    Gaussian model (mu, sigma) and a t-Student model (mu, sigma, delta).\n    In addition, assume that the t model has presented a higher likelihood.\n    The question that the BIC is proposed to answer is: "Is the increase in\n    likelihood due to larger number of parameters?"\n\n    >>> from astropy.stats.info_theory import bayesian_info_criterion\n    >>> lnL_g = -176.4\n    >>> lnL_t = -173.0\n    >>> n_params_g = 2\n    >>> n_params_t = 3\n    >>> n_samples = 100\n    >>> bic_g = bayesian_info_criterion(lnL_g, n_params_g, n_samples)\n    >>> bic_t = bayesian_info_criterion(lnL_t, n_params_t, n_samples)\n    >>> bic_g - bic_t # doctest: +FLOAT_CMP\n    2.1948298140119391\n\n    Therefore, there exist a moderate evidence that the increasing in\n    likelihood for t-Student model is due to the larger number of parameters.\n\n    References\n    ----------\n    .. [1] Richards, D. Maximum Likelihood Estimation and the Bayesian\n       Information Criterion.\n       <https://hea-www.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf>\n    .. [2] Wikipedia. Bayesian Information Criterion.\n       <https://en.wikipedia.org/wiki/Bayesian_information_criterion>\n    .. [3] Origin Lab. Comparing Two Fitting Functions.\n       <http://www.originlab.com/doc/Origin-Help/PostFit-CompareFitFunc>\n    .. [4] Liddle, A. R. Information Criteria for Astrophysical Model\n       Selection. 2008. <http://arxiv.org/pdf/astro-ph/0701113v2.pdf>\n    .. [5] Liddle, A. R. How many cosmological parameters? 2008.\n       <http://arxiv.org/pdf/astro-ph/0401198v3.pdf>\n    '
tempResult = log(n_samples)
	
===================================================================	
lombscargle_chi2: 35	
----------------------------	

"Lomb-Scargle Periodogram\n\n    This implements a chi-squared-based periodogram, which is relatively slow\n    but useful for validating the faster algorithms in the package.\n\n    Parameters\n    ----------\n    t, y, dy : array_like (NOT astropy.Quantities)\n        times, values, and errors of the data points. These should be\n        broadcastable to the same shape.\n    frequency : array_like\n        frequencies (not angular frequencies) at which to calculate periodogram\n    normalization : string (optional, default='standard')\n        Normalization to use for the periodogram.\n        Options are 'standard', 'model', 'log', or 'psd'.\n    fit_mean : bool (optional, default=True)\n        if True, include a constant offset as part of the model at each\n        frequency. This can lead to more accurate results, especially in the\n        case of incomplete phase coverage.\n    center_data : bool (optional, default=True)\n        if True, pre-center the data by subtracting the weighted mean\n        of the input data. This is especially important if ``fit_mean = False``\n    nterms : int (optional, default=1)\n        Number of Fourier terms in the fit\n\n    Returns\n    -------\n    power : array_like\n        Lomb-Scargle power associated with each frequency.\n        Units of the result depend on the normalization.\n\n    References\n    ----------\n    .. [1] M. Zechmeister and M. Kurster, A&A 496, 577-584 (2009)\n    .. [2] W. Press et al, Numerical Recipies in C (2002)\n    .. [3] Scargle, J.D. 1982, ApJ 263:835-853\n    "
if (dy is None):
    dy = 1
(t, y, dy) = numpy.broadcast_arrays(t, y, dy)
frequency = numpy.asarray(frequency)
if (t.ndim != 1):
    raise ValueError('t, y, dy should be one dimensional')
if (frequency.ndim != 1):
    raise ValueError('frequency should be one-dimensional')
w = (dy ** (- 2.0))
w /= w.sum()
if (center_data or fit_mean):
    yw = ((y - numpy.dot(w, y)) / dy)
else:
    yw = (y / dy)
chi2_ref = numpy.dot(yw, yw)

def compute_power(f):
    X = design_matrix(t, f, dy=dy, bias=fit_mean, nterms=nterms)
    XTX = numpy.dot(X.T, X)
    XTy = numpy.dot(X.T, yw)
    return numpy.dot(XTy.T, numpy.linalg.solve(XTX, XTy))
p = numpy.array([compute_power(f) for f in frequency])
if (normalization == 'psd'):
    p *= ((0.5 * t.size) / (dy ** (- 2.0)).sum())
elif (normalization == 'model'):
    p /= (chi2_ref - p)
elif (normalization == 'log'):
    tempResult = log((1 - (p / chi2_ref)))
	
===================================================================	
lombscargle_fastchi2: 51	
----------------------------	

"Lomb-Scargle Periodogram\n\n    This implements a fast chi-squared periodogram using the algorithm\n    outlined in [4]_. The result is identical to the standard Lomb-Scargle\n    periodogram. The advantage of this algorithm is the\n    ability to compute multiterm periodograms relatively quickly.\n\n    Parameters\n    ----------\n    t, y, dy : array_like  (NOT astropy.Quantities)\n        times, values, and errors of the data points. These should be\n        broadcastable to the same shape.\n    f0, df, Nf : (float, float, int)\n        parameters describing the frequency grid, f = f0 + df * arange(Nf).\n    normalization : string (optional, default='standard')\n        Normalization to use for the periodogram.\n        Options are 'standard', 'model', 'log', or 'psd'.\n    fit_mean : bool (optional, default=True)\n        if True, include a constant offset as part of the model at each\n        frequency. This can lead to more accurate results, especially in the\n        case of incomplete phase coverage.\n    center_data : bool (optional, default=True)\n        if True, pre-center the data by subtracting the weighted mean\n        of the input data. This is especially important if ``fit_mean = False``\n    nterms : int (optional, default=1)\n        Number of Fourier terms in the fit\n\n    Returns\n    -------\n    power : array_like\n        Lomb-Scargle power associated with each frequency.\n        Units of the result depend on the normalization.\n\n    References\n    ----------\n    .. [1] M. Zechmeister and M. Kurster, A&A 496, 577-584 (2009)\n    .. [2] W. Press et al, Numerical Recipies in C (2002)\n    .. [3] Scargle, J.D. ApJ 263:835-853 (1982)\n    .. [4] Palmer, J. ApJ 695:496-502 (2009)\n    "
if ((nterms == 0) and (not fit_mean)):
    raise ValueError('Cannot have nterms = 0 without fitting bias')
if (dy is None):
    dy = 1
(t, y, dy) = numpy.broadcast_arrays(t, y, dy)
if (t.ndim != 1):
    raise ValueError('t, y, dy should be one dimensional')
if (f0 < 0):
    raise ValueError('Frequencies must be positive')
if (df <= 0):
    raise ValueError('Frequency steps must be positive')
if (Nf <= 0):
    raise ValueError('Number of frequencies must be positive')
w = (dy ** (- 2.0))
ws = numpy.sum(w)
if (center_data or fit_mean):
    y = (y - (numpy.dot(w, y) / ws))
yw = (y / dy)
chi2_ref = numpy.dot(yw, yw)
kwargs = dict.copy((trig_sum_kwds or {}))
kwargs.update(f0=f0, df=df, use_fft=use_fft, N=Nf)
yws = numpy.sum((y * w))
SCw = [(numpy.zeros(Nf), (ws * numpy.ones(Nf)))]
SCw.extend([trig_sum(t, w, freq_factor=i, **kwargs) for i in range(1, ((2 * nterms) + 1))])
(Sw, Cw) = zip(*SCw)
SCyw = [(numpy.zeros(Nf), (yws * numpy.ones(Nf)))]
SCyw.extend([trig_sum(t, (w * y), freq_factor=i, **kwargs) for i in range(1, (nterms + 1))])
(Syw, Cyw) = zip(*SCyw)
order = ([('C', 0)] if fit_mean else [])
order.extend(sum([[('S', i), ('C', i)] for i in range(1, (nterms + 1))], []))
funcs = dict(S=(lambda m, i: Syw[m][i]), C=(lambda m, i: Cyw[m][i]), SS=(lambda m, n, i: (0.5 * (Cw[abs((m - n))][i] - Cw[(m + n)][i]))), CC=(lambda m, n, i: (0.5 * (Cw[abs((m - n))][i] + Cw[(m + n)][i]))), SC=(lambda m, n, i: (0.5 * ((numpy.sign((m - n)) * Sw[abs((m - n))][i]) + Sw[(m + n)][i]))), CS=(lambda m, n, i: (0.5 * ((numpy.sign((n - m)) * Sw[abs((n - m))][i]) + Sw[(n + m)][i]))))

def compute_power(i):
    XTX = numpy.array([[funcs[(A[0] + B[0])](A[1], B[1], i) for A in order] for B in order])
    XTy = numpy.array([funcs[A[0]](A[1], i) for A in order])
    return numpy.dot(XTy.T, numpy.linalg.solve(XTX, XTy))
p = numpy.array([compute_power(i) for i in range(Nf)])
if (normalization == 'psd'):
    p *= ((0.5 * t.size) / ws)
elif (normalization == 'standard'):
    p /= chi2_ref
elif (normalization == 'log'):
    tempResult = log((1 - (p / chi2_ref)))
	
===================================================================	
lombscargle_fast: 50	
----------------------------	

'Fast Lomb-Scargle Periodogram\n\n    This implements the Press & Rybicki method [1]_ for fast O[N log(N)]\n    Lomb-Scargle periodograms.\n\n    Parameters\n    ----------\n    t, y, dy : array_like  (NOT astropy.Quantities)\n        times, values, and errors of the data points. These should be\n        broadcastable to the same shape.\n    f0, df, Nf : (float, float, int)\n        parameters describing the frequency grid, f = f0 + df * arange(Nf).\n    center_data : bool (default=True)\n        Specify whether to subtract the mean of the data before the fit\n    fit_mean : bool (default=True)\n        If True, then compute the floating-mean periodogram; i.e. let the mean\n        vary with the fit.\n    normalization : string (optional, default=\'standard\')\n        Normalization to use for the periodogram.\n        Options are \'standard\', \'model\', \'log\', or \'psd\'.\n    use_fft : bool (default=True)\n        If True, then use the Press & Rybicki O[NlogN] algorithm to compute\n        the result. Otherwise, use a slower O[N^2] algorithm\n    trig_sum_kwds : dict or None (optional)\n        extra keyword arguments to pass to the ``trig_sum`` utility.\n        Options are ``oversampling`` and ``Mfft``. See documentation\n        of ``trig_sum`` for details.\n\n    Returns\n    -------\n    power : ndarray\n        Lomb-Scargle power associated with each frequency.\n        Units of the result depend on the normalization.\n\n    Notes\n    -----\n    Note that the ``use_fft=True`` algorithm is an approximation to the true\n    Lomb-Scargle periodogram, and as the number of points grows this\n    approximation improves. On the other hand, for very small datasets\n    (<~50 points or so) this approximation may not be useful.\n\n    References\n    ----------\n    .. [1] Press W.H. and Rybicki, G.B, "Fast algorithm for spectral analysis\n        of unevenly sampled data". ApJ 1:338, p277, 1989\n    .. [2] M. Zechmeister and M. Kurster, A&A 496, 577-584 (2009)\n    .. [3] W. Press et al, Numerical Recipies in C (2002)\n    '
if (dy is None):
    dy = 1
(t, y, dy) = numpy.broadcast_arrays(t, y, dy)
if (t.ndim != 1):
    raise ValueError('t, y, dy should be one dimensional')
if (f0 < 0):
    raise ValueError('Frequencies must be positive')
if (df <= 0):
    raise ValueError('Frequency steps must be positive')
if (Nf <= 0):
    raise ValueError('Number of frequencies must be positive')
w = (dy ** (- 2.0))
w /= w.sum()
if (center_data or fit_mean):
    y = (y - numpy.dot(w, y))
kwargs = dict.copy((trig_sum_kwds or {}))
kwargs.update(f0=f0, df=df, use_fft=use_fft, N=Nf)
(Sh, Ch) = trig_sum(t, (w * y), **kwargs)
(S2, C2) = trig_sum(t, w, freq_factor=2, **kwargs)
if fit_mean:
    (S, C) = trig_sum(t, w, **kwargs)
    tan_2omega_tau = ((S2 - ((2 * S) * C)) / (C2 - ((C * C) - (S * S))))
else:
    tan_2omega_tau = (S2 / C2)
S2w = (tan_2omega_tau / numpy.sqrt((1 + (tan_2omega_tau * tan_2omega_tau))))
C2w = (1 / numpy.sqrt((1 + (tan_2omega_tau * tan_2omega_tau))))
Cw = (numpy.sqrt(0.5) * numpy.sqrt((1 + C2w)))
Sw = ((numpy.sqrt(0.5) * numpy.sign(S2w)) * numpy.sqrt((1 - C2w)))
YY = numpy.dot(w, (y ** 2))
YC = ((Ch * Cw) + (Sh * Sw))
YS = ((Sh * Cw) - (Ch * Sw))
CC = (0.5 * ((1 + (C2 * C2w)) + (S2 * S2w)))
SS = (0.5 * ((1 - (C2 * C2w)) - (S2 * S2w)))
if fit_mean:
    CC -= (((C * Cw) + (S * Sw)) ** 2)
    SS -= (((S * Cw) - (C * Sw)) ** 2)
power = (((YC * YC) / CC) + ((YS * YS) / SS))
if (normalization == 'standard'):
    power /= YY
elif (normalization == 'model'):
    power /= (YY - power)
elif (normalization == 'log'):
    tempResult = log((1 - (power / YY)))
	
===================================================================	
lombscargle_scipy: 27	
----------------------------	

"Lomb-Scargle Periodogram\n\n    This is a wrapper of ``scipy.signal.lombscargle`` for computation of the\n    Lomb-Scargle periodogram. This is a relatively fast version of the naive\n    O[N^2] algorithm, but cannot handle heteroskedastic errors.\n\n    Parameters\n    ----------\n    t, y: array_like  (NOT astropy.Quantities)\n        times, values, and errors of the data points. These should be\n        broadcastable to the same shape.\n    frequency : array_like\n        frequencies (not angular frequencies) at which to calculate periodogram\n    normalization : string (optional, default='standard')\n        Normalization to use for the periodogram.\n        Options are 'standard', 'model', 'log', or 'psd'.\n    center_data : bool (optional, default=True)\n        if True, pre-center the data by subtracting the weighted mean\n        of the input data.\n\n    Returns\n    -------\n    power : array_like\n        Lomb-Scargle power associated with each frequency.\n        Units of the result depend on the normalization.\n\n    References\n    ----------\n    .. [1] M. Zechmeister and M. Kurster, A&A 496, 577-584 (2009)\n    .. [2] W. Press et al, Numerical Recipies in C (2002)\n    .. [3] Scargle, J.D. 1982, ApJ 263:835-853\n    "
try:
    from scipy import signal
except ImportError:
    raise ImportError('scipy must be installed to use lombscargle_scipy')
(t, y) = numpy.broadcast_arrays(t, y)
t = numpy.asarray(t, dtype=float)
y = numpy.asarray(y, dtype=float)
frequency = numpy.asarray(frequency, dtype=float)
if (t.ndim != 1):
    raise ValueError('t, y, dy should be one dimensional')
if (frequency.ndim != 1):
    raise ValueError('frequency should be one-dimensional')
if center_data:
    y = (y - y.mean())
p = scipy.signal.lombscargle(t, y, ((2 * numpy.pi) * frequency))
if (normalization == 'psd'):
    pass
elif (normalization == 'standard'):
    p *= (2 / (t.size * numpy.mean((y ** 2))))
elif (normalization == 'log'):
    tempResult = log((1 - ((2 * p) / (t.size * numpy.mean((y ** 2))))))
	
===================================================================	
lombscargle_slow: 55	
----------------------------	

"Lomb-Scargle Periodogram\n\n    This is a pure-python implementation of the original Lomb-Scargle formalism\n    (e.g. [1]_, [2]_), with the addition of the floating mean (e.g. [3]_)\n\n    Parameters\n    ----------\n    t, y, dy : array_like  (NOT astropy.Quantities)\n        times, values, and errors of the data points. These should be\n        broadcastable to the same shape.\n    frequency : array_like\n        frequencies (not angular frequencies) at which to calculate periodogram\n    normalization : string (optional, default='standard')\n        Normalization to use for the periodogram.\n        Options are 'standard', 'model', 'log', or 'psd'.\n    fit_mean : bool (optional, default=True)\n        if True, include a constant offset as part of the model at each\n        frequency. This can lead to more accurate results, especially in the\n        case of incomplete phase coverage.\n    center_data : bool (optional, default=True)\n        if True, pre-center the data by subtracting the weighted mean\n        of the input data. This is especially important if ``fit_mean = False``\n\n    Returns\n    -------\n    power : array_like\n        Lomb-Scargle power associated with each frequency.\n        Units of the result depend on the normalization.\n\n    References\n    ----------\n    .. [1] W. Press et al, Numerical Recipies in C (2002)\n    .. [2] Scargle, J.D. 1982, ApJ 263:835-853\n    .. [3] M. Zechmeister and M. Kurster, A&A 496, 577-584 (2009)\n    "
if (dy is None):
    dy = 1
(t, y, dy) = numpy.broadcast_arrays(t, y, dy)
frequency = numpy.asarray(frequency)
if (t.ndim != 1):
    raise ValueError('t, y, dy should be one dimensional')
if (frequency.ndim != 1):
    raise ValueError('frequency should be one-dimensional')
w = (dy ** (- 2.0))
w /= w.sum()
if (fit_mean or center_data):
    y = (y - numpy.dot(w, y))
omega = ((2 * numpy.pi) * frequency)
omega = omega.ravel()[numpy.newaxis, :]
(t, y, dy, w) = map((lambda x: x[:, numpy.newaxis]), (t, y, dy, w))
sin_omega_t = numpy.sin((omega * t))
cos_omega_t = numpy.cos((omega * t))
S2 = (2 * numpy.dot(w.T, (sin_omega_t * cos_omega_t)))
C2 = (2 * numpy.dot(w.T, (0.5 - (sin_omega_t ** 2))))
if fit_mean:
    S = numpy.dot(w.T, sin_omega_t)
    C = numpy.dot(w.T, cos_omega_t)
    S2 -= ((2 * S) * C)
    C2 -= ((C * C) - (S * S))
omega_t_tau = ((omega * t) - (0.5 * numpy.arctan2(S2, C2)))
sin_omega_t_tau = numpy.sin(omega_t_tau)
cos_omega_t_tau = numpy.cos(omega_t_tau)
Y = numpy.dot(w.T, y)
wy = (w * y)
YCtau = numpy.dot(wy.T, cos_omega_t_tau)
YStau = numpy.dot(wy.T, sin_omega_t_tau)
CCtau = numpy.dot(w.T, (cos_omega_t_tau * cos_omega_t_tau))
SStau = numpy.dot(w.T, (sin_omega_t_tau * sin_omega_t_tau))
if fit_mean:
    Ctau = numpy.dot(w.T, cos_omega_t_tau)
    Stau = numpy.dot(w.T, sin_omega_t_tau)
    YCtau -= (Y * Ctau)
    YStau -= (Y * Stau)
    CCtau -= (Ctau * Ctau)
    SStau -= (Stau * Stau)
p = (((YCtau * YCtau) / CCtau) + ((YStau * YStau) / SStau))
YY = numpy.dot(w.T, (y * y))
if (normalization == 'standard'):
    p /= YY
elif (normalization == 'model'):
    p /= (YY - p)
elif (normalization == 'log'):
    tempResult = log((1 - (p / YY)))
	
===================================================================	
test_gaussian_sigma_to_fwhm: 296	
----------------------------	

tempResult = log(2.0)
	
===================================================================	
test_gaussian_fwhm_to_sigma: 292	
----------------------------	

tempResult = log(2.0)
	
===================================================================	
_logn: 13	
----------------------------	

'Calculate the log base n of x.'
if (out is None):
    tempResult = log(x)
	
===================================================================	
_logn: 13	
----------------------------	

'Calculate the log base n of x.'
if (out is None):
    tempResult = log(n)
	
===================================================================	
_logn: 15	
----------------------------	

'Calculate the log base n of x.'
if (out is None):
    return (numpy.log(x) / numpy.log(n))
else:
    tempResult = log(x, out=out)
	
===================================================================	
_logn: 16	
----------------------------	

'Calculate the log base n of x.'
if (out is None):
    return (numpy.log(x) / numpy.log(n))
else:
    numpy.log(x, out=out)
    tempResult = log(n)
	
===================================================================	
LogStretch.__call__: 145	
----------------------------	

values = _prepare(values, clip=clip, out=out)
numpy.multiply(values, self.exp, out=values)
numpy.add(values, 1.0, out=values)
tempResult = log(values, out=values)
	
===================================================================	
LogStretch.__call__: 146	
----------------------------	

values = _prepare(values, clip=clip, out=out)
numpy.multiply(values, self.exp, out=values)
numpy.add(values, 1.0, out=values)
numpy.log(values, out=values)
tempResult = log((self.exp + 1.0))
	
===================================================================	
InvertedLogStretch.__call__: 163	
----------------------------	

values = _prepare(values, clip=clip, out=out)
tempResult = log((self.exp + 1.0))
	
***************************************************	
scipy_scipy-0.19.0: 256	
===================================================================	
_fractional_power_superdiag_entry: 98	
----------------------------	

'\n    Compute a superdiagonal entry of a fractional matrix power.\n\n    This is Eq. (5.6) in [1]_.\n\n    Parameters\n    ----------\n    l1 : complex\n        A diagonal entry of the matrix.\n    l2 : complex\n        A diagonal entry of the matrix.\n    t12 : complex\n        A superdiagonal entry of the matrix.\n    p : float\n        A fractional power.\n\n    Returns\n    -------\n    f12 : complex\n        A superdiagonal entry of the fractional matrix power.\n\n    Notes\n    -----\n    Care has been taken to return a real number if possible when\n    all of the inputs are real numbers.\n\n    References\n    ----------\n    .. [1] Nicholas J. Higham and Lijing lin (2011)\n           "A Schur-Pade Algorithm for Fractional Powers of a Matrix."\n           SIAM Journal on Matrix Analysis and Applications,\n           32 (3). pp. 1056-1078. ISSN 0895-4798\n\n    '
if (l1 == l2):
    f12 = ((t12 * p) * (l1 ** (p - 1)))
elif (abs((l2 - l1)) > (abs((l1 + l2)) / 2)):
    f12 = ((t12 * ((l2 ** p) - (l1 ** p))) / (l2 - l1))
else:
    z = ((l2 - l1) / (l2 + l1))
    tempResult = log(l1)
	
===================================================================	
_fractional_power_superdiag_entry: 99	
----------------------------	

'\n    Compute a superdiagonal entry of a fractional matrix power.\n\n    This is Eq. (5.6) in [1]_.\n\n    Parameters\n    ----------\n    l1 : complex\n        A diagonal entry of the matrix.\n    l2 : complex\n        A diagonal entry of the matrix.\n    t12 : complex\n        A superdiagonal entry of the matrix.\n    p : float\n        A fractional power.\n\n    Returns\n    -------\n    f12 : complex\n        A superdiagonal entry of the fractional matrix power.\n\n    Notes\n    -----\n    Care has been taken to return a real number if possible when\n    all of the inputs are real numbers.\n\n    References\n    ----------\n    .. [1] Nicholas J. Higham and Lijing lin (2011)\n           "A Schur-Pade Algorithm for Fractional Powers of a Matrix."\n           SIAM Journal on Matrix Analysis and Applications,\n           32 (3). pp. 1056-1078. ISSN 0895-4798\n\n    '
if (l1 == l2):
    f12 = ((t12 * p) * (l1 ** (p - 1)))
elif (abs((l2 - l1)) > (abs((l1 + l2)) / 2)):
    f12 = ((t12 * ((l2 ** p) - (l1 ** p))) / (l2 - l1))
else:
    z = ((l2 - l1) / (l2 + l1))
    log_l1 = numpy.log(l1)
    tempResult = log(l2)
	
===================================================================	
_logm_superdiag_entry: 116	
----------------------------	

'\n    Compute a superdiagonal entry of a matrix logarithm.\n\n    This is like Eq. (11.28) in [1]_, except the determination of whether\n    l1 and l2 are sufficiently far apart has been modified.\n\n    Parameters\n    ----------\n    l1 : complex\n        A diagonal entry of the matrix.\n    l2 : complex\n        A diagonal entry of the matrix.\n    t12 : complex\n        A superdiagonal entry of the matrix.\n\n    Returns\n    -------\n    f12 : complex\n        A superdiagonal entry of the matrix logarithm.\n\n    Notes\n    -----\n    Care has been taken to return a real number if possible when\n    all of the inputs are real numbers.\n\n    References\n    ----------\n    .. [1] Nicholas J. Higham (2008)\n           "Functions of Matrices: Theory and Computation"\n           ISBN 978-0-898716-46-7\n\n    '
if (l1 == l2):
    f12 = (t12 / l1)
elif (abs((l2 - l1)) > (abs((l1 + l2)) / 2)):
    tempResult = log(l2)
	
===================================================================	
_logm_superdiag_entry: 116	
----------------------------	

'\n    Compute a superdiagonal entry of a matrix logarithm.\n\n    This is like Eq. (11.28) in [1]_, except the determination of whether\n    l1 and l2 are sufficiently far apart has been modified.\n\n    Parameters\n    ----------\n    l1 : complex\n        A diagonal entry of the matrix.\n    l2 : complex\n        A diagonal entry of the matrix.\n    t12 : complex\n        A superdiagonal entry of the matrix.\n\n    Returns\n    -------\n    f12 : complex\n        A superdiagonal entry of the matrix logarithm.\n\n    Notes\n    -----\n    Care has been taken to return a real number if possible when\n    all of the inputs are real numbers.\n\n    References\n    ----------\n    .. [1] Nicholas J. Higham (2008)\n           "Functions of Matrices: Theory and Computation"\n           ISBN 978-0-898716-46-7\n\n    '
if (l1 == l2):
    f12 = (t12 / l1)
elif (abs((l2 - l1)) > (abs((l1 + l2)) / 2)):
    tempResult = log(l1)
	
===================================================================	
_logm_superdiag_entry: 119	
----------------------------	

'\n    Compute a superdiagonal entry of a matrix logarithm.\n\n    This is like Eq. (11.28) in [1]_, except the determination of whether\n    l1 and l2 are sufficiently far apart has been modified.\n\n    Parameters\n    ----------\n    l1 : complex\n        A diagonal entry of the matrix.\n    l2 : complex\n        A diagonal entry of the matrix.\n    t12 : complex\n        A superdiagonal entry of the matrix.\n\n    Returns\n    -------\n    f12 : complex\n        A superdiagonal entry of the matrix logarithm.\n\n    Notes\n    -----\n    Care has been taken to return a real number if possible when\n    all of the inputs are real numbers.\n\n    References\n    ----------\n    .. [1] Nicholas J. Higham (2008)\n           "Functions of Matrices: Theory and Computation"\n           ISBN 978-0-898716-46-7\n\n    '
if (l1 == l2):
    f12 = (t12 / l1)
elif (abs((l2 - l1)) > (abs((l1 + l2)) / 2)):
    f12 = ((t12 * (numpy.log(l2) - numpy.log(l1))) / (l2 - l1))
else:
    z = ((l2 - l1) / (l2 + l1))
    tempResult = log(l2)
	
===================================================================	
_logm_superdiag_entry: 119	
----------------------------	

'\n    Compute a superdiagonal entry of a matrix logarithm.\n\n    This is like Eq. (11.28) in [1]_, except the determination of whether\n    l1 and l2 are sufficiently far apart has been modified.\n\n    Parameters\n    ----------\n    l1 : complex\n        A diagonal entry of the matrix.\n    l2 : complex\n        A diagonal entry of the matrix.\n    t12 : complex\n        A superdiagonal entry of the matrix.\n\n    Returns\n    -------\n    f12 : complex\n        A superdiagonal entry of the matrix logarithm.\n\n    Notes\n    -----\n    Care has been taken to return a real number if possible when\n    all of the inputs are real numbers.\n\n    References\n    ----------\n    .. [1] Nicholas J. Higham (2008)\n           "Functions of Matrices: Theory and Computation"\n           ISBN 978-0-898716-46-7\n\n    '
if (l1 == l2):
    f12 = (t12 / l1)
elif (abs((l2 - l1)) > (abs((l1 + l2)) / 2)):
    f12 = ((t12 * (numpy.log(l2) - numpy.log(l1))) / (l2 - l1))
else:
    z = ((l2 - l1) / (l2 + l1))
    tempResult = log(l1)
	
===================================================================	
_logm_triu: 350	
----------------------------	

'\n    Compute matrix logarithm of an upper triangular matrix.\n\n    The matrix logarithm is the inverse of\n    expm: expm(logm(`T`)) == `T`\n\n    Parameters\n    ----------\n    T : (N, N) array_like\n        Upper triangular matrix whose logarithm to evaluate\n\n    Returns\n    -------\n    logm : (N, N) ndarray\n        Matrix logarithm of `T`\n\n    References\n    ----------\n    .. [1] Awad H. Al-Mohy and Nicholas J. Higham (2012)\n           "Improved Inverse Scaling and Squaring Algorithms\n           for the Matrix Logarithm."\n           SIAM Journal on Scientific Computing, 34 (4). C152-C169.\n           ISSN 1095-7197\n\n    .. [2] Nicholas J. Higham (2008)\n           "Functions of Matrices: Theory and Computation"\n           ISBN 978-0-898716-46-7\n\n    .. [3] Nicholas J. Higham and Lijing lin (2011)\n           "A Schur-Pade Algorithm for Fractional Powers of a Matrix."\n           SIAM Journal on Matrix Analysis and Applications,\n           32 (3). pp. 1056-1078. ISSN 0895-4798\n\n    '
T = numpy.asarray(T)
if ((len(T.shape) != 2) or (T.shape[0] != T.shape[1])):
    raise ValueError('expected an upper triangular square matrix')
(n, n) = T.shape
T_diag = numpy.diag(T)
keep_it_real = (numpy.isrealobj(T) and (numpy.min(T_diag) >= 0))
if keep_it_real:
    T0 = T
else:
    T0 = T.astype(complex)
theta = (None, 1.59e-05, 0.00231, 0.0194, 0.0621, 0.128, 0.206, 0.288, 0.367, 0.439, 0.503, 0.56, 0.609, 0.652, 0.689, 0.721, 0.749)
(R, s, m) = _inverse_squaring_helper(T0, theta)
(nodes, weights) = scipy.special.p_roots(m)
nodes = nodes.real
if ((nodes.shape != (m,)) or (weights.shape != (m,))):
    raise Exception('internal error')
nodes = (0.5 + (0.5 * nodes))
weights = (0.5 * weights)
ident = numpy.identity(n)
U = numpy.zeros_like(R)
for (alpha, beta) in zip(weights, nodes):
    U += solve_triangular((ident + (beta * R)), (alpha * R))
U *= numpy.exp2(s)
has_principal_branch = all((((x.real > 0) or (x.imag != 0)) for x in numpy.diag(T0)))
if has_principal_branch:
    tempResult = log(numpy.diag(T0))
	
===================================================================	
TestFixedPoint.func111111: 488	
----------------------------	

tempResult = log(((kl / ks) / n))
	
===================================================================	
TestFixedPoint.func111111: 488	
----------------------------	

tempResult = log(((i0 * n) / (n - 1)))
	
===================================================================	
CheckOptimize.grad: 52	
----------------------------	

self.gradcalls += 1
log_pdot = numpy.dot(self.F, x)
tempResult = log(sum(numpy.exp(log_pdot)))
	
===================================================================	
CheckOptimize.func: 44	
----------------------------	

self.funccalls += 1
if (self.funccalls > 6000):
    raise RuntimeError('too many iterations in optimization routine')
log_pdot = numpy.dot(self.F, x)
tempResult = log(sum(numpy.exp(log_pdot)))
	
===================================================================	
CheckOptimize.hess: 58	
----------------------------	

log_pdot = numpy.dot(self.F, x)
tempResult = log(sum(numpy.exp(log_pdot)))
	
===================================================================	
TestApproxDerivativesDense.fun_vector_scalar: 95	
----------------------------	

tempResult = log(x[0])
	
===================================================================	
TestApproxDerivativesDense.jac_vector_scalar: 101	
----------------------------	

tempResult = log(x[0])
	
===================================================================	
TestApproxDerivativesDense.jac_vector_scalar: 101	
----------------------------	

tempResult = log(x[0])
	
===================================================================	
F_10: 148	
----------------------------	

tempResult = log((1 + x))
	
===================================================================	
minimum_phase: 230	
----------------------------	

'Convert a linear-phase FIR filter to minimum phase\n\n    Parameters\n    ----------\n    h : array\n        Linear-phase FIR filter coefficients.\n    method : {\'hilbert\', \'homomorphic\'}\n        The method to use:\n\n            \'homomorphic\' (default)\n                This method [4]_ [5]_ works best with filters with an\n                odd number of taps, and the resulting minimum phase filter\n                will have a magnitude response that approximates the square\n                root of the the original filter\'s magnitude response.\n\n            \'hilbert\'\n                This method [1]_ is designed to be used with equiripple\n                filters (e.g., from `remez`) with unity or zero gain\n                regions.\n\n    n_fft : int\n        The number of points to use for the FFT. Should be at least a\n        few times larger than the signal length (see Notes).\n\n    Returns\n    -------\n    h_minimum : array\n        The minimum-phase version of the filter, with length\n        ``(length(h) + 1) // 2``.\n\n    See Also\n    --------\n    firwin\n    firwin2\n    remez\n\n    Notes\n    -----\n    Both the Hilbert [1]_ or homomorphic [4]_ [5]_ methods require selection\n    of an FFT length to estimate the complex cepstrum of the filter.\n\n    In the case of the Hilbert method, the deviation from the ideal\n    spectrum ``epsilon`` is related to the number of stopband zeros\n    ``n_stop`` and FFT length ``n_fft`` as::\n\n        epsilon = 2. * n_stop / n_fft\n\n    For example, with 100 stopband zeros and a FFT length of 2048,\n    ``epsilon = 0.0976``. If we conservatively assume that the number of\n    stopband zeros is one less than the filter length, we can take the FFT\n    length to be the next power of 2 that satisfies ``epsilon=0.01`` as::\n\n        n_fft = 2 ** int(np.ceil(np.log2(2 * (len(h) - 1) / 0.01)))\n\n    This gives reasonable results for both the Hilbert and homomorphic\n    methods, and gives the value used when ``n_fft=None``.\n\n    Alternative implementations exist for creating minimum-phase filters,\n    including zero inversion [2]_ and spectral factorization [3]_ [4]_.\n    For more information, see:\n\n        http://dspguru.com/dsp/howtos/how-to-design-minimum-phase-fir-filters\n\n    Examples\n    --------\n    Create an optimal linear-phase filter, then convert it to minimum phase:\n\n    >>> from scipy.signal import remez, minimum_phase, freqz, group_delay\n    >>> import matplotlib.pyplot as plt\n    >>> freq = [0, 0.2, 0.3, 1.0]\n    >>> desired = [1, 0]\n    >>> h_linear = remez(151, freq, desired, Hz=2.)\n\n    Convert it to minimum phase:\n\n    >>> h_min_hom = minimum_phase(h_linear, method=\'homomorphic\')\n    >>> h_min_hil = minimum_phase(h_linear, method=\'hilbert\')\n\n    Compare the three filters:\n\n    >>> fig, axs = plt.subplots(4, figsize=(4, 8))\n    >>> for h, style, color in zip((h_linear, h_min_hom, h_min_hil),\n    ...                            (\'-\', \'-\', \'--\'), (\'k\', \'r\', \'c\')):\n    ...     w, H = freqz(h)\n    ...     w, gd = group_delay((h, 1))\n    ...     w /= np.pi\n    ...     axs[0].plot(h, color=color, linestyle=style)\n    ...     axs[1].plot(w, np.abs(H), color=color, linestyle=style)\n    ...     axs[2].plot(w, 20 * np.log10(np.abs(H)), color=color, linestyle=style)\n    ...     axs[3].plot(w, gd, color=color, linestyle=style)\n    >>> for ax in axs:\n    ...     ax.grid(True, color=\'0.5\')\n    ...     ax.fill_between(freq[1:3], *ax.get_ylim(), color=\'#ffeeaa\', zorder=1)\n    >>> axs[0].set(xlim=[0, len(h_linear) - 1], ylabel=\'Amplitude\', xlabel=\'Samples\')\n    >>> axs[1].legend([\'Linear\', \'Min-Hom\', \'Min-Hil\'], title=\'Phase\')\n    >>> for ax, ylim in zip(axs[1:], ([0, 1.1], [-150, 10], [-60, 60])):\n    ...     ax.set(xlim=[0, 1], ylim=ylim, xlabel=\'Frequency\')\n    >>> axs[1].set(ylabel=\'Magnitude\')\n    >>> axs[2].set(ylabel=\'Magnitude (dB)\')\n    >>> axs[3].set(ylabel=\'Group delay\')\n    >>> plt.tight_layout()\n\n    References\n    ----------\n    .. [1] N. Damera-Venkata and B. L. Evans, "Optimal design of real and\n           complex minimum phase digital FIR filters," Acoustics, Speech,\n           and Signal Processing, 1999. Proceedings., 1999 IEEE International\n           Conference on, Phoenix, AZ, 1999, pp. 1145-1148 vol.3.\n           doi: 10.1109/ICASSP.1999.756179\n    .. [2] X. Chen and T. W. Parks, "Design of optimal minimum phase FIR\n           filters by direct factorization," Signal Processing,\n           vol. 10, no. 4, pp. 369383, Jun. 1986.\n    .. [3] T. Saramaki, "Finite Impulse Response Filter Design," in\n           Handbook for Digital Signal Processing, chapter 4,\n           New York: Wiley-Interscience, 1993.\n    .. [4] J. S. Lim, Advanced Topics in Signal Processing.\n           Englewood Cliffs, N.J.: Prentice Hall, 1988.\n    .. [5] A. V. Oppenheim, R. W. Schafer, and J. R. Buck,\n           "Discrete-Time Signal Processing," 2nd edition.\n           Upper Saddle River, N.J.: Prentice Hall, 1999.\n    '
h = numpy.asarray(h)
if numpy.iscomplexobj(h):
    raise ValueError('Complex filters not supported')
if ((h.ndim != 1) or (h.size <= 2)):
    raise ValueError('h must be 1D and at least 2 samples long')
n_half = (len(h) // 2)
if (not numpy.allclose(h[(- n_half):][::(- 1)], h[:n_half])):
    warnings.warn('h does not appear to by symmetric, conversion may fail', RuntimeWarning)
if ((not isinstance(method, string_types)) or (method not in ('homomorphic', 'hilbert'))):
    raise ValueError(('method must be "homomorphic" or "hilbert", got %r' % (method,)))
if (n_fft is None):
    n_fft = (2 ** int(numpy.ceil(numpy.log2(((2 * (len(h) - 1)) / 0.01)))))
n_fft = int(n_fft)
if (n_fft < len(h)):
    raise ValueError(('n_fft must be at least len(h)==%s' % len(h)))
if (method == 'hilbert'):
    w = (numpy.arange(n_fft) * (((2 * numpy.pi) / n_fft) * n_half))
    H = numpy.real((fft(h, n_fft) * numpy.exp((1j * w))))
    dp = (max(H) - 1)
    ds = (0 - min(H))
    S = (4.0 / ((numpy.sqrt(((1 + dp) + ds)) + numpy.sqrt(((1 - dp) + ds))) ** 2))
    H += ds
    H *= S
    H = numpy.sqrt(H, out=H)
    H += 1e-10
    h_minimum = _dhtm(H)
else:
    h_temp = numpy.abs(fft(h, n_fft))
    h_temp += (1e-07 * h_temp[(h_temp > 0)].min())
    tempResult = log(h_temp, out=h_temp)
	
===================================================================	
_dhtm: 196	
----------------------------	

'Compute the modified 1D discrete Hilbert transform\n\n    Parameters\n    ----------\n    mag : ndarray\n        The magnitude spectrum. Should be 1D with an even length, and\n        preferably a fast length for FFT/IFFT.\n    '
sig = numpy.zeros(len(mag))
midpt = (len(mag) // 2)
sig[1:midpt] = 1
sig[(midpt + 1):] = (- 1)
tempResult = log(mag)
	
===================================================================	
test_filtfilt_gust: 1183	
----------------------------	

(z, p, k) = scipy.signal.ellip(3, 0.01, 120, 0.0875, output='zpk')
eps = 1e-10
r = numpy.max(numpy.abs(p))
tempResult = log(eps)
	
===================================================================	
test_filtfilt_gust: 1183	
----------------------------	

(z, p, k) = scipy.signal.ellip(3, 0.01, 120, 0.0875, output='zpk')
eps = 1e-10
r = numpy.max(numpy.abs(p))
tempResult = log(r)
	
===================================================================	
TestFiltFilt.test_sine: 1045	
----------------------------	

rate = 2000
t = numpy.linspace(0, 1.0, (rate + 1))
xlow = numpy.sin((((5 * 2) * numpy.pi) * t))
xhigh = numpy.sin((((250 * 2) * numpy.pi) * t))
x = (xlow + xhigh)
zpk = butter(8, 0.125, output='zpk')
r = np.abs(zpk[1]).max()
eps = 1e-05
tempResult = log(eps)
	
===================================================================	
TestFiltFilt.test_sine: 1045	
----------------------------	

rate = 2000
t = numpy.linspace(0, 1.0, (rate + 1))
xlow = numpy.sin((((5 * 2) * numpy.pi) * t))
xhigh = numpy.sin((((250 * 2) * numpy.pi) * t))
x = (xlow + xhigh)
zpk = butter(8, 0.125, output='zpk')
r = np.abs(zpk[1]).max()
eps = 1e-05
tempResult = log(r)
	
===================================================================	
count_neighbors_consistency.test_multiple_radius: 571	
----------------------------	

tempResult = log(0.01)
	
===================================================================	
count_neighbors_consistency.test_multiple_radius: 571	
----------------------------	

tempResult = log(10)
	
===================================================================	
multigammaln: 15	
----------------------------	

'Returns the log of multivariate gamma, also sometimes called the\n    generalized gamma.\n\n    Parameters\n    ----------\n    a : ndarray\n        The multivariate gamma is computed for each item of `a`.\n    d : int\n        The dimension of the space of integration.\n\n    Returns\n    -------\n    res : ndarray\n        The values of the log multivariate gamma at the given points `a`.\n\n    Notes\n    -----\n    The formal definition of the multivariate gamma of dimension d for a real\n    `a` is\n\n    .. math::\n\n        \\Gamma_d(a) = \\int_{A>0} e^{-tr(A)} |A|^{a - (d+1)/2} dA\n\n    with the condition :math:`a > (d-1)/2`, and :math:`A > 0` being the set of\n    all the positive definite matrices of dimension `d`.  Note that `a` is a\n    scalar: the integrand only is multivariate, the argument is not (the\n    function is defined over a subset of the real set).\n\n    This can be proven to be equal to the much friendlier equation\n\n    .. math::\n\n        \\Gamma_d(a) = \\pi^{d(d-1)/4} \\prod_{i=1}^{d} \\Gamma(a - (i-1)/2).\n\n    References\n    ----------\n    R. J. Muirhead, Aspects of multivariate statistical theory (Wiley Series in\n    probability and mathematical statistics).\n\n    '
a = numpy.asarray(a)
if ((not numpy.isscalar(d)) or (numpy.floor(d) != d)):
    raise ValueError('d should be a positive integer (dimension)')
if numpy.any((a <= (0.5 * (d - 1)))):
    raise ValueError(('condition a (%f) > 0.5 * (d-1) (%f) not met' % (a, (0.5 * (d - 1)))))
tempResult = log(numpy.pi)
	
===================================================================	
logsumexp: 30	
----------------------------	

'Compute the log of the sum of exponentials of input elements.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    axis : None or int or tuple of ints, optional\n        Axis or axes over which the sum is taken. By default `axis` is None,\n        and all elements are summed.\n\n        .. versionadded:: 0.11.0\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original array.\n\n        .. versionadded:: 0.15.0\n    b : array-like, optional\n        Scaling factor for exp(`a`) must be of the same shape as `a` or\n        broadcastable to `a`. These values may be negative in order to\n        implement subtraction.\n\n        .. versionadded:: 0.12.0\n    return_sign : bool, optional\n        If this is set to True, the result will be a pair containing sign\n        information; if False, results that are negative will be returned\n        as NaN. Default is False (no sign information).\n\n        .. versionadded:: 0.16.0\n    Returns\n    -------\n    res : ndarray\n        The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically\n        more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``\n        is returned.\n    sgn : ndarray\n        If return_sign is True, this will be an array of floating-point\n        numbers matching res and +1, 0, or -1 depending on the sign\n        of the result. If False, only one result is returned.\n\n    See Also\n    --------\n    numpy.logaddexp, numpy.logaddexp2\n\n    Notes\n    -----\n    Numpy has a logaddexp function which is very similar to `logsumexp`, but\n    only handles two arguments. `logaddexp.reduce` is similar to this\n    function, but may be less stable.\n\n    Examples\n    --------\n    >>> from scipy.special import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n\n    With weights\n\n    >>> a = np.arange(10)\n    >>> b = np.arange(10, 0, -1)\n    >>> logsumexp(a, b=b)\n    9.9170178533034665\n    >>> np.log(np.sum(b*np.exp(a)))\n    9.9170178533034647\n\n    Returning a sign flag\n\n    >>> logsumexp([1,2],b=[1,-1],return_sign=True)\n    (1.5413248546129181, -1.0)\n\n    Notice that `logsumexp` does not directly support masked arrays. To use it\n    on a masked array, convert the mask into zero weights:\n\n    >>> a = np.ma.array([np.log(2), 2, np.log(3)],\n    ...                  mask=[False, True, False])\n    >>> b = (~a.mask).astype(int)\n    >>> logsumexp(a.data, b=b), np.log(5)\n    1.6094379124341005, 1.6094379124341005\n\n    '
a = _asarray_validated(a, check_finite=False)
if (b is not None):
    (a, b) = numpy.broadcast_arrays(a, b)
    if numpy.any((b == 0)):
        a = (a + 0.0)
        a[(b == 0)] = (- numpy.inf)
a_max = numpy.amax(a, axis=axis, keepdims=True)
if (a_max.ndim > 0):
    a_max[(~ numpy.isfinite(a_max))] = 0
elif (not numpy.isfinite(a_max)):
    a_max = 0
if (b is not None):
    b = numpy.asarray(b)
    tmp = (b * numpy.exp((a - a_max)))
else:
    tmp = numpy.exp((a - a_max))
with numpy.errstate(divide='ignore'):
    s = numpy.sum(tmp, axis=axis, keepdims=keepdims)
    if return_sign:
        sgn = numpy.sign(s)
        s *= sgn
    tempResult = log(s)
	
===================================================================	
xfunc: 2577	
----------------------------	

if ((x == 0) and (not numpy.isnan(y))):
    return x
else:
    tempResult = log(y)
	
===================================================================	
TestCephes.test_expm1_complex_hard: 264	
----------------------------	

y = numpy.array([0.1, 0.2, 0.3, 5, 11, 20])
tempResult = log(numpy.cos(y))
	
===================================================================	
test_boxcox_underflow: 23	
----------------------------	

x = (1 + 1e-15)
lmbda = 1e-306
y = boxcox(x, lmbda)
tempResult = log(x)
	
===================================================================	
test_boxcox_basic: 10	
----------------------------	

x = numpy.array([0.5, 1, 2, 4])
y = boxcox(x, 0)
tempResult = log(x)
	
===================================================================	
test_identities2: 23	
----------------------------	

x = numpy.array([(- 99.5), (- 9.5), (- 0.5), 0.5, 9.5, 99.5])
y = x.copy()
(x, y) = numpy.meshgrid(x, y)
z = (x + (1j * y)).flatten()
tempResult = log(z)
	
===================================================================	
test_logsumexp_b: 36	
----------------------------	

a = numpy.arange(200)
b = numpy.arange(200, 0, (- 1))
tempResult = log(numpy.sum((b * numpy.exp(a))))
	
===================================================================	
test_logsumexp_b: 40	
----------------------------	

a = numpy.arange(200)
b = numpy.arange(200, 0, (- 1))
desired = numpy.log(numpy.sum((b * numpy.exp(a))))
assert_almost_equal(logsumexp(a, b=b), desired)
a = [1000, 1000]
b = [1.2, 1.2]
tempResult = log((2 * 1.2))
	
===================================================================	
test_logsumexp_b: 44	
----------------------------	

a = numpy.arange(200)
b = numpy.arange(200, 0, (- 1))
desired = numpy.log(numpy.sum((b * numpy.exp(a))))
assert_almost_equal(logsumexp(a, b=b), desired)
a = [1000, 1000]
b = [1.2, 1.2]
desired = (1000 + numpy.log((2 * 1.2)))
assert_almost_equal(logsumexp(a, b=b), desired)
x = numpy.array(([1e-40] * 100000))
b = numpy.linspace(1, 1000, 100000)
tempResult = log(x)
	
===================================================================	
test_logsumexp: 9	
----------------------------	

a = numpy.arange(200)
tempResult = log(numpy.sum(numpy.exp(a)))
	
===================================================================	
test_logsumexp: 12	
----------------------------	

a = numpy.arange(200)
desired = numpy.log(numpy.sum(numpy.exp(a)))
assert_almost_equal(logsumexp(a), desired)
b = [1000, 1000]
tempResult = log(2.0)
	
===================================================================	
test_logsumexp: 16	
----------------------------	

a = numpy.arange(200)
desired = numpy.log(numpy.sum(numpy.exp(a)))
assert_almost_equal(logsumexp(a), desired)
b = [1000, 1000]
desired = (1000.0 + numpy.log(2.0))
assert_almost_equal(logsumexp(b), desired)
n = 1000
b = (numpy.ones(n) * 10000)
tempResult = log(n)
	
===================================================================	
test_logsumexp: 19	
----------------------------	

a = numpy.arange(200)
desired = numpy.log(numpy.sum(numpy.exp(a)))
assert_almost_equal(logsumexp(a), desired)
b = [1000, 1000]
desired = (1000.0 + numpy.log(2.0))
assert_almost_equal(logsumexp(b), desired)
n = 1000
b = (numpy.ones(n) * 10000)
desired = (10000.0 + numpy.log(n))
assert_almost_equal(logsumexp(b), desired)
x = numpy.array(([1e-40] * 1000000))
tempResult = log(x)
	
===================================================================	
TestSystematic.test_exprel: 612	
----------------------------	

tempResult = log(np.finfo(np.double).max)
	
===================================================================	
TestSystematic.test_exprel: 612	
----------------------------	

tempResult = log(np.finfo(np.double).max)
	
===================================================================	
TestMultiGammaLn.test2: 17	
----------------------------	

a = numpy.array([2.5, 10.0])
result = multigammaln(a, 2)
tempResult = log(numpy.sqrt(numpy.pi))
	
===================================================================	
boxcox_llf: 200	
----------------------------	

'The boxcox log-likelihood function.\n\n    Parameters\n    ----------\n    lmb : scalar\n        Parameter for Box-Cox transformation.  See `boxcox` for details.\n    data : array_like\n        Data to calculate Box-Cox log-likelihood for.  If `data` is\n        multi-dimensional, the log-likelihood is calculated along the first\n        axis.\n\n    Returns\n    -------\n    llf : float or ndarray\n        Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,\n        an array otherwise.\n\n    See Also\n    --------\n    boxcox, probplot, boxcox_normplot, boxcox_normmax\n\n    Notes\n    -----\n    The Box-Cox log-likelihood function is defined here as\n\n    .. math::\n\n        llf = (\\lambda - 1) \\sum_i(\\log(x_i)) -\n              N/2 \\log(\\sum_i (y_i - \\bar{y})^2 / N),\n\n    where ``y`` is the Box-Cox transformed input data ``x``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n    >>> np.random.seed(1245)\n\n    Generate some random variates and calculate Box-Cox log-likelihood values\n    for them for a range of ``lmbda`` values:\n\n    >>> x = stats.loggamma.rvs(5, loc=10, size=1000)\n    >>> lmbdas = np.linspace(-2, 10)\n    >>> llf = np.zeros(lmbdas.shape, dtype=float)\n    >>> for ii, lmbda in enumerate(lmbdas):\n    ...     llf[ii] = stats.boxcox_llf(lmbda, x)\n\n    Also find the optimal lmbda value with `boxcox`:\n\n    >>> x_most_normal, lmbda_optimal = stats.boxcox(x)\n\n    Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n    horizontal line to check that that\'s really the optimum:\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(lmbdas, llf, \'b.-\')\n    >>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color=\'r\')\n    >>> ax.set_xlabel(\'lmbda parameter\')\n    >>> ax.set_ylabel(\'Box-Cox log-likelihood\')\n\n    Now add some probability plots to show that where the log-likelihood is\n    maximized the data transformed with `boxcox` looks closest to normal:\n\n    >>> locs = [3, 10, 4]  # \'lower left\', \'center\', \'lower right\'\n    >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n    ...     xt = stats.boxcox(x, lmbda=lmbda)\n    ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n    ...     ax_inset = inset_axes(ax, width="20%", height="20%", loc=loc)\n    ...     ax_inset.plot(osm, osr, \'c.\', osm, slope*osm + intercept, \'k-\')\n    ...     ax_inset.set_xticklabels([])\n    ...     ax_inset.set_yticklabels([])\n    ...     ax_inset.set_title(\'$\\lambda=%1.2f$\' % lmbda)\n\n    >>> plt.show()\n\n    '
data = numpy.asarray(data)
N = data.shape[0]
if (N == 0):
    return numpy.nan
y = boxcox(data, lmb)
y_mean = numpy.mean(y, axis=0)
tempResult = log(data)
	
===================================================================	
boxcox_llf: 201	
----------------------------	

'The boxcox log-likelihood function.\n\n    Parameters\n    ----------\n    lmb : scalar\n        Parameter for Box-Cox transformation.  See `boxcox` for details.\n    data : array_like\n        Data to calculate Box-Cox log-likelihood for.  If `data` is\n        multi-dimensional, the log-likelihood is calculated along the first\n        axis.\n\n    Returns\n    -------\n    llf : float or ndarray\n        Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,\n        an array otherwise.\n\n    See Also\n    --------\n    boxcox, probplot, boxcox_normplot, boxcox_normmax\n\n    Notes\n    -----\n    The Box-Cox log-likelihood function is defined here as\n\n    .. math::\n\n        llf = (\\lambda - 1) \\sum_i(\\log(x_i)) -\n              N/2 \\log(\\sum_i (y_i - \\bar{y})^2 / N),\n\n    where ``y`` is the Box-Cox transformed input data ``x``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n    >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n    >>> np.random.seed(1245)\n\n    Generate some random variates and calculate Box-Cox log-likelihood values\n    for them for a range of ``lmbda`` values:\n\n    >>> x = stats.loggamma.rvs(5, loc=10, size=1000)\n    >>> lmbdas = np.linspace(-2, 10)\n    >>> llf = np.zeros(lmbdas.shape, dtype=float)\n    >>> for ii, lmbda in enumerate(lmbdas):\n    ...     llf[ii] = stats.boxcox_llf(lmbda, x)\n\n    Also find the optimal lmbda value with `boxcox`:\n\n    >>> x_most_normal, lmbda_optimal = stats.boxcox(x)\n\n    Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n    horizontal line to check that that\'s really the optimum:\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(lmbdas, llf, \'b.-\')\n    >>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color=\'r\')\n    >>> ax.set_xlabel(\'lmbda parameter\')\n    >>> ax.set_ylabel(\'Box-Cox log-likelihood\')\n\n    Now add some probability plots to show that where the log-likelihood is\n    maximized the data transformed with `boxcox` looks closest to normal:\n\n    >>> locs = [3, 10, 4]  # \'lower left\', \'center\', \'lower right\'\n    >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n    ...     xt = stats.boxcox(x, lmbda=lmbda)\n    ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n    ...     ax_inset = inset_axes(ax, width="20%", height="20%", loc=loc)\n    ...     ax_inset.plot(osm, osr, \'c.\', osm, slope*osm + intercept, \'k-\')\n    ...     ax_inset.set_xticklabels([])\n    ...     ax_inset.set_yticklabels([])\n    ...     ax_inset.set_title(\'$\\lambda=%1.2f$\' % lmbda)\n\n    >>> plt.show()\n\n    '
data = numpy.asarray(data)
N = data.shape[0]
if (N == 0):
    return numpy.nan
y = boxcox(data, lmb)
y_mean = numpy.mean(y, axis=0)
llf = ((lmb - 1) * numpy.sum(numpy.log(data), axis=0))
tempResult = log(numpy.sum((((y - y_mean) ** 2.0) / N), axis=0))
	
===================================================================	
combine_pvalues: 1344	
----------------------------	

'\n    Methods for combining the p-values of independent tests bearing upon the\n    same hypothesis.\n\n    Parameters\n    ----------\n    pvalues : array_like, 1-D\n        Array of p-values assumed to come from independent tests.\n    method : {\'fisher\', \'stouffer\'}, optional\n        Name of method to use to combine p-values. The following methods are\n        available:\n\n        - "fisher": Fisher\'s method (Fisher\'s combined probability test),\n          the default.\n        - "stouffer": Stouffer\'s Z-score method.\n    weights : array_like, 1-D, optional\n        Optional array of weights used only for Stouffer\'s Z-score method.\n\n    Returns\n    -------\n    statistic: float\n        The statistic calculated by the specified method:\n        - "fisher": The chi-squared statistic\n        - "stouffer": The Z-score\n    pval: float\n        The combined p-value.\n\n    Notes\n    -----\n    Fisher\'s method (also known as Fisher\'s combined probability test) [1]_ uses\n    a chi-squared statistic to compute a combined p-value. The closely related\n    Stouffer\'s Z-score method [2]_ uses Z-scores rather than p-values. The\n    advantage of Stouffer\'s method is that it is straightforward to introduce\n    weights, which can make Stouffer\'s method more powerful than Fisher\'s\n    method when the p-values are from studies of different size [3]_ [4]_.\n\n    Fisher\'s method may be extended to combine p-values from dependent tests\n    [5]_. Extensions such as Brown\'s method and Kost\'s method are not currently\n    implemented.\n\n    .. versionadded:: 0.15.0\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Fisher%27s_method\n    .. [2] http://en.wikipedia.org/wiki/Fisher\'s_method#Relation_to_Stouffer.27s_Z-score_method\n    .. [3] Whitlock, M. C. "Combining probability from independent tests: the\n           weighted Z-method is superior to Fisher\'s approach." Journal of\n           Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n    .. [4] Zaykin, Dmitri V. "Optimally weighted Z-test is a powerful method\n           for combining probabilities in meta-analysis." Journal of\n           Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n    .. [5] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n\n    '
pvalues = numpy.asarray(pvalues)
if (pvalues.ndim != 1):
    raise ValueError('pvalues is not 1-D')
if (method == 'fisher'):
    tempResult = log(pvalues)
	
===================================================================	
skewtest: 343	
----------------------------	

'\n    Tests whether the skew is different from the normal distribution.\n\n    This function tests the null hypothesis that the skewness of\n    the population that the sample was drawn from is the same\n    as that of a corresponding normal distribution.\n\n    Parameters\n    ----------\n    a : array\n        The data to be tested\n    axis : int or None, optional\n       Axis along which statistics are calculated. Default is 0.\n       If None, compute over the whole array `a`.\n    nan_policy : {\'propagate\', \'raise\', \'omit\'}, optional\n        Defines how to handle when input contains nan. \'propagate\' returns nan,\n        \'raise\' throws an error, \'omit\' performs the calculations ignoring nan\n        values. Default is \'propagate\'.\n\n    Returns\n    -------\n    statistic : float\n        The computed z-score for this test.\n    pvalue : float\n        a 2-sided p-value for the hypothesis test\n\n    Notes\n    -----\n    The sample size must be at least 8.\n\n    References\n    ----------\n    .. [1] R. B. D\'Agostino, A. J. Belanger and R. B. D\'Agostino Jr.,\n            "A suggestion for using powerful and informative tests of \n            normality", American Statistician 44, pp. 316-321, 1990.\n\n    '
(a, axis) = _chk_asarray(a, axis)
(contains_nan, nan_policy) = _contains_nan(a, nan_policy)
if (contains_nan and (nan_policy == 'omit')):
    a = numpy.ma.masked_invalid(a)
    return mstats_basic.skewtest(a, axis)
if (axis is None):
    a = numpy.ravel(a)
    axis = 0
b2 = skew(a, axis)
n = float(a.shape[axis])
if (n < 8):
    raise ValueError(('skewtest is not valid with less than 8 samples; %i samples were given.' % int(n)))
y = (b2 * math.sqrt((((n + 1) * (n + 3)) / (6.0 * (n - 2)))))
beta2 = ((((3.0 * (((n ** 2) + (27 * n)) - 70)) * (n + 1)) * (n + 3)) / ((((n - 2.0) * (n + 5)) * (n + 7)) * (n + 9)))
W2 = ((- 1) + math.sqrt((2 * (beta2 - 1))))
delta = (1 / math.sqrt((0.5 * math.log(W2))))
alpha = math.sqrt((2.0 / (W2 - 1)))
y = numpy.where((y == 0), 1, y)
tempResult = log(((y / alpha) + numpy.sqrt((((y / alpha) ** 2) + 1))))
	
===================================================================	
gmean: 64	
----------------------------	

'\n    Compute the geometric mean along the specified axis.\n\n    Returns the geometric average of the array elements.\n    That is:  n-th root of (x1 * x2 * ... * xn)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the geometric mean is computed. Default is 0.\n        If None, compute over the whole array `a`.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed. If dtype is not specified, it defaults to the\n        dtype of a, unless a has an integer dtype with a precision less than\n        that of the default platform integer. In that case, the default\n        platform integer is used.\n\n    Returns\n    -------\n    gmean : ndarray\n        see dtype parameter above\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.average : Weighted average\n    hmean : Harmonic mean\n\n    Notes\n    -----\n    The geometric average is computed over a single dimension of the input\n    array, axis=0 by default, or all values in the array if axis=None.\n    float64 intermediate and return values are used for integer inputs.\n\n    Use masked arrays to ignore any non-finite values in the input or that\n    arise in the calculations such as Not a Number and infinity because masked\n    arrays automatically mask any non-finite values.\n\n    '
if (not isinstance(a, numpy.ndarray)):
    tempResult = log(numpy.array(a, dtype=dtype))
	
===================================================================	
gmean: 67	
----------------------------	

'\n    Compute the geometric mean along the specified axis.\n\n    Returns the geometric average of the array elements.\n    That is:  n-th root of (x1 * x2 * ... * xn)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the geometric mean is computed. Default is 0.\n        If None, compute over the whole array `a`.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed. If dtype is not specified, it defaults to the\n        dtype of a, unless a has an integer dtype with a precision less than\n        that of the default platform integer. In that case, the default\n        platform integer is used.\n\n    Returns\n    -------\n    gmean : ndarray\n        see dtype parameter above\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.average : Weighted average\n    hmean : Harmonic mean\n\n    Notes\n    -----\n    The geometric average is computed over a single dimension of the input\n    array, axis=0 by default, or all values in the array if axis=None.\n    float64 intermediate and return values are used for integer inputs.\n\n    Use masked arrays to ignore any non-finite values in the input or that\n    arise in the calculations such as Not a Number and infinity because masked\n    arrays automatically mask any non-finite values.\n\n    '
if (not isinstance(a, numpy.ndarray)):
    log_a = numpy.log(numpy.array(a, dtype=dtype))
elif dtype:
    if isinstance(a, numpy.ma.MaskedArray):
        tempResult = log(numpy.ma.asarray(a, dtype=dtype))
	
===================================================================	
gmean: 69	
----------------------------	

'\n    Compute the geometric mean along the specified axis.\n\n    Returns the geometric average of the array elements.\n    That is:  n-th root of (x1 * x2 * ... * xn)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the geometric mean is computed. Default is 0.\n        If None, compute over the whole array `a`.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed. If dtype is not specified, it defaults to the\n        dtype of a, unless a has an integer dtype with a precision less than\n        that of the default platform integer. In that case, the default\n        platform integer is used.\n\n    Returns\n    -------\n    gmean : ndarray\n        see dtype parameter above\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.average : Weighted average\n    hmean : Harmonic mean\n\n    Notes\n    -----\n    The geometric average is computed over a single dimension of the input\n    array, axis=0 by default, or all values in the array if axis=None.\n    float64 intermediate and return values are used for integer inputs.\n\n    Use masked arrays to ignore any non-finite values in the input or that\n    arise in the calculations such as Not a Number and infinity because masked\n    arrays automatically mask any non-finite values.\n\n    '
if (not isinstance(a, numpy.ndarray)):
    log_a = numpy.log(numpy.array(a, dtype=dtype))
elif dtype:
    if isinstance(a, numpy.ma.MaskedArray):
        log_a = numpy.log(numpy.ma.asarray(a, dtype=dtype))
    else:
        tempResult = log(numpy.asarray(a, dtype=dtype))
	
===================================================================	
gmean: 71	
----------------------------	

'\n    Compute the geometric mean along the specified axis.\n\n    Returns the geometric average of the array elements.\n    That is:  n-th root of (x1 * x2 * ... * xn)\n\n    Parameters\n    ----------\n    a : array_like\n        Input array or object that can be converted to an array.\n    axis : int or None, optional\n        Axis along which the geometric mean is computed. Default is 0.\n        If None, compute over the whole array `a`.\n    dtype : dtype, optional\n        Type of the returned array and of the accumulator in which the\n        elements are summed. If dtype is not specified, it defaults to the\n        dtype of a, unless a has an integer dtype with a precision less than\n        that of the default platform integer. In that case, the default\n        platform integer is used.\n\n    Returns\n    -------\n    gmean : ndarray\n        see dtype parameter above\n\n    See Also\n    --------\n    numpy.mean : Arithmetic average\n    numpy.average : Weighted average\n    hmean : Harmonic mean\n\n    Notes\n    -----\n    The geometric average is computed over a single dimension of the input\n    array, axis=0 by default, or all values in the array if axis=None.\n    float64 intermediate and return values are used for integer inputs.\n\n    Use masked arrays to ignore any non-finite values in the input or that\n    arise in the calculations such as Not a Number and infinity because masked\n    arrays automatically mask any non-finite values.\n\n    '
if (not isinstance(a, numpy.ndarray)):
    log_a = numpy.log(numpy.array(a, dtype=dtype))
elif dtype:
    if isinstance(a, numpy.ma.MaskedArray):
        log_a = numpy.log(numpy.ma.asarray(a, dtype=dtype))
    else:
        log_a = numpy.log(numpy.asarray(a, dtype=dtype))
else:
    tempResult = log(a)
	
===================================================================	
module: 7	
----------------------------	

'\nStatistics-related constants.\n\n'
from __future__ import division, print_function, absolute_import
import numpy as np
_EPS = np.finfo(float).eps
_XMAX = np.finfo(float).max
tempResult = log(_XMAX)
	
===================================================================	
module: 38	
----------------------------	

from __future__ import division, print_function, absolute_import
import warnings
import numpy as np
from scipy.misc.doccer import inherit_docstring_from
from scipy import optimize
from scipy import integrate
import scipy.special as sc
from scipy._lib._numpy_compat import broadcast_to
from . import _stats
from ._tukeylambda_stats import tukeylambda_variance as _tlvar, tukeylambda_kurtosis as _tlkurt
from ._distn_infrastructure import get_distribution_names, _kurtosis, _lazyselect, _lazywhere, _ncx2_cdf, _ncx2_log_pdf, _ncx2_pdf, rv_continuous, _skew, valarray
from ._constants import _XMIN, _EULER, _ZETA3, _XMAX, _LOGXMAX

class ksone_gen(rv_continuous):
    'General Kolmogorov-Smirnov one-sided test.\n\n    %(default)s\n\n    '

    def _cdf(self, x, n):
        return (1.0 - scipy.special.smirnov(n, x))

    def _ppf(self, q, n):
        return scipy.special.smirnovi(n, (1.0 - q))
ksone = ksone_gen(a=0.0, name='ksone')

class kstwobign_gen(rv_continuous):
    'Kolmogorov-Smirnov two-sided test for large N.\n\n    %(default)s\n\n    '

    def _cdf(self, x):
        return (1.0 - scipy.special.kolmogorov(x))

    def _sf(self, x):
        return scipy.special.kolmogorov(x)

    def _ppf(self, q):
        return scipy.special.kolmogi((1.0 - q))
kstwobign = kstwobign_gen(a=0.0, name='kstwobign')
_norm_pdf_C = numpy.sqrt((2 * numpy.pi))
tempResult = log(_norm_pdf_C)
	
===================================================================	
gennorm_gen._logpdf: 2730	
----------------------------	

tempResult = log((0.5 * beta))
	
===================================================================	
frechet_r_gen._logpdf: 830	
----------------------------	

tempResult = log(c)
	
===================================================================	
norm_gen._entropy: 98	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
loggamma_gen._rvs: 1657	
----------------------------	

tempResult = log(self._random_state.gamma(c, size=self._size))
	
===================================================================	
exponnorm_gen._logpdf: 644	
----------------------------	

invK = (1.0 / K)
exparg = ((0.5 * (invK ** 2)) - (invK * x))
tempResult = log(((0.5 * invK) * scipy.special.erfc(((- (x - invK)) / numpy.sqrt(2)))))
	
===================================================================	
halfnorm_gen._entropy: 1364	
----------------------------	

tempResult = log((numpy.pi / 2.0))
	
===================================================================	
lognorm_gen._logcdf: 1717	
----------------------------	

tempResult = log(x)
	
===================================================================	
johnsonsb_gen._cdf: 1498	
----------------------------	

tempResult = log((x / (1.0 - x)))
	
===================================================================	
gompertz_gen._entropy: 1231	
----------------------------	

tempResult = log(c)
	
===================================================================	
gumbel_l_gen._ppf: 1272	
----------------------------	

tempResult = log((- scipy.special.log1p((- q))))
	
===================================================================	
_lognorm_logpdf: 1698	
----------------------------	

tempResult = log(x)
	
===================================================================	
_lognorm_logpdf: 1698	
----------------------------	

tempResult = log(((s * x) * numpy.sqrt((2 * numpy.pi))))
	
===================================================================	
lognorm_gen._sf: 1723	
----------------------------	

tempResult = log(x)
	
===================================================================	
laplace_gen._entropy: 1541	
----------------------------	

tempResult = log(2)
	
===================================================================	
rayleigh_gen._isf: 2348	
----------------------------	

tempResult = log(q)
	
===================================================================	
powerlognorm_gen._cdf: 2282	
----------------------------	

tempResult = log(x)
	
===================================================================	
halfcauchy_gen._entropy: 1309	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
vonmises_gen._entropy: 2660	
----------------------------	

tempResult = log(((2 * numpy.pi) * scipy.special.i0(kappa)))
	
===================================================================	
frechet_r_gen._entropy: 848	
----------------------------	

tempResult = log(c)
	
===================================================================	
gennorm_gen._entropy: 2751	
----------------------------	

tempResult = log((0.5 * beta))
	
===================================================================	
johnsonsb_gen._pdf: 1494	
----------------------------	

tempResult = log((x / (1.0 - x)))
	
===================================================================	
fisk_gen._entropy: 416	
----------------------------	

tempResult = log(c)
	
===================================================================	
maxwell_gen._entropy: 1791	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
hypsecant_gen._entropy: 1383	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
wrapcauchy_gen._entropy: 2720	
----------------------------	

tempResult = log(((2 * numpy.pi) * (1 - (c * c))))
	
===================================================================	
invweibull_gen._ppf: 1477	
----------------------------	

tempResult = log(q)
	
===================================================================	
lomax_gen._entropy: 2179	
----------------------------	

tempResult = log(c)
	
===================================================================	
genextreme_gen._isf: 1007	
----------------------------	

tempResult = log((- scipy.special.log1p((- q))))
	
===================================================================	
gengamma_gen._logpdf: 1156	
----------------------------	

tempResult = log(abs(c))
	
===================================================================	
exponpow_gen._isf: 702	
----------------------------	

tempResult = log(x)
	
===================================================================	
halflogistic_gen._logpdf: 1319	
----------------------------	

tempResult = log(2)
	
===================================================================	
reciprocal_gen._cdf: 2374	
----------------------------	

tempResult = log(x)
	
===================================================================	
reciprocal_gen._cdf: 2374	
----------------------------	

tempResult = log(a)
	
===================================================================	
invweibull_gen._entropy: 1483	
----------------------------	

tempResult = log(c)
	
===================================================================	
gilbrat_gen._entropy: 1768	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
bradford_gen._entropy: 348	
----------------------------	

tempResult = log((1 + c))
	
===================================================================	
bradford_gen._entropy: 349	
----------------------------	

k = numpy.log((1 + c))
tempResult = log((c / k))
	
===================================================================	
dgamma_gen._logpdf: 540	
----------------------------	

ax = abs(x)
tempResult = log(2)
	
===================================================================	
chi2_gen._logpdf: 487	
----------------------------	

tempResult = log(2)
	
===================================================================	
triang_gen._entropy: 2523	
----------------------------	

tempResult = log(2)
	
===================================================================	
halflogistic_gen._munp: 1329	
----------------------------	

if (n == 1):
    tempResult = log(2)
	
===================================================================	
gilbrat_gen._cdf: 1754	
----------------------------	

tempResult = log(x)
	
===================================================================	
lognorm_gen._logsf: 1726	
----------------------------	

tempResult = log(x)
	
===================================================================	
gengamma_gen._entropy: 1185	
----------------------------	

val = scipy.special.psi(a)
tempResult = log(abs(c))
	
===================================================================	
reciprocal_gen._entropy: 2383	
----------------------------	

tempResult = log((a * b))
	
===================================================================	
reciprocal_gen._entropy: 2383	
----------------------------	

tempResult = log(numpy.log((b / a)))
	
===================================================================	
reciprocal_gen._entropy: 2383	
----------------------------	

tempResult = log((b / a))
	
===================================================================	
genlogistic_gen._logpdf: 893	
----------------------------	

tempResult = log(c)
	
===================================================================	
powerlaw_gen._logpdf: 2256	
----------------------------	

tempResult = log(a)
	
===================================================================	
frechet_l_gen._ppf: 871	
----------------------------	

tempResult = log(q)
	
===================================================================	
tukeylambda_gen.integ: 2618	
----------------------------	

tempResult = log((pow(p, (lam - 1)) + pow((1 - p), (lam - 1))))
	
===================================================================	
genextreme_gen._ppf: 1003	
----------------------------	

tempResult = log((- numpy.log(q)))
	
===================================================================	
genextreme_gen._ppf: 1003	
----------------------------	

tempResult = log(q)
	
===================================================================	
halfnorm_gen._logpdf: 1352	
----------------------------	

tempResult = log((2.0 / numpy.pi))
	
===================================================================	
cauchy_gen._entropy: 441	
----------------------------	

tempResult = log((4 * numpy.pi))
	
===================================================================	
powerlaw_gen._logcdf: 2262	
----------------------------	

tempResult = log(x)
	
===================================================================	
rayleigh_gen._entropy: 2355	
----------------------------	

tempResult = log(2)
	
===================================================================	
lognorm_gen._entropy: 1737	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
lognorm_gen._entropy: 1737	
----------------------------	

tempResult = log(s)
	
===================================================================	
dweibull_gen._ppf: 583	
----------------------------	

fac = (2.0 * numpy.where((q <= 0.5), q, (1.0 - q)))
tempResult = log(fac)
	
===================================================================	
exponpow_gen._logpdf: 692	
----------------------------	

xb = (x ** b)
tempResult = log(b)
	
===================================================================	
dweibull_gen._logpdf: 575	
----------------------------	

ax = abs(x)
tempResult = log(c)
	
===================================================================	
dweibull_gen._logpdf: 575	
----------------------------	

ax = abs(x)
tempResult = log(2.0)
	
===================================================================	
exponweib_gen._logpdf: 673	
----------------------------	

negxc = (- (x ** c))
exm1c = (- scipy.special.expm1(negxc))
tempResult = log(a)
	
===================================================================	
exponweib_gen._logpdf: 673	
----------------------------	

negxc = (- (x ** c))
exm1c = (- scipy.special.expm1(negxc))
tempResult = log(c)
	
===================================================================	
powerlaw_gen._entropy: 2271	
----------------------------	

tempResult = log(a)
	
===================================================================	
fatiguelife_gen._logpdf: 723	
----------------------------	

tempResult = log((x + 1))
	
===================================================================	
fatiguelife_gen._logpdf: 723	
----------------------------	

tempResult = log((2 * c))
	
===================================================================	
fatiguelife_gen._logpdf: 723	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
fatiguelife_gen._logpdf: 723	
----------------------------	

tempResult = log(x)
	
===================================================================	
hypsecant_gen._ppf: 1377	
----------------------------	

tempResult = log(numpy.tan(((numpy.pi * q) / 2.0)))
	
===================================================================	
halfgennorm_gen._entropy: 2776	
----------------------------	

tempResult = log(beta)
	
===================================================================	
rayleigh_gen._logpdf: 2333	
----------------------------	

tempResult = log(r)
	
===================================================================	
nct_gen._pdf: 2069	
----------------------------	

n = (df * 1.0)
nc = (nc * 1.0)
x2 = (x * x)
ncx2 = ((nc * nc) * x2)
fac1 = (n + x2)
tempResult = log(n)
	
===================================================================	
nct_gen._pdf: 2070	
----------------------------	

n = (df * 1.0)
nc = (nc * 1.0)
x2 = (x * x)
ncx2 = ((nc * nc) * x2)
fac1 = (n + x2)
trm1 = (((n / 2.0) * numpy.log(n)) + scipy.special.gammaln((n + 1)))
tempResult = log(2)
	
===================================================================	
nct_gen._pdf: 2070	
----------------------------	

n = (df * 1.0)
nc = (nc * 1.0)
x2 = (x * x)
ncx2 = ((nc * nc) * x2)
fac1 = (n + x2)
trm1 = (((n / 2.0) * numpy.log(n)) + scipy.special.gammaln((n + 1)))
tempResult = log(fac1)
	
===================================================================	
lognorm_gen._cdf: 1714	
----------------------------	

tempResult = log(x)
	
===================================================================	
invgamma_gen._logpdf: 1411	
----------------------------	

tempResult = log(x)
	
===================================================================	
powernorm_gen._logpdf: 2295	
----------------------------	

tempResult = log(c)
	
===================================================================	
levy_stable_gen.alpha1func: 1588	
----------------------------	

tempResult = log(((((numpy.pi / 2) * W) * cosTH) / ((numpy.pi / 2) + bTH)))
	
===================================================================	
halflogistic_gen._entropy: 1339	
----------------------------	

tempResult = log(2)
	
===================================================================	
gumbel_r_gen._ppf: 1250	
----------------------------	

tempResult = log((- numpy.log(q)))
	
===================================================================	
gumbel_r_gen._ppf: 1250	
----------------------------	

tempResult = log(q)
	
===================================================================	
halfgennorm_gen._logpdf: 2761	
----------------------------	

tempResult = log(beta)
	
===================================================================	
burr12_gen._logpdf: 378	
----------------------------	

tempResult = log(c)
	
===================================================================	
burr12_gen._logpdf: 378	
----------------------------	

tempResult = log(d)
	
===================================================================	
gamma_gen.fit: 1112	
----------------------------	

f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))
floc = kwds.get('floc', None)
fscale = kwds.get('fscale', None)
if (floc is None):
    return super(gamma_gen, self).fit(data, *args, **kwds)
if ((f0 is not None) and (fscale is not None)):
    raise ValueError('All parameters fixed. There is nothing to optimize.')
data = numpy.asarray(data)
if numpy.any((data <= floc)):
    raise FitDataError('gamma', lower=floc, upper=numpy.inf)
if (floc != 0):
    data = (data - floc)
xbar = data.mean()
if (fscale is None):
    if (f0 is not None):
        a = f0
    else:
        tempResult = log(xbar)
	
===================================================================	
gamma_gen.fit: 1112	
----------------------------	

f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))
floc = kwds.get('floc', None)
fscale = kwds.get('fscale', None)
if (floc is None):
    return super(gamma_gen, self).fit(data, *args, **kwds)
if ((f0 is not None) and (fscale is not None)):
    raise ValueError('All parameters fixed. There is nothing to optimize.')
data = numpy.asarray(data)
if numpy.any((data <= floc)):
    raise FitDataError('gamma', lower=floc, upper=numpy.inf)
if (floc != 0):
    data = (data - floc)
xbar = data.mean()
if (fscale is None):
    if (f0 is not None):
        a = f0
    else:
        tempResult = log(data)
	
===================================================================	
gamma_gen.fit: 1113	
----------------------------	

f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))
floc = kwds.get('floc', None)
fscale = kwds.get('fscale', None)
if (floc is None):
    return super(gamma_gen, self).fit(data, *args, **kwds)
if ((f0 is not None) and (fscale is not None)):
    raise ValueError('All parameters fixed. There is nothing to optimize.')
data = numpy.asarray(data)
if numpy.any((data <= floc)):
    raise FitDataError('gamma', lower=floc, upper=numpy.inf)
if (floc != 0):
    data = (data - floc)
xbar = data.mean()
if (fscale is None):
    if (f0 is not None):
        a = f0
    else:
        s = (numpy.log(xbar) - np.log(data).mean())
        tempResult = log(a)
	
===================================================================	
gamma_gen.fit: 1120	
----------------------------	

f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))
floc = kwds.get('floc', None)
fscale = kwds.get('fscale', None)
if (floc is None):
    return super(gamma_gen, self).fit(data, *args, **kwds)
if ((f0 is not None) and (fscale is not None)):
    raise ValueError('All parameters fixed. There is nothing to optimize.')
data = numpy.asarray(data)
if numpy.any((data <= floc)):
    raise FitDataError('gamma', lower=floc, upper=numpy.inf)
if (floc != 0):
    data = (data - floc)
xbar = data.mean()
if (fscale is None):
    if (f0 is not None):
        a = f0
    else:
        s = (numpy.log(xbar) - np.log(data).mean())
        func = (lambda a: ((numpy.log(a) - scipy.special.digamma(a)) - s))
        aest = (((3 - s) + numpy.sqrt((((s - 3) ** 2) + (24 * s)))) / (12 * s))
        xa = (aest * (1 - 0.4))
        xb = (aest * (1 + 0.4))
        a = scipy.optimize.brentq(func, xa, xb, disp=0)
    scale = (xbar / a)
else:
    tempResult = log(data)
	
===================================================================	
gamma_gen.fit: 1120	
----------------------------	

f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))
floc = kwds.get('floc', None)
fscale = kwds.get('fscale', None)
if (floc is None):
    return super(gamma_gen, self).fit(data, *args, **kwds)
if ((f0 is not None) and (fscale is not None)):
    raise ValueError('All parameters fixed. There is nothing to optimize.')
data = numpy.asarray(data)
if numpy.any((data <= floc)):
    raise FitDataError('gamma', lower=floc, upper=numpy.inf)
if (floc != 0):
    data = (data - floc)
xbar = data.mean()
if (fscale is None):
    if (f0 is not None):
        a = f0
    else:
        s = (numpy.log(xbar) - np.log(data).mean())
        func = (lambda a: ((numpy.log(a) - scipy.special.digamma(a)) - s))
        aest = (((3 - s) + numpy.sqrt((((s - 3) ** 2) + (24 * s)))) / (12 * s))
        xa = (aest * (1 - 0.4))
        xb = (aest * (1 + 0.4))
        a = scipy.optimize.brentq(func, xa, xb, disp=0)
    scale = (xbar / a)
else:
    tempResult = log(fscale)
	
===================================================================	
gompertz_gen._logpdf: 1222	
----------------------------	

tempResult = log(c)
	
===================================================================	
genhalflogistic_gen._entropy: 1212	
----------------------------	

tempResult = log(2)
	
===================================================================	
kappa4_gen.f3111: 1899	
----------------------------	

tempResult = log((- numpy.log(q)))
	
===================================================================	
kappa4_gen.f3111: 1899	
----------------------------	

tempResult = log(q)
	
===================================================================	
recipinvgauss_gen._logpdf: 2419	
----------------------------	

tempResult = log(((2 * numpy.pi) * x))
	
===================================================================	
alpha_gen._logpdf: 127	
----------------------------	

tempResult = log(x)
	
===================================================================	
alpha_gen._logpdf: 127	
----------------------------	

tempResult = log(_norm_cdf(a))
	
===================================================================	
genexpon_gen._logpdf: 966	
----------------------------	

tempResult = log((a + (b * (- scipy.special.expm1(((- c) * x))))))
	
===================================================================	
kappa4_gen.f1: 1818	
----------------------------	

tempResult = log(h)
	
===================================================================	
johnsonsu_gen._cdf: 1516	
----------------------------	

tempResult = log((x + numpy.sqrt(((x * x) + 1))))
	
===================================================================	
t_gen._logpdf: 2029	
----------------------------	

r = (df * 1.0)
lPx = (scipy.special.gammaln(((r + 1) / 2)) - scipy.special.gammaln((r / 2)))
tempResult = log((r * numpy.pi))
	
===================================================================	
t_gen._logpdf: 2029	
----------------------------	

r = (df * 1.0)
lPx = (scipy.special.gammaln(((r + 1) / 2)) - scipy.special.gammaln((r / 2)))
tempResult = log((1 + ((x ** 2) / r)))
	
===================================================================	
johnsonsu_gen._pdf: 1512	
----------------------------	

x2 = (x * x)
tempResult = log((x + numpy.sqrt((x2 + 1))))
	
===================================================================	
lomax_gen._logpdf: 2160	
----------------------------	

tempResult = log(c)
	
===================================================================	
pearson3_gen._logpdf: 2221	
----------------------------	

(ans, x, transx, mask, invmask, beta, alpha, _) = self._preprocess(x, skew)
tempResult = log(_norm_pdf(x[mask]))
	
===================================================================	
pearson3_gen._logpdf: 2222	
----------------------------	

(ans, x, transx, mask, invmask, beta, alpha, _) = self._preprocess(x, skew)
ans[mask] = numpy.log(_norm_pdf(x[mask]))
tempResult = log(abs(beta))
	
===================================================================	
kappa4_gen.f11111: 1892	
----------------------------	

tempResult = log(q)
	
===================================================================	
gumbel_l_gen._isf: 1281	
----------------------------	

tempResult = log((- numpy.log(x)))
	
===================================================================	
gumbel_l_gen._isf: 1281	
----------------------------	

tempResult = log(x)
	
===================================================================	
beta_gen.fit: 270	
----------------------------	

'%(super)s\n        In the special case where both `floc` and `fscale` are given, a\n        `ValueError` is raised if any value `x` in `data` does not satisfy\n        `floc < x < floc + fscale`.\n        '
f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))
f1 = (kwds.get('f1', None) or kwds.get('fb', None) or kwds.get('fix_b', None))
floc = kwds.get('floc', None)
fscale = kwds.get('fscale', None)
if ((floc is None) or (fscale is None)):
    return super(beta_gen, self).fit(data, *args, **kwds)
if ((f0 is not None) and (f1 is not None)):
    raise ValueError('All parameters fixed. There is nothing to optimize.')
data = ((numpy.ravel(data) - floc) / fscale)
if (numpy.any((data <= 0)) or numpy.any((data >= 1))):
    raise FitDataError('beta', lower=floc, upper=(floc + fscale))
xbar = data.mean()
if ((f0 is not None) or (f1 is not None)):
    if (f0 is not None):
        b = f0
        data = (1 - data)
        xbar = (1 - xbar)
    else:
        b = f1
    a = ((b * xbar) / (1 - xbar))
    tempResult = log(data)
	
===================================================================	
beta_gen.fit: 277	
----------------------------	

'%(super)s\n        In the special case where both `floc` and `fscale` are given, a\n        `ValueError` is raised if any value `x` in `data` does not satisfy\n        `floc < x < floc + fscale`.\n        '
f0 = (kwds.get('f0', None) or kwds.get('fa', None) or kwds.get('fix_a', None))
f1 = (kwds.get('f1', None) or kwds.get('fb', None) or kwds.get('fix_b', None))
floc = kwds.get('floc', None)
fscale = kwds.get('fscale', None)
if ((floc is None) or (fscale is None)):
    return super(beta_gen, self).fit(data, *args, **kwds)
if ((f0 is not None) and (f1 is not None)):
    raise ValueError('All parameters fixed. There is nothing to optimize.')
data = ((numpy.ravel(data) - floc) / fscale)
if (numpy.any((data <= 0)) or numpy.any((data >= 1))):
    raise FitDataError('beta', lower=floc, upper=(floc + fscale))
xbar = data.mean()
if ((f0 is not None) or (f1 is not None)):
    if (f0 is not None):
        b = f0
        data = (1 - data)
        xbar = (1 - xbar)
    else:
        b = f1
    a = ((b * xbar) / (1 - xbar))
    (theta, info, ier, mesg) = scipy.optimize.fsolve(_beta_mle_a, a, args=(b, len(data), np.log(data).sum()), full_output=True)
    if (ier != 1):
        raise FitSolverError(mesg=mesg)
    a = theta[0]
    if (f0 is not None):
        (a, b) = (b, a)
else:
    tempResult = log(data)
	
===================================================================	
chi_gen._logpdf: 459	
----------------------------	

tempResult = log(2)
	
===================================================================	
chi_gen._logpdf: 459	
----------------------------	

tempResult = log(2)
	
===================================================================	
powerlognorm_gen._pdf: 2279	
----------------------------	

tempResult = log(x)
	
===================================================================	
powerlognorm_gen._pdf: 2279	
----------------------------	

tempResult = log(x)
	
===================================================================	
truncnorm_gen._argcheck: 2569	
----------------------------	

self.a = a
self.b = b
self._nb = _norm_cdf(b)
self._na = _norm_cdf(a)
self._sb = _norm_sf(b)
self._sa = _norm_sf(a)
self._delta = numpy.where((self.a > 0), (- (self._sb - self._sa)), (self._nb - self._na))
tempResult = log(self._delta)
	
===================================================================	
truncexpon_gen._logpdf: 2537	
----------------------------	

tempResult = log((- scipy.special.expm1((- b))))
	
===================================================================	
bradford_gen._stats: 334	
----------------------------	

tempResult = log((1.0 + c))
	
===================================================================	
pareto_gen._entropy: 2150	
----------------------------	

tempResult = log(c)
	
===================================================================	
expon_gen._isf: 618	
----------------------------	

tempResult = log(q)
	
===================================================================	
truncexpon_gen._entropy: 2555	
----------------------------	

eB = numpy.exp(b)
tempResult = log((eB - 1))
	
===================================================================	
halfcauchy_gen._logpdf: 1297	
----------------------------	

tempResult = log((2.0 / numpy.pi))
	
===================================================================	
reciprocal_gen._argcheck: 2364	
----------------------------	

self.a = a
self.b = b
tempResult = log(((b * 1.0) / a))
	
===================================================================	
frechet_l_gen._entropy: 882	
----------------------------	

tempResult = log(c)
	
===================================================================	
anglit_gen._entropy: 155	
----------------------------	

tempResult = log(2)
	
===================================================================	
laplace_gen._ppf: 1535	
----------------------------	

tempResult = log((2 * (1 - q)))
	
===================================================================	
laplace_gen._ppf: 1535	
----------------------------	

tempResult = log((2 * q))
	
===================================================================	
invgauss_gen._logpdf: 1450	
----------------------------	

tempResult = log((2 * numpy.pi))
	
===================================================================	
invgauss_gen._logpdf: 1450	
----------------------------	

tempResult = log(x)
	
===================================================================	
loglaplace_gen._entropy: 1694	
----------------------------	

tempResult = log((2.0 / c))
	
===================================================================	
loggamma_gen._ppf: 1666	
----------------------------	

tempResult = log(scipy.special.gammaincinv(c, q))
	
===================================================================	
cosine_gen._entropy: 522	
----------------------------	

tempResult = log((4 * numpy.pi))
	
===================================================================	
frechet_l_gen._logpdf: 859	
----------------------------	

tempResult = log(c)
	
===================================================================	
reciprocal_gen._logpdf: 2371	
----------------------------	

tempResult = log(x)
	
===================================================================	
reciprocal_gen._logpdf: 2371	
----------------------------	

tempResult = log(self.d)
	
===================================================================	
f_gen._logpdf: 770	
----------------------------	

n = (1.0 * dfn)
m = (1.0 * dfd)
tempResult = log(m)
	
===================================================================	
f_gen._logpdf: 770	
----------------------------	

n = (1.0 * dfn)
m = (1.0 * dfd)
tempResult = log(n)
	
===================================================================	
f_gen._logpdf: 770	
----------------------------	

n = (1.0 * dfn)
m = (1.0 * dfd)
tempResult = log(x)
	
===================================================================	
f_gen._logpdf: 771	
----------------------------	

n = (1.0 * dfn)
m = (1.0 * dfd)
lPx = ((((m / 2) * numpy.log(m)) + ((n / 2) * numpy.log(n))) + (((n / 2) - 1) * numpy.log(x)))
tempResult = log((m + (n * x)))
	
===================================================================	
genlogistic_gen._ppf: 900	
----------------------------	

tempResult = log((pow(q, ((- 1.0) / c)) - 1))
	
===================================================================	
kappa4_gen.f211: 1896	
----------------------------	

'ppf = -np.log((1.0 - (q**h))/h)\n            '
tempResult = log(h)
	
===================================================================	
_ncx2_log_pdf: 242	
----------------------------	

df2 = ((df / 2.0) - 1.0)
(xs, ns) = (numpy.sqrt(x), numpy.sqrt(nc))
res = (xlogy((df2 / 2.0), (x / nc)) - (0.5 * ((xs - ns) ** 2)))
tempResult = log((ive(df2, (xs * ns)) / 2.0))
	
===================================================================	
module: 12	
----------------------------	

from __future__ import division, print_function, absolute_import
import math
import numpy as np
import scipy.linalg
from scipy.misc import doccer
from scipy.special import gammaln, psi, multigammaln, xlogy, entr
from scipy._lib._util import check_random_state
from scipy.linalg.blas import drot
from ._discrete_distns import binom
__all__ = ['multivariate_normal', 'matrix_normal', 'dirichlet', 'wishart', 'invwishart', 'multinomial', 'special_ortho_group', 'ortho_group', 'random_correlation']
tempResult = log((2 * numpy.pi))
	
===================================================================	
module: 13	
----------------------------	

from __future__ import division, print_function, absolute_import
import math
import numpy as np
import scipy.linalg
from scipy.misc import doccer
from scipy.special import gammaln, psi, multigammaln, xlogy, entr
from scipy._lib._util import check_random_state
from scipy.linalg.blas import drot
from ._discrete_distns import binom
__all__ = ['multivariate_normal', 'matrix_normal', 'dirichlet', 'wishart', 'invwishart', 'multinomial', 'special_ortho_group', 'ortho_group', 'random_correlation']
_LOG_2PI = numpy.log((2 * numpy.pi))
tempResult = log(2)
	
===================================================================	
module: 14	
----------------------------	

from __future__ import division, print_function, absolute_import
import math
import numpy as np
import scipy.linalg
from scipy.misc import doccer
from scipy.special import gammaln, psi, multigammaln, xlogy, entr
from scipy._lib._util import check_random_state
from scipy.linalg.blas import drot
from ._discrete_distns import binom
__all__ = ['multivariate_normal', 'matrix_normal', 'dirichlet', 'wishart', 'invwishart', 'multinomial', 'special_ortho_group', 'ortho_group', 'random_correlation']
_LOG_2PI = numpy.log((2 * numpy.pi))
_LOG_2 = numpy.log(2)
tempResult = log(numpy.pi)
	
===================================================================	
invwishart_frozen.__init__: 867	
----------------------------	

'\n        Create a frozen inverse Wishart distribution.\n\n        Parameters\n        ----------\n        df : array_like\n            Degrees of freedom of the distribution\n        scale : array_like\n            Scale matrix of the distribution\n        seed : None or int or np.random.RandomState instance, optional\n            This parameter defines the RandomState object to use for drawing\n            random variates.\n            If None (or np.random), the global np.random state is used.\n            If integer, it is used to seed the local RandomState instance\n            Default is None.\n\n        '
self._dist = invwishart_gen(seed)
(self.dim, self.df, self.scale) = self._dist._process_parameters(df, scale)
(C, lower) = scipy.linalg.cho_factor(self.scale, lower=True)
tempResult = log(C.diagonal())
	
===================================================================	
_PSD.__init__: 54	
----------------------------	

(s, u) = scipy.linalg.eigh(M, lower=lower, check_finite=check_finite)
eps = _eigvalsh_to_eps(s, cond, rcond)
if (numpy.min(s) < (- eps)):
    raise ValueError('the input matrix must be positive semidefinite')
d = s[(s > eps)]
if ((len(d) < len(s)) and (not allow_singular)):
    raise numpy.linalg.LinAlgError('singular matrix')
s_pinv = _pinv_1d(s, eps)
U = numpy.multiply(u, numpy.sqrt(s_pinv))
self.rank = len(d)
self.U = U
tempResult = log(d)
	
===================================================================	
dirichlet_gen._logpdf: 423	
----------------------------	

"\n        Parameters\n        ----------\n        x : ndarray\n            Points at which to evaluate the log of the probability\n            density function\n        %(_dirichlet_doc_default_callparams)s\n\n        Notes\n        -----\n        As this function does no argument checking, it should not be\n        called directly; use 'logpdf' instead.\n\n        "
lnB = _lnB(alpha)
tempResult = log(x.T)
	
===================================================================	
invwishart_gen._logpdf: 768	
----------------------------	

"\n        Parameters\n        ----------\n        x : ndarray\n            Points at which to evaluate the log of the probability\n            density function.\n        dim : int\n            Dimension of the scale matrix\n        df : int\n            Degrees of freedom\n        scale : ndarray\n            Scale matrix\n        log_det_scale : float\n            Logarithm of the determinant of the scale matrix\n\n        Notes\n        -----\n        As this function does no argument checking, it should not be\n        called directly; use 'logpdf' instead.\n\n        "
log_det_x = numpy.zeros(x.shape[(- 1)])
x_inv = np.copy(x).T
if (dim > 1):
    _cho_inv_batch(x_inv)
else:
    x_inv = (1.0 / x_inv)
tr_scale_x_inv = numpy.zeros(x.shape[(- 1)])
for i in range(x.shape[(- 1)]):
    (C, lower) = scipy.linalg.cho_factor(x[:, :, i], lower=True)
    tempResult = log(C.diagonal())
	
===================================================================	
wishart_gen._cholesky_logdet: 673	
----------------------------	

'\n        Compute Cholesky decomposition and determine (log(det(scale)).\n\n        Parameters\n        ----------\n        scale : ndarray\n            Scale matrix.\n\n        Returns\n        -------\n        c_decomp : ndarray\n            The Cholesky decomposition of `scale`.\n        logdet : scalar\n            The log of the determinant of `scale`.\n\n        Notes\n        -----\n        This computation of ``logdet`` is equivalent to\n        ``np.linalg.slogdet(scale)``.  It is ~2x faster though.\n\n        '
c_decomp = scipy.linalg.cholesky(scale, lower=True)
tempResult = log(c_decomp.diagonal())
	
===================================================================	
check_cdf_logcdf: 250	
----------------------------	

points = numpy.array([0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])
vals = distfn.ppf(points, *args)
cdf = distfn.cdf(vals, *args)
logcdf = distfn.logcdf(vals, *args)
cdf = cdf[(cdf != 0)]
logcdf = logcdf[numpy.isfinite(logcdf)]
msg += ' - logcdf-log(cdf) relationship'
tempResult = log(cdf)
	
===================================================================	
check_pdf_logpdf: 230	
----------------------------	

points = numpy.array([0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])
vals = distfn.ppf(points, *args)
pdf = distfn.pdf(vals, *args)
logpdf = distfn.logpdf(vals, *args)
pdf = pdf[(pdf != 0)]
logpdf = logpdf[numpy.isfinite(logpdf)]
msg += ' - logpdf-log(pdf) relationship'
tempResult = log(pdf)
	
===================================================================	
check_sf_logsf: 240	
----------------------------	

points = numpy.array([0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])
vals = distfn.ppf(points, *args)
sf = distfn.sf(vals, *args)
logsf = distfn.logsf(vals, *args)
sf = sf[(sf != 0)]
logsf = logsf[numpy.isfinite(logsf)]
msg += ' - logsf-log(sf) relationship'
tempResult = log(sf)
	
===================================================================	
TestBernoulli.test_entropy: 181	
----------------------------	

b = scipy.stats.bernoulli(0.25)
tempResult = log(0.25)
	
===================================================================	
TestBernoulli.test_entropy: 181	
----------------------------	

b = scipy.stats.bernoulli(0.25)
tempResult = log(0.75)
	
===================================================================	
TestExpect.test_logser: 1463	
----------------------------	

(p, loc) = (0.3, 3)
res_0 = scipy.stats.logser.expect((lambda k: k), args=(p,))
tempResult = log((1.0 - p))
	
===================================================================	
TestEntropy.test_entropy_base: 1065	
----------------------------	

pk = numpy.ones(16, float)
S = scipy.stats.entropy(pk, base=2.0)
assert_((abs((S - 4.0)) < 1e-05))
qk = numpy.ones(16, float)
qk[:8] = 2.0
S = scipy.stats.entropy(pk, qk)
S2 = scipy.stats.entropy(pk, qk, base=2.0)
tempResult = log(2.0)
	
===================================================================	
TestLognorm.test_logcdf: 951	
----------------------------	

(x2, mu, sigma) = (201.68, 195, 0.149)
tempResult = log((x2 - mu))
	
===================================================================	
TestLognorm.test_logcdf: 952	
----------------------------	

(x2, mu, sigma) = (201.68, 195, 0.149)
assert_allclose(scipy.stats.lognorm.sf((x2 - mu), s=sigma), scipy.stats.norm.sf((numpy.log((x2 - mu)) / sigma)))
tempResult = log((x2 - mu))
	
===================================================================	
TestGeom.test_logpmf: 236	
----------------------------	

tempResult = log(scipy.stats.geom.pmf([1, 2, 3], 0.5))
	
===================================================================	
TestFitMethod.test_fix_fit_gamma: 1167	
----------------------------	

x = numpy.arange(1, 6)
tempResult = log(x)
	
===================================================================	
TestFitMethod.test_fix_fit_gamma: 1170	
----------------------------	

x = numpy.arange(1, 6)
meanlog = np.log(x).mean()
floc = 0
(a, loc, scale) = scipy.stats.gamma.fit(x, floc=floc)
tempResult = log(x.mean())
	
===================================================================	
TestFitMethod.test_fix_fit_gamma: 1171	
----------------------------	

x = numpy.arange(1, 6)
meanlog = np.log(x).mean()
floc = 0
(a, loc, scale) = scipy.stats.gamma.fit(x, floc=floc)
s = (numpy.log(x.mean()) - meanlog)
tempResult = log(a)
	
===================================================================	
TestFitMethod.test_fix_fit_gamma: 1191	
----------------------------	

x = numpy.arange(1, 6)
meanlog = np.log(x).mean()
floc = 0
(a, loc, scale) = scipy.stats.gamma.fit(x, floc=floc)
s = (numpy.log(x.mean()) - meanlog)
assert_almost_equal((numpy.log(a) - scipy.special.digamma(a)), s, decimal=5)
assert_equal(loc, floc)
assert_almost_equal(scale, (x.mean() / a), decimal=8)
f0 = 1
floc = 0
(a, loc, scale) = scipy.stats.gamma.fit(x, f0=f0, floc=floc)
assert_equal(a, f0)
assert_equal(loc, floc)
assert_almost_equal(scale, (x.mean() / a), decimal=8)
f0 = 2
floc = 0
(a, loc, scale) = scipy.stats.gamma.fit(x, f0=f0, floc=floc)
assert_equal(a, f0)
assert_equal(loc, floc)
assert_almost_equal(scale, (x.mean() / a), decimal=8)
floc = 0
fscale = 2
(a, loc, scale) = scipy.stats.gamma.fit(x, floc=floc, fscale=fscale)
assert_equal(loc, floc)
assert_equal(scale, fscale)
tempResult = log(fscale)
	
===================================================================	
TestPoisson.test_pmf_basic: 652	
----------------------------	

tempResult = log(2)
	
===================================================================	
TestFitMethod.mlefunc: 1198	
----------------------------	

n = len(x)
tempResult = log(x)
	
===================================================================	
TestFitMethod.mlefunc: 1199	
----------------------------	

n = len(x)
s1 = np.log(x).sum()
tempResult = log((1 - x))
	
===================================================================	
TestDLaplace.test_stats2: 729	
----------------------------	

tempResult = log(2.0)
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1606	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
tempResult = log(3)
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1610	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
tempResult = log((- scipy.special.expm1((- 0.25))))
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1623	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
assert_allclose(lc, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_min.sf(x, a, scale=b)
assert_allclose(s, numpy.exp((- 0.25)))
ls = scipy.stats.weibull_min.logsf(x, a, scale=b)
assert_allclose(ls, (- 0.25))
s = scipy.stats.weibull_min.sf(30, 2, scale=3)
assert_allclose(s, numpy.exp((- 100)))
ls = scipy.stats.weibull_min.logsf(30, 2, scale=3)
assert_allclose(ls, (- 100))
x = (- 1.5)
p = scipy.stats.weibull_max.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_max.logpdf(x, a, scale=b)
tempResult = log(3)
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1631	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
assert_allclose(lc, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_min.sf(x, a, scale=b)
assert_allclose(s, numpy.exp((- 0.25)))
ls = scipy.stats.weibull_min.logsf(x, a, scale=b)
assert_allclose(ls, (- 0.25))
s = scipy.stats.weibull_min.sf(30, 2, scale=3)
assert_allclose(s, numpy.exp((- 100)))
ls = scipy.stats.weibull_min.logsf(30, 2, scale=3)
assert_allclose(ls, (- 100))
x = (- 1.5)
p = scipy.stats.weibull_max.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_max.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_max.cdf(x, a, scale=b)
assert_allclose(c, numpy.exp((- 0.25)))
lc = scipy.stats.weibull_max.logcdf(x, a, scale=b)
assert_allclose(lc, (- 0.25))
s = scipy.stats.weibull_max.sf(x, a, scale=b)
assert_allclose(s, (- scipy.special.expm1((- 0.25))))
ls = scipy.stats.weibull_max.logsf(x, a, scale=b)
tempResult = log((- scipy.special.expm1((- 0.25))))
	
===================================================================	
TestWeibull.test_with_maxima_distrib: 1635	
----------------------------	

x = 1.5
a = 2.0
b = 3.0
p = scipy.stats.weibull_min.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_min.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_min.cdf(x, a, scale=b)
assert_allclose(c, (- scipy.special.expm1((- 0.25))))
lc = scipy.stats.weibull_min.logcdf(x, a, scale=b)
assert_allclose(lc, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_min.sf(x, a, scale=b)
assert_allclose(s, numpy.exp((- 0.25)))
ls = scipy.stats.weibull_min.logsf(x, a, scale=b)
assert_allclose(ls, (- 0.25))
s = scipy.stats.weibull_min.sf(30, 2, scale=3)
assert_allclose(s, numpy.exp((- 100)))
ls = scipy.stats.weibull_min.logsf(30, 2, scale=3)
assert_allclose(ls, (- 100))
x = (- 1.5)
p = scipy.stats.weibull_max.pdf(x, a, scale=b)
assert_allclose(p, (numpy.exp((- 0.25)) / 3))
lp = scipy.stats.weibull_max.logpdf(x, a, scale=b)
assert_allclose(lp, ((- 0.25) - numpy.log(3)))
c = scipy.stats.weibull_max.cdf(x, a, scale=b)
assert_allclose(c, numpy.exp((- 0.25)))
lc = scipy.stats.weibull_max.logcdf(x, a, scale=b)
assert_allclose(lc, (- 0.25))
s = scipy.stats.weibull_max.sf(x, a, scale=b)
assert_allclose(s, (- scipy.special.expm1((- 0.25))))
ls = scipy.stats.weibull_max.logsf(x, a, scale=b)
assert_allclose(ls, numpy.log((- scipy.special.expm1((- 0.25)))))
s = scipy.stats.weibull_max.sf((- 1e-09), 2, scale=3)
assert_allclose(s, (- scipy.special.expm1(((- 1) / 9000000000000000000))))
ls = scipy.stats.weibull_max.logsf((- 1e-09), 2, scale=3)
tempResult = log((- scipy.special.expm1(((- 1) / 9000000000000000000))))
	
===================================================================	
TestGeom.test_logcdf_logsf: 253	
----------------------------	

vals = scipy.stats.geom.logcdf([1, 2, 3], 0.5)
vals_sf = scipy.stats.geom.logsf([1, 2, 3], 0.5)
expected = array([0.5, 0.75, 0.875])
tempResult = log(expected)
	
===================================================================	
test_genextreme_entropy: 2156	
----------------------------	

euler_gamma = 0.5772156649015329
h = scipy.stats.genextreme.entropy((- 1.0))
assert_allclose(h, ((2 * euler_gamma) + 1), rtol=1e-14)
h = scipy.stats.genextreme.entropy(0)
assert_allclose(h, (euler_gamma + 1), rtol=1e-14)
h = scipy.stats.genextreme.entropy(1.0)
assert_equal(h, 1)
h = scipy.stats.genextreme.entropy((- 2.0), scale=10)
tempResult = log(10)
	
===================================================================	
test_pdf_logpdf: 161	
----------------------------	

numpy.random.seed(1)
n_basesample = 50
xn = numpy.random.randn(n_basesample)
gkde = scipy.stats.gaussian_kde(xn)
xs = numpy.linspace((- 15), 12, 25)
pdf = gkde.evaluate(xs)
pdf2 = gkde.pdf(xs)
assert_almost_equal(pdf, pdf2, decimal=12)
tempResult = log(pdf)
	
===================================================================	
test_pdf_logpdf: 165	
----------------------------	

numpy.random.seed(1)
n_basesample = 50
xn = numpy.random.randn(n_basesample)
gkde = scipy.stats.gaussian_kde(xn)
xs = numpy.linspace((- 15), 12, 25)
pdf = gkde.evaluate(xs)
pdf2 = gkde.pdf(xs)
assert_almost_equal(pdf, pdf2, decimal=12)
logpdf = numpy.log(pdf)
logpdf2 = gkde.logpdf(xs)
assert_almost_equal(logpdf, logpdf2, decimal=12)
gkde = scipy.stats.gaussian_kde(xs)
tempResult = log(gkde.evaluate(xn))
	
===================================================================	
TestBoxcox_llf.test_basic: 723	
----------------------------	

numpy.random.seed(54321)
x = scipy.stats.norm.rvs(size=10000, loc=10)
lmbda = 1
llf = scipy.stats.boxcox_llf(lmbda, x)
tempResult = log(numpy.sum((x.std() ** 2)))
	
===================================================================	
TestBoxcox.test_fixed_lmbda: 755	
----------------------------	

numpy.random.seed(12345)
x = (scipy.stats.loggamma.rvs(5, size=50) + 5)
xt = scipy.stats.boxcox(x, lmbda=1)
assert_allclose(xt, (x - 1))
xt = scipy.stats.boxcox(x, lmbda=(- 1))
assert_allclose(xt, (1 - (1 / x)))
xt = scipy.stats.boxcox(x, lmbda=0)
tempResult = log(x)
	
===================================================================	
TestBoxcox.test_fixed_lmbda: 757	
----------------------------	

numpy.random.seed(12345)
x = (scipy.stats.loggamma.rvs(5, size=50) + 5)
xt = scipy.stats.boxcox(x, lmbda=1)
assert_allclose(xt, (x - 1))
xt = scipy.stats.boxcox(x, lmbda=(- 1))
assert_allclose(xt, (1 - (1 / x)))
xt = scipy.stats.boxcox(x, lmbda=0)
assert_allclose(xt, numpy.log(x))
xt = scipy.stats.boxcox(list(x), lmbda=0)
tempResult = log(x)
	
===================================================================	
TestMultivariateNormal.test_logpdf_default_values: 58	
----------------------------	

numpy.random.seed(1234)
x = numpy.random.randn(5)
d1 = scipy.stats.multivariate_normal.logpdf(x)
d2 = scipy.stats.multivariate_normal.pdf(x)
d3 = scipy.stats.multivariate_normal.logpdf(x, None, 1)
d4 = scipy.stats.multivariate_normal.pdf(x, None, 1)
tempResult = log(d2)
	
===================================================================	
TestMultivariateNormal.test_logpdf_default_values: 59	
----------------------------	

numpy.random.seed(1234)
x = numpy.random.randn(5)
d1 = scipy.stats.multivariate_normal.logpdf(x)
d2 = scipy.stats.multivariate_normal.pdf(x)
d3 = scipy.stats.multivariate_normal.logpdf(x, None, 1)
d4 = scipy.stats.multivariate_normal.pdf(x, None, 1)
assert_allclose(d1, numpy.log(d2))
tempResult = log(d4)
	
===================================================================	
TestMultivariateNormal.test_logpdf: 49	
----------------------------	

numpy.random.seed(1234)
x = numpy.random.randn(5)
mean = numpy.random.randn(5)
cov = numpy.abs(numpy.random.randn(5))
d1 = scipy.stats.multivariate_normal.logpdf(x, mean, cov)
d2 = scipy.stats.multivariate_normal.pdf(x, mean, cov)
tempResult = log(d2)
	
===================================================================	
TestMultivariateNormal.test_entropy: 256	
----------------------------	

numpy.random.seed(2846)
n = 3
mean = numpy.random.randn(n)
M = numpy.random.randn(n, n)
cov = numpy.dot(M, M.T)
rv = multivariate_normal(mean, cov)
assert_almost_equal(rv.entropy(), scipy.stats.multivariate_normal.entropy(mean, cov))
eigs = numpy.linalg.eig(cov)[0]
tempResult = log((2 * numpy.pi))
	
===================================================================	
TestMultivariateNormal.test_entropy: 256	
----------------------------	

numpy.random.seed(2846)
n = 3
mean = numpy.random.randn(n)
M = numpy.random.randn(n, n)
cov = numpy.dot(M, M.T)
rv = multivariate_normal(mean, cov)
assert_almost_equal(rv.entropy(), scipy.stats.multivariate_normal.entropy(mean, cov))
eigs = numpy.linalg.eig(cov)[0]
tempResult = log(eigs)
	
===================================================================	
TestMultivariateNormal.test_pseudodet_pinv: 180	
----------------------------	

numpy.random.seed(1234)
n = 7
x = numpy.random.randn(n, n)
cov = numpy.dot(x, x.T)
(s, u) = scipy.linalg.eigh(cov)
s = (0.5 * numpy.ones(n))
s[0] = 1.0
s[(- 1)] = 1e-07
cov = numpy.dot(u, numpy.dot(numpy.diag(s), u.T))
cond = 1e-05
psd = _PSD(cov, cond=cond)
psd_pinv = _PSD(psd.pinv, cond=cond)
tempResult = log(s[:(- 1)])
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((4 / 8))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((12 / 8))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((8 / 4))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((8 / 12))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((4 / 2))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((8 / 16))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((8 / 2))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((2 / 4))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((16 / 8))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((2 / 8))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((4 / 8))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((12 / 8))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((8 / 4))
	
===================================================================	
module: 1667	
----------------------------	

" Test functions for stats module\n\n    WRITTEN BY LOUIS LUANGKESORN <lluang@yahoo.com> FOR THE STATS MODULE\n    BASED ON WILKINSON'S STATISTICS QUIZ\n    http://www.stanford.edu/~clint/bench/wilk.txt\n\n    Additional tests by a host of SciPy developers.\n"
from __future__ import division, print_function, absolute_import
import os
import sys
import warnings
from collections import namedtuple
from numpy.testing import TestCase, assert_, assert_equal, assert_almost_equal, assert_array_almost_equal, assert_array_equal, assert_approx_equal, assert_raises, run_module_suite, assert_allclose, dec
from scipy._lib._numpy_compat import assert_raises_regex
import numpy.ma.testutils as mat
from numpy import array, arange, float32, float64, power
import numpy as np
import scipy.stats as stats
import scipy.stats.mstats as mstats
import scipy.stats.mstats_basic as mstats_basic
from scipy._lib._version import NumpyVersion
from scipy._lib.six import xrange
from common_tests import check_named_results
" Numbers in docstrings beginning with 'W' refer to the section numbers\n    and headings found in the STATISTICS QUIZ of Leland Wilkinson.  These are\n    considered to be essential functionality.  True testing and\n    evaluation of a statistics package requires use of the\n    NIST Statistical test data.  See McCoullough(1999) Assessing The Reliability\n    of Statistical Software for a test methodology and its\n    implementation in testing SAS, SPSS, and S-Plus\n"
X = array([1, 2, 3, 4, 5, 6, 7, 8, 9], float)
ZERO = array([0, 0, 0, 0, 0, 0, 0, 0, 0], float)
BIG = array([99999991, 99999992, 99999993, 99999994, 99999995, 99999996, 99999997, 99999998, 99999999], float)
LITTLE = array([0.99999991, 0.99999992, 0.99999993, 0.99999994, 0.99999995, 0.99999996, 0.99999997, 0.99999998, 0.99999999], float)
HUGE = array([1000000000000.0, 2000000000000.0, 3000000000000.0, 4000000000000.0, 5000000000000.0, 6000000000000.0, 7000000000000.0, 8000000000000.0, 9000000000000.0], float)
TINY = array([1e-12, 2e-12, 3e-12, 4e-12, 5e-12, 6e-12, 7e-12, 8e-12, 9e-12], float)
ROUND = array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5], float)

class TestTrimmedStats(TestCase):
    dprec = np.finfo(np.float64).precision

    def test_tmean(self):
        y = scipy.stats.tmean(X, (2, 8), (True, True))
        assert_approx_equal(y, 5.0, significant=self.dprec)
        y1 = scipy.stats.tmean(X, limits=(2, 8), inclusive=(False, False))
        y2 = scipy.stats.tmean(X, limits=None)
        assert_approx_equal(y1, y2, significant=self.dprec)

    def test_tvar(self):
        y = scipy.stats.tvar(X, limits=(2, 8), inclusive=(True, True))
        assert_approx_equal(y, 4.666666666666666, significant=self.dprec)
        y = scipy.stats.tvar(X, limits=None)
        assert_approx_equal(y, X.var(ddof=1), significant=self.dprec)

    def test_tstd(self):
        y = scipy.stats.tstd(X, (2, 8), (True, True))
        assert_approx_equal(y, 2.1602468994692865, significant=self.dprec)
        y = scipy.stats.tstd(X, limits=None)
        assert_approx_equal(y, X.std(ddof=1), significant=self.dprec)

    def test_tmin(self):
        assert_equal(scipy.stats.tmin(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmin(x), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
        assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
        assert_equal(scipy.stats.tmin(x, axis=None), 0)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.tmin(x), numpy.nan)
        assert_equal(scipy.stats.tmin(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmin, x, nan_policy='foobar')
        assert_raises_regex(ValueError, "'propagate', 'raise', 'omit'", scipy.stats.tmin, x, nan_policy='foo')

    def test_tmax(self):
        assert_equal(scipy.stats.tmax(4), 4)
        x = numpy.arange(10)
        assert_equal(scipy.stats.tmax(x), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
        x = x.reshape((5, 2))
        assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
        assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
        assert_equal(scipy.stats.tmax(x, axis=None), 9)
        x = numpy.arange(10.0)
        x[6] = numpy.nan
        assert_equal(scipy.stats.tmax(x), numpy.nan)
        assert_equal(scipy.stats.tmax(x, nan_policy='omit'), 9.0)
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.tmax, x, nan_policy='foobar')

    def test_tsem(self):
        y = scipy.stats.tsem(X, limits=(3, 8), inclusive=(False, True))
        y_ref = numpy.array([4, 5, 6, 7, 8])
        assert_approx_equal(y, (y_ref.std(ddof=1) / numpy.sqrt(y_ref.size)), significant=self.dprec)
        assert_approx_equal(scipy.stats.tsem(X, limits=[(- 1), 10]), scipy.stats.tsem(X, limits=None), significant=self.dprec)

class TestCorrPearsonr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_pXX(self):
        y = scipy.stats.pearsonr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXBIG(self):
        y = scipy.stats.pearsonr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXLITTLE(self):
        y = scipy.stats.pearsonr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXHUGE(self):
        y = scipy.stats.pearsonr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXTINY(self):
        y = scipy.stats.pearsonr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pXROUND(self):
        y = scipy.stats.pearsonr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGBIG(self):
        y = scipy.stats.pearsonr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGLITTLE(self):
        y = scipy.stats.pearsonr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGHUGE(self):
        y = scipy.stats.pearsonr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGTINY(self):
        y = scipy.stats.pearsonr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pBIGROUND(self):
        y = scipy.stats.pearsonr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLELITTLE(self):
        y = scipy.stats.pearsonr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEHUGE(self):
        y = scipy.stats.pearsonr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLETINY(self):
        y = scipy.stats.pearsonr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pLITTLEROUND(self):
        y = scipy.stats.pearsonr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEHUGE(self):
        y = scipy.stats.pearsonr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGETINY(self):
        y = scipy.stats.pearsonr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pHUGEROUND(self):
        y = scipy.stats.pearsonr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYTINY(self):
        y = scipy.stats.pearsonr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pTINYROUND(self):
        y = scipy.stats.pearsonr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_pROUNDROUND(self):
        y = scipy.stats.pearsonr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_r_exactly_pos1(self):
        a = arange(3.0)
        b = a
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, 1.0)
        assert_equal(prob, 0.0)

    def test_r_exactly_neg1(self):
        a = arange(3.0)
        b = (- a)
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_equal(r, (- 1.0))
        assert_equal(prob, 0.0)

    def test_basic(self):
        a = array([(- 1), 0, 1])
        b = array([0, 0, 3])
        (r, prob) = scipy.stats.pearsonr(a, b)
        assert_approx_equal(r, (numpy.sqrt(3) / 2))
        assert_approx_equal(prob, (1.0 / 3))

class TestFisherExact(TestCase):
    'Some tests to show that fisher_exact() works correctly.\n\n    Note that in SciPy 0.9.0 this was not working well for large numbers due to\n    inaccuracy of the hypergeom distribution (see #1218). Fixed now.\n\n    Also note that R and Scipy have different argument formats for their\n    hypergeometric distribution functions.\n\n    R:\n    > phyper(18999, 99000, 110000, 39000, lower.tail = FALSE)\n    [1] 1.701815e-09\n    '

    def test_basic(self):
        fisher_exact = scipy.stats.fisher_exact
        res = fisher_exact([[14500, 20000], [30000, 40000]])[1]
        assert_approx_equal(res, 0.01106, significant=4)
        res = fisher_exact([[100, 2], [1000, 5]])[1]
        assert_approx_equal(res, 0.1301, significant=4)
        res = fisher_exact([[2, 7], [8, 2]])[1]
        assert_approx_equal(res, 0.0230141, significant=6)
        res = fisher_exact([[5, 1], [10, 10]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 15], [20, 20]])[1]
        assert_approx_equal(res, 0.0958044, significant=6)
        res = fisher_exact([[5, 16], [20, 25]])[1]
        assert_approx_equal(res, 0.1725862, significant=6)
        res = fisher_exact([[10, 5], [10, 1]])[1]
        assert_approx_equal(res, 0.1973244, significant=6)
        res = fisher_exact([[5, 0], [1, 4]])[1]
        assert_approx_equal(res, 0.04761904, significant=6)
        res = fisher_exact([[0, 1], [3, 2]])[1]
        assert_approx_equal(res, 1.0)
        res = fisher_exact([[0, 2], [6, 4]])[1]
        assert_approx_equal(res, 0.4545454545)
        res = fisher_exact([[2, 7], [8, 2]])
        assert_approx_equal(res[1], 0.0230141, significant=6)
        assert_approx_equal(res[0], (4.0 / 56))

    def test_precise(self):
        tablist = [([[100, 2], [1000, 5]], (0.2505583993422285, 0.1300759363430016)), ([[2, 7], [8, 2]], (0.08586235135736206, 0.02301413756522114)), ([[5, 1], [10, 10]], (4.725646047336584, 0.197324414715719)), ([[5, 15], [20, 20]], (0.3394396617440852, 0.09580440012477637)), ([[5, 16], [20, 25]], (0.3960558326183334, 0.1725864953812994)), ([[10, 5], [10, 1]], (0.2116112781158483, 0.197324414715719)), ([[10, 5], [10, 0]], (0.0, 0.06126482213438734)), ([[5, 0], [1, 4]], (numpy.inf, 0.04761904761904762)), ([[0, 5], [1, 4]], (0.0, 1.0)), ([[5, 1], [0, 4]], (numpy.inf, 0.04761904761904758)), ([[0, 1], [3, 2]], (0.0, 1.0))]
        for (table, res_r) in tablist:
            res = scipy.stats.fisher_exact(numpy.asarray(table))
            numpy.testing.assert_almost_equal(res[1], res_r[1], decimal=11, verbose=True)

    @numpy.testing.dec.slow
    def test_large_numbers(self):
        pvals = [5.56e-11, 2.666e-11, 1.363e-11]
        for (pval, num) in zip(pvals, [75, 76, 77]):
            res = scipy.stats.fisher_exact([[17704, 496], [1065, num]])[1]
            assert_approx_equal(res, pval, significant=4)
        res = scipy.stats.fisher_exact([[18000, 80000], [20000, 90000]])[1]
        assert_approx_equal(res, 0.2751, significant=4)

    def test_raises(self):
        assert_raises(ValueError, scipy.stats.fisher_exact, np.arange(6).reshape(2, 3))

    def test_row_or_col_zero(self):
        tables = ([[0, 0], [5, 10]], [[5, 10], [0, 0]], [[0, 5], [0, 10]], [[5, 0], [10, 0]])
        for table in tables:
            (oddsratio, pval) = scipy.stats.fisher_exact(table)
            assert_equal(pval, 1.0)
            assert_equal(oddsratio, numpy.nan)

    def test_less_greater(self):
        tables = ([[2, 7], [8, 2]], [[200, 7], [8, 300]], [[28, 21], [6, 1957]], [[190, 800], [200, 900]], [[0, 2], [3, 0]], [[1, 1], [2, 1]], [[2, 0], [1, 2]], [[0, 1], [2, 3]], [[1, 0], [1, 4]])
        pvals = ([0.0185217259520665, 0.9990149169715733], [1.0, 2.0056578803889148e-122], [1.0, 5.728437460831983e-44], [0.7416227, 0.2959826], [0.1, 1.0], [0.7, 0.9], [1.0, 0.3], [(2.0 / 3), 1.0], [1.0, (1.0 / 3)])
        for (table, pval) in zip(tables, pvals):
            res = []
            res.append(scipy.stats.fisher_exact(table, alternative='less')[1])
            res.append(scipy.stats.fisher_exact(table, alternative='greater')[1])
            assert_allclose(res, pval, atol=0, rtol=1e-07)

    def test_gh3014(self):
        (odds, pvalue) = scipy.stats.fisher_exact([[1, 2], [9, 84419233]])

class TestCorrSpearmanr(TestCase):
    ' W.II.D. Compute a correlation matrix on all the variables.\n\n        All the correlations, except for ZERO and MISS, shoud be exactly 1.\n        ZERO and MISS should have undefined or missing correlations with the\n        other variables.  The same should go for SPEARMAN corelations, if\n        your program has them.\n    '

    def test_scalar(self):
        y = scipy.stats.spearmanr(4.0, 2.0)
        assert_(np.isnan(y).all())

    def test_uneven_lengths(self):
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], [8, 9])
        assert_raises(ValueError, scipy.stats.spearmanr, [1, 2, 1], 8)

    def test_nan_policy(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
        assert_array_equal(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0.0))
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')

    def test_sXX(self):
        y = scipy.stats.spearmanr(X, X)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXBIG(self):
        y = scipy.stats.spearmanr(X, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXLITTLE(self):
        y = scipy.stats.spearmanr(X, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXHUGE(self):
        y = scipy.stats.spearmanr(X, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXTINY(self):
        y = scipy.stats.spearmanr(X, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sXROUND(self):
        y = scipy.stats.spearmanr(X, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGBIG(self):
        y = scipy.stats.spearmanr(BIG, BIG)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGLITTLE(self):
        y = scipy.stats.spearmanr(BIG, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGHUGE(self):
        y = scipy.stats.spearmanr(BIG, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGTINY(self):
        y = scipy.stats.spearmanr(BIG, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sBIGROUND(self):
        y = scipy.stats.spearmanr(BIG, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLELITTLE(self):
        y = scipy.stats.spearmanr(LITTLE, LITTLE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEHUGE(self):
        y = scipy.stats.spearmanr(LITTLE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLETINY(self):
        y = scipy.stats.spearmanr(LITTLE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sLITTLEROUND(self):
        y = scipy.stats.spearmanr(LITTLE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEHUGE(self):
        y = scipy.stats.spearmanr(HUGE, HUGE)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGETINY(self):
        y = scipy.stats.spearmanr(HUGE, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sHUGEROUND(self):
        y = scipy.stats.spearmanr(HUGE, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYTINY(self):
        y = scipy.stats.spearmanr(TINY, TINY)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sTINYROUND(self):
        y = scipy.stats.spearmanr(TINY, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_sROUNDROUND(self):
        y = scipy.stats.spearmanr(ROUND, ROUND)
        r = y[0]
        assert_approx_equal(r, 1.0)

    def test_spearmanr_result_attributes(self):
        res = scipy.stats.spearmanr(X, X)
        attributes = ('correlation', 'pvalue')
        check_named_results(res, attributes)

def test_spearmanr():
    x1 = [1, 2, 3, 4, 5]
    x2 = [5, 6, 7, 8, 7]
    expected = (0.8207826816681233, 0.0885870053135438)
    res = scipy.stats.spearmanr(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.spearmanr(x1, x2)
    check_named_results(res, attributes)
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
        assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.28659685838743354, 6.579862219051161e-11)
    res = scipy.stats.spearmanr(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.spearmanr, x, y)
    x1 = [1, 2, 3, 4]
    x2 = [8, 7, 6, numpy.nan]
    res1 = scipy.stats.spearmanr(x1, x2, nan_policy='omit')
    res2 = scipy.stats.spearmanr(x1[:3], x2[:3], nan_policy='omit')
    assert_equal(res1, res2)

class TestCorrSpearmanrTies(TestCase):
    'Some tests of tie-handling by the spearmanr function.'

    def test_tie1(self):
        x = [1.0, 2.0, 3.0, 4.0]
        y = [1.0, 2.0, 2.0, 3.0]
        xr = [1.0, 2.0, 3.0, 4.0]
        yr = [1.0, 2.5, 2.5, 4.0]
        sr = scipy.stats.spearmanr(x, y)
        pr = scipy.stats.pearsonr(xr, yr)
        assert_almost_equal(sr, pr)

def test_kendalltau():
    x1 = [12, 2, 1, 12, 2]
    x2 = [1, 4, 7, 1, 0]
    expected = ((- 0.47140452079103173), 0.2827454599327748)
    res = scipy.stats.kendalltau(x1, x2)
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    attributes = ('correlation', 'pvalue')
    res = scipy.stats.kendalltau(x1, x2)
    check_named_results(res, attributes)
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
    numpy.random.seed(7546)
    x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
    corr = [[1.0, 0.3], [0.3, 1.0]]
    x = numpy.dot(numpy.linalg.cholesky(corr), x)
    expected = (0.19291382765531062, 1.1337095377742629e-10)
    res = scipy.stats.kendalltau(x[0], x[1])
    assert_approx_equal(res[0], expected[0])
    assert_approx_equal(res[1], expected[1])
    assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
    x = numpy.arange(10.0)
    x[9] = numpy.nan
    assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
    assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
    x = numpy.arange(10.0)
    y = numpy.arange(20.0)
    assert_raises(ValueError, scipy.stats.kendalltau, x, y)
    (tau, p_value) = scipy.stats.kendalltau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.kendalltau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)

def test_kendalltau_vs_mstats_basic():
    numpy.random.seed(42)
    for s in range(2, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        expected = scipy.stats.mstats_basic.kendalltau(a, b)
        actual = scipy.stats.kendalltau(a, b)
        assert_approx_equal(actual[0], expected[0])
        assert_approx_equal(actual[1], expected[1])

def test_kendalltau_nan_2nd_arg():
    x = [1.0, 2.0, 3.0, 4.0]
    y = [numpy.nan, 2.4, 3.4, 3.4]
    r1 = scipy.stats.kendalltau(x, y, nan_policy='omit')
    r2 = scipy.stats.kendalltau(x[1:], y[1:])
    assert_allclose(r1.correlation, r2.correlation, atol=1e-15)

def test_weightedtau():
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, 0]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, additive=False)
    assert_approx_equal(tau, (- 0.6220571695180104))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None)
    assert_approx_equal(tau, (- 0.4157652301037516))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None)
    assert_approx_equal(tau, (- 0.7181341329699029))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.4064485096624689))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=None, additive=False)
    assert_approx_equal(tau, (- 0.8376658293735517))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=False)
    assert_approx_equal(tau, (- 0.5160439794026185))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(x, y, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(y, x, rank=True, weigher=(lambda x: 1))
    assert_approx_equal(tau, (- 0.47140452079103173))
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.int16), y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau(numpy.asarray(x, dtype=numpy.float64), numpy.asarray(y, dtype=numpy.float64))
    assert_approx_equal(tau, (- 0.5669496815368272))
    (tau, p_value) = scipy.stats.weightedtau([], [])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    (tau, p_value) = scipy.stats.weightedtau([0], [0])
    assert_equal(numpy.nan, tau)
    assert_equal(numpy.nan, p_value)
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1, 2])
    assert_raises(ValueError, scipy.stats.weightedtau, [0, 1], [0, 1], [0])
    x = [12, 2, 1, 12, 2]
    y = [1, 4, 7, 1, numpy.nan]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))
    x = [12, 2, numpy.nan, 12, 2]
    (tau, p_value) = scipy.stats.weightedtau(x, y)
    assert_approx_equal(tau, (- 0.5669496815368272))

def test_weightedtau_vs_quadratic():

    def wkq(x, y, rank, weigher, add):
        tot = conc = disc = u = v = 0
        for i in range(len(x)):
            for j in range(len(x)):
                w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
                tot += w
                if (x[i] == x[j]):
                    u += w
                if (y[i] == y[j]):
                    v += w
                if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                    conc += w
                elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                    disc += w
        return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
    numpy.random.seed(42)
    for s in range(3, 10):
        a = []
        for i in range(s):
            a += ([i] * i)
        b = list(a)
        numpy.random.shuffle(a)
        numpy.random.shuffle(b)
        rank = numpy.arange(len(a), dtype=numpy.intp)
        for _ in range(2):
            for add in [True, False]:
                expected = wkq(a, b, rank, (lambda x: (1.0 / (x + 1))), add)
                actual = stats.weightedtau(a, b, rank, (lambda x: (1.0 / (x + 1))), add).correlation
                assert_approx_equal(expected, actual)
            numpy.random.shuffle(rank)

class TestFindRepeats(TestCase):

    def test_basic(self):
        a = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 5]
        (res, nums) = scipy.stats.find_repeats(a)
        assert_array_equal(res, [1, 2, 3, 4])
        assert_array_equal(nums, [3, 3, 2, 2])

    def test_empty_result(self):
        for a in [[10, 20, 50, 30, 40], []]:
            (repeated, counts) = scipy.stats.find_repeats(a)
            assert_array_equal(repeated, [])
            assert_array_equal(counts, [])

class TestRegression(TestCase):

    def test_linregressBIGX(self):
        y = scipy.stats.linregress(X, BIG)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 99999990)
        assert_almost_equal(r, 1.0)

    def test_regressXX(self):
        y = scipy.stats.linregress(X, X)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 1.0)

    def test_regressZEROX(self):
        y = scipy.stats.linregress(X, ZERO)
        intercept = y[1]
        r = y[2]
        assert_almost_equal(intercept, 0.0)
        assert_almost_equal(r, 0.0)

    def test_regress_simple(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_rows(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        rows = numpy.vstack((x, y))
        res = scipy.stats.linregress(rows)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_simple_onearg_cols(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        cols = numpy.hstack((numpy.expand_dims(x, 1), numpy.expand_dims(y, 1)))
        res = scipy.stats.linregress(cols)
        assert_almost_equal(res[4], 0.0023957814497838803)

    def test_regress_shape_error(self):
        assert_raises(ValueError, scipy.stats.linregress, numpy.ones((3, 3)))

    def test_linregress(self):
        x = numpy.arange(11)
        y = numpy.arange(5, 16)
        y[[1, (- 2)]] -= 1
        y[[0, (- 1)]] += 1
        res = (1.0, 5.0, 0.9822994862575, 7.45259691e-08, 0.06356417261637273)
        assert_array_almost_equal(scipy.stats.linregress(x, y), res, decimal=14)

    def test_regress_simple_negative_cor(self):
        (a, n) = (1e-71, 100000)
        x = numpy.linspace(a, (2 * a), n)
        y = numpy.linspace((2 * a), a, n)
        scipy.stats.linregress(x, y)
        res = scipy.stats.linregress(x, y)
        assert_((res[2] >= (- 1)))
        assert_almost_equal(res[2], (- 1))
        assert_((not numpy.isnan(res[4])))

    def test_linregress_result_attributes(self):
        x = numpy.linspace(0, 100, 100)
        y = ((0.2 * numpy.linspace(0, 100, 100)) + 10)
        y += numpy.sin(numpy.linspace(0, 20, 100))
        res = scipy.stats.linregress(x, y)
        attributes = ('slope', 'intercept', 'rvalue', 'pvalue', 'stderr')
        check_named_results(res, attributes)

    def test_regress_two_inputs(self):
        x = numpy.arange(2)
        y = numpy.arange(3, 5)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 0.0)
        assert_almost_equal(res[4], 0.0)

    def test_regress_two_inputs_horizontal_line(self):
        x = numpy.arange(2)
        y = numpy.ones(2)
        res = scipy.stats.linregress(x, y)
        assert_almost_equal(res[3], 1.0)
        assert_almost_equal(res[4], 0.0)

    def test_nist_norris(self):
        x = [0.2, 337.4, 118.2, 884.6, 10.1, 226.5, 666.3, 996.3, 448.6, 777.0, 558.2, 0.4, 0.6, 775.5, 666.9, 338.0, 447.5, 11.6, 556.0, 228.1, 995.8, 887.6, 120.2, 0.3, 0.3, 556.8, 339.1, 887.2, 999.0, 779.0, 11.1, 118.3, 229.2, 669.1, 448.9, 0.5]
        y = [0.1, 338.8, 118.1, 888.0, 9.2, 228.1, 668.5, 998.5, 449.1, 778.9, 559.2, 0.3, 0.1, 778.1, 668.8, 339.3, 448.9, 10.8, 557.7, 228.3, 998.0, 888.8, 119.6, 0.3, 0.6, 557.6, 339.3, 888.0, 998.5, 778.9, 10.2, 117.6, 228.9, 668.4, 449.2, 0.2]
        exp_slope = 1.00211681802045
        exp_intercept = (- 0.262323073774029)
        exp_rvalue = 0.999993745883712
        actual = scipy.stats.linregress(x, y)
        assert_almost_equal(actual.slope, exp_slope)
        assert_almost_equal(actual.intercept, exp_intercept)
        assert_almost_equal(actual.rvalue, exp_rvalue, decimal=5)

    def test_empty_input(self):
        assert_raises(ValueError, scipy.stats.linregress, [], [])

    def test_nan_input(self):
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            assert_array_equal(scipy.stats.linregress(x, x), (numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan))

def test_theilslopes():
    (slope, intercept, lower, upper) = scipy.stats.theilslopes([0, 1, 1])
    assert_almost_equal(slope, 0.5)
    assert_almost_equal(intercept, 0.5)
    x = [1, 2, 3, 4, 10, 12, 18]
    y = [9, 15, 19, 20, 45, 55, 78]
    (slope, intercept, lower, upper) = scipy.stats.theilslopes(y, x, 0.07)
    assert_almost_equal(slope, 4)
    assert_almost_equal(upper, 4.38, decimal=2)
    assert_almost_equal(lower, 3.71, decimal=2)

class TestHistogram(TestCase):
    low_values = numpy.array([0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.2], dtype=float)
    high_range = numpy.array([2, 3, 4, 2, 21, 32, 78, 95, 65, 66, 66, 66, 66, 4], dtype=float)
    low_range = numpy.array([2, 3, 3, 2, 3, 2.4, 2.1, 3.1, 2.9, 2.6, 2.7, 2.8, 2.2, 2.001], dtype=float)
    few_values = numpy.array([2.0, 3.0, (- 1.0), 0.0], dtype=float)

    def test_simple(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]), 0.14444444444444446, 0.11111111111111112, 0)), (self.high_range, (numpy.array([5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 5.0, 1.0, 0.0, 1.0]), (- 3.166666666666666), 10.333333333333332, 0)), (self.low_range, (numpy.array([3.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 1.0]), 1.9388888888888889, 0.12222222222222223, 0)), (self.few_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), (- 1.2222222222222223), 0.4444444444444445, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_empty(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram([])
        e_count = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        e_lowerlimit = 0
        e_binsize = 0.1
        e_extrapoints = 0
        assert_allclose(res.count, e_count, rtol=1e-15)
        assert_equal(res.lowerlimit, e_lowerlimit)
        assert_almost_equal(res.binsize, e_binsize)
        assert_equal(res.extrapoints, e_extrapoints)

    def test_reduced_bins(self):
        basic_tests = ((self.low_values, (numpy.array([2.0, 3.0, 3.0, 1.0, 2.0]), 0.07500000000000001, 0.25, 0)), (self.high_range, (numpy.array([5.0, 2.0, 0.0, 6.0, 1.0]), (- 9.625), 23.25, 0)), (self.low_range, (numpy.array([4.0, 2.0, 1.0, 3.0, 4.0]), 1.8625, 0.275, 0)), (self.few_values, (numpy.array([1.0, 1.0, 0.0, 1.0, 1.0]), (- 1.5), 1.0, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=5)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_increased_bins(self):
        basic_tests = ((self.low_values, (numpy.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]), 0.1736842105263158, 0.05263157894736842, 0)), (self.high_range, (numpy.array([5.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]), (- 0.4473684210526314), 4.894736842105263, 0)), (self.low_range, (numpy.array([3.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0]), 1.9710526315789474, 0.05789473684210526, 0)), (self.few_values, (numpy.array([1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]), (- 1.1052631578947367), 0.21052631578947367, 0)))
        for (inputs, expected_results) in basic_tests:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', DeprecationWarning)
                given_results = scipy.stats.histogram(inputs, numbins=20)
            assert_array_almost_equal(expected_results[0], given_results[0], decimal=2)
            for i in range(1, 4):
                assert_almost_equal(expected_results[i], given_results[i], decimal=2)

    def test_histogram_result_attributes(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            res = scipy.stats.histogram(self.low_range, numbins=20)
        attributes = ('count', 'lowerlimit', 'binsize', 'extrapoints')
        check_named_results(res, attributes)

def test_cumfreq():
    x = [1, 4, 2, 1, 3, 1]
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4)
    assert_array_almost_equal(cumfreqs, numpy.array([3.0, 4.0, 5.0, 6.0]))
    (cumfreqs, lowlim, binsize, extrapoints) = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    assert_((extrapoints == 3))
    attributes = ('cumcount', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))
    check_named_results(res, attributes)

def test_relfreq():
    a = numpy.array([1, 4, 2, 1, 3, 1])
    (relfreqs, lowlim, binsize, extrapoints) = scipy.stats.relfreq(a, numbins=4)
    assert_array_almost_equal(relfreqs, array([0.5, 0.16666667, 0.16666667, 0.16666667]))
    attributes = ('frequency', 'lowerlimit', 'binsize', 'extrapoints')
    res = scipy.stats.relfreq(a, numbins=4)
    check_named_results(res, attributes)
    (relfreqs2, lowlim, binsize, extrapoints) = scipy.stats.relfreq([1, 4, 2, 1, 3, 1], numbins=4)
    assert_array_almost_equal(relfreqs, relfreqs2)

class TestGMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float32)
        actual = scipy.stats.gmean(a)
        desired = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        assert_almost_equal(actual, desired, decimal=7)
        desired1 = scipy.stats.gmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=7)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a)
        desired = array((1, 2, 3, 4))
        assert_array_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.gmean(a, axis=0)
        assert_array_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.gmean(a, axis=1)
        v = power((((1 * 2) * 3) * 4), (1.0 / 4.0))
        desired = array((v, v, v))
        assert_array_almost_equal(actual, desired, decimal=14)

    def test_large_values(self):
        a = array([1e+100, 1e+200, 1e+300])
        actual = scipy.stats.gmean(a)
        assert_approx_equal(actual, 1e+200, significant=13)

class TestHMean(TestCase):

    def test_1D_list(self):
        a = (1, 2, 3, 4)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(array(a), axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_1D_array(self):
        a = array((1, 2, 3, 4), float64)
        actual = scipy.stats.hmean(a)
        desired = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        assert_almost_equal(actual, desired, decimal=14)
        desired1 = scipy.stats.hmean(a, axis=(- 1))
        assert_almost_equal(actual, desired1, decimal=14)

    def test_2D_array_default(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        actual = scipy.stats.hmean(a)
        desired = array((1.0, 2.0, 3.0, 4.0))
        assert_array_almost_equal(actual, desired, decimal=14)
        actual1 = scipy.stats.hmean(a, axis=0)
        assert_array_almost_equal(actual1, desired, decimal=14)

    def test_2D_array_dim1(self):
        a = array(((1, 2, 3, 4), (1, 2, 3, 4), (1, 2, 3, 4)))
        v = (4.0 / ((((1.0 / 1) + (1.0 / 2)) + (1.0 / 3)) + (1.0 / 4)))
        desired1 = array((v, v, v))
        actual1 = scipy.stats.hmean(a, axis=1)
        assert_array_almost_equal(actual1, desired1, decimal=14)

class TestScoreatpercentile(TestCase):

    def setUp(self):
        self.a1 = [3, 4, 5, 10, (- 3), (- 5), 6]
        self.a2 = [3, (- 6), (- 2), 8, 7, 4, 2, 1]
        self.a3 = [3.0, 4, 5, 10, (- 3), (- 5), (- 6), 7.0]

    def test_basic(self):
        x = (arange(8) * 0.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 0), 0.0)
        assert_equal(scipy.stats.scoreatpercentile(x, 100), 3.5)
        assert_equal(scipy.stats.scoreatpercentile(x, 50), 1.75)

    def test_fraction(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7)), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8)), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100)), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10)), 5.5)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(list(range(100)), 50, limit=(1, 8), interpolation_method='fraction'), 4.5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='fraction'), 55)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='fraction'), 5.5)

    def test_lower_higher(self):
        scoreatperc = scipy.stats.scoreatpercentile
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(10)), 50, (2, 7), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(10)), 50, limit=(2, 7), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='lower'), 4)
        assert_equal(scoreatperc(list(range(100)), 50, (1, 8), interpolation_method='higher'), 5)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (10, 100), interpolation_method='lower'), 10)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(10, 100), interpolation_method='higher'), 100)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, (1, 10), interpolation_method='lower'), 1)
        assert_equal(scoreatperc(numpy.array([1, 10, 100]), 50, limit=(1, 10), interpolation_method='higher'), 10)

    def test_sequence_per(self):
        x = (arange(8) * 0.5)
        expected = numpy.array([0, 3.5, 1.75])
        res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
        assert_allclose(res, expected)
        assert_(isinstance(res, numpy.ndarray))
        assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
        res2 = scipy.stats.scoreatpercentile(np.arange(12).reshape((3, 4)), numpy.array([0, 1, 100, 100]), axis=1)
        expected2 = array([[0, 4, 8], [0.03, 4.03, 8.03], [3, 7, 11], [3, 7, 11]])
        assert_allclose(res2, expected2)

    def test_axis(self):
        scoreatperc = scipy.stats.scoreatpercentile
        x = arange(12).reshape(3, 4)
        assert_equal(scoreatperc(x, (25, 50, 100)), [2.75, 5.5, 11.0])
        r0 = [[2, 3, 4, 5], [4, 5, 6, 7], [8, 9, 10, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=0), r0)
        r1 = [[0.75, 4.75, 8.75], [1.5, 5.5, 9.5], [3, 7, 11]]
        assert_equal(scoreatperc(x, (25, 50, 100), axis=1), r1)
        x = array([[1, 1, 1], [1, 1, 1], [4, 4, 3], [1, 1, 1], [1, 1, 1]])
        score = scipy.stats.scoreatpercentile(x, 50)
        assert_equal(score.shape, ())
        assert_equal(score, 1.0)
        score = scipy.stats.scoreatpercentile(x, 50, axis=0)
        assert_equal(score.shape, (3,))
        assert_equal(score, [1, 1, 1])

    def test_exception(self):
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1, 2], 56, interpolation_method='foobar')
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], 101)
        assert_raises(ValueError, scipy.stats.scoreatpercentile, [1], (- 1))

    def test_empty(self):
        assert_equal(scipy.stats.scoreatpercentile([], 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile(numpy.array([[], []]), 50), numpy.nan)
        assert_equal(scipy.stats.scoreatpercentile([], [50, 99]), [numpy.nan, numpy.nan])

class TestItemfreq(object):
    a = ([5, 7, 1, 2, 1, 5, 7] * 10)
    b = [1, 2, 5, 7]

    def test_numeric_types(self):

        def _check_itemfreq(dt):
            a = numpy.array(self.a, dt)
            v = scipy.stats.itemfreq(a)
            assert_array_equal(v[:, 0], [1, 2, 5, 7])
            assert_array_equal(v[:, 1], numpy.array([20, 10, 20, 20], dtype=dt))
        dtypes = [numpy.int32, numpy.int64, numpy.float32, numpy.float64, numpy.complex64, numpy.complex128]
        for dt in dtypes:
            (yield (_check_itemfreq, dt))

    def test_object_arrays(self):
        (a, b) = (self.a, self.b)
        dt = 'O'
        aa = numpy.empty(len(a), dt)
        aa[:] = a
        bb = numpy.empty(len(b), dt)
        bb[:] = b
        v = scipy.stats.itemfreq(aa)
        assert_array_equal(v[:, 0], bb)

    def test_structured_arrays(self):
        (a, b) = (self.a, self.b)
        dt = [('', 'i'), ('', 'i')]
        aa = numpy.array(list(zip(a, a)), dt)
        bb = numpy.array(list(zip(b, b)), dt)
        v = scipy.stats.itemfreq(aa)
        assert_equal(tuple(v[(2, 0)]), tuple(bb[2]))

class TestMode(TestCase):

    def test_empty(self):
        (vals, counts) = scipy.stats.mode([])
        assert_equal(vals, numpy.array([]))
        assert_equal(counts, numpy.array([]))

    def test_scalar(self):
        (vals, counts) = scipy.stats.mode(4.0)
        assert_equal(vals, numpy.array([4.0]))
        assert_equal(counts, numpy.array([1]))

    def test_basic(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 6)
        assert_equal(vals[1][0], 3)

    def test_axes(self):
        data1 = [10, 10, 30, 40]
        data2 = [10, 10, 10, 10]
        data3 = [20, 10, 20, 20]
        data4 = [30, 30, 30, 30]
        data5 = [40, 30, 30, 30]
        arr = numpy.array([data1, data2, data3, data4, data5])
        vals = scipy.stats.mode(arr, axis=None)
        assert_equal(vals[0], numpy.array([30]))
        assert_equal(vals[1], numpy.array([8]))
        vals = scipy.stats.mode(arr, axis=0)
        assert_equal(vals[0], numpy.array([[10, 10, 30, 30]]))
        assert_equal(vals[1], numpy.array([[2, 3, 3, 2]]))
        vals = scipy.stats.mode(arr, axis=1)
        assert_equal(vals[0], numpy.array([[10], [10], [20], [30], [30]]))
        assert_equal(vals[1], numpy.array([[2], [4], [3], [4], [3]]))

    def test_strings(self):
        data1 = ['rain', 'showers', 'showers']
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(data1)
        assert_equal(vals[0][0], 'showers')
        assert_equal(vals[1][0], 2)

    @numpy.testing.dec.knownfailureif((sys.version_info > (3,)), 'numpy github issue 641')
    def test_mixed_objects(self):
        objects = [10, True, numpy.nan, 'hello', 10]
        arr = numpy.empty((5,), dtype=object)
        arr[:] = objects
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], 10)
        assert_equal(vals[1][0], 2)

    def test_objects(self):

        class Point(object):

            def __init__(self, x):
                self.x = x

            def __eq__(self, other):
                return (self.x == other.x)

            def __ne__(self, other):
                return (self.x != other.x)

            def __lt__(self, other):
                return (self.x < other.x)

            def __hash__(self):
                return hash(self.x)
        points = [Point(x) for x in [1, 2, 3, 4, 3, 2, 2, 2]]
        arr = numpy.empty((8,), dtype=object)
        arr[:] = points
        assert_((len(set(points)) == 4))
        assert_equal(np.unique(arr).shape, (4,))
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            vals = scipy.stats.mode(arr)
        assert_equal(vals[0][0], Point(2))
        assert_equal(vals[1][0], 4)

    def test_mode_result_attributes(self):
        data1 = [3, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        data2 = []
        actual = scipy.stats.mode(data1)
        attributes = ('mode', 'count')
        check_named_results(actual, attributes)
        actual2 = scipy.stats.mode(data2)
        check_named_results(actual2, attributes)

    def test_mode_nan(self):
        data1 = [3, numpy.nan, 5, 1, 10, 23, 3, 2, 6, 8, 6, 10, 6]
        actual = scipy.stats.mode(data1)
        assert_equal(actual, (6, 3))
        actual = scipy.stats.mode(data1, nan_policy='omit')
        assert_equal(actual, (6, 3))
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.mode, data1, nan_policy='foobar')

class TestVariability(TestCase):
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0

    def test_signaltonoise(self):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', DeprecationWarning)
            y = scipy.stats.signaltonoise(self.testcase)
        assert_approx_equal(y, 2.236067977)

    def test_sem(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            y = scipy.stats.sem(self.scalar_testcase)
        assert_(numpy.isnan(y))
        y = scipy.stats.sem(self.testcase)
        assert_approx_equal(y, 0.6454972244)
        n = len(self.testcase)
        assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.sem(x), numpy.nan)
        assert_equal(scipy.stats.sem(x, nan_policy='omit'), 0.9128709291752769)
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.sem, x, nan_policy='foobar')

    def test_zmap(self):
        y = scipy.stats.zmap(self.testcase, self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zmap_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zmap(x, x, axis=0)
        z1 = scipy.stats.zmap(x, x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zmap_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zmap(x, x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

    def test_zscore(self):
        y = scipy.stats.zscore(self.testcase)
        desired = [(- 1.3416407864999), (- 0.44721359549996), 0.44721359549996, 1.3416407864999]
        assert_array_almost_equal(desired, y, decimal=12)

    def test_zscore_axis(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 2.0], [2.0, 0.0, 2.0, 0.0]])
        t1 = (1.0 / numpy.sqrt((2.0 / 3)))
        t2 = (numpy.sqrt(3.0) / 3)
        t3 = numpy.sqrt(2.0)
        z0 = scipy.stats.zscore(x, axis=0)
        z1 = scipy.stats.zscore(x, axis=1)
        z0_expected = [[(- t1), ((- t3) / 2), ((- t3) / 2), 0.0], [0.0, t3, ((- t3) / 2), t1], [t1, ((- t3) / 2), t3, (- t1)]]
        z1_expected = [[(- 1.0), (- 1.0), 1.0, 1.0], [(- t2), (- t2), (- t2), numpy.sqrt(3.0)], [1.0, (- 1.0), 1.0, (- 1.0)]]
        assert_array_almost_equal(z0, z0_expected)
        assert_array_almost_equal(z1, z1_expected)

    def test_zscore_ddof(self):
        x = numpy.array([[0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 2.0, 3.0]])
        z = scipy.stats.zscore(x, axis=1, ddof=1)
        z0_expected = (numpy.array([(- 0.5), (- 0.5), 0.5, 0.5]) / (1.0 / numpy.sqrt(3)))
        z1_expected = (numpy.array([(- 1.5), (- 0.5), 0.5, 1.5]) / numpy.sqrt((5.0 / 3)))
        assert_array_almost_equal(z[0], z0_expected)
        assert_array_almost_equal(z[1], z1_expected)

class _numpy_version_warn_context_mgr(object):
    '\n    A simple context maneger class to avoid retyping the same code for\n    different versions of numpy when the only difference is that older\n    versions raise warnings.\n\n    This manager does not apply for cases where the old code returns\n    different values.\n    '

    def __init__(self, min_numpy_version, warning_type, num_warnings):
        if (NumpyVersion(numpy.__version__) < min_numpy_version):
            self.numpy_is_old = True
            self.warning_type = warning_type
            self.num_warnings = num_warnings
            self.delegate = warnings.catch_warnings(record=True)
        else:
            self.numpy_is_old = False

    def __enter__(self):
        if self.numpy_is_old:
            self.warn_list = self.delegate.__enter__()
            warnings.simplefilter('always')
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        if self.numpy_is_old:
            self.delegate.__exit__(exc_type, exc_value, traceback)
            _check_warnings(self.warn_list, self.warning_type, self.num_warnings)

def _check_warnings(warn_list, expected_type, expected_len):
    '\n    Checks that all of the warnings from a list returned by\n    `warnings.catch_all(record=True)` are of the required type and that the list\n    contains expected number of warnings.\n    '
    assert_equal(len(warn_list), expected_len, 'number of warnings')
    for warn_ in warn_list:
        assert_((warn_.category is expected_type))

class TestIQR(TestCase):

    def test_basic(self):
        x = (numpy.arange(8) * 0.5)
        numpy.random.shuffle(x)
        assert_equal(scipy.stats.iqr(x), 1.75)

    def test_api(self):
        d = numpy.ones((5, 5))
        scipy.stats.iqr(d)
        scipy.stats.iqr(d, None)
        scipy.stats.iqr(d, 1)
        scipy.stats.iqr(d, (0, 1))
        scipy.stats.iqr(d, None, (10, 90))
        scipy.stats.iqr(d, None, (30, 20), 'raw')
        scipy.stats.iqr(d, None, (25, 75), 1.5, 'propagate')
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            scipy.stats.iqr(d, None, (50, 50), 'normal', 'raise', 'linear')
            scipy.stats.iqr(d, None, (25, 75), (- 0.4), 'omit', 'lower', True)

    def test_empty(self):
        assert_equal(scipy.stats.iqr([]), numpy.nan)
        assert_equal(scipy.stats.iqr(numpy.arange(0)), numpy.nan)

    def test_constant(self):
        x = numpy.ones((7, 4))
        assert_equal(scipy.stats.iqr(x), 0.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
        assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
        with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
        y = (numpy.ones((4, 5, 6)) * numpy.arange(6))
        assert_array_equal(scipy.stats.iqr(y, axis=0), numpy.zeros((5, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=1), numpy.zeros((4, 6)))
        assert_array_equal(scipy.stats.iqr(y, axis=2), (2.5 * numpy.ones((4, 5))))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 1)), numpy.zeros(6))
        assert_array_equal(scipy.stats.iqr(y, axis=(0, 2)), (3.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(y, axis=(1, 2)), (3.0 * numpy.ones(4)))

    def test_scalarlike(self):
        x = (numpy.arange(1) + 7.0)
        assert_equal(scipy.stats.iqr(x[0]), 0.0)
        assert_equal(scipy.stats.iqr(x), 0.0)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_array_equal(scipy.stats.iqr(x, keepdims=True), [0.0])
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_array_equal(scipy.stats.iqr(x, keepdims=True), 0.0)
                _check_warnings(w, RuntimeWarning, 1)

    def test_2D(self):
        x = np.arange(15).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=0), (5.0 * numpy.ones(5)))
        assert_array_equal(scipy.stats.iqr(x, axis=1), (2.0 * numpy.ones(3)))
        assert_array_equal(scipy.stats.iqr(x, axis=(0, 1)), 7.0)
        assert_array_equal(scipy.stats.iqr(x, axis=(1, 0)), 7.0)

    def test_axis(self):
        o = numpy.random.normal(size=(71, 23))
        x = numpy.dstack(([o] * 10))
        q = scipy.stats.iqr(o)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
        x = numpy.rollaxis(x, (- 1), 0)
        assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
        x = x.swapaxes(0, 1)
        assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
        assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
        d = numpy.arange((((3 * 5) * 7) * 11))
        numpy.random.shuffle(d)
        d = d.reshape((3, 5, 7, 11))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 2))[0], scipy.stats.iqr(d[:, :, :, 0].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(0, 1, 3))[1], scipy.stats.iqr(d[:, :, 1, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, (- 4)))[2], scipy.stats.iqr(d[:, :, 2, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 1, 2))[2], scipy.stats.iqr(d[2, :, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(3, 2))[(2, 1)], scipy.stats.iqr(d[2, 1, :, :].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, (- 2)))[(2, 1)], scipy.stats.iqr(d[2, :, :, 1].ravel()))
        assert_equal(scipy.stats.iqr(d, axis=(1, 3))[(2, 2)], scipy.stats.iqr(d[2, :, 2, :].ravel()))
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(IndexError, scipy.stats.iqr, d, axis=4)
        else:
            assert_raises(ValueError, scipy.stats.iqr, d, axis=4)
        assert_raises(ValueError, scipy.stats.iqr, d, axis=(0, 0))

    def test_rng(self):
        x = numpy.arange(5)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(x, rng=(25, 87.5)), 2.5)
        assert_equal(scipy.stats.iqr(x, rng=(12.5, 75)), 2.5)
        assert_almost_equal(scipy.stats.iqr(x, rng=(10, 50)), 1.6)
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(0, 101))
        assert_raises(ValueError, scipy.stats.iqr, x, rng=(numpy.nan, 25))
        assert_raises(TypeError, scipy.stats.iqr, x, rng=(0, 50, 60))

    def test_interpolation(self):
        x = numpy.arange(5)
        y = numpy.arange(4)
        assert_equal(scipy.stats.iqr(x), 2)
        assert_equal(scipy.stats.iqr(y), 1.5)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
            assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 3)
            assert_equal(scipy.stats.iqr(y, interpolation='higher'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='lower'), 2)
            assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
            assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1)
            if (NumpyVersion(numpy.__version__) >= '1.11.0a'):
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.5)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
            else:
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 2)
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='linear'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='linear'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='higher'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='higher'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='higher'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='lower'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='lower'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='lower'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 2)
                assert_equal(scipy.stats.iqr(y, interpolation='nearest'), 1.5)
                assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 2)
                assert_almost_equal(scipy.stats.iqr(x, rng=(25, 80), interpolation='midpoint'), 2.2)
                assert_equal(scipy.stats.iqr(y, interpolation='midpoint'), 1.5)
                _check_warnings(w, RuntimeWarning, 11)
        if (NumpyVersion(numpy.__version__) >= '1.9.0a'):
            assert_raises(ValueError, scipy.stats.iqr, x, interpolation='foobar')
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, interpolation='foobar'), 2)
                _check_warnings(w, RuntimeWarning, 1)

    def test_keepdims(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = numpy.ones((3, 5, 7, 11))
        assert_equal(stats.iqr(x, axis=None, keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=2, keepdims=False).shape, (3, 5, 11))
        assert_equal(stats.iqr(x, axis=(0, 1), keepdims=False).shape, (7, 11))
        assert_equal(stats.iqr(x, axis=(0, 3), keepdims=False).shape, (5, 7))
        assert_equal(stats.iqr(x, axis=(1,), keepdims=False).shape, (3, 7, 11))
        assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=False).shape, ())
        assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=False).shape, (7,))
        if (numpy_version >= '1.9.0a'):
            assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 1, 11))
            assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (1, 1, 7, 11))
            assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (1, 5, 7, 1))
            assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 1, 7, 11))
            assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, (1, 1, 1, 1))
            assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (1, 1, 7, 1))
        else:
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(stats.iqr(x, axis=None, keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=2, keepdims=True).shape, (3, 5, 11))
                assert_equal(stats.iqr(x, axis=(0, 1), keepdims=True).shape, (7, 11))
                assert_equal(stats.iqr(x, axis=(0, 3), keepdims=True).shape, (5, 7))
                assert_equal(stats.iqr(x, axis=(1,), keepdims=True).shape, (3, 7, 11))
                assert_equal(stats.iqr(x, (0, 1, 2, 3), keepdims=True).shape, ())
                assert_equal(stats.iqr(x, axis=(0, 1, 3), keepdims=True).shape, (7,))
                _check_warnings(w, RuntimeWarning, 7)

    def test_nanpolicy(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7)
        assert_equal(scipy.stats.iqr(x, nan_policy='raise'), 7)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='propagate'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.9.0a'):
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 8)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), [5, 5, numpy.nan, 5, 5])
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 3, 2])
                _check_warnings(w, RuntimeWarning, 3)
            else:
                assert_equal(scipy.stats.iqr(x, nan_policy='omit'), 7.5)
                assert_equal(scipy.stats.iqr(x, axis=0, nan_policy='omit'), (5 * numpy.ones(5)))
                assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='omit'), [2, 2.5, 2])
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=0, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, axis=1, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.iqr, x, nan_policy='barfood')

    def test_scale(self):
        numpy_version = NumpyVersion(numpy.__version__)
        x = np.arange(15.0).reshape((3, 5))
        assert_equal(scipy.stats.iqr(x, scale='raw'), 7)
        assert_almost_equal(scipy.stats.iqr(x, scale='normal'), (7 / 1.3489795))
        assert_equal(scipy.stats.iqr(x, scale=2.0), 3.5)
        x[(1, 2)] = numpy.nan
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            if (numpy_version < '1.10.0a'):
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), 4)
                if (numpy_version < '1.9.0a'):
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, 3, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, 3, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, 1.5, 1])
                else:
                    assert_equal(scipy.stats.iqr(x, axis=1, nan_policy='propagate'), [2, numpy.nan, 2])
                    assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                    assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
            else:
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale='normal', nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='propagate'), numpy.nan)
                assert_equal(scipy.stats.iqr(x, axis=1, scale='raw', nan_policy='propagate'), [2, numpy.nan, 2])
                assert_almost_equal(scipy.stats.iqr(x, axis=1, scale='normal', nan_policy='propagate'), (numpy.array([2, numpy.nan, 2]) / 1.3489795))
                assert_equal(scipy.stats.iqr(x, axis=1, scale=2.0, nan_policy='propagate'), [1, numpy.nan, 1])
                _check_warnings(w, RuntimeWarning, 6)
        if (numpy_version < '1.9.0a'):
            with warnings.catch_warnings(record=True) as w:
                warnings.simplefilter('always')
                assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 8)
                assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (8 / 1.3489795))
                assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 4)
                _check_warnings(w, RuntimeWarning, 3)
        else:
            assert_equal(scipy.stats.iqr(x, scale='raw', nan_policy='omit'), 7.5)
            assert_almost_equal(scipy.stats.iqr(x, scale='normal', nan_policy='omit'), (7.5 / 1.3489795))
            assert_equal(scipy.stats.iqr(x, scale=2.0, nan_policy='omit'), 3.75)
        assert_raises(ValueError, scipy.stats.iqr, x, scale='foobar')

class TestMoments(TestCase):
    '\n        Comparison numbers are found using R v.1.5.1\n        note that length(testcase) = 4\n        testmathworks comes from documentation for the\n        Statistics Toolbox for Matlab and can be found at both\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/kurtosis.shtml\n        http://www.mathworks.com/access/helpdesk/help/toolbox/stats/skewness.shtml\n        Note that both test cases came from here.\n    '
    testcase = [1, 2, 3, 4]
    scalar_testcase = 4.0
    numpy.random.seed(1234)
    testcase_moment_accuracy = numpy.random.rand(42)
    testmathworks = [1.165, 0.6268, 0.0751, 0.3516, (- 0.6965)]

    def test_moment(self):
        y = scipy.stats.moment(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 0)
        assert_approx_equal(y, 1.0)
        y = scipy.stats.moment(self.testcase, 1)
        assert_approx_equal(y, 0.0, 10)
        y = scipy.stats.moment(self.testcase, 2)
        assert_approx_equal(y, 1.25)
        y = scipy.stats.moment(self.testcase, 3)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.moment(self.testcase, 4)
        assert_approx_equal(y, 2.5625)
        y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment(self.testcase, 0.0)
        assert_approx_equal(y, 1.0)
        assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
        y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
        assert_allclose(y, [0, 1.25, 0, 2.5625])
        y = scipy.stats.moment([])
        assert_equal(y, numpy.nan)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.moment(x, 2), numpy.nan)
        assert_almost_equal(scipy.stats.moment(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.moment, x, nan_policy='foobar')

    def test_moment_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        mm = scipy.stats.moment(a, 2, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(mm, [1.25, numpy.nan], atol=1e-15)

    def test_variation(self):
        y = scipy.stats.variation(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.variation(self.testcase)
        assert_approx_equal(y, 0.44721359549996, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.variation(x), numpy.nan)
        assert_almost_equal(scipy.stats.variation(x, nan_policy='omit'), 0.6454972243679028)
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.variation, x, nan_policy='foobar')

    def test_variation_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        vv = scipy.stats.variation(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(vv, [0.7453559924999299, numpy.nan], atol=1e-15)

    def test_skewness(self):
        y = scipy.stats.skew(self.scalar_testcase)
        assert_approx_equal(y, 0.0)
        y = scipy.stats.skew(self.testmathworks)
        assert_approx_equal(y, (- 0.29322304336607), 10)
        y = scipy.stats.skew(self.testmathworks, bias=0)
        assert_approx_equal(y, (- 0.43711110502394), 10)
        y = scipy.stats.skew(self.testcase)
        assert_approx_equal(y, 0.0, 10)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.skew(x), numpy.nan)
        assert_equal(scipy.stats.skew(x, nan_policy='omit'), 0.0)
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.skew, x, nan_policy='foobar')

    def test_skewness_scalar(self):
        assert_equal(scipy.stats.skew(arange(10)), 0.0)

    def test_skew_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        s = scipy.stats.skew(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(s, [0, numpy.nan], atol=1e-15)

    def test_kurtosis(self):
        y = scipy.stats.kurtosis(self.scalar_testcase)
        assert_approx_equal(y, (- 3.0))
        y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
        assert_approx_equal(y, 2.1658856802973, 10)
        y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
        assert_approx_equal(y, 3.663542721189047, 10)
        y = scipy.stats.kurtosis(self.testcase, 0, 0)
        assert_approx_equal(y, 1.64)
        x = numpy.arange(10.0)
        x[9] = numpy.nan
        assert_equal(scipy.stats.kurtosis(x), numpy.nan)
        assert_almost_equal(scipy.stats.kurtosis(x, nan_policy='omit'), (- 1.23))
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='raise')
        assert_raises(ValueError, scipy.stats.kurtosis, x, nan_policy='foobar')

    def test_kurtosis_array_scalar(self):
        assert_equal(type(scipy.stats.kurtosis([1, 2, 3])), float)

    def test_kurtosis_propagate_nan(self):
        a = np.arange(8).reshape(2, (- 1)).astype(float)
        a[(1, 0)] = numpy.nan
        k = scipy.stats.kurtosis(a, axis=1, nan_policy='propagate')
        numpy.testing.assert_allclose(k, [(- 1.36), numpy.nan], atol=1e-15)

    def test_moment_accuracy(self):
        tc_no_mean = (self.testcase_moment_accuracy - numpy.mean(self.testcase_moment_accuracy))
        assert_allclose(np.power(tc_no_mean, 42).mean(), scipy.stats.moment(self.testcase_moment_accuracy, 42))

class TestThreshold(TestCase):

    def test_basic(self):
        a = [(- 1), 2, 3, 4, 5, (- 1), (- 2)]
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=DeprecationWarning)
            assert_array_equal(scipy.stats.threshold(a), a)
            assert_array_equal(scipy.stats.threshold(a, 3, None, 0), [0, 0, 3, 4, 5, 0, 0])
            assert_array_equal(scipy.stats.threshold(a, None, 3, 0), [(- 1), 2, 3, 0, 0, (- 1), (- 2)])
            assert_array_equal(scipy.stats.threshold(a, 2, 4, 0), [0, 2, 3, 4, 0, 0, 0])

class TestStudentTest(TestCase):
    X1 = numpy.array([(- 1), 0, 1])
    X2 = numpy.array([0, 1, 2])
    T1_0 = 0
    P1_0 = 1
    T1_1 = (- 1.732051)
    P1_1 = 0.2254033
    T1_2 = (- 3.464102)
    P1_2 = 0.0741799
    T2_0 = 1.732051
    P2_0 = 0.2254033

    def test_onesample(self):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            (t, p) = scipy.stats.ttest_1samp(4.0, 3.0)
        assert_(numpy.isnan(t))
        assert_(numpy.isnan(p))
        (t, p) = scipy.stats.ttest_1samp(self.X1, 0)
        assert_array_almost_equal(t, self.T1_0)
        assert_array_almost_equal(p, self.P1_0)
        res = scipy.stats.ttest_1samp(self.X1, 0)
        attributes = ('statistic', 'pvalue')
        check_named_results(res, attributes)
        (t, p) = scipy.stats.ttest_1samp(self.X2, 0)
        assert_array_almost_equal(t, self.T2_0)
        assert_array_almost_equal(p, self.P2_0)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 1)
        assert_array_almost_equal(t, self.T1_1)
        assert_array_almost_equal(p, self.P1_1)
        (t, p) = scipy.stats.ttest_1samp(self.X1, 2)
        assert_array_almost_equal(t, self.T1_2)
        assert_array_almost_equal(p, self.P1_2)
        numpy.random.seed(7654567)
        x = scipy.stats.norm.rvs(loc=5, scale=10, size=51)
        x[50] = numpy.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', category=RuntimeWarning)
            assert_array_equal(scipy.stats.ttest_1samp(x, 5.0), (numpy.nan, numpy.nan))
            assert_array_almost_equal(scipy.stats.ttest_1samp(x, 5.0, nan_policy='omit'), ((- 1.641262407436716), 0.107147027334048))
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='raise')
            assert_raises(ValueError, scipy.stats.ttest_1samp, x, 5.0, nan_policy='foobar')

def test_percentileofscore():
    pcos = scipy.stats.percentileofscore
    assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos((numpy.arange(10) + 1), 4, kind=kind), result))
    for (kind, result) in [('rank', 45.0), ('strict', 30.0), ('weak', 50.0), ('mean', 40.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 5, 6, 7, 8, 9], 4, kind=kind), result))
    assert_equal(pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4), 50.0)
    for (kind, result) in [('rank', 50.0), ('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([1, 2, 3, 4, 4, 4, 5, 6, 7, 8], 4, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([1, 2, 3, 5, 6, 7, 8, 9, 10, 11], 4, kind=kind), 30))
    for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 50, 60, 70, 80, 90, 100], 40, kind=kind), result))
    for (kind, result) in [('mean', 45.0), ('strict', 30.0), ('weak', 60.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 40, 40, 40, 50, 60, 70, 80], 40, kind=kind), result))
    for kind in ('rank', 'mean', 'strict', 'weak'):
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 40, kind=kind), 30.0))
    for (kind, result) in [('rank', 10.0), ('mean', 5.0), ('strict', 0.0), ('weak', 10.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 10, kind=kind), result))
    for (kind, result) in [('rank', 100.0), ('mean', 95.0), ('strict', 90.0), ('weak', 100.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], 110, kind=kind), result))
    for (kind, score, result) in [('rank', 200, 100.0), ('mean', 200, 100.0), ('mean', 0, 0.0)]:
        (yield (assert_equal, pcos([10, 20, 30, 50, 60, 70, 80, 90, 100, 110], score, kind=kind), result))
    assert_raises(ValueError, pcos, [1, 2, 3, 3, 4], 3, kind='unrecognized')
PowerDivCase = namedtuple('Case', ['f_obs', 'f_exp', 'ddof', 'axis', 'chi2', 'log', 'mod_log', 'cr'])
tempResult = log((8 / 12))
	
===================================================================	
test_chisquare_masked_arrays: 1756	
----------------------------	

obs = np.array([[8, 8, 16, 32, (- 1)], [(- 1), (- 1), 3, 4, 5]]).T
mask = np.array([[0, 0, 0, 0, 1], [1, 1, 0, 0, 0]]).T
mobs = numpy.ma.masked_array(obs, mask)
expected_chisq = numpy.array([24.0, 0.5])
tempResult = log(0.5)
	
===================================================================	
test_chisquare_masked_arrays: 1756	
----------------------------	

obs = np.array([[8, 8, 16, 32, (- 1)], [(- 1), (- 1), 3, 4, 5]]).T
mask = np.array([[0, 0, 0, 0, 1], [1, 1, 0, 0, 0]]).T
mobs = numpy.ma.masked_array(obs, mask)
expected_chisq = numpy.array([24.0, 0.5])
tempResult = log(2.0)
	
===================================================================	
test_chisquare_masked_arrays: 1756	
----------------------------	

obs = np.array([[8, 8, 16, 32, (- 1)], [(- 1), (- 1), 3, 4, 5]]).T
mask = np.array([[0, 0, 0, 0, 1], [1, 1, 0, 0, 0]]).T
mobs = numpy.ma.masked_array(obs, mask)
expected_chisq = numpy.array([24.0, 0.5])
tempResult = log(0.75)
	
===================================================================	
test_chisquare_masked_arrays: 1756	
----------------------------	

obs = np.array([[8, 8, 16, 32, (- 1)], [(- 1), (- 1), 3, 4, 5]]).T
mask = np.array([[0, 0, 0, 0, 1], [1, 1, 0, 0, 0]]).T
mobs = numpy.ma.masked_array(obs, mask)
expected_chisq = numpy.array([24.0, 0.5])
tempResult = log(1.25)
	
***************************************************	
sklearn_sklearn-0.18.0: 171	
===================================================================	
objective: 159	
----------------------------	

E = numpy.exp(((AB[0] * F) + AB[1]))
P = (1.0 / (1.0 + E))
tempResult = log((P + tiny))
	
===================================================================	
objective: 159	
----------------------------	

E = numpy.exp(((AB[0] * F) + AB[1]))
P = (1.0 / (1.0 + E))
tempResult = log(((1.0 - P) + tiny))
	
===================================================================	
LinearDiscriminantAnalysis.predict_log_proba: 187	
----------------------------	

'Estimate log probability.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Input data.\n\n        Returns\n        -------\n        C : array, shape (n_samples, n_classes)\n            Estimated log probabilities.\n        '
tempResult = log(self.predict_proba(X))
	
===================================================================	
LinearDiscriminantAnalysis._solve_eigen: 90	
----------------------------	

"Eigenvalue solver.\n\n        The eigenvalue solver computes the optimal solution of the Rayleigh\n        coefficient (basically the ratio of between class scatter to within\n        class scatter). This solver supports both classification and\n        dimensionality reduction (with optional shrinkage).\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n\n        shrinkage : string or float, optional\n            Shrinkage parameter, possible values:\n              - None: no shrinkage (default).\n              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n              - float between 0 and 1: fixed shrinkage constant.\n\n        Notes\n        -----\n        This solver is based on [1]_, section 3.8.3, pp. 121-124.\n\n        References\n        ----------\n        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\n           (Second Edition). John Wiley & Sons, Inc., New York, 2001. ISBN\n           0-471-05669-3.\n        "
self.means_ = _class_means(X, y)
self.covariance_ = _class_cov(X, y, self.priors_, shrinkage)
Sw = self.covariance_
St = _cov(X, shrinkage)
Sb = (St - Sw)
(evals, evecs) = scipy.linalg.eigh(Sb, Sw)
self.explained_variance_ratio_ = numpy.sort((evals / numpy.sum(evals)))[::(- 1)]
evecs = evecs[:, numpy.argsort(evals)[::(- 1)]]
evecs /= numpy.apply_along_axis(numpy.linalg.norm, 0, evecs)
self.scalings_ = evecs
self.coef_ = np.dot(self.means_, evecs).dot(evecs.T)
tempResult = log(self.priors_)
	
===================================================================	
LinearDiscriminantAnalysis._solve_lsqr: 75	
----------------------------	

"Least squares solver.\n\n        The least squares solver computes a straightforward solution of the\n        optimal decision rule based directly on the discriminant functions. It\n        can only be used for classification (with optional shrinkage), because\n        estimation of eigenvectors is not performed. Therefore, dimensionality\n        reduction with the transform is not supported.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_classes)\n            Target values.\n\n        shrinkage : string or float, optional\n            Shrinkage parameter, possible values:\n              - None: no shrinkage (default).\n              - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.\n              - float between 0 and 1: fixed shrinkage parameter.\n\n        Notes\n        -----\n        This solver is based on [1]_, section 2.6.2, pp. 39-41.\n\n        References\n        ----------\n        .. [1] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\n           (Second Edition). John Wiley & Sons, Inc., New York, 2001. ISBN\n           0-471-05669-3.\n        "
self.means_ = _class_means(X, y)
self.covariance_ = _class_cov(X, y, self.priors_, shrinkage)
self.coef_ = linalg.lstsq(self.covariance_, self.means_.T)[0].T
tempResult = log(self.priors_)
	
===================================================================	
QuadraticDiscriminantAnalysis.predict_log_proba: 283	
----------------------------	

'Return posterior probabilities of classification.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Array of samples/test vectors.\n\n        Returns\n        -------\n        C : array, shape = [n_samples, n_classes]\n            Posterior log-probabilities of classification per class.\n        '
probas_ = self.predict_proba(X)
tempResult = log(probas_)
	
===================================================================	
QuadraticDiscriminantAnalysis._decision_function: 258	
----------------------------	

check_is_fitted(self, 'classes_')
X = check_array(X)
norm2 = []
for i in range(len(self.classes_)):
    R = self.rotations_[i]
    S = self.scalings_[i]
    Xm = (X - self.means_[i])
    X2 = numpy.dot(Xm, (R * (S ** (- 0.5))))
    norm2.append(numpy.sum((X2 ** 2), 1))
norm2 = np.array(norm2).T
tempResult = log(s)
	
===================================================================	
QuadraticDiscriminantAnalysis._decision_function: 259	
----------------------------	

check_is_fitted(self, 'classes_')
X = check_array(X)
norm2 = []
for i in range(len(self.classes_)):
    R = self.rotations_[i]
    S = self.scalings_[i]
    Xm = (X - self.means_[i])
    X2 = numpy.dot(Xm, (R * (S ** (- 0.5))))
    norm2.append(numpy.sum((X2 ** 2), 1))
norm2 = np.array(norm2).T
u = numpy.asarray([numpy.sum(numpy.log(s)) for s in self.scalings_])
tempResult = log(self.priors_)
	
===================================================================	
LinearDiscriminantAnalysis._solve_svd: 120	
----------------------------	

'SVD solver.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        y : array-like, shape (n_samples,) or (n_samples, n_targets)\n            Target values.\n        '
(n_samples, n_features) = X.shape
n_classes = len(self.classes_)
self.means_ = _class_means(X, y)
if self.store_covariance:
    self.covariance_ = _class_cov(X, y, self.priors_)
Xc = []
for (idx, group) in enumerate(self.classes_):
    Xg = X[(y == group), :]
    Xc.append((Xg - self.means_[idx]))
self.xbar_ = numpy.dot(self.priors_, self.means_)
Xc = numpy.concatenate(Xc, axis=0)
std = Xc.std(axis=0)
std[(std == 0)] = 1.0
fac = (1.0 / (n_samples - n_classes))
X = (numpy.sqrt(fac) * (Xc / std))
(U, S, V) = scipy.linalg.svd(X, full_matrices=False)
rank = numpy.sum((S > self.tol))
if (rank < n_features):
    warnings.warn('Variables are collinear.')
scalings = ((V[:rank] / std).T / S[:rank])
X = numpy.dot((np.sqrt(((n_samples * self.priors_) * fac)) * (self.means_ - self.xbar_).T).T, scalings)
(_, S, V) = scipy.linalg.svd(X, full_matrices=0)
self.explained_variance_ratio_ = ((S ** 2) / numpy.sum((S ** 2)))[:self.n_components]
rank = numpy.sum((S > (self.tol * S[0])))
self.scalings_ = numpy.dot(scalings, V.T[:, :rank])
coef = numpy.dot((self.means_ - self.xbar_), self.scalings_)
tempResult = log(self.priors_)
	
===================================================================	
DummyClassifier.predict_log_proba: 138	
----------------------------	

'\n        Return log probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        P : array-like or list of array-like of shape = [n_samples, n_classes]\n            Returns the log probability of the sample for each class in\n            the model, where classes are ordered arithmetically for each\n            output.\n        '
proba = self.predict_proba(X)
if (self.n_outputs_ == 1):
    tempResult = log(proba)
	
===================================================================	
DummyClassifier.predict_log_proba: 140	
----------------------------	

'\n        Return log probability estimates for the test vectors X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        P : array-like or list of array-like of shape = [n_samples, n_classes]\n            Returns the log probability of the sample for each class in\n            the model, where classes are ordered arithmetically for each\n            output.\n        '
proba = self.predict_proba(X)
if (self.n_outputs_ == 1):
    return numpy.log(proba)
else:
    tempResult = log(p)
	
===================================================================	
SkewedChi2Sampler.fit: 55	
----------------------------	

'Fit the model with X.\n\n        Samples random projection according to n_features.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        self : object\n            Returns the transformer.\n        '
X = check_array(X)
random_state = check_random_state(self.random_state)
n_features = X.shape[1]
uniform = random_state.uniform(size=(n_features, self.n_components))
tempResult = log(numpy.tan(((numpy.pi / 2.0) * uniform)))
	
===================================================================	
AdditiveChi2Sampler._transform_dense: 114	
----------------------------	

non_zero = (X != 0.0)
X_nz = X[non_zero]
X_step = numpy.zeros_like(X)
X_step[non_zero] = numpy.sqrt((X_nz * self.sample_interval_))
X_new = [X_step]
tempResult = log(X_nz)
	
===================================================================	
SkewedChi2Sampler.transform: 67	
----------------------------	

'Apply the approximate feature map to X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            New data, where n_samples in the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        X_new : array-like, shape (n_samples, n_components)\n        '
check_is_fitted(self, 'random_weights_')
X = as_float_array(X, copy=True)
X = check_array(X, copy=False)
if (X < 0).any():
    raise ValueError('X may not contain entries smaller than zero.')
X += self.skewedness
tempResult = log(X, X)
	
===================================================================	
AdditiveChi2Sampler._transform_sparse: 132	
----------------------------	

indices = X.indices.copy()
indptr = X.indptr.copy()
data_step = numpy.sqrt((X.data * self.sample_interval_))
X_step = scipy.sparse.csr_matrix((data_step, indices, indptr), shape=X.shape, dtype=X.dtype, copy=False)
X_new = [X_step]
tempResult = log(X.data)
	
===================================================================	
MultinomialNB._update_feature_log_prob: 233	
----------------------------	

'Apply smoothing to raw counts and recompute log probabilities'
smoothed_fc = (self.feature_count_ + self.alpha)
smoothed_cc = smoothed_fc.sum(axis=1)
tempResult = log(smoothed_fc)
	
===================================================================	
MultinomialNB._update_feature_log_prob: 233	
----------------------------	

'Apply smoothing to raw counts and recompute log probabilities'
smoothed_fc = (self.feature_count_ + self.alpha)
smoothed_cc = smoothed_fc.sum(axis=1)
tempResult = log(smoothed_cc.reshape((- 1), 1))
	
===================================================================	
BernoulliNB._joint_log_likelihood: 273	
----------------------------	

'Calculate the posterior log probability of the samples X'
check_is_fitted(self, 'classes_')
X = check_array(X, accept_sparse='csr')
if (self.binarize is not None):
    X = binarize(X, threshold=self.binarize)
(n_classes, n_features) = self.feature_log_prob_.shape
(n_samples, n_features_X) = X.shape
if (n_features_X != n_features):
    raise ValueError(('Expected input with %d features, got %d instead' % (n_features, n_features_X)))
tempResult = log((1 - numpy.exp(self.feature_log_prob_)))
	
===================================================================	
GaussianNB._joint_log_likelihood: 135	
----------------------------	

check_is_fitted(self, 'classes_')
X = check_array(X)
joint_log_likelihood = []
for i in range(numpy.size(self.classes_)):
    tempResult = log(self.class_prior_[i])
	
===================================================================	
GaussianNB._joint_log_likelihood: 136	
----------------------------	

check_is_fitted(self, 'classes_')
X = check_array(X)
joint_log_likelihood = []
for i in range(numpy.size(self.classes_)):
    jointi = numpy.log(self.class_prior_[i])
    tempResult = log(((2.0 * numpy.pi) * self.sigma_[i, :]))
	
===================================================================	
BaseDiscreteNB._update_class_log_prior: 150	
----------------------------	

n_classes = len(self.classes_)
if (class_prior is not None):
    if (len(class_prior) != n_classes):
        raise ValueError('Number of priors must match number of classes.')
    tempResult = log(class_prior)
	
===================================================================	
BaseDiscreteNB._update_class_log_prior: 152	
----------------------------	

n_classes = len(self.classes_)
if (class_prior is not None):
    if (len(class_prior) != n_classes):
        raise ValueError('Number of priors must match number of classes.')
    self.class_log_prior_ = numpy.log(class_prior)
elif self.fit_prior:
    tempResult = log(self.class_count_)
	
===================================================================	
BaseDiscreteNB._update_class_log_prior: 152	
----------------------------	

n_classes = len(self.classes_)
if (class_prior is not None):
    if (len(class_prior) != n_classes):
        raise ValueError('Number of priors must match number of classes.')
    self.class_log_prior_ = numpy.log(class_prior)
elif self.fit_prior:
    tempResult = log(self.class_count_.sum())
	
===================================================================	
BaseDiscreteNB._update_class_log_prior: 154	
----------------------------	

n_classes = len(self.classes_)
if (class_prior is not None):
    if (len(class_prior) != n_classes):
        raise ValueError('Number of priors must match number of classes.')
    self.class_log_prior_ = numpy.log(class_prior)
elif self.fit_prior:
    self.class_log_prior_ = (numpy.log(self.class_count_) - numpy.log(self.class_count_.sum()))
else:
    tempResult = log(n_classes)
	
===================================================================	
BernoulliNB._update_feature_log_prob: 261	
----------------------------	

'Apply smoothing to raw counts and recompute log probabilities'
smoothed_fc = (self.feature_count_ + self.alpha)
smoothed_cc = (self.class_count_ + (self.alpha * 2))
tempResult = log(smoothed_fc)
	
===================================================================	
BernoulliNB._update_feature_log_prob: 261	
----------------------------	

'Apply smoothing to raw counts and recompute log probabilities'
smoothed_fc = (self.feature_count_ + self.alpha)
smoothed_cc = (self.class_count_ + (self.alpha * 2))
tempResult = log(smoothed_cc.reshape((- 1), 1))
	
===================================================================	
johnson_lindenstrauss_min_dim: 29	
----------------------------	

'Find a \'safe\' number of components to randomly project to\n\n    The distortion introduced by a random projection `p` only changes the\n    distance between two points by a factor (1 +- eps) in an euclidean space\n    with good probability. The projection `p` is an eps-embedding as defined\n    by:\n\n      (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2\n\n    Where u and v are any rows taken from a dataset of shape [n_samples,\n    n_features], eps is in ]0, 1[ and p is a projection by a random Gaussian\n    N(0, 1) matrix with shape [n_components, n_features] (or a sparse\n    Achlioptas matrix).\n\n    The minimum number of components to guarantee the eps-embedding is\n    given by:\n\n      n_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3)\n\n    Note that the number of dimensions is independent of the original\n    number of features but instead depends on the size of the dataset:\n    the larger the dataset, the higher is the minimal dimensionality of\n    an eps-embedding.\n\n    Read more in the :ref:`User Guide <johnson_lindenstrauss>`.\n\n    Parameters\n    ----------\n    n_samples : int or numpy array of int greater than 0,\n        Number of samples. If an array is given, it will compute\n        a safe number of components array-wise.\n\n    eps : float or numpy array of float in ]0,1[, optional (default=0.1)\n        Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma.\n        If an array is given, it will compute a safe number of components\n        array-wise.\n\n    Returns\n    -------\n    n_components : int or numpy array of int,\n        The minimal number of components to guarantee with good probability\n        an eps-embedding with n_samples.\n\n    Examples\n    --------\n\n    >>> johnson_lindenstrauss_min_dim(1e6, eps=0.5)\n    663\n\n    >>> johnson_lindenstrauss_min_dim(1e6, eps=[0.5, 0.1, 0.01])\n    array([    663,   11841, 1112658])\n\n    >>> johnson_lindenstrauss_min_dim([1e4, 1e5, 1e6], eps=0.1)\n    array([ 7894,  9868, 11841])\n\n    References\n    ----------\n\n    .. [1] https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma\n\n    .. [2] Sanjoy Dasgupta and Anupam Gupta, 1999,\n           "An elementary proof of the Johnson-Lindenstrauss Lemma."\n           http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654\n\n    '
eps = numpy.asarray(eps)
n_samples = numpy.asarray(n_samples)
if (numpy.any((eps <= 0.0)) or numpy.any((eps >= 1))):
    raise ValueError(('The JL bound is defined for eps in ]0, 1[, got %r' % eps))
if (numpy.any(n_samples) <= 0):
    raise ValueError(('The JL bound is defined for n_samples greater than zero, got %r' % n_samples))
denominator = (((eps ** 2) / 2) - ((eps ** 3) / 3))
tempResult = log(n_samples)
	
===================================================================	
_log_normalize: 53	
----------------------------	

"Normalize ``X`` according to Kluger's log-interactions scheme."
X = make_nonnegative(X, min_value=1)
if issparse(X):
    raise ValueError('Cannot compute log of a sparse matrix, because log(x) diverges to -infinity as x goes to 0.')
tempResult = log(X)
	
===================================================================	
_k_init: 31	
----------------------------	

'Init n_clusters seeds according to k-means++\n\n    Parameters\n    -----------\n    X: array or sparse matrix, shape (n_samples, n_features)\n        The data to pick seeds for. To avoid memory copy, the input data\n        should be double precision (dtype=np.float64).\n\n    n_clusters: integer\n        The number of seeds to choose\n\n    x_squared_norms: array, shape (n_samples,)\n        Squared Euclidean norm of each data point.\n\n    random_state: numpy.RandomState\n        The generator used to initialize the centers.\n\n    n_local_trials: integer, optional\n        The number of seeding trials for each center (except the first),\n        of which the one reducing inertia the most is greedily chosen.\n        Set to None to make the number of trials depend logarithmically\n        on the number of seeds (2+log(k)); this is the default.\n\n    Notes\n    -----\n    Selects initial cluster centers for k-mean clustering in a smart way\n    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n    "k-means++: the advantages of careful seeding". ACM-SIAM symposium\n    on Discrete algorithms. 2007\n\n    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n    which is the implementation used in the aforementioned paper.\n    '
(n_samples, n_features) = X.shape
centers = numpy.empty((n_clusters, n_features), dtype=X.dtype)
assert (x_squared_norms is not None), 'x_squared_norms None in _k_init'
if (n_local_trials is None):
    tempResult = log(n_clusters)
	
===================================================================	
log_likelihood: 15	
----------------------------	

'Computes the sample mean of the log_likelihood under a covariance model\n\n    computes the empirical expected log-likelihood (accounting for the\n    normalization terms and scaling), allowing for universal comparison (beyond\n    this software package)\n\n    Parameters\n    ----------\n    emp_cov : 2D ndarray (n_features, n_features)\n        Maximum Likelihood Estimator of covariance\n\n    precision : 2D ndarray (n_features, n_features)\n        The precision matrix of the covariance model to be tested\n\n    Returns\n    -------\n    sample mean of the log-likelihood\n    '
p = precision.shape[0]
log_likelihood_ = ((- numpy.sum((emp_cov * precision))) + fast_logdet(precision))
tempResult = log((2 * numpy.pi))
	
===================================================================	
_objective: 22	
----------------------------	

'Evaluation of the graph-lasso objective function\n\n    the objective function is made of a shifted scaled version of the\n    normalized log-likelihood (i.e. its empirical mean over the samples) and a\n    penalisation term to promote sparsity\n    '
p = precision_.shape[0]
tempResult = log((2 * numpy.pi))
	
===================================================================	
graph_lasso: 46	
----------------------------	

"l1-penalized covariance estimator\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    Parameters\n    ----------\n    emp_cov : 2D ndarray, shape (n_features, n_features)\n        Empirical covariance from which to compute the covariance estimate.\n\n    alpha : positive float\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n\n    cov_init : 2D array (n_features, n_features), optional\n        The initial guess for the covariance.\n\n    mode : {'cd', 'lars'}\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    tol : positive float, optional\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped.\n\n    enet_tol : positive float, optional\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'.\n\n    max_iter : integer, optional\n        The maximum number of iterations.\n\n    verbose : boolean, optional\n        If verbose is True, the objective function and dual gap are\n        printed at each iteration.\n\n    return_costs : boolean, optional\n        If return_costs is True, the objective function and dual gap\n        at each iteration are returned.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems.\n\n    return_n_iter : bool, optional\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    covariance : 2D ndarray, shape (n_features, n_features)\n        The estimated covariance matrix.\n\n    precision : 2D ndarray, shape (n_features, n_features)\n        The estimated (sparse) precision matrix.\n\n    costs : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n    n_iter : int\n        Number of iterations. Returned only if `return_n_iter` is set to True.\n\n    See Also\n    --------\n    GraphLasso, GraphLassoCV\n\n    Notes\n    -----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.\n\n    "
(_, n_features) = emp_cov.shape
if (alpha == 0):
    if return_costs:
        precision_ = scipy.linalg.inv(emp_cov)
        cost = ((- 2.0) * log_likelihood(emp_cov, precision_))
        tempResult = log((2 * numpy.pi))
	
===================================================================	
fetch_kddcup99: 47	
----------------------------	

"Load and return the kddcup 99 dataset (classification).\n\n    The KDD Cup '99 dataset was created by processing the tcpdump portions\n    of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n    created by MIT Lincoln Lab [1] . The artificial data was generated using\n    a closed network and hand-injected attacks to produce a large number of\n    different types of attack with normal activity in the background.\n    As the initial goal was to produce a large training set for supervised\n    learning algorithms, there is a large proportion (80.1%) of abnormal\n    data which is unrealistic in real world, and inappropriate for unsupervised\n    anomaly detection which aims at detecting 'abnormal' data, ie\n\n    1) qualitatively different from normal data.\n\n    2) in large minority among the observations.\n\n    We thus transform the KDD Data set into two different data sets: SA and SF.\n\n    - SA is obtained by simply selecting all the normal data, and a small\n      proportion of abnormal data to gives an anomaly proportion of 1%.\n\n    - SF is obtained as in [2]\n      by simply picking up the data whose attribute logged_in is positive, thus\n      focusing on the intrusion attack, which gives a proportion of 0.3% of\n      attack.\n\n    - http and smtp are two subsets of SF corresponding with third feature\n      equal to 'http' (resp. to 'smtp')\n\n\n    General KDD structure :\n\n    ================      ==========================================\n    Samples total         4898431\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SA structure :\n\n    ================      ==========================================\n    Samples total         976158\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SF structure :\n\n    ================      ==========================================\n    Samples total         699691\n    Dimensionality        4\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    http structure :\n\n    ================      ==========================================\n    Samples total         619052\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    smtp structure :\n\n    ================      ==========================================\n    Samples total         95373\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    subset : None, 'SA', 'SF', 'http', 'smtp'\n        To return the corresponding classical subsets of kddcup 99.\n        If None, return the entire kddcup 99 dataset.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Random state for shuffling the dataset.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    percent10 : bool, default=False\n        Whether to load only 10 percent of the data.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    Returns\n    -------\n    data : Bunch\n        Dictionary-like object, the interesting attributes are:\n        'data', the data to learn and 'target', the regression target for each\n        sample.\n\n\n    References\n    ----------\n    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n           Detection Evaluation Richard Lippmann, Joshua W. Haines,\n           David J. Fried, Jonathan Korba, Kumar Das\n\n    .. [2] A Geometric Framework for Unsupervised Anomaly Detection: Detecting\n           Intrusions in Unlabeled Data (2002) by Eleazar Eskin, Andrew Arnold,\n           Michael Prerau, Leonid Portnoy, Sal Stolfo\n\n    "
kddcup99 = _fetch_brute_kddcup99(shuffle=shuffle, percent10=percent10, download_if_missing=download_if_missing)
data = kddcup99.data
target = kddcup99.target
if (subset == 'SA'):
    s = (target == b'normal.')
    t = numpy.logical_not(s)
    normal_samples = data[s, :]
    normal_targets = target[s]
    abnormal_samples = data[t, :]
    abnormal_targets = target[t]
    n_samples_abnormal = abnormal_samples.shape[0]
    random_state = check_random_state(random_state)
    r = random_state.randint(0, n_samples_abnormal, 3377)
    abnormal_samples = abnormal_samples[r]
    abnormal_targets = abnormal_targets[r]
    data = numpy.r_[(normal_samples, abnormal_samples)]
    target = numpy.r_[(normal_targets, abnormal_targets)]
if ((subset == 'SF') or (subset == 'http') or (subset == 'smtp')):
    s = (data[:, 11] == 1)
    data = numpy.c_[(data[s, :11], data[s, 12:])]
    target = target[s]
    tempResult = log((data[:, 0] + 0.1).astype(float))
	
===================================================================	
fetch_kddcup99: 48	
----------------------------	

"Load and return the kddcup 99 dataset (classification).\n\n    The KDD Cup '99 dataset was created by processing the tcpdump portions\n    of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n    created by MIT Lincoln Lab [1] . The artificial data was generated using\n    a closed network and hand-injected attacks to produce a large number of\n    different types of attack with normal activity in the background.\n    As the initial goal was to produce a large training set for supervised\n    learning algorithms, there is a large proportion (80.1%) of abnormal\n    data which is unrealistic in real world, and inappropriate for unsupervised\n    anomaly detection which aims at detecting 'abnormal' data, ie\n\n    1) qualitatively different from normal data.\n\n    2) in large minority among the observations.\n\n    We thus transform the KDD Data set into two different data sets: SA and SF.\n\n    - SA is obtained by simply selecting all the normal data, and a small\n      proportion of abnormal data to gives an anomaly proportion of 1%.\n\n    - SF is obtained as in [2]\n      by simply picking up the data whose attribute logged_in is positive, thus\n      focusing on the intrusion attack, which gives a proportion of 0.3% of\n      attack.\n\n    - http and smtp are two subsets of SF corresponding with third feature\n      equal to 'http' (resp. to 'smtp')\n\n\n    General KDD structure :\n\n    ================      ==========================================\n    Samples total         4898431\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SA structure :\n\n    ================      ==========================================\n    Samples total         976158\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SF structure :\n\n    ================      ==========================================\n    Samples total         699691\n    Dimensionality        4\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    http structure :\n\n    ================      ==========================================\n    Samples total         619052\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    smtp structure :\n\n    ================      ==========================================\n    Samples total         95373\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    subset : None, 'SA', 'SF', 'http', 'smtp'\n        To return the corresponding classical subsets of kddcup 99.\n        If None, return the entire kddcup 99 dataset.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Random state for shuffling the dataset.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    percent10 : bool, default=False\n        Whether to load only 10 percent of the data.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    Returns\n    -------\n    data : Bunch\n        Dictionary-like object, the interesting attributes are:\n        'data', the data to learn and 'target', the regression target for each\n        sample.\n\n\n    References\n    ----------\n    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n           Detection Evaluation Richard Lippmann, Joshua W. Haines,\n           David J. Fried, Jonathan Korba, Kumar Das\n\n    .. [2] A Geometric Framework for Unsupervised Anomaly Detection: Detecting\n           Intrusions in Unlabeled Data (2002) by Eleazar Eskin, Andrew Arnold,\n           Michael Prerau, Leonid Portnoy, Sal Stolfo\n\n    "
kddcup99 = _fetch_brute_kddcup99(shuffle=shuffle, percent10=percent10, download_if_missing=download_if_missing)
data = kddcup99.data
target = kddcup99.target
if (subset == 'SA'):
    s = (target == b'normal.')
    t = numpy.logical_not(s)
    normal_samples = data[s, :]
    normal_targets = target[s]
    abnormal_samples = data[t, :]
    abnormal_targets = target[t]
    n_samples_abnormal = abnormal_samples.shape[0]
    random_state = check_random_state(random_state)
    r = random_state.randint(0, n_samples_abnormal, 3377)
    abnormal_samples = abnormal_samples[r]
    abnormal_targets = abnormal_targets[r]
    data = numpy.r_[(normal_samples, abnormal_samples)]
    target = numpy.r_[(normal_targets, abnormal_targets)]
if ((subset == 'SF') or (subset == 'http') or (subset == 'smtp')):
    s = (data[:, 11] == 1)
    data = numpy.c_[(data[s, :11], data[s, 12:])]
    target = target[s]
    data[:, 0] = numpy.log((data[:, 0] + 0.1).astype(float))
    tempResult = log((data[:, 4] + 0.1).astype(float))
	
===================================================================	
fetch_kddcup99: 49	
----------------------------	

"Load and return the kddcup 99 dataset (classification).\n\n    The KDD Cup '99 dataset was created by processing the tcpdump portions\n    of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n    created by MIT Lincoln Lab [1] . The artificial data was generated using\n    a closed network and hand-injected attacks to produce a large number of\n    different types of attack with normal activity in the background.\n    As the initial goal was to produce a large training set for supervised\n    learning algorithms, there is a large proportion (80.1%) of abnormal\n    data which is unrealistic in real world, and inappropriate for unsupervised\n    anomaly detection which aims at detecting 'abnormal' data, ie\n\n    1) qualitatively different from normal data.\n\n    2) in large minority among the observations.\n\n    We thus transform the KDD Data set into two different data sets: SA and SF.\n\n    - SA is obtained by simply selecting all the normal data, and a small\n      proportion of abnormal data to gives an anomaly proportion of 1%.\n\n    - SF is obtained as in [2]\n      by simply picking up the data whose attribute logged_in is positive, thus\n      focusing on the intrusion attack, which gives a proportion of 0.3% of\n      attack.\n\n    - http and smtp are two subsets of SF corresponding with third feature\n      equal to 'http' (resp. to 'smtp')\n\n\n    General KDD structure :\n\n    ================      ==========================================\n    Samples total         4898431\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SA structure :\n\n    ================      ==========================================\n    Samples total         976158\n    Dimensionality        41\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    SF structure :\n\n    ================      ==========================================\n    Samples total         699691\n    Dimensionality        4\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    http structure :\n\n    ================      ==========================================\n    Samples total         619052\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    smtp structure :\n\n    ================      ==========================================\n    Samples total         95373\n    Dimensionality        3\n    Features              discrete (int) or continuous (float)\n    Targets               str, 'normal.' or name of the anomaly type\n    ================      ==========================================\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    subset : None, 'SA', 'SF', 'http', 'smtp'\n        To return the corresponding classical subsets of kddcup 99.\n        If None, return the entire kddcup 99 dataset.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Random state for shuffling the dataset.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    percent10 : bool, default=False\n        Whether to load only 10 percent of the data.\n\n    download_if_missing : bool, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    Returns\n    -------\n    data : Bunch\n        Dictionary-like object, the interesting attributes are:\n        'data', the data to learn and 'target', the regression target for each\n        sample.\n\n\n    References\n    ----------\n    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n           Detection Evaluation Richard Lippmann, Joshua W. Haines,\n           David J. Fried, Jonathan Korba, Kumar Das\n\n    .. [2] A Geometric Framework for Unsupervised Anomaly Detection: Detecting\n           Intrusions in Unlabeled Data (2002) by Eleazar Eskin, Andrew Arnold,\n           Michael Prerau, Leonid Portnoy, Sal Stolfo\n\n    "
kddcup99 = _fetch_brute_kddcup99(shuffle=shuffle, percent10=percent10, download_if_missing=download_if_missing)
data = kddcup99.data
target = kddcup99.target
if (subset == 'SA'):
    s = (target == b'normal.')
    t = numpy.logical_not(s)
    normal_samples = data[s, :]
    normal_targets = target[s]
    abnormal_samples = data[t, :]
    abnormal_targets = target[t]
    n_samples_abnormal = abnormal_samples.shape[0]
    random_state = check_random_state(random_state)
    r = random_state.randint(0, n_samples_abnormal, 3377)
    abnormal_samples = abnormal_samples[r]
    abnormal_targets = abnormal_targets[r]
    data = numpy.r_[(normal_samples, abnormal_samples)]
    target = numpy.r_[(normal_targets, abnormal_targets)]
if ((subset == 'SF') or (subset == 'http') or (subset == 'smtp')):
    s = (data[:, 11] == 1)
    data = numpy.c_[(data[s, :11], data[s, 12:])]
    target = target[s]
    data[:, 0] = numpy.log((data[:, 0] + 0.1).astype(float))
    data[:, 4] = numpy.log((data[:, 4] + 0.1).astype(float))
    tempResult = log((data[:, 5] + 0.1).astype(float))
	
===================================================================	
FactorAnalysis.fit: 70	
----------------------------	

'Fit the FactorAnalysis model to X using EM\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        self\n        '
X = check_array(X, copy=self.copy, dtype=numpy.float64)
(n_samples, n_features) = X.shape
n_components = self.n_components
if (n_components is None):
    n_components = n_features
self.mean_ = numpy.mean(X, axis=0)
X -= self.mean_
nsqrt = sqrt(n_samples)
llconst = ((n_features * log((2.0 * numpy.pi))) + n_components)
var = numpy.var(X, axis=0)
if (self.noise_variance_init is None):
    psi = numpy.ones(n_features, dtype=X.dtype)
else:
    if (len(self.noise_variance_init) != n_features):
        raise ValueError(('noise_variance_init dimension does not with number of features : %d != %d' % (len(self.noise_variance_init), n_features)))
    psi = numpy.array(self.noise_variance_init)
loglike = []
old_ll = (- numpy.inf)
SMALL = 1e-12
if (self.svd_method == 'lapack'):

    def my_svd(X):
        (_, s, V) = scipy.linalg.svd(X, full_matrices=False)
        return (s[:n_components], V[:n_components], squared_norm(s[n_components:]))
elif (self.svd_method == 'randomized'):
    random_state = check_random_state(self.random_state)

    def my_svd(X):
        (_, s, V) = randomized_svd(X, n_components, random_state=random_state, n_iter=self.iterated_power)
        return (s, V, (squared_norm(X) - squared_norm(s)))
else:
    raise ValueError(('SVD method %s is not supported. Please consider the documentation' % self.svd_method))
for i in xrange(self.max_iter):
    sqrt_psi = (numpy.sqrt(psi) + SMALL)
    (s, V, unexp_var) = my_svd((X / (sqrt_psi * nsqrt)))
    s **= 2
    W = (numpy.sqrt(numpy.maximum((s - 1.0), 0.0))[:, numpy.newaxis] * V)
    del V
    W *= sqrt_psi
    tempResult = log(s)
	
===================================================================	
FactorAnalysis.fit: 71	
----------------------------	

'Fit the FactorAnalysis model to X using EM\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data.\n\n        Returns\n        -------\n        self\n        '
X = check_array(X, copy=self.copy, dtype=numpy.float64)
(n_samples, n_features) = X.shape
n_components = self.n_components
if (n_components is None):
    n_components = n_features
self.mean_ = numpy.mean(X, axis=0)
X -= self.mean_
nsqrt = sqrt(n_samples)
llconst = ((n_features * log((2.0 * numpy.pi))) + n_components)
var = numpy.var(X, axis=0)
if (self.noise_variance_init is None):
    psi = numpy.ones(n_features, dtype=X.dtype)
else:
    if (len(self.noise_variance_init) != n_features):
        raise ValueError(('noise_variance_init dimension does not with number of features : %d != %d' % (len(self.noise_variance_init), n_features)))
    psi = numpy.array(self.noise_variance_init)
loglike = []
old_ll = (- numpy.inf)
SMALL = 1e-12
if (self.svd_method == 'lapack'):

    def my_svd(X):
        (_, s, V) = scipy.linalg.svd(X, full_matrices=False)
        return (s[:n_components], V[:n_components], squared_norm(s[n_components:]))
elif (self.svd_method == 'randomized'):
    random_state = check_random_state(self.random_state)

    def my_svd(X):
        (_, s, V) = randomized_svd(X, n_components, random_state=random_state, n_iter=self.iterated_power)
        return (s, V, (squared_norm(X) - squared_norm(s)))
else:
    raise ValueError(('SVD method %s is not supported. Please consider the documentation' % self.svd_method))
for i in xrange(self.max_iter):
    sqrt_psi = (numpy.sqrt(psi) + SMALL)
    (s, V, unexp_var) = my_svd((X / (sqrt_psi * nsqrt)))
    s **= 2
    W = (numpy.sqrt(numpy.maximum((s - 1.0), 0.0))[:, numpy.newaxis] * V)
    del V
    W *= sqrt_psi
    ll = (llconst + numpy.sum(numpy.log(s)))
    tempResult = log(psi)
	
===================================================================	
_assess_dimension_: 24	
----------------------------	

'Compute the likelihood of a rank ``rank`` dataset\n\n    The dataset is assumed to be embedded in gaussian noise of shape(n,\n    dimf) having spectrum ``spectrum``.\n\n    Parameters\n    ----------\n    spectrum: array of shape (n)\n        Data spectrum.\n    rank: int\n        Tested rank value.\n    n_samples: int\n        Number of samples.\n    n_features: int\n        Number of features.\n\n    Returns\n    -------\n    ll: float,\n        The log-likelihood\n\n    Notes\n    -----\n    This implements the method of `Thomas P. Minka:\n    Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`\n    '
if (rank > len(spectrum)):
    raise ValueError('The tested rank cannot exceed the rank of the dataset')
pu = ((- rank) * log(2.0))
for i in range(rank):
    pu += (gammaln(((n_features - i) / 2.0)) - ((log(numpy.pi) * (n_features - i)) / 2.0))
tempResult = log(spectrum[:rank])
	
===================================================================	
_assess_dimension_: 31	
----------------------------	

'Compute the likelihood of a rank ``rank`` dataset\n\n    The dataset is assumed to be embedded in gaussian noise of shape(n,\n    dimf) having spectrum ``spectrum``.\n\n    Parameters\n    ----------\n    spectrum: array of shape (n)\n        Data spectrum.\n    rank: int\n        Tested rank value.\n    n_samples: int\n        Number of samples.\n    n_features: int\n        Number of features.\n\n    Returns\n    -------\n    ll: float,\n        The log-likelihood\n\n    Notes\n    -----\n    This implements the method of `Thomas P. Minka:\n    Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`\n    '
if (rank > len(spectrum)):
    raise ValueError('The tested rank cannot exceed the rank of the dataset')
pu = ((- rank) * log(2.0))
for i in range(rank):
    pu += (gammaln(((n_features - i) / 2.0)) - ((log(numpy.pi) * (n_features - i)) / 2.0))
pl = numpy.sum(numpy.log(spectrum[:rank]))
pl = (((- pl) * n_samples) / 2.0)
if (rank == n_features):
    pv = 0
    v = 1
else:
    v = (numpy.sum(spectrum[rank:]) / (n_features - rank))
    tempResult = log(v)
	
===================================================================	
test_pca_score: 275	
----------------------------	

(n, p) = (1000, 3)
rng = numpy.random.RandomState(0)
X = ((rng.randn(n, p) * 0.1) + numpy.array([3, 4, 5]))
for solver in solver_list:
    pca = PCA(n_components=2, svd_solver=solver)
    pca.fit(X)
    ll1 = pca.score(X)
    tempResult = log((((2 * numpy.pi) * numpy.exp(1)) * (0.1 ** 2)))
	
===================================================================	
module: 13	
----------------------------	

'Test truncated SVD transformer.'
import numpy as np
import scipy.sparse as sp
from sklearn.decomposition import TruncatedSVD
from sklearn.utils import check_random_state
from sklearn.utils.testing import assert_array_almost_equal, assert_equal, assert_raises, assert_greater, assert_array_less
shape = (60, 55)
(n_samples, n_features) = shape
rng = check_random_state(42)
X = rng.randint((- 100), 20, np.product(shape)).reshape(shape)
X = scipy.sparse.csr_matrix(numpy.maximum(X, 0), dtype=numpy.float64)
tempResult = log(X.data)
	
===================================================================	
BaggingClassifier.predict_log_proba: 280	
----------------------------	

'Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the base\n        estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        p : array of shape = [n_samples, n_classes]\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        '
check_is_fitted(self, 'classes_')
if hasattr(self.base_estimator_, 'predict_log_proba'):
    X = check_array(X, accept_sparse=['csr', 'csc'])
    if (self.n_features_ != X.shape[1]):
        raise ValueError('Number of features of the model must match the input. Model n_features is {0} and input n_features is {1} '.format(self.n_features_, X.shape[1]))
    (n_jobs, n_estimators, starts) = _partition_estimators(self.n_estimators, self.n_jobs)
    all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_log_proba)(self.estimators_[starts[i]:starts[(i + 1)]], self.estimators_features_[starts[i]:starts[(i + 1)]], X, self.n_classes_) for i in range(n_jobs)))
    log_proba = all_log_proba[0]
    for j in range(1, len(all_log_proba)):
        log_proba = numpy.logaddexp(log_proba, all_log_proba[j])
    tempResult = log(self.n_estimators)
	
===================================================================	
BaggingClassifier.predict_log_proba: 283	
----------------------------	

'Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the base\n        estimators in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n            The training input samples. Sparse matrices are accepted only if\n            they are supported by the base estimator.\n\n        Returns\n        -------\n        p : array of shape = [n_samples, n_classes]\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        '
check_is_fitted(self, 'classes_')
if hasattr(self.base_estimator_, 'predict_log_proba'):
    X = check_array(X, accept_sparse=['csr', 'csc'])
    if (self.n_features_ != X.shape[1]):
        raise ValueError('Number of features of the model must match the input. Model n_features is {0} and input n_features is {1} '.format(self.n_features_, X.shape[1]))
    (n_jobs, n_estimators, starts) = _partition_estimators(self.n_estimators, self.n_jobs)
    all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_log_proba)(self.estimators_[starts[i]:starts[(i + 1)]], self.estimators_features_[starts[i]:starts[(i + 1)]], X, self.n_classes_) for i in range(n_jobs)))
    log_proba = all_log_proba[0]
    for j in range(1, len(all_log_proba)):
        log_proba = numpy.logaddexp(log_proba, all_log_proba[j])
    log_proba -= numpy.log(self.n_estimators)
    return log_proba
else:
    tempResult = log(self.predict_proba(X))
	
===================================================================	
ForestClassifier.predict_log_proba: 269	
----------------------------	

'Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the trees in the\n        forest.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape = [n_samples, n_features]\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n            such arrays if n_outputs > 1.\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        '
proba = self.predict_proba(X)
if (self.n_outputs_ == 1):
    tempResult = log(proba)
	
===================================================================	
ForestClassifier.predict_log_proba: 272	
----------------------------	

'Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the log of the mean predicted class probabilities of the trees in the\n        forest.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape = [n_samples, n_features]\n            The input samples. Internally, its dtype will be converted to\n            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n            converted into a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n            such arrays if n_outputs > 1.\n            The class probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        '
proba = self.predict_proba(X)
if (self.n_outputs_ == 1):
    return numpy.log(proba)
else:
    for k in range(self.n_outputs_):
        tempResult = log(proba[k])
	
===================================================================	
LogOddsEstimator.fit: 88	
----------------------------	

if (sample_weight is None):
    pos = numpy.sum(y)
    neg = (y.shape[0] - pos)
else:
    pos = numpy.sum((sample_weight * y))
    neg = numpy.sum((sample_weight * (1 - y)))
if ((neg == 0) or (pos == 0)):
    raise ValueError('y contains non binary labels.')
tempResult = log((pos / neg))
	
===================================================================	
GradientBoostingClassifier.predict_log_proba: 833	
----------------------------	

'Predict class log-probabilities for X.\n\n        Parameters\n        ----------\n        X : array-like of shape = [n_samples, n_features]\n            The input samples.\n\n        Raises\n        ------\n        AttributeError\n            If the ``loss`` does not support probabilities.\n\n        Returns\n        -------\n        p : array of shape = [n_samples]\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        '
proba = self.predict_proba(X)
tempResult = log(proba)
	
===================================================================	
_average_path_length: 82	
----------------------------	

' The average path length in a n_samples iTree, which is equal to\n    the average path length of an unsuccessful BST search since the\n    latter has the same structure as an isolation tree.\n    Parameters\n    ----------\n    n_samples_leaf : array-like of shape (n_samples, n_estimators), or int.\n        The number of training samples in each test sample leaf, for\n        each estimators.\n\n    Returns\n    -------\n    average_path_length : array, same shape as n_samples_leaf\n\n    '
if isinstance(n_samples_leaf, INTEGER_TYPES):
    if (n_samples_leaf <= 1):
        return 1.0
    else:
        tempResult = log(n_samples_leaf)
	
===================================================================	
_average_path_length: 90	
----------------------------	

' The average path length in a n_samples iTree, which is equal to\n    the average path length of an unsuccessful BST search since the\n    latter has the same structure as an isolation tree.\n    Parameters\n    ----------\n    n_samples_leaf : array-like of shape (n_samples, n_estimators), or int.\n        The number of training samples in each test sample leaf, for\n        each estimators.\n\n    Returns\n    -------\n    average_path_length : array, same shape as n_samples_leaf\n\n    '
if isinstance(n_samples_leaf, INTEGER_TYPES):
    if (n_samples_leaf <= 1):
        return 1.0
    else:
        return ((2.0 * (numpy.log(n_samples_leaf) + 0.5772156649)) - ((2.0 * (n_samples_leaf - 1.0)) / n_samples_leaf))
else:
    n_samples_leaf_shape = n_samples_leaf.shape
    n_samples_leaf = n_samples_leaf.reshape((1, (- 1)))
    average_path_length = numpy.zeros(n_samples_leaf.shape)
    mask = (n_samples_leaf <= 1)
    not_mask = numpy.logical_not(mask)
    average_path_length[mask] = 1.0
    tempResult = log(n_samples_leaf[not_mask])
	
===================================================================	
AdaBoostClassifier._boost_real: 154	
----------------------------	

'Implement a single boost using the SAMME.R real algorithm.'
estimator = self._make_estimator(random_state=random_state)
estimator.fit(X, y, sample_weight=sample_weight)
y_predict_proba = estimator.predict_proba(X)
if (iboost == 0):
    self.classes_ = getattr(estimator, 'classes_', None)
    self.n_classes_ = len(self.classes_)
y_predict = self.classes_.take(numpy.argmax(y_predict_proba, axis=1), axis=0)
incorrect = (y_predict != y)
estimator_error = numpy.mean(numpy.average(incorrect, weights=sample_weight, axis=0))
if (estimator_error <= 0):
    return (sample_weight, 1.0, 0.0)
n_classes = self.n_classes_
classes = self.classes_
y_codes = numpy.array([((- 1.0) / (n_classes - 1)), 1.0])
y_coding = y_codes.take((classes == y[:, numpy.newaxis]))
proba = y_predict_proba
proba[(proba < np.finfo(proba.dtype).eps)] = np.finfo(proba.dtype).eps
tempResult = log(y_predict_proba)
	
===================================================================	
AdaBoostClassifier._boost_discrete: 177	
----------------------------	

'Implement a single boost using the SAMME discrete algorithm.'
estimator = self._make_estimator(random_state=random_state)
estimator.fit(X, y, sample_weight=sample_weight)
y_predict = estimator.predict(X)
if (iboost == 0):
    self.classes_ = getattr(estimator, 'classes_', None)
    self.n_classes_ = len(self.classes_)
incorrect = (y_predict != y)
estimator_error = numpy.mean(numpy.average(incorrect, weights=sample_weight, axis=0))
if (estimator_error <= 0):
    return (sample_weight, 1.0, 0.0)
n_classes = self.n_classes_
if (estimator_error >= (1.0 - (1.0 / n_classes))):
    self.estimators_.pop((- 1))
    if (len(self.estimators_) == 0):
        raise ValueError('BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.')
    return (None, None, None)
tempResult = log(((1.0 - estimator_error) / estimator_error))
	
===================================================================	
AdaBoostClassifier._boost_discrete: 177	
----------------------------	

'Implement a single boost using the SAMME discrete algorithm.'
estimator = self._make_estimator(random_state=random_state)
estimator.fit(X, y, sample_weight=sample_weight)
y_predict = estimator.predict(X)
if (iboost == 0):
    self.classes_ = getattr(estimator, 'classes_', None)
    self.n_classes_ = len(self.classes_)
incorrect = (y_predict != y)
estimator_error = numpy.mean(numpy.average(incorrect, weights=sample_weight, axis=0))
if (estimator_error <= 0):
    return (sample_weight, 1.0, 0.0)
n_classes = self.n_classes_
if (estimator_error >= (1.0 - (1.0 / n_classes))):
    self.estimators_.pop((- 1))
    if (len(self.estimators_) == 0):
        raise ValueError('BaseClassifier in AdaBoostClassifier ensemble is worse than random, ensemble can not be fit.')
    return (None, None, None)
tempResult = log((n_classes - 1.0))
	
===================================================================	
AdaBoostRegressor._boost: 329	
----------------------------	

'Implement a single boost for regression\n\n        Perform a single boost according to the AdaBoost.R2 algorithm and\n        return the updated sample weights.\n\n        Parameters\n        ----------\n        iboost : int\n            The index of the current boost iteration.\n\n        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. DOK and LIL are converted to CSR.\n\n        y : array-like of shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n\n        sample_weight : array-like of shape = [n_samples]\n            The current sample weights.\n\n        random_state : numpy.RandomState\n            The current random number generator\n\n        Returns\n        -------\n        sample_weight : array-like of shape = [n_samples] or None\n            The reweighted sample weights.\n            If None then boosting has terminated early.\n\n        estimator_weight : float\n            The weight for the current boost.\n            If None then boosting has terminated early.\n\n        estimator_error : float\n            The regression error for the current boost.\n            If None then boosting has terminated early.\n        '
estimator = self._make_estimator(random_state=random_state)
cdf = sample_weight.cumsum()
cdf /= cdf[(- 1)]
uniform_samples = random_state.random_sample(X.shape[0])
bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
bootstrap_idx = numpy.array(bootstrap_idx, copy=False)
estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
y_predict = estimator.predict(X)
error_vect = numpy.abs((y_predict - y))
error_max = error_vect.max()
if (error_max != 0.0):
    error_vect /= error_max
if (self.loss == 'square'):
    error_vect **= 2
elif (self.loss == 'exponential'):
    error_vect = (1.0 - numpy.exp((- error_vect)))
estimator_error = (sample_weight * error_vect).sum()
if (estimator_error <= 0):
    return (sample_weight, 1.0, 0.0)
elif (estimator_error >= 0.5):
    if (len(self.estimators_) > 1):
        self.estimators_.pop((- 1))
    return (None, None, None)
beta = (estimator_error / (1.0 - estimator_error))
tempResult = log((1.0 / beta))
	
===================================================================	
AdaBoostClassifier.predict_log_proba: 283	
----------------------------	

'Predict class log-probabilities for X.\n\n        The predicted class log-probabilities of an input sample is computed as\n        the weighted mean predicted class log-probabilities of the classifiers\n        in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. DOK and LIL are converted to CSR.\n\n        Returns\n        -------\n        p : array of shape = [n_samples]\n            The class probabilities of the input samples. The order of\n            outputs is the same of that of the `classes_` attribute.\n        '
tempResult = log(self.predict_proba(X))
	
===================================================================	
_samme_proba: 103	
----------------------------	

'Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n\n    References\n    ----------\n    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.\n\n    '
proba = estimator.predict_proba(X)
proba[(proba < np.finfo(proba.dtype).eps)] = np.finfo(proba.dtype).eps
tempResult = log(proba)
	
===================================================================	
TfidfTransformer.fit: 420	
----------------------------	

'Learn the idf vector (global term weights)\n\n        Parameters\n        ----------\n        X : sparse matrix, [n_samples, n_features]\n            a matrix of term/token counts\n        '
if (not scipy.sparse.issparse(X)):
    X = scipy.sparse.csc_matrix(X)
if self.use_idf:
    (n_samples, n_features) = X.shape
    df = _document_frequency(X)
    df += int(self.smooth_idf)
    n_samples += int(self.smooth_idf)
    tempResult = log((float(n_samples) / df))
	
===================================================================	
TfidfTransformer.transform: 432	
----------------------------	

'Transform a count matrix to a tf or tf-idf representation\n\n        Parameters\n        ----------\n        X : sparse matrix, [n_samples, n_features]\n            a matrix of term/token counts\n\n        copy : boolean, default True\n            Whether to copy X and operate on the copy or perform in-place\n            operations.\n\n        Returns\n        -------\n        vectors : sparse matrix, [n_samples, n_features]\n        '
if (hasattr(X, 'dtype') and numpy.issubdtype(X.dtype, numpy.float)):
    X = scipy.sparse.csr_matrix(X, copy=copy)
else:
    X = scipy.sparse.csr_matrix(X, dtype=numpy.float64, copy=copy)
(n_samples, n_features) = X.shape
if self.sublinear_tf:
    tempResult = log(X.data, X.data)
	
===================================================================	
test_compute_mi_dd: 12	
----------------------------	

x = numpy.array([0, 1, 1, 0, 0])
y = numpy.array([1, 0, 0, 0, 1])
tempResult = log((3 / 5))
	
===================================================================	
test_compute_mi_dd: 12	
----------------------------	

x = numpy.array([0, 1, 1, 0, 0])
y = numpy.array([1, 0, 0, 0, 1])
tempResult = log((2 / 5))
	
===================================================================	
test_compute_mi_dd: 13	
----------------------------	

x = numpy.array([0, 1, 1, 0, 0])
y = numpy.array([1, 0, 0, 0, 1])
H_x = H_y = (((- (3 / 5)) * numpy.log((3 / 5))) - ((2 / 5) * numpy.log((2 / 5))))
tempResult = log((1 / 5))
	
===================================================================	
test_compute_mi_dd: 13	
----------------------------	

x = numpy.array([0, 1, 1, 0, 0])
y = numpy.array([1, 0, 0, 0, 1])
H_x = H_y = (((- (3 / 5)) * numpy.log((3 / 5))) - ((2 / 5) * numpy.log((2 / 5))))
tempResult = log((2 / 5))
	
===================================================================	
test_compute_mi_dd: 13	
----------------------------	

x = numpy.array([0, 1, 1, 0, 0])
y = numpy.array([1, 0, 0, 0, 1])
H_x = H_y = (((- (3 / 5)) * numpy.log((3 / 5))) - ((2 / 5) * numpy.log((2 / 5))))
tempResult = log((2 / 5))
	
===================================================================	
test_compute_mi_cd: 40	
----------------------------	

n_samples = 1000
numpy.random.seed(0)
for p in [0.3, 0.5, 0.7]:
    x = (numpy.random.uniform(size=n_samples) > p)
    y = numpy.empty(n_samples)
    mask = (x == 0)
    y[mask] = numpy.random.uniform((- 1), 1, size=numpy.sum(mask))
    y[(~ mask)] = numpy.random.uniform(0, 2, size=numpy.sum((~ mask)))
    tempResult = log((0.5 * (1 - p)))
	
===================================================================	
test_compute_mi_cd: 40	
----------------------------	

n_samples = 1000
numpy.random.seed(0)
for p in [0.3, 0.5, 0.7]:
    x = (numpy.random.uniform(size=n_samples) > p)
    y = numpy.empty(n_samples)
    mask = (x == 0)
    y[mask] = numpy.random.uniform((- 1), 1, size=numpy.sum(mask))
    y[(~ mask)] = numpy.random.uniform(0, 2, size=numpy.sum((~ mask)))
    tempResult = log((0.5 * p))
	
===================================================================	
test_compute_mi_cd: 40	
----------------------------	

n_samples = 1000
numpy.random.seed(0)
for p in [0.3, 0.5, 0.7]:
    x = (numpy.random.uniform(size=n_samples) > p)
    y = numpy.empty(n_samples)
    mask = (x == 0)
    y[mask] = numpy.random.uniform((- 1), 1, size=numpy.sum(mask))
    y[(~ mask)] = numpy.random.uniform(0, 2, size=numpy.sum((~ mask)))
    tempResult = log(0.5)
	
===================================================================	
test_compute_mi_cd: 40	
----------------------------	

n_samples = 1000
numpy.random.seed(0)
for p in [0.3, 0.5, 0.7]:
    x = (numpy.random.uniform(size=n_samples) > p)
    y = numpy.empty(n_samples)
    mask = (x == 0)
    y[mask] = numpy.random.uniform((- 1), 1, size=numpy.sum(mask))
    y[(~ mask)] = numpy.random.uniform(0, 2, size=numpy.sum((~ mask)))
    tempResult = log(2)
	
===================================================================	
test_compute_mi_cc: 23	
----------------------------	

mean = numpy.zeros(2)
sigma_1 = 1
sigma_2 = 10
corr = 0.5
cov = numpy.array([[(sigma_1 ** 2), ((corr * sigma_1) * sigma_2)], [((corr * sigma_1) * sigma_2), (sigma_2 ** 2)]])
tempResult = log(sigma_1)
	
===================================================================	
test_compute_mi_cc: 23	
----------------------------	

mean = numpy.zeros(2)
sigma_1 = 1
sigma_2 = 10
corr = 0.5
cov = numpy.array([[(sigma_1 ** 2), ((corr * sigma_1) * sigma_2)], [((corr * sigma_1) * sigma_2), (sigma_2 ** 2)]])
tempResult = log(sigma_2)
	
===================================================================	
test_compute_mi_cc: 23	
----------------------------	

mean = numpy.zeros(2)
sigma_1 = 1
sigma_2 = 10
corr = 0.5
cov = numpy.array([[(sigma_1 ** 2), ((corr * sigma_1) * sigma_2)], [((corr * sigma_1) * sigma_2), (sigma_2 ** 2)]])
tempResult = log(numpy.linalg.det(cov))
	
===================================================================	
_BinaryGaussianProcessClassifierLaplace._posterior_mode: 133	
----------------------------	

"Mode-finding for binary Laplace GPC and fixed kernel.\n\n        This approximates the posterior of the latent function values for given\n        inputs and target observations with a Gaussian approximation and uses\n        Newton's iteration to find the mode of this approximation.\n        "
if (self.warm_start and hasattr(self, 'f_cached') and (self.f_cached.shape == self.y_train_.shape)):
    f = self.f_cached
else:
    f = numpy.zeros_like(self.y_train_, dtype=numpy.float64)
log_marginal_likelihood = (- numpy.inf)
for _ in range(self.max_iter_predict):
    pi = (1 / (1 + numpy.exp((- f))))
    W = (pi * (1 - pi))
    W_sr = numpy.sqrt(W)
    W_sr_K = (W_sr[:, numpy.newaxis] * K)
    B = (numpy.eye(W.shape[0]) + (W_sr_K * W_sr))
    L = cholesky(B, lower=True)
    b = ((W * f) + (self.y_train_ - pi))
    a = (b - (W_sr * cho_solve((L, True), W_sr_K.dot(b))))
    f = K.dot(a)
    tempResult = log((1 + numpy.exp(((- ((self.y_train_ * 2) - 1)) * f))))
	
===================================================================	
_BinaryGaussianProcessClassifierLaplace._posterior_mode: 133	
----------------------------	

"Mode-finding for binary Laplace GPC and fixed kernel.\n\n        This approximates the posterior of the latent function values for given\n        inputs and target observations with a Gaussian approximation and uses\n        Newton's iteration to find the mode of this approximation.\n        "
if (self.warm_start and hasattr(self, 'f_cached') and (self.f_cached.shape == self.y_train_.shape)):
    f = self.f_cached
else:
    f = numpy.zeros_like(self.y_train_, dtype=numpy.float64)
log_marginal_likelihood = (- numpy.inf)
for _ in range(self.max_iter_predict):
    pi = (1 / (1 + numpy.exp((- f))))
    W = (pi * (1 - pi))
    W_sr = numpy.sqrt(W)
    W_sr_K = (W_sr[:, numpy.newaxis] * K)
    B = (numpy.eye(W.shape[0]) + (W_sr_K * W_sr))
    L = cholesky(B, lower=True)
    b = ((W * f) + (self.y_train_ - pi))
    a = (b - (W_sr * cho_solve((L, True), W_sr_K.dot(b))))
    f = K.dot(a)
    tempResult = log(numpy.diag(L))
	
===================================================================	
GaussianProcessRegressor.log_marginal_likelihood: 140	
----------------------------	

'Returns log-marginal likelihood of theta for training data.\n\n        Parameters\n        ----------\n        theta : array-like, shape = (n_kernel_params,) or None\n            Kernel hyperparameters for which the log-marginal likelihood is\n            evaluated. If None, the precomputed log_marginal_likelihood\n            of ``self.kernel_.theta`` is returned.\n\n        eval_gradient : bool, default: False\n            If True, the gradient of the log-marginal likelihood with respect\n            to the kernel hyperparameters at position theta is returned\n            additionally. If True, theta must not be None.\n\n        Returns\n        -------\n        log_likelihood : float\n            Log-marginal likelihood of theta for training data.\n\n        log_likelihood_gradient : array, shape = (n_kernel_params,), optional\n            Gradient of the log-marginal likelihood with respect to the kernel\n            hyperparameters at position theta.\n            Only returned when eval_gradient is True.\n        '
if (theta is None):
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated for theta!=None')
    return self.log_marginal_likelihood_value_
kernel = self.kernel_.clone_with_theta(theta)
if eval_gradient:
    (K, K_gradient) = kernel(self.X_train_, eval_gradient=True)
else:
    K = kernel(self.X_train_)
K[numpy.diag_indices_from(K)] += self.alpha
try:
    L = cholesky(K, lower=True)
except numpy.linalg.LinAlgError:
    return (((- numpy.inf), numpy.zeros_like(theta)) if eval_gradient else (- numpy.inf))
y_train = self.y_train_
if (y_train.ndim == 1):
    y_train = y_train[:, numpy.newaxis]
alpha = cho_solve((L, True), y_train)
log_likelihood_dims = ((- 0.5) * numpy.einsum('ik,ik->k', y_train, alpha))
tempResult = log(numpy.diag(L))
	
===================================================================	
GaussianProcessRegressor.log_marginal_likelihood: 141	
----------------------------	

'Returns log-marginal likelihood of theta for training data.\n\n        Parameters\n        ----------\n        theta : array-like, shape = (n_kernel_params,) or None\n            Kernel hyperparameters for which the log-marginal likelihood is\n            evaluated. If None, the precomputed log_marginal_likelihood\n            of ``self.kernel_.theta`` is returned.\n\n        eval_gradient : bool, default: False\n            If True, the gradient of the log-marginal likelihood with respect\n            to the kernel hyperparameters at position theta is returned\n            additionally. If True, theta must not be None.\n\n        Returns\n        -------\n        log_likelihood : float\n            Log-marginal likelihood of theta for training data.\n\n        log_likelihood_gradient : array, shape = (n_kernel_params,), optional\n            Gradient of the log-marginal likelihood with respect to the kernel\n            hyperparameters at position theta.\n            Only returned when eval_gradient is True.\n        '
if (theta is None):
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated for theta!=None')
    return self.log_marginal_likelihood_value_
kernel = self.kernel_.clone_with_theta(theta)
if eval_gradient:
    (K, K_gradient) = kernel(self.X_train_, eval_gradient=True)
else:
    K = kernel(self.X_train_)
K[numpy.diag_indices_from(K)] += self.alpha
try:
    L = cholesky(K, lower=True)
except numpy.linalg.LinAlgError:
    return (((- numpy.inf), numpy.zeros_like(theta)) if eval_gradient else (- numpy.inf))
y_train = self.y_train_
if (y_train.ndim == 1):
    y_train = y_train[:, numpy.newaxis]
alpha = cho_solve((L, True), y_train)
log_likelihood_dims = ((- 0.5) * numpy.einsum('ik,ik->k', y_train, alpha))
log_likelihood_dims -= np.log(np.diag(L)).sum()
tempResult = log((2 * numpy.pi))
	
===================================================================	
RationalQuadratic.__call__: 644	
----------------------------	

'Return the kernel k(X, Y) and optionally its gradient.\n\n        Parameters\n        ----------\n        X : array, shape (n_samples_X, n_features)\n            Left argument of the returned kernel k(X, Y)\n\n        Y : array, shape (n_samples_Y, n_features), (optional, default=None)\n            Right argument of the returned kernel k(X, Y). If None, k(X, X)\n            if evaluated instead.\n\n        eval_gradient : bool (optional, default=False)\n            Determines whether the gradient with respect to the kernel\n            hyperparameter is determined. Only supported when Y is None.\n\n        Returns\n        -------\n        K : array, shape (n_samples_X, n_samples_Y)\n            Kernel k(X, Y)\n\n        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)\n            The gradient of the kernel k(X, X) with respect to the\n            hyperparameter of the kernel. Only returned when eval_gradient\n            is True.\n        '
X = numpy.atleast_2d(X)
if (Y is None):
    dists = squareform(pdist(X, metric='sqeuclidean'))
    tmp = (dists / ((2 * self.alpha) * (self.length_scale ** 2)))
    base = (1 + tmp)
    K = (base ** (- self.alpha))
    numpy.fill_diagonal(K, 1)
else:
    if eval_gradient:
        raise ValueError('Gradient can only be evaluated when Y is None.')
    dists = cdist(X, Y, metric='sqeuclidean')
    K = ((1 + (dists / ((2 * self.alpha) * (self.length_scale ** 2)))) ** (- self.alpha))
if eval_gradient:
    if (not self.hyperparameter_length_scale.fixed):
        length_scale_gradient = ((dists * K) / ((self.length_scale ** 2) * base))
        length_scale_gradient = length_scale_gradient[:, :, numpy.newaxis]
    else:
        length_scale_gradient = numpy.empty((K.shape[0], K.shape[1], 0))
    if (not self.hyperparameter_alpha.fixed):
        tempResult = log(base)
	
===================================================================	
Kernel.theta: 110	
----------------------------	

"Returns the (flattened, log-transformed) non-fixed hyperparameters.\n\n        Note that theta are typically the log-transformed values of the\n        kernel's hyperparameters as this representation of the search space\n        is more amenable for hyperparameter search, as hyperparameters like\n        length-scales naturally live on a log-scale.\n\n        Returns\n        -------\n        theta : array, shape (n_dims,)\n            The non-fixed, log-transformed hyperparameters of the kernel\n        "
theta = []
params = self.get_params()
for hyperparameter in self.hyperparameters:
    if (not hyperparameter.fixed):
        theta.append(params[hyperparameter.name])
if (len(theta) > 0):
    tempResult = log(numpy.hstack(theta))
	
===================================================================	
Kernel.bounds: 140	
----------------------------	

"Returns the log-transformed bounds on the theta.\n\n        Returns\n        -------\n        bounds : array, shape (n_dims, 2)\n            The log-transformed bounds on the kernel's hyperparameters theta\n        "
bounds = []
for hyperparameter in self.hyperparameters:
    if (not hyperparameter.fixed):
        bounds.append(hyperparameter.bounds)
if (len(bounds) > 0):
    tempResult = log(numpy.vstack(bounds))
	
===================================================================	
test_kernel_anisotropic: 95	
----------------------------	

kernel = (3.0 * RBF([0.5, 2.0]))
K = kernel(X)
X1 = numpy.array(X)
X1[:, 0] *= 4
K1 = (3.0 * RBF(2.0)(X1))
assert_almost_equal(K, K1)
X2 = numpy.array(X)
X2[:, 1] /= 4
K2 = (3.0 * RBF(0.5)(X2))
assert_almost_equal(K, K2)
tempResult = log(2)
	
===================================================================	
test_kernel_anisotropic: 96	
----------------------------	

kernel = (3.0 * RBF([0.5, 2.0]))
K = kernel(X)
X1 = numpy.array(X)
X1[:, 0] *= 4
K1 = (3.0 * RBF(2.0)(X1))
assert_almost_equal(K, K1)
X2 = numpy.array(X)
X2[:, 1] /= 4
K2 = (3.0 * RBF(0.5)(X2))
assert_almost_equal(K, K2)
kernel.theta = (kernel.theta + numpy.log(2))
tempResult = log([6.0, 1.0, 4.0])
	
===================================================================	
test_kernel_theta: 44	
----------------------------	

for kernel in kernels:
    if (isinstance(kernel, KernelOperator) or isinstance(kernel, Exponentiation)):
        continue
    theta = kernel.theta
    (_, K_gradient) = kernel(X, eval_gradient=True)
    init_sign = signature(kernel.__class__.__init__).parameters.values()
    args = [p.name for p in init_sign if (p.name != 'self')]
    theta_vars = map((lambda s: s.rstrip('_bounds')), filter((lambda s: s.endswith('_bounds')), args))
    assert_equal(set((hyperparameter.name for hyperparameter in kernel.hyperparameters)), set(theta_vars))
    for (i, hyperparameter) in enumerate(kernel.hyperparameters):
        tempResult = log(getattr(kernel, hyperparameter.name))
	
===================================================================	
test_kernel_theta: 60	
----------------------------	

for kernel in kernels:
    if (isinstance(kernel, KernelOperator) or isinstance(kernel, Exponentiation)):
        continue
    theta = kernel.theta
    (_, K_gradient) = kernel(X, eval_gradient=True)
    init_sign = signature(kernel.__class__.__init__).parameters.values()
    args = [p.name for p in init_sign if (p.name != 'self')]
    theta_vars = map((lambda s: s.rstrip('_bounds')), filter((lambda s: s.endswith('_bounds')), args))
    assert_equal(set((hyperparameter.name for hyperparameter in kernel.hyperparameters)), set(theta_vars))
    for (i, hyperparameter) in enumerate(kernel.hyperparameters):
        assert_equal(theta[i], numpy.log(getattr(kernel, hyperparameter.name)))
    for (i, hyperparameter) in enumerate(kernel.hyperparameters):
        params = kernel.get_params()
        params[(hyperparameter.name + '_bounds')] = 'fixed'
        kernel_class = kernel.__class__
        new_kernel = kernel_class(**params)
        (_, K_gradient_new) = new_kernel(X, eval_gradient=True)
        assert_equal(theta.shape[0], (new_kernel.theta.shape[0] + 1))
        assert_equal(K_gradient.shape[2], (K_gradient_new.shape[2] + 1))
        if (i > 0):
            assert_equal(theta[:i], new_kernel.theta[:i])
            assert_array_equal(K_gradient[..., :i], K_gradient_new[..., :i])
        if ((i + 1) < len(kernel.hyperparameters)):
            assert_equal(theta[(i + 1):], new_kernel.theta[i:])
            assert_array_equal(K_gradient[..., (i + 1):], K_gradient_new[..., i:])
    for (i, hyperparameter) in enumerate(kernel.hyperparameters):
        tempResult = log(42)
	
===================================================================	
test_kernel_theta: 64	
----------------------------	

for kernel in kernels:
    if (isinstance(kernel, KernelOperator) or isinstance(kernel, Exponentiation)):
        continue
    theta = kernel.theta
    (_, K_gradient) = kernel(X, eval_gradient=True)
    init_sign = signature(kernel.__class__.__init__).parameters.values()
    args = [p.name for p in init_sign if (p.name != 'self')]
    theta_vars = map((lambda s: s.rstrip('_bounds')), filter((lambda s: s.endswith('_bounds')), args))
    assert_equal(set((hyperparameter.name for hyperparameter in kernel.hyperparameters)), set(theta_vars))
    for (i, hyperparameter) in enumerate(kernel.hyperparameters):
        assert_equal(theta[i], numpy.log(getattr(kernel, hyperparameter.name)))
    for (i, hyperparameter) in enumerate(kernel.hyperparameters):
        params = kernel.get_params()
        params[(hyperparameter.name + '_bounds')] = 'fixed'
        kernel_class = kernel.__class__
        new_kernel = kernel_class(**params)
        (_, K_gradient_new) = new_kernel(X, eval_gradient=True)
        assert_equal(theta.shape[0], (new_kernel.theta.shape[0] + 1))
        assert_equal(K_gradient.shape[2], (K_gradient_new.shape[2] + 1))
        if (i > 0):
            assert_equal(theta[:i], new_kernel.theta[:i])
            assert_array_equal(K_gradient[..., :i], K_gradient_new[..., :i])
        if ((i + 1) < len(kernel.hyperparameters)):
            assert_equal(theta[(i + 1):], new_kernel.theta[i:])
            assert_array_equal(K_gradient[..., (i + 1):], K_gradient_new[..., i:])
    for (i, hyperparameter) in enumerate(kernel.hyperparameters):
        theta[i] = numpy.log(42)
        kernel.theta = theta
        assert_almost_equal(getattr(kernel, hyperparameter.name), 42)
        setattr(kernel, hyperparameter.name, 43)
        tempResult = log(43)
	
===================================================================	
BayesianRidge.fit: 50	
----------------------------	

'Fit the model\n\n        Parameters\n        ----------\n        X : numpy array of shape [n_samples,n_features]\n            Training data\n        y : numpy array of shape [n_samples]\n            Target values\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
(X, y) = check_X_y(X, y, dtype=numpy.float64, y_numeric=True)
(X, y, X_offset, y_offset, X_scale) = self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
(n_samples, n_features) = X.shape
alpha_ = (1.0 / numpy.var(y))
lambda_ = 1.0
verbose = self.verbose
lambda_1 = self.lambda_1
lambda_2 = self.lambda_2
alpha_1 = self.alpha_1
alpha_2 = self.alpha_2
self.scores_ = list()
coef_old_ = None
XT_y = numpy.dot(X.T, y)
(U, S, Vh) = scipy.linalg.svd(X, full_matrices=False)
eigen_vals_ = (S ** 2)
for iter_ in range(self.n_iter):
    if (n_samples > n_features):
        coef_ = numpy.dot(Vh.T, (Vh / (eigen_vals_ + (lambda_ / alpha_))[:, None]))
        coef_ = numpy.dot(coef_, XT_y)
        if self.compute_score:
            tempResult = log((lambda_ + (alpha_ * eigen_vals_)))
	
===================================================================	
BayesianRidge.fit: 57	
----------------------------	

'Fit the model\n\n        Parameters\n        ----------\n        X : numpy array of shape [n_samples,n_features]\n            Training data\n        y : numpy array of shape [n_samples]\n            Target values\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
(X, y) = check_X_y(X, y, dtype=numpy.float64, y_numeric=True)
(X, y, X_offset, y_offset, X_scale) = self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
(n_samples, n_features) = X.shape
alpha_ = (1.0 / numpy.var(y))
lambda_ = 1.0
verbose = self.verbose
lambda_1 = self.lambda_1
lambda_2 = self.lambda_2
alpha_1 = self.alpha_1
alpha_2 = self.alpha_2
self.scores_ = list()
coef_old_ = None
XT_y = numpy.dot(X.T, y)
(U, S, Vh) = scipy.linalg.svd(X, full_matrices=False)
eigen_vals_ = (S ** 2)
for iter_ in range(self.n_iter):
    if (n_samples > n_features):
        coef_ = numpy.dot(Vh.T, (Vh / (eigen_vals_ + (lambda_ / alpha_))[:, None]))
        coef_ = numpy.dot(coef_, XT_y)
        if self.compute_score:
            logdet_sigma_ = (- numpy.sum(numpy.log((lambda_ + (alpha_ * eigen_vals_)))))
    else:
        coef_ = numpy.dot(X.T, numpy.dot((U / (eigen_vals_ + (lambda_ / alpha_))[None, :]), U.T))
        coef_ = numpy.dot(coef_, y)
        if self.compute_score:
            logdet_sigma_ = (lambda_ * numpy.ones(n_features))
            logdet_sigma_[:n_samples] += (alpha_ * eigen_vals_)
            tempResult = log(logdet_sigma_)
	
===================================================================	
ARDRegression.fit: 124	
----------------------------	

'Fit the ARDRegression model according to the given training data\n        and parameters.\n\n        Iterative procedure to maximize the evidence\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array, shape = [n_samples]\n            Target values (integers)\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
(X, y) = check_X_y(X, y, dtype=numpy.float64, y_numeric=True)
(n_samples, n_features) = X.shape
coef_ = numpy.zeros(n_features)
(X, y, X_offset, y_offset, X_scale) = self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
keep_lambda = numpy.ones(n_features, dtype=bool)
lambda_1 = self.lambda_1
lambda_2 = self.lambda_2
alpha_1 = self.alpha_1
alpha_2 = self.alpha_2
verbose = self.verbose
alpha_ = (1.0 / numpy.var(y))
lambda_ = numpy.ones(n_features)
self.scores_ = list()
coef_old_ = None
for iter_ in range(self.n_iter):
    sigma_ = pinvh(((numpy.eye(n_samples) / alpha_) + numpy.dot((X[:, keep_lambda] * numpy.reshape((1.0 / lambda_[keep_lambda]), [1, (- 1)])), X[:, keep_lambda].T)))
    sigma_ = numpy.dot(sigma_, (X[:, keep_lambda] * numpy.reshape((1.0 / lambda_[keep_lambda]), [1, (- 1)])))
    sigma_ = (- numpy.dot((numpy.reshape((1.0 / lambda_[keep_lambda]), [(- 1), 1]) * X[:, keep_lambda].T), sigma_))
    sigma_.flat[::(sigma_.shape[1] + 1)] += (1.0 / lambda_[keep_lambda])
    coef_[keep_lambda] = (alpha_ * numpy.dot(sigma_, numpy.dot(X[:, keep_lambda].T, y)))
    rmse_ = numpy.sum(((y - numpy.dot(X, coef_)) ** 2))
    gamma_ = (1.0 - (lambda_[keep_lambda] * numpy.diag(sigma_)))
    lambda_[keep_lambda] = ((gamma_ + (2.0 * lambda_1)) / ((coef_[keep_lambda] ** 2) + (2.0 * lambda_2)))
    alpha_ = (((n_samples - gamma_.sum()) + (2.0 * alpha_1)) / (rmse_ + (2.0 * alpha_2)))
    keep_lambda = (lambda_ < self.threshold_lambda)
    coef_[(~ keep_lambda)] = 0
    if self.compute_score:
        tempResult = log(lambda_)
	
===================================================================	
ARDRegression.fit: 126	
----------------------------	

'Fit the ARDRegression model according to the given training data\n        and parameters.\n\n        Iterative procedure to maximize the evidence\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n        y : array, shape = [n_samples]\n            Target values (integers)\n\n        Returns\n        -------\n        self : returns an instance of self.\n        '
(X, y) = check_X_y(X, y, dtype=numpy.float64, y_numeric=True)
(n_samples, n_features) = X.shape
coef_ = numpy.zeros(n_features)
(X, y, X_offset, y_offset, X_scale) = self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
keep_lambda = numpy.ones(n_features, dtype=bool)
lambda_1 = self.lambda_1
lambda_2 = self.lambda_2
alpha_1 = self.alpha_1
alpha_2 = self.alpha_2
verbose = self.verbose
alpha_ = (1.0 / numpy.var(y))
lambda_ = numpy.ones(n_features)
self.scores_ = list()
coef_old_ = None
for iter_ in range(self.n_iter):
    sigma_ = pinvh(((numpy.eye(n_samples) / alpha_) + numpy.dot((X[:, keep_lambda] * numpy.reshape((1.0 / lambda_[keep_lambda]), [1, (- 1)])), X[:, keep_lambda].T)))
    sigma_ = numpy.dot(sigma_, (X[:, keep_lambda] * numpy.reshape((1.0 / lambda_[keep_lambda]), [1, (- 1)])))
    sigma_ = (- numpy.dot((numpy.reshape((1.0 / lambda_[keep_lambda]), [(- 1), 1]) * X[:, keep_lambda].T), sigma_))
    sigma_.flat[::(sigma_.shape[1] + 1)] += (1.0 / lambda_[keep_lambda])
    coef_[keep_lambda] = (alpha_ * numpy.dot(sigma_, numpy.dot(X[:, keep_lambda].T, y)))
    rmse_ = numpy.sum(((y - numpy.dot(X, coef_)) ** 2))
    gamma_ = (1.0 - (lambda_[keep_lambda] * numpy.diag(sigma_)))
    lambda_[keep_lambda] = ((gamma_ + (2.0 * lambda_1)) / ((coef_[keep_lambda] ** 2) + (2.0 * lambda_2)))
    alpha_ = (((n_samples - gamma_.sum()) + (2.0 * alpha_1)) / (rmse_ + (2.0 * alpha_2)))
    keep_lambda = (lambda_ < self.threshold_lambda)
    coef_[(~ keep_lambda)] = 0
    if self.compute_score:
        s = ((lambda_1 * np.log(lambda_)) - (lambda_2 * lambda_)).sum()
        s += ((alpha_1 * log(alpha_)) - (alpha_2 * alpha_))
        tempResult = log(lambda_)
	
===================================================================	
LassoLarsIC.fit: 443	
----------------------------	

'Fit the model using X, y as training data.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            training data.\n\n        y : array-like, shape (n_samples,)\n            target values.\n\n        copy_X : boolean, optional, default True\n            If ``True``, X will be copied; else, it may be overwritten.\n\n        Returns\n        -------\n        self : object\n            returns an instance of self.\n        '
self.fit_path = True
(X, y) = check_X_y(X, y, y_numeric=True)
(X, y, Xmean, ymean, Xstd) = base.LinearModel._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
max_iter = self.max_iter
Gram = self._get_gram()
(alphas_, active_, coef_path_, self.n_iter_) = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
n_samples = X.shape[0]
if (self.criterion == 'aic'):
    K = 2
elif (self.criterion == 'bic'):
    K = log(n_samples)
else:
    raise ValueError('criterion should be either bic or aic')
R = (y[:, numpy.newaxis] - numpy.dot(X, coef_path_))
mean_squared_error = numpy.mean((R ** 2), axis=0)
df = numpy.zeros(coef_path_.shape[1], dtype=numpy.int)
for (k, coef) in enumerate(coef_path_.T):
    mask = (numpy.abs(coef) > np.finfo(coef.dtype).eps)
    if (not numpy.any(mask)):
        continue
    df[k] = numpy.sum(mask)
self.alphas_ = alphas_
with numpy.errstate(divide='ignore'):
    tempResult = log(mean_squared_error)
	
===================================================================	
LogisticRegression.predict_log_proba: 415	
----------------------------	

'Log of probability estimates.\n\n        The returned estimates for all classes are ordered by the\n        label of classes.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n\n        Returns\n        -------\n        T : array-like, shape = [n_samples, n_classes]\n            Returns the log-probability of the sample for each class in the\n            model, where classes are ordered as they are in ``self.classes_``.\n        '
tempResult = log(self.predict_proba(X))
	
===================================================================	
_dynamic_max_trials: 21	
----------------------------	

'Determine number trials such that at least one outlier-free subset is\n    sampled for the given inlier/outlier ratio.\n\n    Parameters\n    ----------\n    n_inliers : int\n        Number of inliers in the data.\n\n    n_samples : int\n        Total number of samples in the data.\n\n    min_samples : int\n        Minimum number of samples chosen randomly from original data.\n\n    probability : float\n        Probability (confidence) that one outlier-free sample is generated.\n\n    Returns\n    -------\n    trials : int\n        Number of trials.\n\n    '
inlier_ratio = (n_inliers / float(n_samples))
nom = max(_EPSILON, (1 - probability))
denom = max(_EPSILON, (1 - (inlier_ratio ** min_samples)))
if (nom == 1):
    return 0
if (denom == 1):
    return float('inf')
tempResult = log(nom)
	
===================================================================	
_dynamic_max_trials: 21	
----------------------------	

'Determine number trials such that at least one outlier-free subset is\n    sampled for the given inlier/outlier ratio.\n\n    Parameters\n    ----------\n    n_inliers : int\n        Number of inliers in the data.\n\n    n_samples : int\n        Total number of samples in the data.\n\n    min_samples : int\n        Minimum number of samples chosen randomly from original data.\n\n    probability : float\n        Probability (confidence) that one outlier-free sample is generated.\n\n    Returns\n    -------\n    trials : int\n        Number of trials.\n\n    '
inlier_ratio = (n_inliers / float(n_samples))
nom = max(_EPSILON, (1 - probability))
denom = max(_EPSILON, (1 - (inlier_ratio ** min_samples)))
if (nom == 1):
    return 0
if (denom == 1):
    return float('inf')
tempResult = log(denom)
	
===================================================================	
SGDClassifier._predict_log_proba: 352	
----------------------------	

tempResult = log(self.predict_proba(X))
	
===================================================================	
log_loss: 33	
----------------------------	

tempResult = log((1.0 + numpy.exp(((- y) * p))))
	
===================================================================	
DenseSGDClassifierTestCase.test_sgd_proba: 347	
----------------------------	

clf = SGDClassifier(loss='hinge', alpha=0.01, n_iter=10).fit(X, Y)
assert_false(hasattr(clf, 'predict_proba'))
assert_false(hasattr(clf, 'predict_log_proba'))
for loss in ['log', 'modified_huber']:
    clf = self.factory(loss='modified_huber', alpha=0.01, n_iter=10)
    clf.fit(X, Y)
    p = clf.predict_proba([[3, 2]])
    assert_true((p[(0, 1)] > 0.5))
    p = clf.predict_proba([[(- 1), (- 1)]])
    assert_true((p[(0, 1)] < 0.5))
    p = clf.predict_log_proba([[3, 2]])
    assert_true((p[(0, 1)] > p[(0, 0)]))
    p = clf.predict_log_proba([[(- 1), (- 1)]])
    assert_true((p[(0, 1)] < p[(0, 0)]))
clf = self.factory(loss='log', alpha=0.01, n_iter=10).fit(X2, Y2)
d = clf.decision_function([[0.1, (- 0.1)], [0.3, 0.2]])
p = clf.predict_proba([[0.1, (- 0.1)], [0.3, 0.2]])
assert_array_equal(numpy.argmax(p, axis=1), numpy.argmax(d, axis=1))
assert_almost_equal(p[0].sum(), 1)
assert_true(numpy.all((p[0] >= 0)))
p = clf.predict_proba([[(- 1), (- 1)]])
d = clf.decision_function([[(- 1), (- 1)]])
assert_array_equal(numpy.argsort(p[0]), numpy.argsort(d[0]))
l = clf.predict_log_proba([[3, 2]])
p = clf.predict_proba([[3, 2]])
tempResult = log(p)
	
===================================================================	
DenseSGDClassifierTestCase.test_sgd_proba: 350	
----------------------------	

clf = SGDClassifier(loss='hinge', alpha=0.01, n_iter=10).fit(X, Y)
assert_false(hasattr(clf, 'predict_proba'))
assert_false(hasattr(clf, 'predict_log_proba'))
for loss in ['log', 'modified_huber']:
    clf = self.factory(loss='modified_huber', alpha=0.01, n_iter=10)
    clf.fit(X, Y)
    p = clf.predict_proba([[3, 2]])
    assert_true((p[(0, 1)] > 0.5))
    p = clf.predict_proba([[(- 1), (- 1)]])
    assert_true((p[(0, 1)] < 0.5))
    p = clf.predict_log_proba([[3, 2]])
    assert_true((p[(0, 1)] > p[(0, 0)]))
    p = clf.predict_log_proba([[(- 1), (- 1)]])
    assert_true((p[(0, 1)] < p[(0, 0)]))
clf = self.factory(loss='log', alpha=0.01, n_iter=10).fit(X2, Y2)
d = clf.decision_function([[0.1, (- 0.1)], [0.3, 0.2]])
p = clf.predict_proba([[0.1, (- 0.1)], [0.3, 0.2]])
assert_array_equal(numpy.argmax(p, axis=1), numpy.argmax(d, axis=1))
assert_almost_equal(p[0].sum(), 1)
assert_true(numpy.all((p[0] >= 0)))
p = clf.predict_proba([[(- 1), (- 1)]])
d = clf.decision_function([[(- 1), (- 1)]])
assert_array_equal(numpy.argsort(p[0]), numpy.argsort(d[0]))
l = clf.predict_log_proba([[3, 2]])
p = clf.predict_proba([[3, 2]])
assert_array_almost_equal(numpy.log(p), l)
l = clf.predict_log_proba([[(- 1), (- 1)]])
p = clf.predict_proba([[(- 1), (- 1)]])
tempResult = log(p)
	
===================================================================	
_kl_divergence_error: 70	
----------------------------	

"t-SNE objective function: the absolute error of the\n    KL divergence of p_ijs and q_ijs.\n\n    Parameters\n    ----------\n    params : array, shape (n_params,)\n        Unraveled embedding.\n\n    P : array, shape (n_samples * (n_samples-1) / 2,)\n        Condensed joint probability matrix.\n\n    neighbors : array (n_samples, K)\n        The neighbors is not actually required to calculate the\n        divergence, but is here to match the signature of the\n        gradient function\n\n    degrees_of_freedom : float\n        Degrees of freedom of the Student's-t distribution.\n\n    n_samples : int\n        Number of samples.\n\n    n_components : int\n        Dimension of the embedded space.\n\n    Returns\n    -------\n    kl_divergence : float\n        Kullback-Leibler divergence of p_ij and q_ij.\n\n    grad : array, shape (n_params,)\n        Unraveled gradient of the Kullback-Leibler divergence with respect to\n        the embedding.\n    "
X_embedded = params.reshape(n_samples, n_components)
n = pdist(X_embedded, 'sqeuclidean')
n += 1.0
n /= degrees_of_freedom
n **= ((degrees_of_freedom + 1.0) / (- 2.0))
Q = numpy.maximum((n / (2.0 * numpy.sum(n))), MACHINE_EPSILON)
if (len(P.shape) == 2):
    P = squareform(P)
tempResult = log((P / Q))
	
===================================================================	
_kl_divergence: 50	
----------------------------	

"t-SNE objective function: gradient of the KL divergence\n    of p_ijs and q_ijs and the absolute error.\n\n    Parameters\n    ----------\n    params : array, shape (n_params,)\n        Unraveled embedding.\n\n    P : array, shape (n_samples * (n_samples-1) / 2,)\n        Condensed joint probability matrix.\n\n    degrees_of_freedom : float\n        Degrees of freedom of the Student's-t distribution.\n\n    n_samples : int\n        Number of samples.\n\n    n_components : int\n        Dimension of the embedded space.\n\n    skip_num_points : int (optional, default:0)\n        This does not compute the gradient for points with indices below\n        `skip_num_points`. This is useful when computing transforms of new\n        data where you'd like to keep the old data fixed.\n\n    Returns\n    -------\n    kl_divergence : float\n        Kullback-Leibler divergence of p_ij and q_ij.\n\n    grad : array, shape (n_params,)\n        Unraveled gradient of the Kullback-Leibler divergence with respect to\n        the embedding.\n    "
X_embedded = params.reshape(n_samples, n_components)
n = pdist(X_embedded, 'sqeuclidean')
n += 1.0
n /= degrees_of_freedom
n **= ((degrees_of_freedom + 1.0) / (- 2.0))
Q = numpy.maximum((n / (2.0 * numpy.sum(n))), MACHINE_EPSILON)
tempResult = log((P / Q))
	
===================================================================	
test_binary_search: 96	
----------------------------	

random_state = check_random_state(0)
distances = random_state.randn(50, 2).astype(numpy.float32)
distances = numpy.abs(distances.dot(distances.T))
numpy.fill_diagonal(distances, 0.0)
desired_perplexity = 25.0
P = _binary_search_perplexity(distances, None, desired_perplexity, verbose=0)
P = numpy.maximum(P, np.finfo(np.double).eps)
tempResult = log(P[i])
	
===================================================================	
log_loss: 380	
----------------------------	

'Log loss, aka logistic loss or cross-entropy loss.\n\n    This is the loss function used in (multinomial) logistic regression\n    and extensions of it such as neural networks, defined as the negative\n    log-likelihood of the true labels given a probabilistic classifier\'s\n    predictions. The log loss is only defined for two or more labels.\n    For a single sample with true label yt in {0,1} and\n    estimated probability yp that yt = 1, the log loss is\n\n        -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n\n    Read more in the :ref:`User Guide <log_loss>`.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels for n_samples samples.\n\n    y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n        Predicted probabilities, as returned by a classifier\'s\n        predict_proba method. If ``y_pred.shape = (n_samples,)``\n        the probabilities provided are assumed to be that of the\n        positive class. The labels in ``y_pred`` are assumed to be\n        ordered alphabetically, as done by\n        :class:`preprocessing.LabelBinarizer`.\n\n    eps : float\n        Log loss is undefined for p=0 or p=1, so probabilities are\n        clipped to max(eps, min(1 - eps, p)).\n\n    normalize : bool, optional (default=True)\n        If true, return the mean loss per sample.\n        Otherwise, return the sum of the per-sample losses.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    labels : array-like, optional (default=None)\n        If not provided, labels will be inferred from y_true. If ``labels``\n        is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n        assumed to be binary and are inferred from ``y_true``.\n        .. versionadded:: 0.18\n\n    Returns\n    -------\n    loss : float\n\n    Examples\n    --------\n    >>> log_loss(["spam", "ham", "ham", "spam"],  # doctest: +ELLIPSIS\n    ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n    0.21616...\n\n    References\n    ----------\n    C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n    p. 209.\n\n    Notes\n    -----\n    The logarithm used is the natural logarithm (base-e).\n    '
y_pred = check_array(y_pred, ensure_2d=False)
check_consistent_length(y_pred, y_true)
lb = LabelBinarizer()
if (labels is not None):
    lb.fit(labels)
else:
    lb.fit(y_true)
if (len(lb.classes_) == 1):
    if (labels is None):
        raise ValueError('y_true contains only one label ({0}). Please provide the true labels explicitly through the labels argument.'.format(lb.classes_[0]))
    else:
        raise ValueError('The labels array needs to contain at least two labels for log_loss, got {0}.'.format(lb.classes_))
transformed_labels = lb.transform(y_true)
if (transformed_labels.shape[1] == 1):
    transformed_labels = numpy.append((1 - transformed_labels), transformed_labels, axis=1)
y_pred = numpy.clip(y_pred, eps, (1 - eps))
if (y_pred.ndim == 1):
    y_pred = y_pred[:, numpy.newaxis]
if (y_pred.shape[1] == 1):
    y_pred = numpy.append((1 - y_pred), y_pred, axis=1)
transformed_labels = check_array(transformed_labels)
if (len(lb.classes_) != y_pred.shape[1]):
    if (labels is None):
        raise ValueError('y_true and y_pred contain different number of classes {0}, {1}. Please provide the true labels explicitly through the labels argument. Classes found in y_true: {2}'.format(transformed_labels.shape[1], y_pred.shape[1], lb.classes_))
    else:
        raise ValueError('The number of classes in labels is different from that in y_pred. Classes found in labels: {0}'.format(lb.classes_))
y_pred /= y_pred.sum(axis=1)[:, numpy.newaxis]
tempResult = log(y_pred)
	
===================================================================	
mutual_info_score: 107	
----------------------------	

"Mutual Information between two clusterings.\n\n    The Mutual Information is a measure of the similarity between two labels of\n    the same data. Where :math:`P(i)` is the probability of a random sample\n    occurring in cluster :math:`U_i` and :math:`P'(j)` is the probability of a\n    random sample occurring in cluster :math:`V_j`, the Mutual Information\n    between clusterings :math:`U` and :math:`V` is given as:\n\n    .. math::\n\n        MI(U,V)=\\sum_{i=1}^R \\sum_{j=1}^C P(i,j)\\log\\frac{P(i,j)}{P(i)P'(j)}\n\n    This is equal to the Kullback-Leibler divergence of the joint distribution\n    with the product distribution of the marginals.\n\n    This metric is independent of the absolute values of the labels:\n    a permutation of the class or cluster label values won't change the\n    score value in any way.\n\n    This metric is furthermore symmetric: switching ``label_true`` with\n    ``label_pred`` will return the same score value. This can be useful to\n    measure the agreement of two independent label assignments strategies\n    on the same dataset when the real ground truth is not known.\n\n    Read more in the :ref:`User Guide <mutual_info_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    labels_pred : array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    contingency : {None, array, sparse matrix},\n                  shape = [n_classes_true, n_classes_pred]\n        A contingency matrix given by the :func:`contingency_matrix` function.\n        If value is ``None``, it will be computed, otherwise the given value is\n        used, with ``labels_true`` and ``labels_pred`` ignored.\n\n    Returns\n    -------\n    mi: float\n       Mutual information, a non-negative value\n\n    See also\n    --------\n    adjusted_mutual_info_score: Adjusted against chance Mutual Information\n    normalized_mutual_info_score: Normalized Mutual Information\n    "
if (contingency is None):
    (labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
else:
    contingency = check_array(contingency, accept_sparse=['csr', 'csc', 'coo'], dtype=[int, numpy.int32, numpy.int64])
if isinstance(contingency, numpy.ndarray):
    (nzx, nzy) = numpy.nonzero(contingency)
    nz_val = contingency[(nzx, nzy)]
elif scipy.sparse.issparse(contingency):
    (nzx, nzy, nz_val) = scipy.sparse.find(contingency)
else:
    raise ValueError(("Unsupported type for 'contingency': %s" % type(contingency)))
contingency_sum = contingency.sum()
pi = numpy.ravel(contingency.sum(axis=1))
pj = numpy.ravel(contingency.sum(axis=0))
tempResult = log(nz_val)
	
===================================================================	
mutual_info_score: 110	
----------------------------	

"Mutual Information between two clusterings.\n\n    The Mutual Information is a measure of the similarity between two labels of\n    the same data. Where :math:`P(i)` is the probability of a random sample\n    occurring in cluster :math:`U_i` and :math:`P'(j)` is the probability of a\n    random sample occurring in cluster :math:`V_j`, the Mutual Information\n    between clusterings :math:`U` and :math:`V` is given as:\n\n    .. math::\n\n        MI(U,V)=\\sum_{i=1}^R \\sum_{j=1}^C P(i,j)\\log\\frac{P(i,j)}{P(i)P'(j)}\n\n    This is equal to the Kullback-Leibler divergence of the joint distribution\n    with the product distribution of the marginals.\n\n    This metric is independent of the absolute values of the labels:\n    a permutation of the class or cluster label values won't change the\n    score value in any way.\n\n    This metric is furthermore symmetric: switching ``label_true`` with\n    ``label_pred`` will return the same score value. This can be useful to\n    measure the agreement of two independent label assignments strategies\n    on the same dataset when the real ground truth is not known.\n\n    Read more in the :ref:`User Guide <mutual_info_score>`.\n\n    Parameters\n    ----------\n    labels_true : int array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    labels_pred : array, shape = [n_samples]\n        A clustering of the data into disjoint subsets.\n\n    contingency : {None, array, sparse matrix},\n                  shape = [n_classes_true, n_classes_pred]\n        A contingency matrix given by the :func:`contingency_matrix` function.\n        If value is ``None``, it will be computed, otherwise the given value is\n        used, with ``labels_true`` and ``labels_pred`` ignored.\n\n    Returns\n    -------\n    mi: float\n       Mutual information, a non-negative value\n\n    See also\n    --------\n    adjusted_mutual_info_score: Adjusted against chance Mutual Information\n    normalized_mutual_info_score: Normalized Mutual Information\n    "
if (contingency is None):
    (labels_true, labels_pred) = check_clusterings(labels_true, labels_pred)
    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
else:
    contingency = check_array(contingency, accept_sparse=['csr', 'csc', 'coo'], dtype=[int, numpy.int32, numpy.int64])
if isinstance(contingency, numpy.ndarray):
    (nzx, nzy) = numpy.nonzero(contingency)
    nz_val = contingency[(nzx, nzy)]
elif scipy.sparse.issparse(contingency):
    (nzx, nzy, nz_val) = scipy.sparse.find(contingency)
else:
    raise ValueError(("Unsupported type for 'contingency': %s" % type(contingency)))
contingency_sum = contingency.sum()
pi = numpy.ravel(contingency.sum(axis=1))
pj = numpy.ravel(contingency.sum(axis=0))
log_contingency_nm = numpy.log(nz_val)
contingency_nm = (nz_val / contingency_sum)
outer = (pi.take(nzx) * pj.take(nzy))
tempResult = log(outer)
	
===================================================================	
entropy: 162	
----------------------------	

'Calculates the entropy for a labeling.'
if (len(labels) == 0):
    return 1.0
label_idx = numpy.unique(labels, return_inverse=True)[1]
pi = bincount(label_idx).astype(numpy.float64)
pi = pi[(pi > 0)]
pi_sum = numpy.sum(pi)
tempResult = log(pi)
	
===================================================================	
test_log_loss: 729	
----------------------------	

y_true = ['no', 'no', 'no', 'yes', 'yes', 'yes']
y_pred = numpy.array([[0.5, 0.5], [0.1, 0.9], [0.01, 0.99], [0.9, 0.1], [0.75, 0.25], [0.001, 0.999]])
loss = log_loss(y_true, y_pred)
assert_almost_equal(loss, 1.8817971)
y_true = [1, 0, 2]
y_pred = [[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]]
loss = log_loss(y_true, y_pred, normalize=True)
assert_almost_equal(loss, 0.6904911)
y_true *= 2
y_pred *= 2
loss = log_loss(y_true, y_pred, normalize=False)
assert_almost_equal(loss, (0.6904911 * 6), decimal=6)
y_pred = (numpy.asarray(y_pred) > 0.5)
loss = log_loss(y_true, y_pred, normalize=True, eps=0.1)
assert_almost_equal(loss, log_loss(y_true, numpy.clip(y_pred, 0.1, 0.9)))
y_true = [1, 0, 2]
y_pred = [[0.2, 0.7], [0.6, 0.5], [0.4, 0.1]]
assert_raises(ValueError, log_loss, y_true, y_pred)
y_true = ['ham', 'spam', 'spam', 'ham']
y_pred = [[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]]
loss = log_loss(y_true, y_pred)
assert_almost_equal(loss, 1.0383217, decimal=6)
y_true = [2, 2]
y_pred = [[0.2, 0.7], [0.6, 0.5]]
y_score = numpy.array([[0.1, 0.9], [0.1, 0.9]])
error_str = 'y_true contains only one label (2). Please provide the true labels explicitly through the labels argument.'
assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred)
y_pred = [[0.2, 0.7], [0.6, 0.5], [0.2, 0.3]]
error_str = 'Found input variables with inconsistent numbers of samples: [3, 2]'
assert_raise_message(ValueError, error_str, log_loss, y_true, y_pred)
tempResult = log(y_score[:, 1])
	
===================================================================	
BayesianGaussianMixture._estimate_log_prob: 180	
----------------------------	

(_, n_features) = X.shape
tempResult = log(self.degrees_of_freedom_)
	
===================================================================	
BayesianGaussianMixture._estimate_log_prob: 181	
----------------------------	

(_, n_features) = X.shape
log_gauss = (_estimate_log_gaussian_prob(X, self.means_, self.precisions_cholesky_, self.covariance_type) - ((0.5 * n_features) * numpy.log(self.degrees_of_freedom_)))
tempResult = log(2.0)
	
===================================================================	
BayesianGaussianMixture._compute_lower_bound: 187	
----------------------------	

'Estimate the lower bound of the model.\n\n        The lower bound on the likelihood (of the training data with respect to\n        the model) is used to detect the convergence and has to decrease at\n        each iteration.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n\n        log_prob_norm : float\n            Logarithm of the probability of each sample in X.\n\n        Returns\n        -------\n        lower_bound : float\n        '
(n_features,) = self.mean_prior_.shape
tempResult = log(self.degrees_of_freedom_)
	
===================================================================	
BayesianGaussianMixture._compute_lower_bound: 196	
----------------------------	

'Estimate the lower bound of the model.\n\n        The lower bound on the likelihood (of the training data with respect to\n        the model) is used to detect the convergence and has to decrease at\n        each iteration.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n\n        log_resp : array, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n\n        log_prob_norm : float\n            Logarithm of the probability of each sample in X.\n\n        Returns\n        -------\n        lower_bound : float\n        '
(n_features,) = self.mean_prior_.shape
log_det_precisions_chol = (_compute_log_det_cholesky(self.precisions_cholesky_, self.covariance_type, n_features) - ((0.5 * n_features) * numpy.log(self.degrees_of_freedom_)))
if (self.covariance_type == 'tied'):
    log_wishart = (self.n_components * numpy.float64(_log_wishart_norm(self.degrees_of_freedom_, log_det_precisions_chol, n_features)))
else:
    log_wishart = numpy.sum(_log_wishart_norm(self.degrees_of_freedom_, log_det_precisions_chol, n_features))
if (self.weight_concentration_prior_type == 'dirichlet_process'):
    log_norm_weight = (- numpy.sum(betaln(self.weight_concentration_[0], self.weight_concentration_[1])))
else:
    log_norm_weight = _log_dirichlet_norm(self.weight_concentration_)
tempResult = log(self.mean_precision_)
	
===================================================================	
_DPGMMBase._fit: 304	
----------------------------	

"Estimate model parameters with the variational\n        algorithm.\n\n        For a full derivation and description of the algorithm see\n        doc/modules/dp-derivation.rst\n        or\n        http://scikit-learn.org/stable/modules/dp-derivation.html\n\n        A initialization step is performed before entering the em\n        algorithm. If you want to avoid this step, set the keyword\n        argument init_params to the empty string '' when creating\n        the object. Likewise, if you would like just to do an\n        initialization, set n_iter=0.\n\n        Parameters\n        ----------\n        X : array_like, shape (n, n_features)\n            List of n_features-dimensional data points.  Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        responsibilities : array, shape (n_samples, n_components)\n            Posterior probabilities of each mixture component for each\n            observation.\n        "
self.random_state_ = check_random_state(self.random_state)
X = check_array(X)
if (X.ndim == 1):
    X = X[:, numpy.newaxis]
(n_samples, n_features) = X.shape
z = numpy.ones((n_samples, self.n_components))
z /= self.n_components
tempResult = log((2 * numpy.pi))
	
===================================================================	
_DPGMMBase._fit: 305	
----------------------------	

"Estimate model parameters with the variational\n        algorithm.\n\n        For a full derivation and description of the algorithm see\n        doc/modules/dp-derivation.rst\n        or\n        http://scikit-learn.org/stable/modules/dp-derivation.html\n\n        A initialization step is performed before entering the em\n        algorithm. If you want to avoid this step, set the keyword\n        argument init_params to the empty string '' when creating\n        the object. Likewise, if you would like just to do an\n        initialization, set n_iter=0.\n\n        Parameters\n        ----------\n        X : array_like, shape (n, n_features)\n            List of n_features-dimensional data points.  Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        responsibilities : array, shape (n_samples, n_components)\n            Posterior probabilities of each mixture component for each\n            observation.\n        "
self.random_state_ = check_random_state(self.random_state)
X = check_array(X)
if (X.ndim == 1):
    X = X[:, numpy.newaxis]
(n_samples, n_features) = X.shape
z = numpy.ones((n_samples, self.n_components))
z /= self.n_components
self._initial_bound = (((- 0.5) * n_features) * numpy.log((2 * numpy.pi)))
tempResult = log(((2 * numpy.pi) * numpy.e))
	
===================================================================	
_DPGMMBase._fit: 317	
----------------------------	

"Estimate model parameters with the variational\n        algorithm.\n\n        For a full derivation and description of the algorithm see\n        doc/modules/dp-derivation.rst\n        or\n        http://scikit-learn.org/stable/modules/dp-derivation.html\n\n        A initialization step is performed before entering the em\n        algorithm. If you want to avoid this step, set the keyword\n        argument init_params to the empty string '' when creating\n        the object. Likewise, if you would like just to do an\n        initialization, set n_iter=0.\n\n        Parameters\n        ----------\n        X : array_like, shape (n, n_features)\n            List of n_features-dimensional data points.  Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        responsibilities : array, shape (n_samples, n_components)\n            Posterior probabilities of each mixture component for each\n            observation.\n        "
self.random_state_ = check_random_state(self.random_state)
X = check_array(X)
if (X.ndim == 1):
    X = X[:, numpy.newaxis]
(n_samples, n_features) = X.shape
z = numpy.ones((n_samples, self.n_components))
z /= self.n_components
self._initial_bound = (((- 0.5) * n_features) * numpy.log((2 * numpy.pi)))
self._initial_bound -= numpy.log(((2 * numpy.pi) * numpy.e))
if ((self.init_params != '') or (not hasattr(self, 'gamma_'))):
    self._initialize_gamma()
if (('m' in self.init_params) or (not hasattr(self, 'means_'))):
    self.means_ = cluster.KMeans(n_clusters=self.n_components, random_state=self.random_state_).fit(X).cluster_centers_[::(- 1)]
if (('w' in self.init_params) or (not hasattr(self, 'weights_'))):
    self.weights_ = numpy.tile((1.0 / self.n_components), self.n_components)
if (('c' in self.init_params) or (not hasattr(self, 'precs_'))):
    if (self.covariance_type == 'spherical'):
        self.dof_ = numpy.ones(self.n_components)
        self.scale_ = numpy.ones(self.n_components)
        self.precs_ = numpy.ones((self.n_components, n_features))
        tempResult = log(self.scale_)
	
===================================================================	
_DPGMMBase._fit: 323	
----------------------------	

"Estimate model parameters with the variational\n        algorithm.\n\n        For a full derivation and description of the algorithm see\n        doc/modules/dp-derivation.rst\n        or\n        http://scikit-learn.org/stable/modules/dp-derivation.html\n\n        A initialization step is performed before entering the em\n        algorithm. If you want to avoid this step, set the keyword\n        argument init_params to the empty string '' when creating\n        the object. Likewise, if you would like just to do an\n        initialization, set n_iter=0.\n\n        Parameters\n        ----------\n        X : array_like, shape (n, n_features)\n            List of n_features-dimensional data points.  Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        responsibilities : array, shape (n_samples, n_components)\n            Posterior probabilities of each mixture component for each\n            observation.\n        "
self.random_state_ = check_random_state(self.random_state)
X = check_array(X)
if (X.ndim == 1):
    X = X[:, numpy.newaxis]
(n_samples, n_features) = X.shape
z = numpy.ones((n_samples, self.n_components))
z /= self.n_components
self._initial_bound = (((- 0.5) * n_features) * numpy.log((2 * numpy.pi)))
self._initial_bound -= numpy.log(((2 * numpy.pi) * numpy.e))
if ((self.init_params != '') or (not hasattr(self, 'gamma_'))):
    self._initialize_gamma()
if (('m' in self.init_params) or (not hasattr(self, 'means_'))):
    self.means_ = cluster.KMeans(n_clusters=self.n_components, random_state=self.random_state_).fit(X).cluster_centers_[::(- 1)]
if (('w' in self.init_params) or (not hasattr(self, 'weights_'))):
    self.weights_ = numpy.tile((1.0 / self.n_components), self.n_components)
if (('c' in self.init_params) or (not hasattr(self, 'precs_'))):
    if (self.covariance_type == 'spherical'):
        self.dof_ = numpy.ones(self.n_components)
        self.scale_ = numpy.ones(self.n_components)
        self.precs_ = numpy.ones((self.n_components, n_features))
        self.bound_prec_ = ((0.5 * n_features) * (digamma(self.dof_) - numpy.log(self.scale_)))
    elif (self.covariance_type == 'diag'):
        self.dof_ = (1 + (0.5 * n_features))
        self.dof_ *= numpy.ones((self.n_components, n_features))
        self.scale_ = numpy.ones((self.n_components, n_features))
        self.precs_ = numpy.ones((self.n_components, n_features))
        tempResult = log(self.scale_)
	
===================================================================	
wishart_logz: 46	
----------------------------	

'The logarithm of the normalization constant for the wishart distribution'
z = 0.0
tempResult = log(2)
	
===================================================================	
wishart_logz: 47	
----------------------------	

'The logarithm of the normalization constant for the wishart distribution'
z = 0.0
z += (((0.5 * v) * n_features) * numpy.log(2))
tempResult = log(numpy.pi)
	
===================================================================	
wishart_logz: 48	
----------------------------	

'The logarithm of the normalization constant for the wishart distribution'
z = 0.0
z += (((0.5 * v) * n_features) * numpy.log(2))
z += ((0.25 * (n_features * (n_features - 1))) * numpy.log(numpy.pi))
tempResult = log(dets)
	
===================================================================	
_DPGMMBase._update_precisions: 167	
----------------------------	

'Update the variational distributions for the precisions'
n_features = X.shape[1]
if (self.covariance_type == 'spherical'):
    self.dof_ = ((0.5 * n_features) * numpy.sum(z, axis=0))
    for k in range(self.n_components):
        sq_diff = numpy.sum(((X - self.means_[k]) ** 2), axis=1)
        self.scale_[k] = 1.0
        self.scale_[k] += (0.5 * numpy.sum((z.T[k] * (sq_diff + n_features))))
        tempResult = log(self.scale_[k])
	
===================================================================	
_DPGMMBase._update_precisions: 175	
----------------------------	

'Update the variational distributions for the precisions'
n_features = X.shape[1]
if (self.covariance_type == 'spherical'):
    self.dof_ = ((0.5 * n_features) * numpy.sum(z, axis=0))
    for k in range(self.n_components):
        sq_diff = numpy.sum(((X - self.means_[k]) ** 2), axis=1)
        self.scale_[k] = 1.0
        self.scale_[k] += (0.5 * numpy.sum((z.T[k] * (sq_diff + n_features))))
        self.bound_prec_[k] = ((0.5 * n_features) * (digamma(self.dof_[k]) - numpy.log(self.scale_[k])))
    self.precs_ = np.tile((self.dof_ / self.scale_), [n_features, 1]).T
elif (self.covariance_type == 'diag'):
    for k in range(self.n_components):
        self.dof_[k].fill((1.0 + (0.5 * numpy.sum(z.T[k], axis=0))))
        sq_diff = ((X - self.means_[k]) ** 2)
        self.scale_[k] = (numpy.ones(n_features) + (0.5 * numpy.dot(z.T[k], (sq_diff + 1))))
        self.precs_[k] = (self.dof_[k] / self.scale_[k])
        tempResult = log(self.scale_[k])
	
===================================================================	
wishart_log_det: 39	
----------------------------	

'Expected value of the log of the determinant of a Wishart\n\n    The expected value of the logarithm of the determinant of a\n    wishart-distributed random variable with the specified parameters.'
l = numpy.sum(digamma((0.5 * (a - numpy.arange((- 1), (n_features - 1))))))
tempResult = log(2)
	
===================================================================	
VBGMM._bound_proportions: 407	
----------------------------	

logprior = 0.0
dg = digamma(self.gamma_)
dg -= digamma(numpy.sum(self.gamma_))
logprior += numpy.sum((dg.reshape(((- 1), 1)) * z.T))
z_non_zeros = z[(z > np.finfo(np.float32).eps)]
tempResult = log(z_non_zeros)
	
===================================================================	
_DPGMMBase._bound_precisions: 248	
----------------------------	

'Returns the bound term related to precisions'
logprior = 0.0
if (self.covariance_type == 'spherical'):
    logprior += numpy.sum(gammaln(self.dof_))
    logprior -= numpy.sum(((self.dof_ - 1) * digamma(numpy.maximum(0.5, self.dof_))))
    tempResult = log(self.scale_)
	
===================================================================	
_DPGMMBase._bound_precisions: 252	
----------------------------	

'Returns the bound term related to precisions'
logprior = 0.0
if (self.covariance_type == 'spherical'):
    logprior += numpy.sum(gammaln(self.dof_))
    logprior -= numpy.sum(((self.dof_ - 1) * digamma(numpy.maximum(0.5, self.dof_))))
    logprior += numpy.sum((((- numpy.log(self.scale_)) + self.dof_) - self.precs_[:, 0]))
elif (self.covariance_type == 'diag'):
    logprior += numpy.sum(gammaln(self.dof_))
    logprior -= numpy.sum(((self.dof_ - 1) * digamma(numpy.maximum(0.5, self.dof_))))
    tempResult = log(self.scale_)
	
===================================================================	
_DPGMMBase._bound_proportions: 269	
----------------------------	

'Returns the bound term related to proportions'
dg12 = digamma((self.gamma_.T[1] + self.gamma_.T[2]))
dg1 = (digamma(self.gamma_.T[1]) - dg12)
dg2 = (digamma(self.gamma_.T[2]) - dg12)
cz = numpy.cumsum(z[:, ::(- 1)], axis=(- 1))[:, (- 2)::(- 1)]
logprior = (numpy.sum((cz * dg2[:(- 1)])) + numpy.sum((z * dg1)))
del cz
z_non_zeros = z[(z > np.finfo(np.float32).eps)]
tempResult = log(z_non_zeros)
	
===================================================================	
_estimate_log_gaussian_prob: 147	
----------------------------	

"Estimate the log Gaussian probability.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n\n    means : array-like, shape (n_components, n_features)\n\n    precisions_chol : array-like,\n        Cholesky decompositions of the precision matrices.\n        'full' : shape of (n_components, n_features, n_features)\n        'tied' : shape of (n_features, n_features)\n        'diag' : shape of (n_components, n_features)\n        'spherical' : shape of (n_components,)\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n\n    Returns\n    -------\n    log_prob : array, shape (n_samples, n_components)\n    "
(n_samples, n_features) = X.shape
(n_components, _) = means.shape
log_det = _compute_log_det_cholesky(precisions_chol, covariance_type, n_features)
if (covariance_type == 'full'):
    log_prob = numpy.empty((n_samples, n_components))
    for (k, (mu, prec_chol)) in enumerate(zip(means, precisions_chol)):
        y = (numpy.dot(X, prec_chol) - numpy.dot(mu, prec_chol))
        log_prob[:, k] = numpy.sum(numpy.square(y), axis=1)
elif (covariance_type == 'tied'):
    log_prob = numpy.empty((n_samples, n_components))
    for (k, mu) in enumerate(means):
        y = (numpy.dot(X, precisions_chol) - numpy.dot(mu, precisions_chol))
        log_prob[:, k] = numpy.sum(numpy.square(y), axis=1)
elif (covariance_type == 'diag'):
    precisions = (precisions_chol ** 2)
    log_prob = ((numpy.sum(((means ** 2) * precisions), 1) - (2.0 * numpy.dot(X, (means * precisions).T))) + numpy.dot((X ** 2), precisions.T))
elif (covariance_type == 'spherical'):
    precisions = (precisions_chol ** 2)
    log_prob = (((numpy.sum((means ** 2), 1) * precisions) - (2 * numpy.dot(X, (means.T * precisions)))) + numpy.outer(row_norms(X, squared=True), precisions))
tempResult = log((2 * numpy.pi))
	
===================================================================	
GaussianMixture.bic: 238	
----------------------------	

'Bayesian information criterion for the current model on the input X.\n\n        Parameters\n        ----------\n        X : array of shape (n_samples, n_dimensions)\n\n        Returns\n        -------\n        bic: float\n            The greater the better.\n        '
tempResult = log(X.shape[0])
	
===================================================================	
GaussianMixture._estimate_log_weights: 199	
----------------------------	

tempResult = log(self.weights_)
	
===================================================================	
_compute_log_det_cholesky: 117	
----------------------------	

"Compute the log-det of the cholesky decomposition of matrices.\n\n    Parameters\n    ----------\n    matrix_chol : array-like,\n        Cholesky decompositions of the matrices.\n        'full' : shape of (n_components, n_features, n_features)\n        'tied' : shape of (n_features, n_features)\n        'diag' : shape of (n_components, n_features)\n        'spherical' : shape of (n_components,)\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n\n    n_features : int\n        Number of features.\n\n    Returns\n    -------\n    log_det_precision_chol : array-like, shape (n_components,)\n        The determinant of the precision matrix for each component.\n    "
if (covariance_type == 'full'):
    (n_components, _, _) = matrix_chol.shape
    tempResult = log(matrix_chol.reshape(n_components, (- 1))[:, ::(n_features + 1)])
	
===================================================================	
_compute_log_det_cholesky: 119	
----------------------------	

"Compute the log-det of the cholesky decomposition of matrices.\n\n    Parameters\n    ----------\n    matrix_chol : array-like,\n        Cholesky decompositions of the matrices.\n        'full' : shape of (n_components, n_features, n_features)\n        'tied' : shape of (n_features, n_features)\n        'diag' : shape of (n_components, n_features)\n        'spherical' : shape of (n_components,)\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n\n    n_features : int\n        Number of features.\n\n    Returns\n    -------\n    log_det_precision_chol : array-like, shape (n_components,)\n        The determinant of the precision matrix for each component.\n    "
if (covariance_type == 'full'):
    (n_components, _, _) = matrix_chol.shape
    log_det_chol = numpy.sum(numpy.log(matrix_chol.reshape(n_components, (- 1))[:, ::(n_features + 1)]), 1)
elif (covariance_type == 'tied'):
    tempResult = log(numpy.diag(matrix_chol))
	
===================================================================	
_compute_log_det_cholesky: 121	
----------------------------	

"Compute the log-det of the cholesky decomposition of matrices.\n\n    Parameters\n    ----------\n    matrix_chol : array-like,\n        Cholesky decompositions of the matrices.\n        'full' : shape of (n_components, n_features, n_features)\n        'tied' : shape of (n_features, n_features)\n        'diag' : shape of (n_components, n_features)\n        'spherical' : shape of (n_components,)\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n\n    n_features : int\n        Number of features.\n\n    Returns\n    -------\n    log_det_precision_chol : array-like, shape (n_components,)\n        The determinant of the precision matrix for each component.\n    "
if (covariance_type == 'full'):
    (n_components, _, _) = matrix_chol.shape
    log_det_chol = numpy.sum(numpy.log(matrix_chol.reshape(n_components, (- 1))[:, ::(n_features + 1)]), 1)
elif (covariance_type == 'tied'):
    log_det_chol = numpy.sum(numpy.log(numpy.diag(matrix_chol)))
elif (covariance_type == 'diag'):
    tempResult = log(matrix_chol)
	
===================================================================	
_compute_log_det_cholesky: 123	
----------------------------	

"Compute the log-det of the cholesky decomposition of matrices.\n\n    Parameters\n    ----------\n    matrix_chol : array-like,\n        Cholesky decompositions of the matrices.\n        'full' : shape of (n_components, n_features, n_features)\n        'tied' : shape of (n_features, n_features)\n        'diag' : shape of (n_components, n_features)\n        'spherical' : shape of (n_components,)\n\n    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n\n    n_features : int\n        Number of features.\n\n    Returns\n    -------\n    log_det_precision_chol : array-like, shape (n_components,)\n        The determinant of the precision matrix for each component.\n    "
if (covariance_type == 'full'):
    (n_components, _, _) = matrix_chol.shape
    log_det_chol = numpy.sum(numpy.log(matrix_chol.reshape(n_components, (- 1))[:, ::(n_features + 1)]), 1)
elif (covariance_type == 'tied'):
    log_det_chol = numpy.sum(numpy.log(numpy.diag(matrix_chol)))
elif (covariance_type == 'diag'):
    log_det_chol = numpy.sum(numpy.log(matrix_chol), axis=1)
else:
    tempResult = log(matrix_chol)
	
===================================================================	
_GMMBase.bic: 236	
----------------------------	

'Bayesian information criterion for the current model fit\n        and the proposed data.\n\n        Parameters\n        ----------\n        X : array of shape(n_samples, n_dimensions)\n\n        Returns\n        -------\n        bic: float (the lower the better)\n        '
tempResult = log(X.shape[0])
	
===================================================================	
_log_multivariate_normal_density_full: 281	
----------------------------	

'Log probability for full covariance matrices.'
(n_samples, n_dim) = X.shape
nmix = len(means)
log_prob = numpy.empty((n_samples, nmix))
for (c, (mu, cv)) in enumerate(zip(means, covars)):
    try:
        cv_chol = scipy.linalg.cholesky(cv, lower=True)
    except scipy.linalg.LinAlgError:
        try:
            cv_chol = scipy.linalg.cholesky((cv + (min_covar * numpy.eye(n_dim))), lower=True)
        except scipy.linalg.LinAlgError:
            raise ValueError("'covars' must be symmetric, positive-definite")
    tempResult = log(numpy.diagonal(cv_chol))
	
===================================================================	
_log_multivariate_normal_density_full: 283	
----------------------------	

'Log probability for full covariance matrices.'
(n_samples, n_dim) = X.shape
nmix = len(means)
log_prob = numpy.empty((n_samples, nmix))
for (c, (mu, cv)) in enumerate(zip(means, covars)):
    try:
        cv_chol = scipy.linalg.cholesky(cv, lower=True)
    except scipy.linalg.LinAlgError:
        try:
            cv_chol = scipy.linalg.cholesky((cv + (min_covar * numpy.eye(n_dim))), lower=True)
        except scipy.linalg.LinAlgError:
            raise ValueError("'covars' must be symmetric, positive-definite")
    cv_log_det = (2 * numpy.sum(numpy.log(numpy.diagonal(cv_chol))))
    cv_sol = linalg.solve_triangular(cv_chol, (X - mu).T, lower=True).T
    tempResult = log((2 * numpy.pi))
	
===================================================================	
_log_multivariate_normal_density_diag: 251	
----------------------------	

'Compute Gaussian log-density at X for a diagonal model.'
(n_samples, n_dim) = X.shape
tempResult = log((2 * numpy.pi))
	
===================================================================	
_log_multivariate_normal_density_diag: 251	
----------------------------	

'Compute Gaussian log-density at X for a diagonal model.'
(n_samples, n_dim) = X.shape
tempResult = log(covars)
	
===================================================================	
_GMMBase.score_samples: 88	
----------------------------	

'Return the per-sample likelihood of the data under the model.\n\n        Compute the log probability of X under the model and\n        return the posterior distribution (responsibilities) of each\n        mixture component for each element of X.\n\n        Parameters\n        ----------\n        X: array_like, shape (n_samples, n_features)\n            List of n_features-dimensional data points. Each row\n            corresponds to a single data point.\n\n        Returns\n        -------\n        logprob : array_like, shape (n_samples,)\n            Log probabilities of each data point in X.\n\n        responsibilities : array_like, shape (n_samples, n_components)\n            Posterior probabilities of each mixture component for each\n            observation\n        '
check_is_fitted(self, 'means_')
X = check_array(X)
if (X.ndim == 1):
    X = X[:, numpy.newaxis]
if (X.size == 0):
    return (numpy.array([]), numpy.empty((0, self.n_components)))
if (X.shape[1] != self.means_.shape[1]):
    raise ValueError('The shape of X  is not compatible with self')
tempResult = log(self.weights_)
	
===================================================================	
test_log_wishart_norm: 26	
----------------------------	

rng = numpy.random.RandomState(0)
(n_components, n_features) = (5, 2)
degrees_of_freedom = (numpy.abs(rng.rand(n_components)) + 1.0)
tempResult = log(range(2, (2 + n_components)))
	
===================================================================	
test_log_wishart_norm: 29	
----------------------------	

rng = numpy.random.RandomState(0)
(n_components, n_features) = (5, 2)
degrees_of_freedom = (numpy.abs(rng.rand(n_components)) + 1.0)
log_det_precisions_chol = (n_features * numpy.log(range(2, (2 + n_components))))
expected_norm = numpy.empty(5)
for (k, (degrees_of_freedom_k, log_det_k)) in enumerate(zip(degrees_of_freedom, log_det_precisions_chol)):
    tempResult = log(2.0)
	
===================================================================	
test_log_normalize: 84	
----------------------------	

v = numpy.array([0.1, 0.8, 0.01, 0.09])
tempResult = log((2 * v))
	
===================================================================	
test_gaussian_mixture_aic_bic: 420	
----------------------------	

rng = numpy.random.RandomState(0)
(n_samples, n_features, n_components) = (50, 3, 2)
X = rng.randn(n_samples, n_features)
tempResult = log((2 * numpy.pi))
	
===================================================================	
test_gaussian_mixture_aic_bic: 425	
----------------------------	

rng = numpy.random.RandomState(0)
(n_samples, n_features, n_components) = (50, 3, 2)
X = rng.randn(n_samples, n_features)
sgh = (0.5 * (fast_logdet(numpy.cov(X.T, bias=1)) + (n_features * (1 + numpy.log((2 * numpy.pi))))))
for cv_type in COVARIANCE_TYPE:
    g = GaussianMixture(n_components=n_components, covariance_type=cv_type, random_state=rng, max_iter=200)
    g.fit(X)
    aic = (((2 * n_samples) * sgh) + (2 * g._n_parameters()))
    tempResult = log(n_samples)
	
===================================================================	
test_compute_log_det_cholesky: 260	
----------------------------	

n_features = 2
rand_data = RandomData(numpy.random.RandomState(0))
for covar_type in COVARIANCE_TYPE:
    covariance = rand_data.covariances[covar_type]
    if (covar_type == 'full'):
        predected_det = numpy.array([scipy.linalg.det(cov) for cov in covariance])
    elif (covar_type == 'tied'):
        predected_det = scipy.linalg.det(covariance)
    elif (covar_type == 'diag'):
        predected_det = numpy.array([numpy.prod(cov) for cov in covariance])
    elif (covar_type == 'spherical'):
        predected_det = (covariance ** n_features)
    expected_det = _compute_log_det_cholesky(_compute_precision_cholesky(covariance, covar_type), covar_type, n_features=n_features)
    tempResult = log(predected_det)
	
===================================================================	
_naive_lmvnpdf_diag: 41	
----------------------------	

ref = numpy.empty((len(X), len(mu)))
stds = numpy.sqrt(cv)
for (i, (m, std)) in enumerate(zip(mu, stds)):
    tempResult = log(scipy.stats.norm.pdf(X, m, std))
	
===================================================================	
test_aic: 278	
----------------------------	

(n_samples, n_dim, n_components) = (50, 3, 2)
X = rng.randn(n_samples, n_dim)
tempResult = log((2 * numpy.pi))
	
===================================================================	
test_aic: 283	
----------------------------	

(n_samples, n_dim, n_components) = (50, 3, 2)
X = rng.randn(n_samples, n_dim)
SGH = (0.5 * (X.var() + numpy.log((2 * numpy.pi))))
for cv_type in ['full', 'tied', 'diag', 'spherical']:
    g = sklearn.mixture.GMM(n_components=n_components, covariance_type=cv_type, random_state=rng, min_covar=1e-07)
    g.fit(X)
    aic = ((((2 * n_samples) * SGH) * n_dim) + (2 * g._n_parameters()))
    tempResult = log(n_samples)
	
===================================================================	
KernelDensity.score_samples: 63	
----------------------------	

'Evaluate the density model on the data.\n\n        Parameters\n        ----------\n        X : array_like, shape (n_samples, n_features)\n            An array of points to query.  Last dimension should match dimension\n            of training data (n_features).\n\n        Returns\n        -------\n        density : ndarray, shape (n_samples,)\n            The array of log(density) evaluations.\n        '
X = check_array(X, order='C', dtype=DTYPE)
N = self.tree_.data.shape[0]
atol_N = (self.atol * N)
log_density = self.tree_.kernel_density(X, h=self.bandwidth, kernel=self.kernel, atol=atol_N, rtol=self.rtol, breadth_first=self.breadth_first, return_log=True)
tempResult = log(N)
	
===================================================================	
MLPClassifier.predict_log_proba: 396	
----------------------------	

'Return the log of probability estimates.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input data.\n\n        Returns\n        -------\n        log_y_prob : array-like, shape (n_samples, n_classes)\n            The predicted log-probability of the sample for each class\n            in the model, where classes are ordered as they are in\n            `self.classes_`. Equivalent to log(predict_proba(X))\n        '
y_prob = self.predict_proba(X)
tempResult = log(y_prob, out=y_prob)
	
===================================================================	
log_loss: 59	
----------------------------	

"Compute Logistic loss for classification.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels.\n\n    y_prob : array-like of float, shape = (n_samples, n_classes)\n        Predicted probabilities, as returned by a classifier's\n        predict_proba method.\n\n    Returns\n    -------\n    loss : float\n        The degree to which the samples are correctly predicted.\n    "
y_prob = numpy.clip(y_prob, 1e-10, (1 - 1e-10))
if (y_prob.shape[1] == 1):
    y_prob = numpy.append((1 - y_prob), y_prob, axis=1)
if (y_true.shape[1] == 1):
    y_true = numpy.append((1 - y_true), y_true, axis=1)
tempResult = log(y_prob)
	
===================================================================	
binary_log_loss: 64	
----------------------------	

"Compute binary logistic loss for classification.\n\n    This is identical to log_loss in binary classification case,\n    but is kept for its use in multilabel case.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels.\n\n    y_prob : array-like of float, shape = (n_samples, n_classes)\n        Predicted probabilities, as returned by a classifier's\n        predict_proba method.\n\n    Returns\n    -------\n    loss : float\n        The degree to which the samples are correctly predicted.\n    "
y_prob = numpy.clip(y_prob, 1e-10, (1 - 1e-10))
tempResult = log(y_prob)
	
===================================================================	
binary_log_loss: 64	
----------------------------	

"Compute binary logistic loss for classification.\n\n    This is identical to log_loss in binary classification case,\n    but is kept for its use in multilabel case.\n\n    Parameters\n    ----------\n    y_true : array-like or label indicator matrix\n        Ground truth (correct) labels.\n\n    y_prob : array-like of float, shape = (n_samples, n_classes)\n        Predicted probabilities, as returned by a classifier's\n        predict_proba method.\n\n    Returns\n    -------\n    loss : float\n        The degree to which the samples are correctly predicted.\n    "
y_prob = numpy.clip(y_prob, 1e-10, (1 - 1e-10))
tempResult = log((1 - y_prob))
	
===================================================================	
test_predict_proba_multiclass: 272	
----------------------------	

X = X_digits_multi[:10]
y = y_digits_multi[:10]
clf = MLPClassifier(hidden_layer_sizes=5)
with ignore_warnings(category=ConvergenceWarning):
    clf.fit(X, y)
y_proba = clf.predict_proba(X)
y_log_proba = clf.predict_log_proba(X)
(n_samples, n_classes) = (y.shape[0], np.unique(y).size)
proba_max = y_proba.argmax(axis=1)
proba_log_max = y_log_proba.argmax(axis=1)
assert_equal(y_proba.shape, (n_samples, n_classes))
assert_array_equal(proba_max, proba_log_max)
tempResult = log(y_proba)
	
===================================================================	
test_predict_proba_binary: 256	
----------------------------	

X = X_digits_binary[:50]
y = y_digits_binary[:50]
clf = MLPClassifier(hidden_layer_sizes=5)
with ignore_warnings(category=ConvergenceWarning):
    clf.fit(X, y)
y_proba = clf.predict_proba(X)
y_log_proba = clf.predict_log_proba(X)
(n_samples, n_classes) = (y.shape[0], 2)
proba_max = y_proba.argmax(axis=1)
proba_log_max = y_log_proba.argmax(axis=1)
assert_equal(y_proba.shape, (n_samples, n_classes))
assert_array_equal(proba_max, proba_log_max)
tempResult = log(y_proba)
	
===================================================================	
test_predict_proba_multilabel: 287	
----------------------------	

(X, Y) = make_multilabel_classification(n_samples=50, random_state=0, return_indicator=True)
(n_samples, n_classes) = Y.shape
clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=30, random_state=0)
clf.fit(X, Y)
y_proba = clf.predict_proba(X)
assert_equal(y_proba.shape, (n_samples, n_classes))
assert_array_equal((y_proba > 0.5), Y)
y_log_proba = clf.predict_log_proba(X)
proba_max = y_proba.argmax(axis=1)
proba_log_max = y_log_proba.argmax(axis=1)
assert_greater((y_proba.sum(1) - 1).dot((y_proba.sum(1) - 1)), 1e-10)
assert_array_equal(proba_max, proba_log_max)
tempResult = log(y_proba)
	
===================================================================	
test_standard_scaler_numerical_stability: 147	
----------------------------	

tempResult = log(1e-05, dtype=numpy.float64)
	
===================================================================	
test_standard_scaler_numerical_stability: 155	
----------------------------	

x = (numpy.zeros(8, dtype=numpy.float64) + numpy.log(1e-05, dtype=numpy.float64))
if (LooseVersion(numpy.__version__) >= LooseVersion('1.9')):
    x_scaled = assert_no_warnings(scale, x)
    assert_array_almost_equal(scale(x), numpy.zeros(8))
else:
    w = 'standard deviation of the data is probably very close to 0'
    x_scaled = assert_warns_message(UserWarning, w, scale, x)
    assert_array_almost_equal(x_scaled, numpy.zeros(8))
tempResult = log(1e-05, dtype=numpy.float64)
	
===================================================================	
BaseSVC._predict_log_proba: 307	
----------------------------	

tempResult = log(self.predict_proba(X))
	
===================================================================	
_check_predict_proba: 33	
----------------------------	

proba = clf.predict_proba(X)
log_proba = clf.predict_log_proba(X)
y = numpy.atleast_1d(y)
if (y.ndim == 1):
    y = numpy.reshape(y, ((- 1), 1))
n_outputs = y.shape[1]
n_samples = len(X)
if (n_outputs == 1):
    proba = [proba]
    log_proba = [log_proba]
for k in range(n_outputs):
    assert_equal(proba[k].shape[0], n_samples)
    assert_equal(proba[k].shape[1], len(numpy.unique(y[:, k])))
    assert_array_equal(proba[k].sum(axis=1), numpy.ones(len(X)))
    tempResult = log(proba[k])
	
===================================================================	
test_isotonic_sample_weight_parameter_default_value: 137	
----------------------------	

ir = IsotonicRegression()
rng = numpy.random.RandomState(42)
n = 100
x = numpy.arange(n)
tempResult = log((1 + numpy.arange(n)))
	
===================================================================	
test_skewed_chi2_sampler: 55	
----------------------------	

c = 0.03
X_c = (X + c)[:, numpy.newaxis, :]
Y_c = (Y + c)[numpy.newaxis, :, :]
tempResult = log(X_c)
	
===================================================================	
test_skewed_chi2_sampler: 55	
----------------------------	

c = 0.03
X_c = (X + c)[:, numpy.newaxis, :]
Y_c = (Y + c)[numpy.newaxis, :, :]
tempResult = log(Y_c)
	
===================================================================	
test_skewed_chi2_sampler: 55	
----------------------------	

c = 0.03
X_c = (X + c)[:, numpy.newaxis, :]
Y_c = (Y + c)[numpy.newaxis, :, :]
tempResult = log(2.0)
	
===================================================================	
test_skewed_chi2_sampler: 55	
----------------------------	

c = 0.03
X_c = (X + c)[:, numpy.newaxis, :]
Y_c = (Y + c)[numpy.newaxis, :, :]
tempResult = log((X_c + Y_c))
	
===================================================================	
test_mnnb: 116	
----------------------------	

for X in [X2, scipy.sparse.csr_matrix(X2)]:
    clf = MultinomialNB()
    assert_raises(ValueError, clf.fit, (- X), y2)
    y_pred = clf.fit(X, y2).predict(X)
    assert_array_equal(y_pred, y2)
    y_pred_proba = clf.predict_proba(X)
    y_pred_log_proba = clf.predict_log_proba(X)
    tempResult = log(y_pred_proba)
	
===================================================================	
test_mnnb: 125	
----------------------------	

for X in [X2, scipy.sparse.csr_matrix(X2)]:
    clf = MultinomialNB()
    assert_raises(ValueError, clf.fit, (- X), y2)
    y_pred = clf.fit(X, y2).predict(X)
    assert_array_equal(y_pred, y2)
    y_pred_proba = clf.predict_proba(X)
    y_pred_log_proba = clf.predict_log_proba(X)
    assert_array_almost_equal(numpy.log(y_pred_proba), y_pred_log_proba, 8)
    clf2 = MultinomialNB()
    clf2.partial_fit(X[:2], y2[:2], classes=numpy.unique(y2))
    clf2.partial_fit(X[2:5], y2[2:5])
    clf2.partial_fit(X[5:], y2[5:])
    y_pred2 = clf2.predict(X)
    assert_array_equal(y_pred2, y2)
    y_pred_proba2 = clf2.predict_proba(X)
    y_pred_log_proba2 = clf2.predict_log_proba(X)
    tempResult = log(y_pred_proba2)
	
===================================================================	
test_mnnb: 134	
----------------------------	

for X in [X2, scipy.sparse.csr_matrix(X2)]:
    clf = MultinomialNB()
    assert_raises(ValueError, clf.fit, (- X), y2)
    y_pred = clf.fit(X, y2).predict(X)
    assert_array_equal(y_pred, y2)
    y_pred_proba = clf.predict_proba(X)
    y_pred_log_proba = clf.predict_log_proba(X)
    assert_array_almost_equal(numpy.log(y_pred_proba), y_pred_log_proba, 8)
    clf2 = MultinomialNB()
    clf2.partial_fit(X[:2], y2[:2], classes=numpy.unique(y2))
    clf2.partial_fit(X[2:5], y2[2:5])
    clf2.partial_fit(X[5:], y2[5:])
    y_pred2 = clf2.predict(X)
    assert_array_equal(y_pred2, y2)
    y_pred_proba2 = clf2.predict_proba(X)
    y_pred_log_proba2 = clf2.predict_log_proba(X)
    assert_array_almost_equal(numpy.log(y_pred_proba2), y_pred_log_proba2, 8)
    assert_array_almost_equal(y_pred_proba2, y_pred_proba)
    assert_array_almost_equal(y_pred_log_proba2, y_pred_log_proba)
    clf3 = MultinomialNB()
    clf3.partial_fit(X, y2, classes=numpy.unique(y2))
    y_pred3 = clf3.predict(X)
    assert_array_equal(y_pred3, y2)
    y_pred_proba3 = clf3.predict_proba(X)
    y_pred_log_proba3 = clf3.predict_log_proba(X)
    tempResult = log(y_pred_proba3)
	
===================================================================	
test_gnb: 31	
----------------------------	

clf = GaussianNB()
y_pred = clf.fit(X, y).predict(X)
assert_array_equal(y_pred, y)
y_pred_proba = clf.predict_proba(X)
y_pred_log_proba = clf.predict_log_proba(X)
tempResult = log(y_pred_proba)
	
===================================================================	
test_discrete_prior: 106	
----------------------------	

for cls in [BernoulliNB, MultinomialNB]:
    clf = cls().fit(X2, y2)
    tempResult = log((numpy.array([2, 2, 2]) / 6.0))
	
===================================================================	
test_feature_log_prob_bnb: 302	
----------------------------	

X = numpy.array([[0, 0, 0], [1, 1, 0], [0, 1, 0], [1, 0, 1], [0, 1, 0]])
Y = numpy.array([0, 0, 1, 2, 2])
clf = BernoulliNB(alpha=1.0)
clf.fit(X, Y)
tempResult = log((clf.feature_count_ + 1.0))
	
===================================================================	
test_feature_log_prob_bnb: 303	
----------------------------	

X = numpy.array([[0, 0, 0], [1, 1, 0], [0, 1, 0], [1, 0, 1], [0, 1, 0]])
Y = numpy.array([0, 0, 1, 2, 2])
clf = BernoulliNB(alpha=1.0)
clf.fit(X, Y)
num = numpy.log((clf.feature_count_ + 1.0))
tempResult = log((clf.class_count_ + 2.0))
	
===================================================================	
DecisionTreeClassifier.predict_log_proba: 273	
----------------------------	

'Predict class log-probabilities of the input samples X.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape = [n_samples, n_features]\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n            such arrays if n_outputs > 1.\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        '
proba = self.predict_proba(X)
if (self.n_outputs_ == 1):
    tempResult = log(proba)
	
===================================================================	
DecisionTreeClassifier.predict_log_proba: 276	
----------------------------	

'Predict class log-probabilities of the input samples X.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape = [n_samples, n_features]\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n\n        Returns\n        -------\n        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n            such arrays if n_outputs > 1.\n            The class log-probabilities of the input samples. The order of the\n            classes corresponds to that in the attribute `classes_`.\n        '
proba = self.predict_proba(X)
if (self.n_outputs_ == 1):
    return numpy.log(proba)
else:
    for k in range(self.n_outputs_):
        tempResult = log(proba[k])
	
===================================================================	
logsumexp: 159	
----------------------------	

'Computes the sum of arr assuming arr is in the log domain.\n\n    Returns log(sum(exp(arr))) while minimizing the possibility of\n    over/underflow.\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from sklearn.utils.extmath import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n    '
arr = numpy.rollaxis(arr, axis)
vmax = arr.max(axis=0)
tempResult = log(numpy.sum(numpy.exp((arr - vmax)), axis=0))
	
===================================================================	
test_logsumexp: 69	
----------------------------	

x = numpy.array(([1e-40] * 1000000))
tempResult = log(x)
	
===================================================================	
naive_log_logistic: 242	
----------------------------	

tempResult = log((1 / (1 + numpy.exp((- x)))))
	
===================================================================	
Get no callers of function numpy.log at line 334 col 9.	
***************************************************	
matplotlib_matplotlib-2.0.0: 41	
===================================================================	
MercatorLatitudeTransform.transform_non_affine: 58	
----------------------------	

'\n            This transform takes an Nx1 ``numpy`` array and returns a\n            transformed copy.  Since the range of the Mercator scale\n            is limited by the user-specified threshold, the input\n            array must be masked to contain only valid values.\n            ``matplotlib`` will handle masked arrays and remove the\n            out-of-range data from the plot.  Importantly, the\n            ``transform`` method *must* return an array that is the\n            same shape as the input array, since these values need to\n            remain synchronized with values in the other dimension.\n            '
masked = numpy.ma.masked_where(((a < (- self.thresh)) | (a > self.thresh)), a)
if masked.mask.any():
    return numpy.ma.log(numpy.abs((numpy.ma.tan(masked) + (1.0 / numpy.ma.cos(masked)))))
else:
    tempResult = log(numpy.abs((numpy.tan(a) + (1.0 / numpy.cos(a)))))
	
===================================================================	
module: 7	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
t1 = numpy.arange(0.0, 2.0, 0.1)
t2 = numpy.arange(0.0, 2.0, 0.01)
(l1,) = matplotlib.pyplot.plot(t2, numpy.exp((- t2)))
tempResult = log((1 + t1))
	
===================================================================	
module: 26	
----------------------------	

'\n===================================\nShaded & power normalized rendering\n===================================\n\nThe Mandelbrot set rendering can be improved by using a normalized recount\nassociated with a power normalized colormap (gamma=0.3). Rendering can be\nfurther enhanced thanks to shading.\n\nThe `maxiter` gives the precision of the computation. `maxiter=200` should\ntake a few seconds on most modern laptops.\n'
import numpy as np

def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):
    X = numpy.linspace(xmin, xmax, xn, dtype=numpy.float32)
    Y = numpy.linspace(ymin, ymax, yn, dtype=numpy.float32)
    C = (X + (Y[:, None] * 1j))
    N = numpy.zeros(C.shape, dtype=int)
    Z = numpy.zeros(C.shape, numpy.complex64)
    for n in range(maxiter):
        I = numpy.less(abs(Z), horizon)
        N[I] = n
        Z[I] = ((Z[I] ** 2) + C[I])
    N[(N == (maxiter - 1))] = 0
    return (Z, N)
if (__name__ == '__main__'):
    import time
    import matplotlib
    from matplotlib import colors
    import matplotlib.pyplot as plt
    (xmin, xmax, xn) = ((- 2.25), (+ 0.75), (3000 / 2))
    (ymin, ymax, yn) = ((- 1.25), (+ 1.25), (2500 / 2))
    maxiter = 200
    horizon = (2.0 ** 40)
    tempResult = log(numpy.log(horizon))
	
===================================================================	
module: 26	
----------------------------	

'\n===================================\nShaded & power normalized rendering\n===================================\n\nThe Mandelbrot set rendering can be improved by using a normalized recount\nassociated with a power normalized colormap (gamma=0.3). Rendering can be\nfurther enhanced thanks to shading.\n\nThe `maxiter` gives the precision of the computation. `maxiter=200` should\ntake a few seconds on most modern laptops.\n'
import numpy as np

def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):
    X = numpy.linspace(xmin, xmax, xn, dtype=numpy.float32)
    Y = numpy.linspace(ymin, ymax, yn, dtype=numpy.float32)
    C = (X + (Y[:, None] * 1j))
    N = numpy.zeros(C.shape, dtype=int)
    Z = numpy.zeros(C.shape, numpy.complex64)
    for n in range(maxiter):
        I = numpy.less(abs(Z), horizon)
        N[I] = n
        Z[I] = ((Z[I] ** 2) + C[I])
    N[(N == (maxiter - 1))] = 0
    return (Z, N)
if (__name__ == '__main__'):
    import time
    import matplotlib
    from matplotlib import colors
    import matplotlib.pyplot as plt
    (xmin, xmax, xn) = ((- 2.25), (+ 0.75), (3000 / 2))
    (ymin, ymax, yn) = ((- 1.25), (+ 1.25), (2500 / 2))
    maxiter = 200
    horizon = (2.0 ** 40)
    tempResult = log(horizon)
	
===================================================================	
module: 26	
----------------------------	

'\n===================================\nShaded & power normalized rendering\n===================================\n\nThe Mandelbrot set rendering can be improved by using a normalized recount\nassociated with a power normalized colormap (gamma=0.3). Rendering can be\nfurther enhanced thanks to shading.\n\nThe `maxiter` gives the precision of the computation. `maxiter=200` should\ntake a few seconds on most modern laptops.\n'
import numpy as np

def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):
    X = numpy.linspace(xmin, xmax, xn, dtype=numpy.float32)
    Y = numpy.linspace(ymin, ymax, yn, dtype=numpy.float32)
    C = (X + (Y[:, None] * 1j))
    N = numpy.zeros(C.shape, dtype=int)
    Z = numpy.zeros(C.shape, numpy.complex64)
    for n in range(maxiter):
        I = numpy.less(abs(Z), horizon)
        N[I] = n
        Z[I] = ((Z[I] ** 2) + C[I])
    N[(N == (maxiter - 1))] = 0
    return (Z, N)
if (__name__ == '__main__'):
    import time
    import matplotlib
    from matplotlib import colors
    import matplotlib.pyplot as plt
    (xmin, xmax, xn) = ((- 2.25), (+ 0.75), (3000 / 2))
    (ymin, ymax, yn) = ((- 1.25), (+ 1.25), (2500 / 2))
    maxiter = 200
    horizon = (2.0 ** 40)
    tempResult = log(2)
	
===================================================================	
module: 29	
----------------------------	

'\n===================================\nShaded & power normalized rendering\n===================================\n\nThe Mandelbrot set rendering can be improved by using a normalized recount\nassociated with a power normalized colormap (gamma=0.3). Rendering can be\nfurther enhanced thanks to shading.\n\nThe `maxiter` gives the precision of the computation. `maxiter=200` should\ntake a few seconds on most modern laptops.\n'
import numpy as np

def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):
    X = numpy.linspace(xmin, xmax, xn, dtype=numpy.float32)
    Y = numpy.linspace(ymin, ymax, yn, dtype=numpy.float32)
    C = (X + (Y[:, None] * 1j))
    N = numpy.zeros(C.shape, dtype=int)
    Z = numpy.zeros(C.shape, numpy.complex64)
    for n in range(maxiter):
        I = numpy.less(abs(Z), horizon)
        N[I] = n
        Z[I] = ((Z[I] ** 2) + C[I])
    N[(N == (maxiter - 1))] = 0
    return (Z, N)
if (__name__ == '__main__'):
    import time
    import matplotlib
    from matplotlib import colors
    import matplotlib.pyplot as plt
    (xmin, xmax, xn) = ((- 2.25), (+ 0.75), (3000 / 2))
    (ymin, ymax, yn) = ((- 1.25), (+ 1.25), (2500 / 2))
    maxiter = 200
    horizon = (2.0 ** 40)
    log_horizon = (numpy.log(numpy.log(horizon)) / numpy.log(2))
    (Z, N) = mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon)
    with numpy.errstate(invalid='ignore'):
        tempResult = log(numpy.log(abs(Z)))
	
===================================================================	
module: 29	
----------------------------	

'\n===================================\nShaded & power normalized rendering\n===================================\n\nThe Mandelbrot set rendering can be improved by using a normalized recount\nassociated with a power normalized colormap (gamma=0.3). Rendering can be\nfurther enhanced thanks to shading.\n\nThe `maxiter` gives the precision of the computation. `maxiter=200` should\ntake a few seconds on most modern laptops.\n'
import numpy as np

def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):
    X = numpy.linspace(xmin, xmax, xn, dtype=numpy.float32)
    Y = numpy.linspace(ymin, ymax, yn, dtype=numpy.float32)
    C = (X + (Y[:, None] * 1j))
    N = numpy.zeros(C.shape, dtype=int)
    Z = numpy.zeros(C.shape, numpy.complex64)
    for n in range(maxiter):
        I = numpy.less(abs(Z), horizon)
        N[I] = n
        Z[I] = ((Z[I] ** 2) + C[I])
    N[(N == (maxiter - 1))] = 0
    return (Z, N)
if (__name__ == '__main__'):
    import time
    import matplotlib
    from matplotlib import colors
    import matplotlib.pyplot as plt
    (xmin, xmax, xn) = ((- 2.25), (+ 0.75), (3000 / 2))
    (ymin, ymax, yn) = ((- 1.25), (+ 1.25), (2500 / 2))
    maxiter = 200
    horizon = (2.0 ** 40)
    log_horizon = (numpy.log(numpy.log(horizon)) / numpy.log(2))
    (Z, N) = mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon)
    with numpy.errstate(invalid='ignore'):
        tempResult = log(abs(Z))
	
===================================================================	
module: 29	
----------------------------	

'\n===================================\nShaded & power normalized rendering\n===================================\n\nThe Mandelbrot set rendering can be improved by using a normalized recount\nassociated with a power normalized colormap (gamma=0.3). Rendering can be\nfurther enhanced thanks to shading.\n\nThe `maxiter` gives the precision of the computation. `maxiter=200` should\ntake a few seconds on most modern laptops.\n'
import numpy as np

def mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon=2.0):
    X = numpy.linspace(xmin, xmax, xn, dtype=numpy.float32)
    Y = numpy.linspace(ymin, ymax, yn, dtype=numpy.float32)
    C = (X + (Y[:, None] * 1j))
    N = numpy.zeros(C.shape, dtype=int)
    Z = numpy.zeros(C.shape, numpy.complex64)
    for n in range(maxiter):
        I = numpy.less(abs(Z), horizon)
        N[I] = n
        Z[I] = ((Z[I] ** 2) + C[I])
    N[(N == (maxiter - 1))] = 0
    return (Z, N)
if (__name__ == '__main__'):
    import time
    import matplotlib
    from matplotlib import colors
    import matplotlib.pyplot as plt
    (xmin, xmax, xn) = ((- 2.25), (+ 0.75), (3000 / 2))
    (ymin, ymax, yn) = ((- 1.25), (+ 1.25), (2500 / 2))
    maxiter = 200
    horizon = (2.0 ** 40)
    log_horizon = (numpy.log(numpy.log(horizon)) / numpy.log(2))
    (Z, N) = mandelbrot_set(xmin, xmax, ymin, ymax, xn, yn, maxiter, horizon)
    with numpy.errstate(invalid='ignore'):
        tempResult = log(2)
	
===================================================================	
hinton: 10	
----------------------------	

'Draw Hinton diagram for visualizing a weight matrix.'
ax = (ax if (ax is not None) else matplotlib.pyplot.gca())
if (not max_weight):
    tempResult = log(np.abs(matrix).max())
	
===================================================================	
hinton: 10	
----------------------------	

'Draw Hinton diagram for visualizing a weight matrix.'
ax = (ax if (ax is not None) else matplotlib.pyplot.gca())
if (not max_weight):
    tempResult = log(2)
	
===================================================================	
SymLogNorm._transform: 571	
----------------------------	

'\n        Inplace transformation.\n        '
masked = (numpy.abs(a) > self.linthresh)
sign = numpy.sign(a[masked])
tempResult = log((numpy.abs(a[masked]) / self.linthresh))
	
===================================================================	
LogNorm.__call__: 501	
----------------------------	

if (clip is None):
    clip = self.clip
(result, is_scalar) = self.process_value(value)
result = numpy.ma.masked_less_equal(result, 0, copy=False)
self.autoscale_None(result)
(vmin, vmax) = (self.vmin, self.vmax)
if (vmin > vmax):
    raise ValueError('minvalue must be less than or equal to maxvalue')
elif (vmin <= 0):
    raise ValueError('values must all be positive')
elif (vmin == vmax):
    result.fill(0)
else:
    if clip:
        mask = numpy.ma.getmask(result)
        result = numpy.ma.array(numpy.clip(result.filled(vmax), vmin, vmax), mask=mask)
    resdat = result.data
    mask = result.mask
    if (mask is numpy.ma.nomask):
        mask = (resdat <= 0)
    else:
        mask |= (resdat <= 0)
    matplotlib.cbook._putmask(resdat, mask, 1)
    tempResult = log(resdat, resdat)
	
===================================================================	
LogNorm.__call__: 502	
----------------------------	

if (clip is None):
    clip = self.clip
(result, is_scalar) = self.process_value(value)
result = numpy.ma.masked_less_equal(result, 0, copy=False)
self.autoscale_None(result)
(vmin, vmax) = (self.vmin, self.vmax)
if (vmin > vmax):
    raise ValueError('minvalue must be less than or equal to maxvalue')
elif (vmin <= 0):
    raise ValueError('values must all be positive')
elif (vmin == vmax):
    result.fill(0)
else:
    if clip:
        mask = numpy.ma.getmask(result)
        result = numpy.ma.array(numpy.clip(result.filled(vmax), vmin, vmax), mask=mask)
    resdat = result.data
    mask = result.mask
    if (mask is numpy.ma.nomask):
        mask = (resdat <= 0)
    else:
        mask |= (resdat <= 0)
    matplotlib.cbook._putmask(resdat, mask, 1)
    numpy.log(resdat, resdat)
    tempResult = log(vmin)
	
===================================================================	
LogNorm.__call__: 503	
----------------------------	

if (clip is None):
    clip = self.clip
(result, is_scalar) = self.process_value(value)
result = numpy.ma.masked_less_equal(result, 0, copy=False)
self.autoscale_None(result)
(vmin, vmax) = (self.vmin, self.vmax)
if (vmin > vmax):
    raise ValueError('minvalue must be less than or equal to maxvalue')
elif (vmin <= 0):
    raise ValueError('values must all be positive')
elif (vmin == vmax):
    result.fill(0)
else:
    if clip:
        mask = numpy.ma.getmask(result)
        result = numpy.ma.array(numpy.clip(result.filled(vmax), vmin, vmax), mask=mask)
    resdat = result.data
    mask = result.mask
    if (mask is numpy.ma.nomask):
        mask = (resdat <= 0)
    else:
        mask |= (resdat <= 0)
    matplotlib.cbook._putmask(resdat, mask, 1)
    numpy.log(resdat, resdat)
    resdat -= numpy.log(vmin)
    tempResult = log(vmax)
	
===================================================================	
LogNorm.__call__: 503	
----------------------------	

if (clip is None):
    clip = self.clip
(result, is_scalar) = self.process_value(value)
result = numpy.ma.masked_less_equal(result, 0, copy=False)
self.autoscale_None(result)
(vmin, vmax) = (self.vmin, self.vmax)
if (vmin > vmax):
    raise ValueError('minvalue must be less than or equal to maxvalue')
elif (vmin <= 0):
    raise ValueError('values must all be positive')
elif (vmin == vmax):
    result.fill(0)
else:
    if clip:
        mask = numpy.ma.getmask(result)
        result = numpy.ma.array(numpy.clip(result.filled(vmax), vmin, vmax), mask=mask)
    resdat = result.data
    mask = result.mask
    if (mask is numpy.ma.nomask):
        mask = (resdat <= 0)
    else:
        mask |= (resdat <= 0)
    matplotlib.cbook._putmask(resdat, mask, 1)
    numpy.log(resdat, resdat)
    resdat -= numpy.log(vmin)
    tempResult = log(vmin)
	
===================================================================	
logspace: 23	
----------------------------	

'\n    Return N values logarithmically spaced between xmin and xmax.\n\n    '
tempResult = log(xmin)
	
===================================================================	
logspace: 23	
----------------------------	

'\n    Return N values logarithmically spaced between xmin and xmax.\n\n    '
tempResult = log(xmax)
	
===================================================================	
entropy: 428	
----------------------------	

'\n    Return the entropy of the data in *y* in units of nat.\n\n    .. math::\n\n      -\\sum p_i \\ln(p_i)\n\n    where :math:`p_i` is the probability of observing *y* in the\n    :math:`i^{th}` bin of *bins*.  *bins* can be a number of bins or a\n    range of bins; see :func:`numpy.histogram`.\n\n    Compare *S* with analytic calculation for a Gaussian::\n\n      x = mu + sigma * randn(200000)\n      Sanalytic = 0.5 * ( 1.0 + log(2*pi*sigma**2.0) )\n    '
(n, bins) = numpy.histogram(y, bins)
n = n.astype(numpy.float_)
n = numpy.take(n, numpy.nonzero(n)[0])
p = numpy.divide(n, len(y))
delta = (bins[1] - bins[0])
tempResult = log(p)
	
===================================================================	
entropy: 428	
----------------------------	

'\n    Return the entropy of the data in *y* in units of nat.\n\n    .. math::\n\n      -\\sum p_i \\ln(p_i)\n\n    where :math:`p_i` is the probability of observing *y* in the\n    :math:`i^{th}` bin of *bins*.  *bins* can be a number of bins or a\n    range of bins; see :func:`numpy.histogram`.\n\n    Compare *S* with analytic calculation for a Gaussian::\n\n      x = mu + sigma * randn(200000)\n      Sanalytic = 0.5 * ( 1.0 + log(2*pi*sigma**2.0) )\n    '
(n, bins) = numpy.histogram(y, bins)
n = n.astype(numpy.float_)
n = numpy.take(n, numpy.nonzero(n)[0])
p = numpy.divide(n, len(y))
delta = (bins[1] - bins[0])
tempResult = log(delta)
	
===================================================================	
LogTransformBase.transform_non_affine: 61	
----------------------------	

with numpy.errstate(invalid='ignore'):
    a = numpy.where((a <= 0), self._fill_value, a)
tempResult = log(a, out=a)
	
===================================================================	
LogTransformBase.transform_non_affine: 61	
----------------------------	

with numpy.errstate(invalid='ignore'):
    a = numpy.where((a <= 0), self._fill_value, a)
tempResult = log(self.base)
	
===================================================================	
SymmetricalLogTransform.__init__: 191	
----------------------------	

matplotlib.transforms.Transform.__init__(self)
self.base = base
self.linthresh = linthresh
self.linscale = linscale
self._linscale_adj = (linscale / (1.0 - (self.base ** (- 1))))
tempResult = log(base)
	
===================================================================	
is_decade: 1129	
----------------------------	

if (not numpy.isfinite(x)):
    return False
if (x == 0.0):
    return True
tempResult = log(numpy.abs(x))
	
===================================================================	
is_decade: 1129	
----------------------------	

if (not numpy.isfinite(x)):
    return False
if (x == 0.0):
    return True
tempResult = log(base)
	
===================================================================	
SymmetricalLogLocator.get_log_range: 1332	
----------------------------	

tempResult = log(lo)
	
===================================================================	
SymmetricalLogLocator.get_log_range: 1332	
----------------------------	

tempResult = log(b)
	
===================================================================	
SymmetricalLogLocator.get_log_range: 1333	
----------------------------	

lo = numpy.floor((numpy.log(lo) / numpy.log(b)))
tempResult = log(hi)
	
===================================================================	
SymmetricalLogLocator.get_log_range: 1333	
----------------------------	

lo = numpy.floor((numpy.log(lo) / numpy.log(b)))
tempResult = log(b)
	
===================================================================	
decade_down: 1106	
----------------------------	

'floor x to the nearest lower decade'
if (x == 0.0):
    return (- base)
tempResult = log(x)
	
===================================================================	
decade_down: 1106	
----------------------------	

'floor x to the nearest lower decade'
if (x == 0.0):
    return (- base)
tempResult = log(base)
	
===================================================================	
decade_up: 1113	
----------------------------	

'ceil x to the nearest higher decade'
if (x == 0.0):
    return base
tempResult = log(x)
	
===================================================================	
decade_up: 1113	
----------------------------	

'ceil x to the nearest higher decade'
if (x == 0.0):
    return base
tempResult = log(base)
	
===================================================================	
_AxesBase._set_view_from_bbox: 1840	
----------------------------	

"\n        Update view from a selection bbox.\n\n        .. note::\n\n            Intended to be overridden by new projection types, but if not, the\n            default implementation sets the view limits to the bbox directly.\n\n        Parameters\n        ----------\n\n        bbox : tuple\n            The selected bounding box limits, in *display* coordinates.\n\n        direction : str\n            The direction to apply the bounding box.\n                * `'in'` - The bounding box describes the view directly, i.e.,\n                           it zooms in.\n                * `'out'` - The bounding box describes the size to make the\n                            existing view, i.e., it zooms out.\n\n        mode : str or None\n            The selection mode, whether to apply the bounding box in only the\n            `'x'` direction, `'y'` direction or both (`None`).\n\n        twinx : bool\n            Whether this axis is twinned in the *x*-direction.\n\n        twiny : bool\n            Whether this axis is twinned in the *y*-direction.\n        "
(lastx, lasty, x, y) = bbox
inverse = self.transData.inverted()
(lastx, lasty) = inverse.transform_point((lastx, lasty))
(x, y) = inverse.transform_point((x, y))
(Xmin, Xmax) = self.get_xlim()
(Ymin, Ymax) = self.get_ylim()
if twinx:
    (x0, x1) = (Xmin, Xmax)
elif (Xmin < Xmax):
    if (x < lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 < Xmin):
        x0 = Xmin
    if (x1 > Xmax):
        x1 = Xmax
else:
    if (x > lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 > Xmin):
        x0 = Xmin
    if (x1 < Xmax):
        x1 = Xmax
if twiny:
    (y0, y1) = (Ymin, Ymax)
elif (Ymin < Ymax):
    if (y < lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 < Ymin):
        y0 = Ymin
    if (y1 > Ymax):
        y1 = Ymax
else:
    if (y > lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 > Ymin):
        y0 = Ymin
    if (y1 < Ymax):
        y1 = Ymax
if (direction == 'in'):
    if (mode == 'x'):
        self.set_xlim((x0, x1))
    elif (mode == 'y'):
        self.set_ylim((y0, y1))
    else:
        self.set_xlim((x0, x1))
        self.set_ylim((y0, y1))
elif (direction == 'out'):
    if (self.get_xscale() == 'log'):
        tempResult = log((Xmax / Xmin))
	
===================================================================	
_AxesBase._set_view_from_bbox: 1840	
----------------------------	

"\n        Update view from a selection bbox.\n\n        .. note::\n\n            Intended to be overridden by new projection types, but if not, the\n            default implementation sets the view limits to the bbox directly.\n\n        Parameters\n        ----------\n\n        bbox : tuple\n            The selected bounding box limits, in *display* coordinates.\n\n        direction : str\n            The direction to apply the bounding box.\n                * `'in'` - The bounding box describes the view directly, i.e.,\n                           it zooms in.\n                * `'out'` - The bounding box describes the size to make the\n                            existing view, i.e., it zooms out.\n\n        mode : str or None\n            The selection mode, whether to apply the bounding box in only the\n            `'x'` direction, `'y'` direction or both (`None`).\n\n        twinx : bool\n            Whether this axis is twinned in the *x*-direction.\n\n        twiny : bool\n            Whether this axis is twinned in the *y*-direction.\n        "
(lastx, lasty, x, y) = bbox
inverse = self.transData.inverted()
(lastx, lasty) = inverse.transform_point((lastx, lasty))
(x, y) = inverse.transform_point((x, y))
(Xmin, Xmax) = self.get_xlim()
(Ymin, Ymax) = self.get_ylim()
if twinx:
    (x0, x1) = (Xmin, Xmax)
elif (Xmin < Xmax):
    if (x < lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 < Xmin):
        x0 = Xmin
    if (x1 > Xmax):
        x1 = Xmax
else:
    if (x > lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 > Xmin):
        x0 = Xmin
    if (x1 < Xmax):
        x1 = Xmax
if twiny:
    (y0, y1) = (Ymin, Ymax)
elif (Ymin < Ymax):
    if (y < lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 < Ymin):
        y0 = Ymin
    if (y1 > Ymax):
        y1 = Ymax
else:
    if (y > lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 > Ymin):
        y0 = Ymin
    if (y1 < Ymax):
        y1 = Ymax
if (direction == 'in'):
    if (mode == 'x'):
        self.set_xlim((x0, x1))
    elif (mode == 'y'):
        self.set_ylim((y0, y1))
    else:
        self.set_xlim((x0, x1))
        self.set_ylim((y0, y1))
elif (direction == 'out'):
    if (self.get_xscale() == 'log'):
        tempResult = log((x1 / x0))
	
===================================================================	
_AxesBase._set_view_from_bbox: 1848	
----------------------------	

"\n        Update view from a selection bbox.\n\n        .. note::\n\n            Intended to be overridden by new projection types, but if not, the\n            default implementation sets the view limits to the bbox directly.\n\n        Parameters\n        ----------\n\n        bbox : tuple\n            The selected bounding box limits, in *display* coordinates.\n\n        direction : str\n            The direction to apply the bounding box.\n                * `'in'` - The bounding box describes the view directly, i.e.,\n                           it zooms in.\n                * `'out'` - The bounding box describes the size to make the\n                            existing view, i.e., it zooms out.\n\n        mode : str or None\n            The selection mode, whether to apply the bounding box in only the\n            `'x'` direction, `'y'` direction or both (`None`).\n\n        twinx : bool\n            Whether this axis is twinned in the *x*-direction.\n\n        twiny : bool\n            Whether this axis is twinned in the *y*-direction.\n        "
(lastx, lasty, x, y) = bbox
inverse = self.transData.inverted()
(lastx, lasty) = inverse.transform_point((lastx, lasty))
(x, y) = inverse.transform_point((x, y))
(Xmin, Xmax) = self.get_xlim()
(Ymin, Ymax) = self.get_ylim()
if twinx:
    (x0, x1) = (Xmin, Xmax)
elif (Xmin < Xmax):
    if (x < lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 < Xmin):
        x0 = Xmin
    if (x1 > Xmax):
        x1 = Xmax
else:
    if (x > lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 > Xmin):
        x0 = Xmin
    if (x1 < Xmax):
        x1 = Xmax
if twiny:
    (y0, y1) = (Ymin, Ymax)
elif (Ymin < Ymax):
    if (y < lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 < Ymin):
        y0 = Ymin
    if (y1 > Ymax):
        y1 = Ymax
else:
    if (y > lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 > Ymin):
        y0 = Ymin
    if (y1 < Ymax):
        y1 = Ymax
if (direction == 'in'):
    if (mode == 'x'):
        self.set_xlim((x0, x1))
    elif (mode == 'y'):
        self.set_ylim((y0, y1))
    else:
        self.set_xlim((x0, x1))
        self.set_ylim((y0, y1))
elif (direction == 'out'):
    if (self.get_xscale() == 'log'):
        alpha = (numpy.log((Xmax / Xmin)) / numpy.log((x1 / x0)))
        rx1 = (pow((Xmin / x0), alpha) * Xmin)
        rx2 = (pow((Xmax / x0), alpha) * Xmin)
    else:
        alpha = ((Xmax - Xmin) / (x1 - x0))
        rx1 = ((alpha * (Xmin - x0)) + Xmin)
        rx2 = ((alpha * (Xmax - x0)) + Xmin)
    if (self.get_yscale() == 'log'):
        tempResult = log((Ymax / Ymin))
	
===================================================================	
_AxesBase._set_view_from_bbox: 1848	
----------------------------	

"\n        Update view from a selection bbox.\n\n        .. note::\n\n            Intended to be overridden by new projection types, but if not, the\n            default implementation sets the view limits to the bbox directly.\n\n        Parameters\n        ----------\n\n        bbox : tuple\n            The selected bounding box limits, in *display* coordinates.\n\n        direction : str\n            The direction to apply the bounding box.\n                * `'in'` - The bounding box describes the view directly, i.e.,\n                           it zooms in.\n                * `'out'` - The bounding box describes the size to make the\n                            existing view, i.e., it zooms out.\n\n        mode : str or None\n            The selection mode, whether to apply the bounding box in only the\n            `'x'` direction, `'y'` direction or both (`None`).\n\n        twinx : bool\n            Whether this axis is twinned in the *x*-direction.\n\n        twiny : bool\n            Whether this axis is twinned in the *y*-direction.\n        "
(lastx, lasty, x, y) = bbox
inverse = self.transData.inverted()
(lastx, lasty) = inverse.transform_point((lastx, lasty))
(x, y) = inverse.transform_point((x, y))
(Xmin, Xmax) = self.get_xlim()
(Ymin, Ymax) = self.get_ylim()
if twinx:
    (x0, x1) = (Xmin, Xmax)
elif (Xmin < Xmax):
    if (x < lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 < Xmin):
        x0 = Xmin
    if (x1 > Xmax):
        x1 = Xmax
else:
    if (x > lastx):
        (x0, x1) = (x, lastx)
    else:
        (x0, x1) = (lastx, x)
    if (x0 > Xmin):
        x0 = Xmin
    if (x1 < Xmax):
        x1 = Xmax
if twiny:
    (y0, y1) = (Ymin, Ymax)
elif (Ymin < Ymax):
    if (y < lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 < Ymin):
        y0 = Ymin
    if (y1 > Ymax):
        y1 = Ymax
else:
    if (y > lasty):
        (y0, y1) = (y, lasty)
    else:
        (y0, y1) = (lasty, y)
    if (y0 > Ymin):
        y0 = Ymin
    if (y1 < Ymax):
        y1 = Ymax
if (direction == 'in'):
    if (mode == 'x'):
        self.set_xlim((x0, x1))
    elif (mode == 'y'):
        self.set_ylim((y0, y1))
    else:
        self.set_xlim((x0, x1))
        self.set_ylim((y0, y1))
elif (direction == 'out'):
    if (self.get_xscale() == 'log'):
        alpha = (numpy.log((Xmax / Xmin)) / numpy.log((x1 / x0)))
        rx1 = (pow((Xmin / x0), alpha) * Xmin)
        rx2 = (pow((Xmax / x0), alpha) * Xmin)
    else:
        alpha = ((Xmax - Xmin) / (x1 - x0))
        rx1 = ((alpha * (Xmin - x0)) + Xmin)
        rx2 = ((alpha * (Xmax - x0)) + Xmin)
    if (self.get_yscale() == 'log'):
        tempResult = log((y1 / y0))
	
===================================================================	
test_log_scales: 968	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = matplotlib.pyplot.gca()
tempResult = log(numpy.linspace(0.1, 100))
	
===================================================================	
test_nonfinite_limits: 430	
----------------------------	
x = np.arange(0.0, np.e, 0.01)
olderr = np.seterr(divide='ignore')
try:
    tempResult = log(x)	
===================================================================	
test_pcolor_pre_transform_limits: 98	
----------------------------	

ax = matplotlib.pyplot.axes()
(xs, ys) = numpy.meshgrid(numpy.linspace(15, 20, 15), numpy.linspace(12.4, 12.5, 20))
tempResult = log((xs * ys))
	
===================================================================	
test_pcolormesh_pre_transform_limits: 106	
----------------------------	

ax = matplotlib.pyplot.axes()
(xs, ys) = numpy.meshgrid(numpy.linspace(15, 20, 15), numpy.linspace(12.4, 12.5, 20))
tempResult = log((xs * ys))
	
===================================================================	
test_contour_pre_transform_limits: 90	
----------------------------	

ax = matplotlib.pyplot.axes()
(xs, ys) = numpy.meshgrid(numpy.linspace(15, 20, 15), numpy.linspace(12.4, 12.5, 20))
tempResult = log((xs * ys))
	
***************************************************	
ipython_ipython-6.1.0: 0	
***************************************************	
pandas_pandas-0.19.2: 6	
===================================================================	
_get_group_index_sorter: 2550	
----------------------------	

"\n    _algos.groupsort_indexer implements `counting sort` and it is at least\n    O(ngroups), where\n        ngroups = prod(shape)\n        shape = map(len, keys)\n    that is, linear in the number of combinations (cartesian product) of unique\n    values of groupby keys. This can be huge when doing multi-key groupby.\n    np.argsort(kind='mergesort') is O(count x log(count)) where count is the\n    length of the data-frame;\n    Both algorithms are `stable` sort and that is necessary for correctness of\n    groupby operations. e.g. consider:\n        df.groupby(key)[col].transform('first')\n    "
count = len(group_index)
alpha = 0.0
beta = 1.0
tempResult = log(count)
	
===================================================================	
_get_center_of_mass: 969	
----------------------------	

valid_count = len([x for x in [com, span, halflife, alpha] if (x is not None)])
if (valid_count > 1):
    raise ValueError('com, span, halflife, and alpha are mutually exclusive')
if (com is not None):
    if (com < 0):
        raise ValueError('com must satisfy: com >= 0')
elif (span is not None):
    if (span < 1):
        raise ValueError('span must satisfy: span >= 1')
    com = ((span - 1) / 2.0)
elif (halflife is not None):
    if (halflife <= 0):
        raise ValueError('halflife must satisfy: halflife > 0')
    tempResult = log(0.5)
	
===================================================================	
VAR._ic: 229	
----------------------------	

'\n        Returns the Akaike/Bayesian information criteria.\n        '
RSS = self._rss
k = (self._p * ((self._k * self._p) + 1))
n = (self._nobs * self._k)
tempResult = log((RSS / n))
	
===================================================================	
VAR._ic: 229	
----------------------------	

'\n        Returns the Akaike/Bayesian information criteria.\n        '
RSS = self._rss
k = (self._p * ((self._k * self._p) + 1))
n = (self._nobs * self._k)
tempResult = log((RSS / n))
	
===================================================================	
VAR._ic: 229	
----------------------------	

'\n        Returns the Akaike/Bayesian information criteria.\n        '
RSS = self._rss
k = (self._p * ((self._k * self._p) + 1))
n = (self._nobs * self._k)
tempResult = log(n)
	
===================================================================	
TestCategoricalAsBlock.test_numeric_like_ops: 2620	
----------------------------	

for op in ['__add__', '__sub__', '__mul__', '__truediv__']:
    self.assertRaises(TypeError, (lambda : getattr(self.cat, op)(self.cat)))
s = self.cat['value_group']
for op in ['kurt', 'skew', 'var', 'std', 'mean', 'sum', 'median']:
    self.assertRaises(TypeError, (lambda : getattr(s, op)(numeric_only=False)))
s = pandas.Series(pandas.Categorical([1, 2, 3, 4]))
self.assertRaises(TypeError, (lambda : numpy.sum(s)))
for op in ['__add__', '__sub__', '__mul__', '__truediv__']:
    self.assertRaises(TypeError, (lambda : getattr(s, op)(2)))
tempResult = log(s)
	
***************************************************	
dask_dask-0.7.0: 0	
***************************************************	
nengo_nengo-2.0.0: 2	
===================================================================	
Sigmoid.gain_bias: 101	
----------------------------	

'Return gain and bias given maximum firing rate and x-intercept.'
lim = (1.0 / self.tau_ref)
tempResult = log((((2.0 * lim) - max_rates) / (lim - max_rates)))
	
===================================================================	
Sigmoid.gain_bias: 102	
----------------------------	

'Return gain and bias given maximum firing rate and x-intercept.'
lim = (1.0 / self.tau_ref)
gain = (((- 2.0) / (intercepts - 1.0)) * numpy.log((((2.0 * lim) - max_rates) / (lim - max_rates))))
tempResult = log(((lim / max_rates) - 1))
	
***************************************************	
sympy_sympy-1.0.0: 1	
===================================================================	
entropy: 99	
----------------------------	

'Compute the entropy of a matrix/density object.\n\n    This computes -Tr(density*ln(density)) using the eigenvalue decomposition\n    of density, which is given as either a Density instance or a matrix\n    (numpy.ndarray, sympy.Matrix or scipy.sparse).\n\n    Parameters\n    ==========\n\n    density : density matrix of type Density, sympy matrix,\n    scipy.sparse or numpy.ndarray\n\n    Examples\n    ========\n\n    >>> from sympy.physics.quantum.density import Density, entropy\n    >>> from sympy.physics.quantum.represent import represent\n    >>> from sympy.physics.quantum.matrixutils import scipy_sparse_matrix\n    >>> from sympy.physics.quantum.spin import JzKet, Jz\n    >>> from sympy import S, log\n    >>> up = JzKet(S(1)/2,S(1)/2)\n    >>> down = JzKet(S(1)/2,-S(1)/2)\n    >>> d = Density((up,0.5),(down,0.5))\n    >>> entropy(d)\n    log(2)/2\n\n    '
if isinstance(density, Density):
    density = represent(density)
if isinstance(density, scipy_sparse_matrix):
    density = to_numpy(density)
if isinstance(density, Matrix):
    eigvals = density.eigenvals().keys()
    return expand((- sum(((e * log(e)) for e in eigvals))))
elif isinstance(density, numpy_ndarray):
    import numpy as np
    eigvals = numpy.linalg.eigvals(density)
    tempResult = log(eigvals)
	
***************************************************	
daducci_amico-dev: 0	
***************************************************	
aplpy_aplpy-1.1.1: 0	
***************************************************	
markovmodel_msmtools-1.0.2: 15	
===================================================================	
timescales_from_eigenvalues: 204	
----------------------------	

'Compute implied time scales from given eigenvalues\n    \n    Parameters\n    ----------\n    evals : eigenvalues\n    tau : lag time\n\n    Returns\n    -------\n    ts : ndarray\n        The implied time scales to the given eigenvalues, in the same order.\n    \n    '
'Check for dominant eigenvalues with large imaginary part'
if (not numpy.allclose(evals.imag, 0.0)):
    warnings.warn('Using eigenvalues with non-zero imaginary part', ImaginaryEigenValueWarning)
'Check for multiple eigenvalues of magnitude one'
ind_abs_one = numpy.isclose(numpy.abs(evals), 1.0, rtol=0.0, atol=1e-14)
if (sum(ind_abs_one) > 1):
    warnings.warn('Multiple eigenvalues with magnitude one.', SpectralWarning)
'Compute implied time scales'
ts = numpy.zeros(len(evals))
'Eigenvalues of magnitude one imply infinite timescale'
ts[ind_abs_one] = numpy.inf
'All other eigenvalues give rise to finite timescales'
tempResult = log(numpy.abs(evals[numpy.logical_not(ind_abs_one)]))
	
===================================================================	
TestDecomposition.test_timescales: 251	
----------------------------	

P = self.bdc.transition_matrix()
ev = eigvals(P)
'Sort with decreasing magnitude'
ev = ev[numpy.argsort(numpy.abs(ev))[::(- 1)]]
tempResult = log(numpy.abs(ev))
	
===================================================================	
timescale_sensitivity: 83	
----------------------------	

' \n    calculate the sensitivity matrix for timescale k given transition matrix T.\n    Parameters\n    ----------\n    T : numpy.ndarray shape = (n, n)\n        Transition matrix\n    k : int\n        timescale index for timescales of descending order (k = 0 for the infinite one)\n        \n    Returns\n    -------\n    x : ndarray, shape=(n, n)\n        Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.\n    '
(eValues, rightEigenvectors) = numpy.linalg.eig(T)
leftEigenvectors = numpy.linalg.inv(rightEigenvectors)
perm = numpy.argsort(eValues)[::(- 1)]
eValues = eValues[perm]
rightEigenvectors = rightEigenvectors[:, perm]
leftEigenvectors = leftEigenvectors[perm]
eVal = eValues[k]
sensitivity = numpy.outer(leftEigenvectors[k], rightEigenvectors[:, k])
if (eVal < 1.0):
    tempResult = log(eVal)
	
===================================================================	
timescales: 194	
----------------------------	

'Compute implied time scales of given transition matrix\n    \n    Parameters\n    ----------\n    T : transition matrix\n    tau : lag time\n    k : int (optional)\n        Compute the first k implied time scales.\n    ncv : int (optional)\n        The number of Lanczos vectors generated, `ncv` must be greater than k;\n        it is recommended that ncv > 2*k\n    reversible : bool, optional\n        Indicate that transition matrix is reversible \n    mu : (M,) ndarray, optional\n        Stationary distribution of T\n\n    Returns\n    -------\n    ts : ndarray\n        The implied time scales of the transition matrix.\n    '
if (k is None):
    raise ValueError('Number of time scales required for decomposition of sparse matrix')
values = eigenvalues(T, k=k, ncv=ncv, reversible=reversible)
'Check for dominant eigenvalues with large imaginary part'
if (not numpy.allclose(values.imag, 0.0)):
    warnings.warn('Using eigenvalues with non-zero imaginary part for implied time scale computation', ImaginaryEigenValueWarning)
'Check for multiple eigenvalues of magnitude one'
ind_abs_one = numpy.isclose(numpy.abs(values), 1.0)
if (sum(ind_abs_one) > 1):
    warnings.warn('Multiple eigenvalues with magnitude one.', SpectralWarning)
'Compute implied time scales'
ts = numpy.zeros(len(values))
'Eigenvalues of magnitude one imply infinite rate'
ts[ind_abs_one] = numpy.inf
'All other eigenvalues give rise to finite rates'
tempResult = log(numpy.abs(values[numpy.logical_not(ind_abs_one)]))
	
===================================================================	
timescales_from_eigenvalues: 211	
----------------------------	

'Compute implied time scales from given eigenvalues\n    \n    Parameters\n    ----------\n    eval : eigenvalues\n    tau : lag time\n\n    Returns\n    -------\n    ts : ndarray\n        The implied time scales to the given eigenvalues, in the same order.\n    \n    '
'Check for dominant eigenvalues with large imaginary part'
if (not numpy.allclose(ev.imag, 0.0)):
    warnings.warn('Using eigenvalues with non-zero imaginary part for implied time scale computation', ImaginaryEigenValueWarning)
'Check for multiple eigenvalues of magnitude one'
ind_abs_one = numpy.isclose(numpy.abs(ev), 1.0, rtol=0.0, atol=1e-14)
if (sum(ind_abs_one) > 1):
    warnings.warn('Multiple eigenvalues with magnitude one.', SpectralWarning)
'Compute implied time scales'
ts = numpy.zeros(len(ev))
'Eigenvalues of magnitude one imply infinite timescale'
ts[ind_abs_one] = numpy.inf
'All other eigenvalues give rise to finite timescales'
tempResult = log(numpy.abs(ev[numpy.logical_not(ind_abs_one)]))
	
===================================================================	
TestDecomposition.test_timescales: 319	
----------------------------	

P_dense = self.bdc.transition_matrix()
P = self.bdc.transition_matrix_sparse()
ev = eigvals(P_dense)
'Sort with decreasing magnitude'
ev = ev[numpy.argsort(numpy.abs(ev))[::(- 1)]]
tempResult = log(numpy.abs(ev))
	
===================================================================	
TestDecomposition.test_timescales_rev: 341	
----------------------------	

P_dense = self.bdc.transition_matrix()
P = self.bdc.transition_matrix_sparse()
mu = self.bdc.stationary_distribution()
ev = eigvals(P_dense)
'Sort with decreasing magnitude'
ev = ev[numpy.argsort(numpy.abs(ev))[::(- 1)]]
tempResult = log(numpy.abs(ev))
	
===================================================================	
TestDecompositionSparse.test_timescales_rev: 620	
----------------------------	

P_dense = self.bdc.transition_matrix()
P = self.bdc.transition_matrix_sparse()
mu = self.bdc.stationary_distribution()
ev = eigvals(P_dense)
'Sort with decreasing magnitude'
ev = ev[numpy.argsort(numpy.abs(ev))[::(- 1)]]
tempResult = log(numpy.abs(ev))
	
===================================================================	
TestDecompositionSparse.test_timescales: 598	
----------------------------	

P_dense = self.bdc.transition_matrix()
P = self.bdc.transition_matrix_sparse()
ev = eigvals(P_dense)
'Sort with decreasing magnitude'
ev = ev[numpy.argsort(numpy.abs(ev))[::(- 1)]]
tempResult = log(numpy.abs(ev))
	
===================================================================	
TestDecompositionDense.test_timescales_rev: 247	
----------------------------	

P_dense = self.bdc.transition_matrix()
P = self.bdc.transition_matrix()
mu = self.bdc.stationary_distribution()
ev = eigvals(P_dense)
'Sort with decreasing magnitude'
ev = ev[numpy.argsort(numpy.abs(ev))[::(- 1)]]
tempResult = log(numpy.abs(ev))
	
===================================================================	
TestDecompositionDense.test_timescales: 225	
----------------------------	

P = self.bdc.transition_matrix()
ev = eigvals(P)
'Sort with decreasing magnitude'
ev = ev[numpy.argsort(numpy.abs(ev))[::(- 1)]]
tempResult = log(numpy.abs(ev))
	
===================================================================	
log_likelihood: 181	
----------------------------	

'Log-likelihood of the count matrix given a transition matrix.\n\n    Parameters\n    ----------\n    C : (M, M) ndarray or scipy.sparse matrix\n        Count matrix\n    T : (M, M) ndarray orscipy.sparse matrix\n        Transition matrix\n        \n    Returns\n    -------\n    logL : float\n        Log-likelihood of the count matrix\n\n    Notes\n    -----\n        \n    The likelihood of a set of observed transition counts\n    :math:`C=(c_{ij})` for a given matrix of transition counts\n    :math:`T=(t_{ij})` is given by \n\n    .. math:: L(C|P)=\\prod_{i=1}^{M} \\left( \\prod_{j=1}^{M} p_{ij}^{c_{ij}} \\right)\n\n    The log-likelihood is given by\n\n    .. math:: l(C|P)=\\sum_{i,j=1}^{M}c_{ij} \\log p_{ij}.\n\n    The likelihood describes the probability of making an observation\n    :math:`C` for a given model :math:`P`.\n\n    Examples\n    --------\n\n    >>> from msmtools.estimation import log_likelihood\n\n    >>> T = np.array([[0.9, 0.1, 0.0], [0.5, 0.0, 0.5], [0.0, 0.1, 0.9]])\n\n    >>> C = np.array([[58, 7, 0], [6, 0, 4], [0, 3, 21]])\n    >>> logL = log_likelihood(C, T)\n    >>> logL\n    -38.280803472508182    \n\n    >>> C = np.array([[58, 20, 0], [6, 0, 4], [0, 3, 21]])\n    >>> logL = log_likelihood(C, T)\n    >>> logL\n    -68.214409681430766\n\n    References\n    ----------\n    .. [1] Prinz, J H, H Wu, M Sarich, B Keller, M Senne, M Held, J D\n        Chodera, C Schuette and F Noe. 2011. Markov models of\n        molecular kinetics: Generation and validation. J Chem Phys\n        134: 174105     \n        \n    '
if (issparse(C) and issparse(T)):
    return sparse.likelihood.log_likelihood(C, T)
else:
    if (not isinstance(C, numpy.ndarray)):
        C = numpy.array(C)
    if (not isinstance(T, numpy.ndarray)):
        T = numpy.array(T)
    nz = numpy.nonzero(T)
    tempResult = log(T[nz])
	
===================================================================	
log_likelihood: 13	
----------------------------	

'\n        implementation of likelihood of C given T\n    '
C = C.tocsr()
T = T.tocsr()
ind = scipy.nonzero(C)
relT = numpy.array(T[ind])[0, :]
tempResult = log(relT)
	
===================================================================	
TestTransitionMatrix.test_count_matrix: 25	
----------------------------	

'Small test cases'
log = likelihood.log_likelihood(self.C1, self.T1)
tempResult = log((((0.8 * (0.2 ** 3)) * (0.9 ** 3)) * 0.1))
	
===================================================================	
TestTransitionMatrix.test_count_matrix: 25	
----------------------------	

'Small test cases'
log = log_likelihood(self.C1, self.T1)
tempResult = log((((0.8 * (0.2 ** 3)) * (0.9 ** 3)) * 0.1))
	
***************************************************	
nilearn_nilearn-0.4.0: 2	
===================================================================	
create_graph_net_simulation_data: 33	
----------------------------	

'\n    Function to generate data\n\n    '
generator = check_random_state(random_state)
w = numpy.zeros((size, size, size))
for _ in range(n_points):
    point = (generator.randint(0, size), generator.randint(0, size), generator.randint(0, size))
    w[point] = 1.0
mask = numpy.ones((size, size, size), dtype=numpy.bool)
w = scipy.ndimage.gaussian_filter(w, sigma=1)
w = w[mask]
XX = generator.randn(n_samples, size, size, size)
noise = []
for i in range(n_samples):
    Xi = scipy.ndimage.filters.gaussian_filter(XX[i, :, :, :], smooth_X)
    Xi = Xi[mask]
    noise.append(Xi)
noise = numpy.array(noise)
if (task == 'regression'):
    y = generator.randn(n_samples)
elif (task == 'classification'):
    y = numpy.ones(n_samples)
    y[0::2] = (- 1)
X = numpy.dot(y[:, numpy.newaxis], w[numpy.newaxis])
norm_noise = (scipy.linalg.norm(X, 2) / numpy.exp((snr / 20.0)))
noise_coef = (norm_noise / scipy.linalg.norm(noise, 2))
noise *= noise_coef
tempResult = log((scipy.linalg.norm(X, 2) / scipy.linalg.norm(noise, 2)))
	
===================================================================	
_smooth_array: 64	
----------------------------	

"Smooth images by applying a Gaussian filter.\n\n    Apply a Gaussian filter along the three first dimensions of arr.\n\n    Parameters\n    ----------\n    arr: numpy.ndarray\n        4D array, with image number as last dimension. 3D arrays are also\n        accepted.\n\n    affine: numpy.ndarray\n        (4, 4) matrix, giving affine transformation for image. (3, 3) matrices\n        are also accepted (only these coefficients are used).\n        If fwhm='fast', the affine is not used and can be None\n\n    fwhm: scalar, numpy.ndarray, 'fast' or None\n        Smoothing strength, as a full-width at half maximum, in millimeters.\n        If a scalar is given, width is identical on all three directions.\n        A numpy.ndarray must have 3 elements, giving the FWHM along each axis.\n        If fwhm == 'fast', a fast smoothing will be performed with\n        a filter [0.2, 1, 0.2] in each direction and a normalisation\n        to preserve the local average value.\n        If fwhm is None, no filtering is performed (useful when just removal\n        of non-finite values is needed).\n\n\n    ensure_finite: bool\n        if True, replace every non-finite values (like NaNs) by zero before\n        filtering.\n\n    copy: bool\n        if True, input array is not modified. False by default: the filtering\n        is performed in-place.\n\n    Returns\n    -------\n    filtered_arr: numpy.ndarray\n        arr, filtered.\n\n    Notes\n    -----\n    This function is most efficient with arr in C order.\n    "
if (fwhm == 0.0):
    warnings.warn("The parameter 'fwhm' for smoothing is specified as {0}. Converting to None (no smoothing option)".format(fwhm))
    fwhm = None
if (arr.dtype.kind == 'i'):
    if (arr.dtype == numpy.int64):
        arr = arr.astype(numpy.float64)
    else:
        arr = arr.astype(numpy.float32)
if copy:
    arr = arr.copy()
if ensure_finite:
    arr[numpy.logical_not(numpy.isfinite(arr))] = 0
if (fwhm == 'fast'):
    arr = _fast_smooth_array(arr)
elif (fwhm is not None):
    affine = affine[:3, :3]
    tempResult = log(2)
	
***************************************************	
poliastro_poliastro-0.8.0: 2	
===================================================================	
nu_to_F: 26	
----------------------------	

'Hyperbolic eccentric anomaly from true anomaly.\n\n    Parameters\n    ----------\n    nu : float\n        True anomaly (rad).\n    ecc : float\n        Eccentricity (>1).\n\n    Returns\n    -------\n    F : float\n        Hyperbolic eccentric anomaly.\n\n    Note\n    -----\n    Taken from Curtis, H. (2013). *Orbital mechanics for engineering students*. 167\n\n    '
tempResult = log(((numpy.sqrt((ecc + 1)) + (numpy.sqrt((ecc - 1)) * numpy.tan((nu / 2)))) / (numpy.sqrt((ecc + 1)) - (numpy.sqrt((ecc - 1)) * numpy.tan((nu / 2))))))
	
===================================================================	
_kepler: 58	
----------------------------	

dot_r0v0 = numpy.dot(r0, v0)
norm_r0 = (numpy.dot(r0, r0) ** 0.5)
sqrt_mu = (k ** 0.5)
alpha = (((- numpy.dot(v0, v0)) / k) + (2 / norm_r0))
if (alpha > 0):
    xi_new = ((sqrt_mu * tof) * alpha)
elif (alpha < 0):
    tempResult = log((((((- 2) * k) * alpha) * tof) / (dot_r0v0 + ((numpy.sign(tof) * numpy.sqrt(((- k) / alpha))) * (1 - (norm_r0 * alpha))))))
	
***************************************************	
skimage_skimage-0.13.0: 13	
===================================================================	
separate_stains: 331	
----------------------------	

'RGB to stain color space conversion.\n\n    Parameters\n    ----------\n    rgb : array_like\n        The image in RGB format, in a 3-D array of shape ``(.., .., 3)``.\n    conv_matrix: ndarray\n        The stain separation matrix as described by G. Landini [1]_.\n\n    Returns\n    -------\n    out : ndarray\n        The image in stain color space, in a 3-D array of shape\n        ``(.., .., 3)``.\n\n    Raises\n    ------\n    ValueError\n        If `rgb` is not a 3-D array of shape ``(.., .., 3)``.\n\n    Notes\n    -----\n    Stain separation matrices available in the ``color`` module and their\n    respective colorspace:\n\n    * ``hed_from_rgb``: Hematoxylin + Eosin + DAB\n    * ``hdx_from_rgb``: Hematoxylin + DAB\n    * ``fgx_from_rgb``: Feulgen + Light Green\n    * ``bex_from_rgb``: Giemsa stain : Methyl Blue + Eosin\n    * ``rbd_from_rgb``: FastRed + FastBlue +  DAB\n    * ``gdx_from_rgb``: Methyl Green + DAB\n    * ``hax_from_rgb``: Hematoxylin + AEC\n    * ``bro_from_rgb``: Blue matrix Anilline Blue + Red matrix Azocarmine                        + Orange matrix Orange-G\n    * ``bpx_from_rgb``: Methyl Blue + Ponceau Fuchsin\n    * ``ahx_from_rgb``: Alcian Blue + Hematoxylin\n    * ``hpx_from_rgb``: Hematoxylin + PAS\n\n    References\n    ----------\n    .. [1] http://www.dentistry.bham.ac.uk/landinig/software/cdeconv/cdeconv.html\n\n    Examples\n    --------\n    >>> from skimage import data\n    >>> from skimage.color import separate_stains, hdx_from_rgb\n    >>> ihc = data.immunohistochemistry()\n    >>> ihc_hdx = separate_stains(ihc, hdx_from_rgb)\n    '
rgb = util.dtype.img_as_float(rgb, force_copy=True)
rgb += 2
tempResult = log(rgb)
	
===================================================================	
peak_snr: 205	
----------------------------	

'Peak signal to noise ratio of two images\n\n    Parameters\n    ----------\n    img1 : array-like\n    img2 : array-like\n\n    Returns\n    -------\n    peak_snr : float\n        Peak signal to noise ratio\n    '
if (img1.ndim == 3):
    (img1, img2) = (rgb2gray(img1.copy()), rgb2gray(img2.copy()))
img1 = skimage.img_as_float(img1)
img2 = skimage.img_as_float(img2)
mse = ((1.0 / img1.size) * np.square((img1 - img2)).sum())
(_, max_) = dtype_range[img1.dtype.type]
tempResult = log((max_ / mse))
	
===================================================================	
ORB.extract: 90	
----------------------------	

'Extract rBRIEF binary descriptors for given keypoints in image.\n\n        Note that the keypoints must be extracted using the same `downscale`\n        and `n_scales` parameters. Additionally, if you want to extract both\n        keypoints and descriptors you should use the faster\n        `detect_and_extract`.\n\n        Parameters\n        ----------\n        image : 2D array\n            Input image.\n        keypoints : (N, 2) array\n            Keypoint coordinates as ``(row, col)``.\n        scales : (N, ) array\n            Corresponding scales.\n        orientations : (N, ) array\n            Corresponding orientations in radians.\n\n        '
assert_nD(image, 2)
pyramid = self._build_pyramid(image)
descriptors_list = []
mask_list = []
tempResult = log(scales)
	
===================================================================	
ORB.extract: 90	
----------------------------	

'Extract rBRIEF binary descriptors for given keypoints in image.\n\n        Note that the keypoints must be extracted using the same `downscale`\n        and `n_scales` parameters. Additionally, if you want to extract both\n        keypoints and descriptors you should use the faster\n        `detect_and_extract`.\n\n        Parameters\n        ----------\n        image : 2D array\n            Input image.\n        keypoints : (N, 2) array\n            Keypoint coordinates as ``(row, col)``.\n        scales : (N, ) array\n            Corresponding scales.\n        orientations : (N, ) array\n            Corresponding orientations in radians.\n\n        '
assert_nD(image, 2)
pyramid = self._build_pyramid(image)
descriptors_list = []
mask_list = []
tempResult = log(self.downscale)
	
===================================================================	
threshold_li: 150	
----------------------------	

'Return threshold value based on adaptation of Li\'s Minimum Cross Entropy method.\n\n    Parameters\n    ----------\n    image : (N, M) ndarray\n        Input image.\n\n    Returns\n    -------\n    threshold : float\n        Upper threshold value. All pixels with an intensity higher than\n        this value are assumed to be foreground.\n\n    References\n    ----------\n    .. [1] Li C.H. and Lee C.K. (1993) "Minimum Cross Entropy Thresholding"\n           Pattern Recognition, 26(4): 617-625\n           DOI:10.1016/0031-3203(93)90115-D\n    .. [2] Li C.H. and Tam P.K.S. (1998) "An Iterative Algorithm for Minimum\n           Cross Entropy Thresholding" Pattern Recognition Letters, 18(8): 771-776\n           DOI:10.1016/S0167-8655(98)00057-9\n    .. [3] Sezgin M. and Sankur B. (2004) "Survey over Image Thresholding\n           Techniques and Quantitative Performance Evaluation" Journal of\n           Electronic Imaging, 13(1): 146-165\n           DOI:10.1117/1.1631315\n    .. [4] ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\n    Examples\n    --------\n    >>> from skimage.data import camera\n    >>> image = camera()\n    >>> thresh = threshold_li(image)\n    >>> binary = image > thresh\n    '
if numpy.all((image == image.flat[0])):
    raise ValueError('threshold_li is expected to work with images having more than one value. The input image seems to have just one value {0}.'.format(image.flat[0]))
image = image.copy()
immin = numpy.min(image)
image -= immin
imrange = numpy.max(image)
tolerance = ((0.5 * imrange) / 256)
mean = numpy.mean(image)
new_thresh = mean
old_thresh = (new_thresh + (2 * tolerance))
while (abs((new_thresh - old_thresh)) > tolerance):
    old_thresh = new_thresh
    threshold = (old_thresh + tolerance)
    mean_back = image[(image <= threshold)].mean()
    mean_obj = image[(image > threshold)].mean()
    tempResult = log(mean_back)
	
===================================================================	
threshold_li: 150	
----------------------------	

'Return threshold value based on adaptation of Li\'s Minimum Cross Entropy method.\n\n    Parameters\n    ----------\n    image : (N, M) ndarray\n        Input image.\n\n    Returns\n    -------\n    threshold : float\n        Upper threshold value. All pixels with an intensity higher than\n        this value are assumed to be foreground.\n\n    References\n    ----------\n    .. [1] Li C.H. and Lee C.K. (1993) "Minimum Cross Entropy Thresholding"\n           Pattern Recognition, 26(4): 617-625\n           DOI:10.1016/0031-3203(93)90115-D\n    .. [2] Li C.H. and Tam P.K.S. (1998) "An Iterative Algorithm for Minimum\n           Cross Entropy Thresholding" Pattern Recognition Letters, 18(8): 771-776\n           DOI:10.1016/S0167-8655(98)00057-9\n    .. [3] Sezgin M. and Sankur B. (2004) "Survey over Image Thresholding\n           Techniques and Quantitative Performance Evaluation" Journal of\n           Electronic Imaging, 13(1): 146-165\n           DOI:10.1117/1.1631315\n    .. [4] ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\n    Examples\n    --------\n    >>> from skimage.data import camera\n    >>> image = camera()\n    >>> thresh = threshold_li(image)\n    >>> binary = image > thresh\n    '
if numpy.all((image == image.flat[0])):
    raise ValueError('threshold_li is expected to work with images having more than one value. The input image seems to have just one value {0}.'.format(image.flat[0]))
image = image.copy()
immin = numpy.min(image)
image -= immin
imrange = numpy.max(image)
tolerance = ((0.5 * imrange) / 256)
mean = numpy.mean(image)
new_thresh = mean
old_thresh = (new_thresh + (2 * tolerance))
while (abs((new_thresh - old_thresh)) > tolerance):
    old_thresh = new_thresh
    threshold = (old_thresh + tolerance)
    mean_back = image[(image <= threshold)].mean()
    mean_obj = image[(image > threshold)].mean()
    tempResult = log(mean_obj)
	
===================================================================	
threshold_yen: 106	
----------------------------	

'Return threshold value based on Yen\'s method.\n\n    Parameters\n    ----------\n    image : (N, M) ndarray\n        Input image.\n    nbins : int, optional\n        Number of bins used to calculate histogram. This value is ignored for\n        integer arrays.\n\n    Returns\n    -------\n    threshold : float\n        Upper threshold value. All pixels with an intensity higher than\n        this value are assumed to be foreground.\n\n    References\n    ----------\n    .. [1] Yen J.C., Chang F.J., and Chang S. (1995) "A New Criterion\n           for Automatic Multilevel Thresholding" IEEE Trans. on Image\n           Processing, 4(3): 370-378. DOI:10.1109/83.366472\n    .. [2] Sezgin M. and Sankur B. (2004) "Survey over Image Thresholding\n           Techniques and Quantitative Performance Evaluation" Journal of\n           Electronic Imaging, 13(1): 146-165, DOI:10.1117/1.1631315\n           http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf\n    .. [3] ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold\n\n    Examples\n    --------\n    >>> from skimage.data import camera\n    >>> image = camera()\n    >>> thresh = threshold_yen(image)\n    >>> binary = image <= thresh\n    '
(hist, bin_centers) = histogram(image.ravel(), nbins)
if (bin_centers.size == 1):
    return bin_centers[0]
pmf = (hist.astype(numpy.float32) / hist.sum())
P1 = numpy.cumsum(pmf)
P1_sq = numpy.cumsum((pmf ** 2))
P2_sq = numpy.cumsum((pmf[::(- 1)] ** 2))[::(- 1)]
tempResult = log((((P1_sq[:(- 1)] * P2_sq[1:]) ** (- 1)) * ((P1[:(- 1)] * (1.0 - P1[:(- 1)])) ** 2)))
	
===================================================================	
_sigma_prefactor: 9	
----------------------------	

b = bandwidth
tempResult = log(2)
	
===================================================================	
_dynamic_max_trials: 258	
----------------------------	

'Determine number trials such that at least one outlier-free subset is\n    sampled for the given inlier/outlier ratio.\n    Parameters\n    ----------\n    n_inliers : int\n        Number of inliers in the data.\n    n_samples : int\n        Total number of samples in the data.\n    min_samples : int\n        Minimum number of samples chosen randomly from original data.\n    probability : float\n        Probability (confidence) that one outlier-free sample is generated.\n    Returns\n    -------\n    trials : int\n        Number of trials.\n    '
if (n_inliers == 0):
    return numpy.inf
nom = (1 - probability)
if (nom == 0):
    return numpy.inf
inlier_ratio = (n_inliers / float(n_samples))
denom = (1 - (inlier_ratio ** min_samples))
if (denom == 0):
    return 1
elif (denom == 1):
    return numpy.inf
tempResult = log(nom)
	
===================================================================	
_dynamic_max_trials: 259	
----------------------------	

'Determine number trials such that at least one outlier-free subset is\n    sampled for the given inlier/outlier ratio.\n    Parameters\n    ----------\n    n_inliers : int\n        Number of inliers in the data.\n    n_samples : int\n        Total number of samples in the data.\n    min_samples : int\n        Minimum number of samples chosen randomly from original data.\n    probability : float\n        Probability (confidence) that one outlier-free sample is generated.\n    Returns\n    -------\n    trials : int\n        Number of trials.\n    '
if (n_inliers == 0):
    return numpy.inf
nom = (1 - probability)
if (nom == 0):
    return numpy.inf
inlier_ratio = (n_inliers / float(n_samples))
denom = (1 - (inlier_ratio ** min_samples))
if (denom == 0):
    return 1
elif (denom == 1):
    return numpy.inf
nom = numpy.log(nom)
tempResult = log(denom)
	
===================================================================	
test_shannon_ones: 9	
----------------------------	

img = numpy.ones((10, 10))
res = shannon_entropy(img, base=numpy.e)
tempResult = log((10 * 10))
	
===================================================================	
richardson_lucy: 83	
----------------------------	

"Richardson-Lucy deconvolution.\n\n    Parameters\n    ----------\n    image : ndarray\n       Input degraded image (can be N dimensional).\n    psf : ndarray\n       The point spread function.\n    iterations : int\n       Number of iterations. This parameter plays the role of\n       regularisation.\n    clip : boolean, optional\n       True by default. If true, pixel value of the result above 1 or\n       under -1 are thresholded for skimage pipeline compatibility.\n\n    Returns\n    -------\n    im_deconv : ndarray\n       The deconvolved image.\n\n    Examples\n    --------\n    >>> from skimage import color, data, restoration\n    >>> camera = color.rgb2gray(data.camera())\n    >>> from scipy.signal import convolve2d\n    >>> psf = np.ones((5, 5)) / 25\n    >>> camera = convolve2d(camera, psf, 'same')\n    >>> camera += 0.1 * camera.std() * np.random.standard_normal(camera.shape)\n    >>> deconvolved = restoration.richardson_lucy(camera, psf, 5)\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution\n    "
direct_time = numpy.prod((image.shape + psf.shape))
tempResult = log(n)
	
===================================================================	
_swirl_mapping: 96	
----------------------------	

(x, y) = xy.T
(x0, y0) = center
rho = numpy.sqrt((((x - x0) ** 2) + ((y - y0) ** 2)))
tempResult = log(2)
	
***************************************************	
sunpy_sunpy-0.8.0: 0	
***************************************************	
spacetelescope_synphot-0.1: 3	
===================================================================	
SpectralElement.photbw: 701	
----------------------------	

'Calculate the\n        :ref:`bandpass RMS width as in IRAF SYNPHOT <synphot-formula-bandw>`.\n\n        This is a compatibility function. To calculate the actual\n        bandpass RMS width, use :func:`rmswidth`.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, ``self.waveset`` is used.\n\n        threshold : float or `~astropy.units.quantity.Quantity`, optional\n            Data points with throughput below this value are not\n            included in the calculation. By default, all data points\n            are included.\n\n        Returns\n        -------\n        bandw : `~astropy.units.quantity.Quantity`\n            IRAF SYNPHOT RMS width of the bandpass.\n\n        Raises\n        ------\n        synphot.exceptions.SynphotError\n            Threshold is invalid.\n\n        '
x = self._validate_wavelengths(wavelengths).value
y = self(x).value
if (threshold is None):
    wave = x
    thru = y
else:
    if (isinstance(threshold, numbers.Real) or (isinstance(threshold, astropy.units.Quantity) and (threshold.unit == self._internal_flux_unit))):
        mask = (y >= threshold)
    else:
        raise exceptions.SynphotError('{0} is not a valid threshold'.format(threshold))
    wave = x[mask]
    thru = y[mask]
a = self.barlam(wavelengths=wavelengths).value
if (a == 0):
    bandw = 0.0
else:
    tempResult = log((wave / a))
	
===================================================================	
SpectralElement.fwhm: 711	
----------------------------	

'Calculate :ref:`synphot-formula-fwhm` of equivalent gaussian.\n\n        Parameters\n        ----------\n        kwargs : dict\n            See :func:`photbw`.\n\n        Returns\n        -------\n        fwhm_val : `~astropy.units.quantity.Quantity`\n            FWHM of equivalent gaussian.\n\n        '
tempResult = log(2)
	
===================================================================	
BaseSpectrum.barlam: 242	
----------------------------	

'Calculate :ref:`mean log wavelength <synphot-formula-barlam>`.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, `waveset` is used.\n\n        Returns\n        -------\n        bar_lam : `~astropy.units.quantity.Quantity`\n            Mean log wavelength.\n\n        '
x = self._validate_wavelengths(wavelengths).value
y = self(x).value
tempResult = log(x)
	
***************************************************	
librosa_librosa-0.5.1: 13	
===================================================================	
__beat_track_dp: 108	
----------------------------	

'Core dynamic program for beat tracking'
backlink = numpy.zeros_like(localscore, dtype=int)
cumscore = numpy.zeros_like(localscore)
window = numpy.arange(((- 2) * period), ((- numpy.round((period / 2))) + 1), dtype=int)
if (tightness <= 0):
    raise ParameterError('tightness must be strictly positive')
tempResult = log(((- window) / period))
	
===================================================================	
fmt: 185	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    tempResult = log((n - 1))
	
===================================================================	
fmt: 185	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    tempResult = log((n - 2))
	
===================================================================	
fmt: 186	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    tempResult = log((n - 1))
	
===================================================================	
fmt: 186	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    tempResult = log(t_min)
	
===================================================================	
fmt: 190	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    n_fmt = int(numpy.ceil(((over_sample * (numpy.log((n - 1)) - numpy.log(t_min))) / log_base)))
elif (n_fmt < 3):
    raise ParameterError('n_fmt=={:} < 3'.format(n_fmt))
else:
    tempResult = log((n_fmt - 1))
	
===================================================================	
fmt: 190	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    n_fmt = int(numpy.ceil(((over_sample * (numpy.log((n - 1)) - numpy.log(t_min))) / log_base)))
elif (n_fmt < 3):
    raise ParameterError('n_fmt=={:} < 3'.format(n_fmt))
else:
    tempResult = log((n_fmt - 2))
	
===================================================================	
fmt: 197	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    n_fmt = int(numpy.ceil(((over_sample * (numpy.log((n - 1)) - numpy.log(t_min))) / log_base)))
elif (n_fmt < 3):
    raise ParameterError('n_fmt=={:} < 3'.format(n_fmt))
else:
    log_base = ((numpy.log((n_fmt - 1)) - numpy.log((n_fmt - 2))) / over_sample)
if (not numpy.all(numpy.isfinite(y))):
    raise ParameterError('y must be finite everywhere')
base = numpy.exp(log_base)
x = numpy.linspace(0, 1, num=n, endpoint=False)
f_interp = scipy.interpolate.interp1d(x, y, kind=kind, axis=axis)
n_over = int(numpy.ceil(over_sample))
tempResult = log(t_min)
	
===================================================================	
fmt: 197	
----------------------------	

'The fast Mellin transform (FMT) [1]_ of a uniformly sampled signal y.\n\n    When the Mellin parameter (beta) is 1/2, it is also known as the scale transform [2]_.\n    The scale transform can be useful for audio analysis because its magnitude is invariant\n    to scaling of the domain (e.g., time stretching or compression).  This is analogous\n    to the magnitude of the Fourier transform being invariant to shifts in the input domain.\n\n\n    .. [1] De Sena, Antonio, and Davide Rocchesso.\n        "A fast Mellin and scale transform."\n        EURASIP Journal on Applied Signal Processing 2007.1 (2007): 75-75.\n\n    .. [2] Cohen, L.\n        "The scale representation."\n        IEEE Transactions on Signal Processing 41, no. 12 (1993): 3275-3292.\n\n    Parameters\n    ----------\n    y : np.ndarray, real-valued\n        The input signal(s).  Can be multidimensional.\n        The target axis must contain at least 3 samples.\n\n    t_min : float > 0\n        The minimum time spacing (in samples).\n        This value should generally be less than 1 to preserve as much information as\n        possible.\n\n    n_fmt : int > 2 or None\n        The number of scale transform bins to use.\n        If None, then `n_bins = over_sample * ceil(n * log((n-1)/t_min))` is taken,\n        where `n = y.shape[axis]`\n\n    kind : str\n        The type of interpolation to use when re-sampling the input.\n        See `scipy.interpolate.interp1d` for possible values.\n\n        Note that the default is to use high-precision (cubic) interpolation.\n        This can be slow in practice; if speed is preferred over accuracy,\n        then consider using `kind=\'linear\'`.\n\n    beta : float\n        The Mellin parameter.  `beta=0.5` provides the scale transform.\n\n    over_sample : float >= 1\n        Over-sampling factor for exponential resampling.\n\n    axis : int\n        The axis along which to transform `y`\n\n    Returns\n    -------\n    x_scale : np.ndarray [dtype=complex]\n        The scale transform of `y` along the `axis` dimension.\n\n    Raises\n    ------\n    ParameterError\n        if `n_fmt < 2` or `t_min <= 0`\n        or if `y` is not finite\n        or if `y.shape[axis] < 3`.\n\n    Notes\n    -----\n    This function caches at level 30.\n\n\n    Examples\n    --------\n    >>> # Generate a signal and time-stretch it (with energy normalization)\n    >>> scale = 1.25\n    >>> freq = 3.0\n    >>> x1 = np.linspace(0, 1, num=1024, endpoint=False)\n    >>> x2 = np.linspace(0, 1, num=scale * len(x1), endpoint=False)\n    >>> y1 = np.sin(2 * np.pi * freq * x1)\n    >>> y2 = np.sin(2 * np.pi * freq * x2) / np.sqrt(scale)\n    >>> # Verify that the two signals have the same energy\n    >>> np.sum(np.abs(y1)**2), np.sum(np.abs(y2)**2)\n        (255.99999999999997, 255.99999999999969)\n    >>> scale1 = librosa.fmt(y1, n_fmt=512)\n    >>> scale2 = librosa.fmt(y2, n_fmt=512)\n    >>> # And plot the results\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure(figsize=(8, 4))\n    >>> plt.subplot(1, 2, 1)\n    >>> plt.plot(y1, label=\'Original\')\n    >>> plt.plot(y2, linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'time (samples)\')\n    >>> plt.title(\'Input signals\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.subplot(1, 2, 2)\n    >>> plt.semilogy(np.abs(scale1), label=\'Original\')\n    >>> plt.semilogy(np.abs(scale2), linestyle=\'--\', label=\'Stretched\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.title(\'Scale transform magnitude\')\n    >>> plt.legend(frameon=True)\n    >>> plt.axis(\'tight\')\n    >>> plt.tight_layout()\n\n    >>> # Plot the scale transform of an onset strength autocorrelation\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      offset=10.0, duration=30.0)\n    >>> odf = librosa.onset.onset_strength(y=y, sr=sr)\n    >>> # Auto-correlate with up to 10 seconds lag\n    >>> odf_ac = librosa.autocorrelate(odf, max_size=10 * sr // 512)\n    >>> # Normalize\n    >>> odf_ac = librosa.util.normalize(odf_ac, norm=np.inf)\n    >>> # Compute the scale transform\n    >>> odf_ac_scale = librosa.fmt(librosa.util.normalize(odf_ac), n_fmt=512)\n    >>> # Plot the results\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> plt.plot(odf, label=\'Onset strength\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Time (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 2)\n    >>> plt.plot(odf_ac, label=\'Onset autocorrelation\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'Lag (frames)\')\n    >>> plt.xticks([])\n    >>> plt.legend(frameon=True)\n    >>> plt.subplot(3, 1, 3)\n    >>> plt.semilogy(np.abs(odf_ac_scale), label=\'Scale transform magnitude\')\n    >>> plt.axis(\'tight\')\n    >>> plt.xlabel(\'scale coefficients\')\n    >>> plt.legend(frameon=True)\n    >>> plt.tight_layout()\n    '
n = y.shape[axis]
if (n < 3):
    raise ParameterError('y.shape[{:}]=={:} < 3'.format(axis, n))
if (t_min <= 0):
    raise ParameterError('t_min must be a positive number')
if (n_fmt is None):
    if (over_sample < 1):
        raise ParameterError('over_sample must be >= 1')
    log_base = (numpy.log((n - 1)) - numpy.log((n - 2)))
    n_fmt = int(numpy.ceil(((over_sample * (numpy.log((n - 1)) - numpy.log(t_min))) / log_base)))
elif (n_fmt < 3):
    raise ParameterError('n_fmt=={:} < 3'.format(n_fmt))
else:
    log_base = ((numpy.log((n_fmt - 1)) - numpy.log((n_fmt - 2))) / over_sample)
if (not numpy.all(numpy.isfinite(y))):
    raise ParameterError('y must be finite everywhere')
base = numpy.exp(log_base)
x = numpy.linspace(0, 1, num=n, endpoint=False)
f_interp = scipy.interpolate.interp1d(x, y, kind=kind, axis=axis)
n_over = int(numpy.ceil(over_sample))
tempResult = log(n)
	
===================================================================	
mel_to_hz: 125	
----------------------------	

'Convert mel bin numbers to frequencies\n\n    Examples\n    --------\n    >>> librosa.mel_to_hz(3)\n    array([ 200.])\n\n    >>> librosa.mel_to_hz([1,2,3,4,5])\n    array([  66.667,  133.333,  200.   ,  266.667,  333.333])\n\n    Parameters\n    ----------\n    mels          : np.ndarray [shape=(n,)], float\n        mel bins to convert\n    htk           : bool\n        use HTK formula instead of Slaney\n\n    Returns\n    -------\n    frequencies   : np.ndarray [shape=(n,)]\n        input mels in Hz\n\n    See Also\n    --------\n    hz_to_mel\n    '
mels = numpy.atleast_1d(mels)
if htk:
    return (700.0 * ((10.0 ** (mels / 2595.0)) - 1.0))
f_min = 0.0
f_sp = (200.0 / 3)
freqs = (f_min + (f_sp * mels))
min_log_hz = 1000.0
min_log_mel = ((min_log_hz - f_min) / f_sp)
tempResult = log(6.4)
	
===================================================================	
hz_to_mel: 110	
----------------------------	

'Convert Hz to Mels\n\n    Examples\n    --------\n    >>> librosa.hz_to_mel(60)\n    array([ 0.9])\n    >>> librosa.hz_to_mel([110, 220, 440])\n    array([ 1.65,  3.3 ,  6.6 ])\n\n    Parameters\n    ----------\n    frequencies   : np.ndarray [shape=(n,)] , float\n        scalar or array of frequencies\n    htk           : bool\n        use HTK formula instead of Slaney\n\n    Returns\n    -------\n    mels        : np.ndarray [shape=(n,)]\n        input frequencies in Mels\n\n    See Also\n    --------\n    mel_to_hz\n    '
frequencies = numpy.atleast_1d(frequencies)
if htk:
    return (2595.0 * numpy.log10((1.0 + (frequencies / 700.0))))
f_min = 0.0
f_sp = (200.0 / 3)
mels = ((frequencies - f_min) / f_sp)
min_log_hz = 1000.0
min_log_mel = ((min_log_hz - f_min) / f_sp)
tempResult = log(6.4)
	
===================================================================	
hz_to_mel: 112	
----------------------------	

'Convert Hz to Mels\n\n    Examples\n    --------\n    >>> librosa.hz_to_mel(60)\n    array([ 0.9])\n    >>> librosa.hz_to_mel([110, 220, 440])\n    array([ 1.65,  3.3 ,  6.6 ])\n\n    Parameters\n    ----------\n    frequencies   : np.ndarray [shape=(n,)] , float\n        scalar or array of frequencies\n    htk           : bool\n        use HTK formula instead of Slaney\n\n    Returns\n    -------\n    mels        : np.ndarray [shape=(n,)]\n        input frequencies in Mels\n\n    See Also\n    --------\n    mel_to_hz\n    '
frequencies = numpy.atleast_1d(frequencies)
if htk:
    return (2595.0 * numpy.log10((1.0 + (frequencies / 700.0))))
f_min = 0.0
f_sp = (200.0 / 3)
mels = ((frequencies - f_min) / f_sp)
min_log_hz = 1000.0
min_log_mel = ((min_log_hz - f_min) / f_sp)
logstep = (numpy.log(6.4) / 27.0)
log_t = (frequencies >= min_log_hz)
tempResult = log((frequencies[log_t] / min_log_hz))
	
===================================================================	
__test1: 70	
----------------------------	

srand()
data = numpy.random.randn(3, 100)
distance = squareform(pdist(data.T, metric=metric))
rec = librosa.segment.recurrence_matrix(data, mode='affinity', metric=metric, sparse=True, bandwidth=bandwidth)
(i, j, vals) = scipy.sparse.find(rec)
tempResult = log(vals)
	
***************************************************	
mne_python-0.15.0: 13	
===================================================================	
_ico_downsample: 224	
----------------------------	

'Downsample the surface if isomorphic to a subdivided icosahedron.'
n_tri = surf['ntri']
found = (- 1)
bad_msg = ('A surface with %d triangles cannot be isomorphic with a subdivided icosahedron.' % surf['ntri'])
if ((n_tri % 20) != 0):
    raise RuntimeError(bad_msg)
n_tri = (n_tri // 20)
tempResult = log(n_tri)
	
===================================================================	
_ico_downsample: 224	
----------------------------	

'Downsample the surface if isomorphic to a subdivided icosahedron.'
n_tri = surf['ntri']
found = (- 1)
bad_msg = ('A surface with %d triangles cannot be isomorphic with a subdivided icosahedron.' % surf['ntri'])
if ((n_tri % 20) != 0):
    raise RuntimeError(bad_msg)
n_tri = (n_tri // 20)
tempResult = log(4)
	
===================================================================	
_calc_beta: 54	
----------------------------	

'Compute coefficients for calculating the magic vector omega.'
rkk1 = (rk1[0] - rk[0])
size = numpy.linalg.norm(rkk1)
rkk1 /= size
num = (rk_norm + numpy.dot(rk, rkk1))
den = (rk1_norm + numpy.dot(rk1, rkk1))
tempResult = log((num / den))
	
===================================================================	
_logdet: 437	
----------------------------	

'Compute the log det of a symmetric matrix.'
vals = scipy.linalg.eigh(A)[0]
tol = ((vals.max() * vals.size) * np.finfo(np.float64).eps)
vals = numpy.where((vals > tol), vals, tol)
tempResult = log(vals)
	
===================================================================	
minimum_phase: 284	
----------------------------	

'Convert a linear-phase FIR filter to minimum phase.\n\n    Parameters\n    ----------\n    h : array\n        Linear-phase FIR filter coefficients.\n\n    Returns\n    -------\n    h_minimum : array\n        The minimum-phase version of the filter, with length\n        ``(length(h) + 1) // 2``.\n    '
try:
    from scipy.signal import minimum_phase
except Exception:
    pass
else:
    return minimum_phase(h)
from scipy.fftpack import fft, ifft
h = numpy.asarray(h)
if numpy.iscomplexobj(h):
    raise ValueError('Complex filters not supported')
if ((h.ndim != 1) or (h.size <= 2)):
    raise ValueError('h must be 1D and at least 2 samples long')
n_half = (len(h) // 2)
if (not numpy.allclose(h[(- n_half):][::(- 1)], h[:n_half])):
    warnings.warn('h does not appear to by symmetric, conversion may fail', RuntimeWarning)
n_fft = (2 ** int(numpy.ceil(numpy.log2(((2 * (len(h) - 1)) / 0.01)))))
h_temp = numpy.abs(fft(h, n_fft))
h_temp += (1e-07 * h_temp[(h_temp > 0)].min())
tempResult = log(h_temp, out=h_temp)
	
===================================================================	
_tps: 400	
----------------------------	

'Thin-plate function (r ** 2) * np.log(r).'
out = numpy.zeros_like(distsq)
mask = (distsq > 0)
valid = distsq[mask]
tempResult = log(valid)
	
===================================================================	
CSP.fit: 93	
----------------------------	

'Estimate the CSP decomposition on epochs.\n\n        Parameters\n        ----------\n        X : ndarray, shape (n_epochs, n_channels, n_times)\n            The data on which to estimate the CSP.\n        y : array, shape (n_epochs,)\n            The class for each epoch.\n\n        Returns\n        -------\n        self : instance of CSP\n            Returns the modified instance.\n        '
if (not isinstance(X, numpy.ndarray)):
    raise ValueError(('X should be of type ndarray (got %s).' % type(X)))
self._check_Xy(X, y)
n_channels = X.shape[1]
self._classes = numpy.unique(y)
n_classes = len(self._classes)
if (n_classes < 2):
    raise ValueError('n_classes must be >= 2.')
covs = numpy.zeros((n_classes, n_channels, n_channels))
sample_weights = list()
for (class_idx, this_class) in enumerate(self._classes):
    if (self.cov_est == 'concat'):
        class_ = numpy.transpose(X[(y == this_class)], [1, 0, 2])
        class_ = class_.reshape(n_channels, (- 1))
        cov = _regularized_covariance(class_, reg=self.reg)
        weight = sum((y == this_class))
    elif (self.cov_est == 'epoch'):
        class_ = X[(y == this_class)]
        cov = numpy.zeros((n_channels, n_channels))
        for this_X in class_:
            cov += _regularized_covariance(this_X, reg=self.reg)
        cov /= len(class_)
        weight = len(class_)
    covs[class_idx] = cov
    if self.norm_trace:
        covs[class_idx] /= numpy.trace(cov)
    sample_weights.append(weight)
if (n_classes == 2):
    (eigen_values, eigen_vectors) = scipy.linalg.eigh(covs[0], covs.sum(0))
    ix = numpy.argsort(numpy.abs((eigen_values - 0.5)))[::(- 1)]
else:
    (eigen_vectors, D) = _ajd_pham(covs)
    mean_cov = numpy.average(covs, axis=0, weights=sample_weights)
    eigen_vectors = eigen_vectors.T
    for ii in range(eigen_vectors.shape[1]):
        tmp = numpy.dot(numpy.dot(eigen_vectors[:, ii].T, mean_cov), eigen_vectors[:, ii])
        eigen_vectors[:, ii] /= numpy.sqrt(tmp)
    class_probas = [numpy.mean((y == _class)) for _class in self._classes]
    mutual_info = []
    for jj in range(eigen_vectors.shape[1]):
        (aa, bb) = (0, 0)
        for (cov, prob) in zip(covs, class_probas):
            tmp = numpy.dot(numpy.dot(eigen_vectors[:, jj].T, cov), eigen_vectors[:, jj])
            tempResult = log(numpy.sqrt(tmp))
	
===================================================================	
CSP.transform: 120	
----------------------------	

"Estimate epochs sources given the CSP filters.\n\n        Parameters\n        ----------\n        X : array, shape (n_epochs, n_channels, n_times)\n            The data.\n\n        Returns\n        -------\n        X : ndarray\n            If self.transform_into == 'average_power' then returns the power of\n            CSP features averaged over time and shape (n_epochs, n_sources)\n            If self.transform_into == 'csp_space' then returns the data in CSP\n            space and shape is (n_epochs, n_sources, n_times)\n        "
if (not isinstance(X, numpy.ndarray)):
    raise ValueError(('X should be of type ndarray (got %s).' % type(X)))
if (self.filters_ is None):
    raise RuntimeError('No filters available. Please first fit CSP decomposition.')
pick_filters = self.filters_[:self.n_components]
X = numpy.asarray([numpy.dot(pick_filters, epoch) for epoch in X])
if (self.transform_into == 'average_power'):
    X = (X ** 2).mean(axis=2)
    log = (True if (self.log is None) else self.log)
    if log:
        tempResult = log(X)
	
===================================================================	
_prob_kuiper: 57	
----------------------------	

'Test for statistical significance against uniform distribution.\n\n    Parameters\n    ----------\n    d : float\n        The kuiper distance value.\n    n_eff : int\n        The effective number of elements.\n    dtype : str | obj\n        The data type to be used. Defaults to double precision floats.\n\n    Returns\n    -------\n    pk_norm : float\n        The normalized Kuiper value such that 0 < ``pk_norm`` < 1.\n\n    References\n    ----------\n    [1] Stephens MA 1970. Journal of the Royal Statistical Society, ser. B,\n    vol 32, pp 115-122.\n\n    [2] Kuiper NH 1962. Proceedings of the Koninklijke Nederlands Akademie\n    van Wetenschappen, ser Vol 63 pp 38-47\n    '
n_time_slices = numpy.size(d)
n_points = 100
en = math.sqrt(n_eff)
k_lambda = (((en + 0.155) + (0.24 / en)) * d)
l2 = (k_lambda ** 2.0)
j2 = ((numpy.arange(n_points) + 1) ** 2)
j2 = j2.repeat(n_time_slices).reshape(n_points, n_time_slices)
fact = (((4.0 * j2) * l2) - 1.0)
expo = numpy.exp((((- 2.0) * j2) * l2))
term = ((2.0 * fact) * expo)
pk = term.sum(axis=0, dtype=dtype)
pk_norm = numpy.zeros(n_time_slices)
tempResult = log(pk[(pk > 0)])
	
===================================================================	
test_mne_python_vs_eeglab: 51	
----------------------------	

' Test eeglab vs mne_python infomax code.'
random_state = 42
methods = ['infomax', 'extended_infomax']
ch_types = ['eeg', 'mag']
for ch_type in ch_types:
    Y = generate_data_for_comparing_against_eeglab_infomax(ch_type, random_state)
    (N, T) = Y.shape
    for method in methods:
        eeglab_results_file = ('eeglab_%s_results_%s_data.mat' % (method, dict(eeg='eeg', mag='meg')[ch_type]))
        tempResult = log(N)
	
===================================================================	
test_mne_python_vs_eeglab: 52	
----------------------------	

' Test eeglab vs mne_python infomax code.'
random_state = 42
methods = ['infomax', 'extended_infomax']
ch_types = ['eeg', 'mag']
for ch_type in ch_types:
    Y = generate_data_for_comparing_against_eeglab_infomax(ch_type, random_state)
    (N, T) = Y.shape
    for method in methods:
        eeglab_results_file = ('eeglab_%s_results_%s_data.mat' % (method, dict(eeg='eeg', mag='meg')[ch_type]))
        l_rate_eeglab = (0.00065 / numpy.log(N))
        tempResult = log(T)
	
===================================================================	
test_stockwell_api: 85	
----------------------------	

'Test stockwell functions.'
raw = read_raw_fif(raw_fname)
(event_id, tmin, tmax) = (1, (- 0.2), 0.5)
event_name = os.path.join(base_dir, 'test-eve.fif')
events = read_events(event_name)
epochs = Epochs(raw, events, event_id, tmin, tmax, picks=[0, 1, 3])
for (fmin, fmax) in [(None, 50), (5, 50), (5, None)]:
    with warnings.catch_warnings(record=True):
        (power, itc) = tfr_stockwell(epochs, fmin=fmin, fmax=fmax, return_itc=True)
    if (fmax is not None):
        assert_true((power.freqs.max() <= fmax))
    with warnings.catch_warnings(record=True):
        power_evoked = tfr_stockwell(epochs.average(), fmin=fmin, fmax=fmax, return_itc=False)
    assert_array_almost_equal(power_evoked.data, power.data)
assert_true(isinstance(power, AverageTFR))
assert_true(isinstance(itc, AverageTFR))
assert_equal(power.data.shape, itc.data.shape)
assert_true((itc.data.min() >= 0.0))
assert_true((itc.data.max() <= 1.0))
tempResult = log(power.data.max())
	
===================================================================	
test_stockwell_api: 86	
----------------------------	

'Test stockwell functions.'
raw = read_raw_fif(raw_fname)
(event_id, tmin, tmax) = (1, (- 0.2), 0.5)
event_name = os.path.join(base_dir, 'test-eve.fif')
events = read_events(event_name)
epochs = Epochs(raw, events, event_id, tmin, tmax, picks=[0, 1, 3])
for (fmin, fmax) in [(None, 50), (5, 50), (5, None)]:
    with warnings.catch_warnings(record=True):
        (power, itc) = tfr_stockwell(epochs, fmin=fmin, fmax=fmax, return_itc=True)
    if (fmax is not None):
        assert_true((power.freqs.max() <= fmax))
    with warnings.catch_warnings(record=True):
        power_evoked = tfr_stockwell(epochs.average(), fmin=fmin, fmax=fmax, return_itc=False)
    assert_array_almost_equal(power_evoked.data, power.data)
assert_true(isinstance(power, AverageTFR))
assert_true(isinstance(itc, AverageTFR))
assert_equal(power.data.shape, itc.data.shape)
assert_true((itc.data.min() >= 0.0))
assert_true((itc.data.max() <= 1.0))
assert_true(((numpy.log(power.data.max()) * 20) <= 0.0))
tempResult = log(power.data.max())
	
***************************************************	

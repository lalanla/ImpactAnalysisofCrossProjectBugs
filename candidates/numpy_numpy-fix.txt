astropy_astropy-1.3.0: 0	
***************************************************	
scipy_scipy-0.19.0: 0	
***************************************************	
sklearn_sklearn-0.18.0: 0	
***************************************************	
matplotlib_matplotlib-2.0.0: 0	
***************************************************	
ipython_ipython-6.1.0: 0	
***************************************************	
pandas_pandas-0.19.2: 2	
===================================================================	
TestSeriesIndexing.test_getitem_setitem_ellipsis: 72	
----------------------------	

s = Series(numpy.random.randn(10))
tempResult = fix(s)
	
===================================================================	
DatetimeIndex.to_julian_date: 1145	
----------------------------	

'\n        Convert DatetimeIndex to Float64Index of Julian Dates.\n        0 Julian date is noon January 1, 4713 BC.\n        http://en.wikipedia.org/wiki/Julian_day\n        '
year = self.year
month = self.month
day = self.day
testarr = (month < 3)
year[testarr] -= 1
month[testarr] += 12
tempResult = fix((((153 * month) - 457) / 5))
	
***************************************************	
dask_dask-0.7.0: 0	
***************************************************	
nengo_nengo-2.0.0: 0	
***************************************************	
sympy_sympy-1.0.0: 0	
***************************************************	
daducci_amico-dev: 0	
***************************************************	
aplpy_aplpy-1.1.1: 0	
***************************************************	
markovmodel_msmtools-1.0.2: 0	
***************************************************	
nilearn_nilearn-0.4.0: 0	
***************************************************	
poliastro_poliastro-0.8.0: 0	
***************************************************	
skimage_skimage-0.13.0: 2	
===================================================================	
register_translation: 48	
----------------------------	

'\n    Efficient subpixel image translation registration by cross-correlation.\n\n    This code gives the same precision as the FFT upsampled cross-correlation\n    in a fraction of the computation time and with reduced memory requirements.\n    It obtains an initial estimate of the cross-correlation peak by an FFT and\n    then refines the shift estimation by upsampling the DFT only in a small\n    neighborhood of that estimate by means of a matrix-multiply DFT.\n\n    Parameters\n    ----------\n    src_image : ndarray\n        Reference image.\n    target_image : ndarray\n        Image to register.  Must be same dimensionality as ``src_image``.\n    upsample_factor : int, optional\n        Upsampling factor. Images will be registered to within\n        ``1 / upsample_factor`` of a pixel. For example\n        ``upsample_factor == 20`` means the images will be registered\n        within 1/20th of a pixel.  Default is 1 (no upsampling)\n    space : string, one of "real" or "fourier", optional\n        Defines how the algorithm interprets input data.  "real" means data\n        will be FFT\'d to compute the correlation, while "fourier" data will\n        bypass FFT of input data.  Case insensitive.\n\n    Returns\n    -------\n    shifts : ndarray\n        Shift vector (in pixels) required to register ``target_image`` with\n        ``src_image``.  Axis ordering is consistent with numpy (e.g. Z, Y, X)\n    error : float\n        Translation invariant normalized RMS error between ``src_image`` and\n        ``target_image``.\n    phasediff : float\n        Global phase difference between the two images (should be\n        zero if images are non-negative).\n\n    References\n    ----------\n    .. [1] Manuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup,\n           "Efficient subpixel image registration algorithms,"\n           Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156\n    .. [2] James R. Fienup, "Invariant error metrics for image reconstruction"\n           Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352\n    '
if (src_image.shape != target_image.shape):
    raise ValueError('Error: images must be same size for register_translation')
if ((src_image.ndim != 2) and (upsample_factor > 1)):
    raise NotImplementedError('Error: register_translation only supports subpixel registration for 2D images')
if (space.lower() == 'fourier'):
    src_freq = src_image
    target_freq = target_image
elif (space.lower() == 'real'):
    src_image = numpy.array(src_image, dtype=numpy.complex128, copy=False)
    target_image = numpy.array(target_image, dtype=numpy.complex128, copy=False)
    src_freq = numpy.fft.fftn(src_image)
    target_freq = numpy.fft.fftn(target_image)
else:
    raise ValueError('Error: register_translation only knows the "real" and "fourier" values for the ``space`` argument.')
shape = src_freq.shape
image_product = (src_freq * target_freq.conj())
cross_correlation = numpy.fft.ifftn(image_product)
maxima = numpy.unravel_index(numpy.argmax(numpy.abs(cross_correlation)), cross_correlation.shape)
tempResult = fix((axis_size / 2))
	
===================================================================	
register_translation: 58	
----------------------------	

'\n    Efficient subpixel image translation registration by cross-correlation.\n\n    This code gives the same precision as the FFT upsampled cross-correlation\n    in a fraction of the computation time and with reduced memory requirements.\n    It obtains an initial estimate of the cross-correlation peak by an FFT and\n    then refines the shift estimation by upsampling the DFT only in a small\n    neighborhood of that estimate by means of a matrix-multiply DFT.\n\n    Parameters\n    ----------\n    src_image : ndarray\n        Reference image.\n    target_image : ndarray\n        Image to register.  Must be same dimensionality as ``src_image``.\n    upsample_factor : int, optional\n        Upsampling factor. Images will be registered to within\n        ``1 / upsample_factor`` of a pixel. For example\n        ``upsample_factor == 20`` means the images will be registered\n        within 1/20th of a pixel.  Default is 1 (no upsampling)\n    space : string, one of "real" or "fourier", optional\n        Defines how the algorithm interprets input data.  "real" means data\n        will be FFT\'d to compute the correlation, while "fourier" data will\n        bypass FFT of input data.  Case insensitive.\n\n    Returns\n    -------\n    shifts : ndarray\n        Shift vector (in pixels) required to register ``target_image`` with\n        ``src_image``.  Axis ordering is consistent with numpy (e.g. Z, Y, X)\n    error : float\n        Translation invariant normalized RMS error between ``src_image`` and\n        ``target_image``.\n    phasediff : float\n        Global phase difference between the two images (should be\n        zero if images are non-negative).\n\n    References\n    ----------\n    .. [1] Manuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup,\n           "Efficient subpixel image registration algorithms,"\n           Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156\n    .. [2] James R. Fienup, "Invariant error metrics for image reconstruction"\n           Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352\n    '
if (src_image.shape != target_image.shape):
    raise ValueError('Error: images must be same size for register_translation')
if ((src_image.ndim != 2) and (upsample_factor > 1)):
    raise NotImplementedError('Error: register_translation only supports subpixel registration for 2D images')
if (space.lower() == 'fourier'):
    src_freq = src_image
    target_freq = target_image
elif (space.lower() == 'real'):
    src_image = numpy.array(src_image, dtype=numpy.complex128, copy=False)
    target_image = numpy.array(target_image, dtype=numpy.complex128, copy=False)
    src_freq = numpy.fft.fftn(src_image)
    target_freq = numpy.fft.fftn(target_image)
else:
    raise ValueError('Error: register_translation only knows the "real" and "fourier" values for the ``space`` argument.')
shape = src_freq.shape
image_product = (src_freq * target_freq.conj())
cross_correlation = numpy.fft.ifftn(image_product)
maxima = numpy.unravel_index(numpy.argmax(numpy.abs(cross_correlation)), cross_correlation.shape)
midpoints = numpy.array([numpy.fix((axis_size / 2)) for axis_size in shape])
shifts = numpy.array(maxima, dtype=numpy.float64)
shifts[(shifts > midpoints)] -= numpy.array(shape)[(shifts > midpoints)]
if (upsample_factor == 1):
    src_amp = (numpy.sum((numpy.abs(src_freq) ** 2)) / src_freq.size)
    target_amp = (numpy.sum((numpy.abs(target_freq) ** 2)) / target_freq.size)
    CCmax = cross_correlation.max()
else:
    shifts = (numpy.round((shifts * upsample_factor)) / upsample_factor)
    upsampled_region_size = numpy.ceil((upsample_factor * 1.5))
    tempResult = fix((upsampled_region_size / 2.0))
	
***************************************************	
sunpy_sunpy-0.8.0: 0	
***************************************************	
spacetelescope_synphot-0.1: 0	
***************************************************	
librosa_librosa-0.5.1: 0	
***************************************************	
mne_python-0.15.0: 2	
===================================================================	
_read_mff_events: 40	
----------------------------	

'Extract the events.\n\n    Parameters\n    ----------\n    filename : str\n        File path.\n    sfreq : float\n        The sampling frequency\n    '
orig = {}
for xml_file in glob(join(filename, '*.xml')):
    xml_type = splitext(basename(xml_file))[0]
    orig[xml_type] = _parse_xml(xml_file)
xml_files = orig.keys()
xml_events = [x for x in xml_files if (x[:7] == 'Events_')]
for item in orig['info']:
    if ('recordTime' in item):
        start_time = _ns2py_time(item['recordTime'])
        break
markers = []
code = []
for xml in xml_events:
    for event in orig[xml][2:]:
        event_start = _ns2py_time(event['beginTime'])
        start = (event_start - start_time).total_seconds()
        if (event['code'] not in code):
            code.append(event['code'])
        tempResult = fix((start * sfreq))
	
===================================================================	
infomax: 91	
----------------------------	

'Run (extended) Infomax ICA decomposition on raw data.\n\n    Parameters\n    ----------\n    data : np.ndarray, shape (n_samples, n_features)\n        The whitened data to unmix.\n    weights : np.ndarray, shape (n_features, n_features)\n        The initialized unmixing matrix.\n        Defaults to None, which means the identity matrix is used.\n    l_rate : float\n        This quantity indicates the relative size of the change in weights.\n        Defaults to ``0.01 / log(n_features ** 2)``.\n\n        .. note:: Smaller learning rates will slow down the ICA procedure.\n\n    block : int\n        The block size of randomly chosen data segments.\n        Defaults to floor(sqrt(n_times / 3.)).\n    w_change : float\n        The change at which to stop iteration. Defaults to 1e-12.\n    anneal_deg : float\n        The angle (in degrees) at which the learning rate will be reduced.\n        Defaults to 60.0.\n    anneal_step : float\n        The factor by which the learning rate will be reduced once\n        ``anneal_deg`` is exceeded: ``l_rate *= anneal_step.``\n        Defaults to 0.9.\n    extended : bool\n        Whether to use the extended Infomax algorithm or not.\n        Defaults to True.\n    n_subgauss : int\n        The number of subgaussian components. Only considered for extended\n        Infomax. Defaults to 1.\n    kurt_size : int\n        The window size for kurtosis estimation. Only considered for extended\n        Infomax. Defaults to 6000.\n    ext_blocks : int\n        Only considered for extended Infomax. If positive, denotes the number\n        of blocks after which to recompute the kurtosis, which is used to\n        estimate the signs of the sources. In this case, the number of\n        sub-gaussian sources is automatically determined.\n        If negative, the number of sub-gaussian sources to be used is fixed\n        and equal to n_subgauss. In this case, the kurtosis is not estimated.\n        Defaults to 1.\n    max_iter : int\n        The maximum number of iterations. Defaults to 200.\n    random_state : int | np.random.RandomState\n        If random_state is an int, use random_state to seed the random number\n        generator. If random_state is already a np.random.RandomState instance,\n        use random_state as random number generator.\n    blowup : float\n        The maximum difference allowed between two successive estimations of\n        the unmixing matrix. Defaults to 10000.\n    blowup_fac : float\n        The factor by which the learning rate will be reduced if the difference\n        between two successive estimations of the unmixing matrix exceededs\n        ``blowup``: ``l_rate *= blowup_fac``. Defaults to 0.5.\n    n_small_angle : int | None\n        The maximum number of allowed steps in which the angle between two\n        successive estimations of the unmixing matrix is less than\n        ``anneal_deg``. If None, this parameter is not taken into account to\n        stop the iterations. Defaults to 20.\n    use_bias : bool\n        This quantity indicates if the bias should be computed.\n        Defaults to True.\n    verbose : bool, str, int, or None\n        If not None, override default verbosity level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    unmixing_matrix : np.ndarray, shape (n_features, n_features)\n        The linear unmixing operator.\n\n    References\n    ----------\n    .. [1] A. J. Bell, T. J. Sejnowski. An information-maximization approach to\n           blind separation and blind deconvolution. Neural Computation, 7(6),\n           1129-1159, 1995.\n    .. [2] T. W. Lee, M. Girolami, T. J. Sejnowski. Independent component\n           analysis using an extended infomax algorithm for mixed subgaussian\n           and supergaussian sources. Neural Computation, 11(2), 417-441, 1999.\n    '
from scipy.stats import kurtosis
rng = check_random_state(random_state)
max_weight = 100000000.0
restart_fac = 0.9
min_l_rate = 1e-10
degconst = (180.0 / numpy.pi)
extmomentum = 0.5
signsbias = 0.02
signcount_threshold = 25
signcount_step = 2
(n_samples, n_features) = data.shape
n_features_square = (n_features ** 2)
if (l_rate is None):
    l_rate = (0.01 / math.log((n_features ** 2.0)))
if (block is None):
    block = int(math.floor(math.sqrt((n_samples / 3.0))))
utils.logger.info((('computing%sInfomax ICA' % ' Extended ') if extended else ' '))
nblock = (n_samples // block)
lastt = (((nblock - 1) * block) + 1)
if (weights is None):
    weights = numpy.identity(n_features, dtype=numpy.float64)
else:
    weights = weights.T
BI = (block * numpy.identity(n_features, dtype=numpy.float64))
bias = numpy.zeros((n_features, 1), dtype=numpy.float64)
onesrow = numpy.ones((1, block), dtype=numpy.float64)
startweights = weights.copy()
oldweights = startweights.copy()
step = 0
count_small_angle = 0
wts_blowup = False
blockno = 0
signcount = 0
initial_ext_blocks = ext_blocks
if extended:
    signs = numpy.ones(n_features)
    for k in range(n_subgauss):
        signs[k] = (- 1)
    kurt_size = min(kurt_size, n_samples)
    old_kurt = numpy.zeros(n_features, dtype=numpy.float64)
    oldsigns = numpy.zeros(n_features)
(olddelta, oldchange) = (1.0, 0.0)
while (step < max_iter):
    permute = random_permutation(n_samples, rng)
    for t in range(0, lastt, block):
        u = numpy.dot(data[permute[t:(t + block)], :], weights)
        u += np.dot(bias, onesrow).T
        if extended:
            y = numpy.tanh(u)
            weights += (l_rate * numpy.dot(weights, ((BI - (signs[None, :] * numpy.dot(u.T, y))) - numpy.dot(u.T, u))))
            if use_bias:
                bias += (l_rate * numpy.reshape((numpy.sum(y, axis=0, dtype=numpy.float64) * (- 2.0)), (n_features, 1)))
        else:
            y = (1.0 / (1.0 + numpy.exp((- u))))
            weights += (l_rate * numpy.dot(weights, (BI + numpy.dot(u.T, (1.0 - (2.0 * y))))))
            if use_bias:
                bias += (l_rate * numpy.reshape(numpy.sum((1.0 - (2.0 * y)), axis=0, dtype=numpy.float64), (n_features, 1)))
        max_weight_val = numpy.max(numpy.abs(weights))
        if (max_weight_val > max_weight):
            wts_blowup = True
        blockno += 1
        if wts_blowup:
            break
        if extended:
            if ((ext_blocks > 0) and ((blockno % ext_blocks) == 0)):
                if (kurt_size < n_samples):
                    rp = numpy.floor((rng.uniform(0, 1, kurt_size) * (n_samples - 1)))
                    tpartact = np.dot(data[rp.astype(int), :], weights).T
                else:
                    tpartact = np.dot(data, weights).T
                kurt = kurtosis(tpartact, axis=1, fisher=True)
                if (extmomentum != 0):
                    kurt = ((extmomentum * old_kurt) + ((1.0 - extmomentum) * kurt))
                    old_kurt = kurt
                signs = numpy.sign((kurt + signsbias))
                ndiff = ((signs - oldsigns) != 0).sum()
                if (ndiff == 0):
                    signcount += 1
                else:
                    signcount = 0
                oldsigns = signs
                if (signcount >= signcount_threshold):
                    tempResult = fix((ext_blocks * signcount_step))
	
***************************************************	

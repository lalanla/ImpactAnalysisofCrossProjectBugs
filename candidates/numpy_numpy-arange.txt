astropy_astropy-1.3.0: 430	
===================================================================	
discretize_bilinear_2D: 109	
----------------------------	

'\n    Discretize model by performing a bilinear interpolation.\n    '
tempResult = arange((x_range[0] - 0.5), (x_range[1] + 0.5))
	
===================================================================	
discretize_bilinear_2D: 110	
----------------------------	

'\n    Discretize model by performing a bilinear interpolation.\n    '
x = numpy.arange((x_range[0] - 0.5), (x_range[1] + 0.5))
tempResult = arange((y_range[0] - 0.5), (y_range[1] + 0.5))
	
===================================================================	
discretize_integrate_2D: 146	
----------------------------	

'\n    Discretize model by integrating the model over the pixel.\n    '
from scipy.integrate import dblquad
tempResult = arange((x_range[0] - 0.5), (x_range[1] + 0.5))
	
===================================================================	
discretize_integrate_2D: 147	
----------------------------	

'\n    Discretize model by integrating the model over the pixel.\n    '
from scipy.integrate import dblquad
x = numpy.arange((x_range[0] - 0.5), (x_range[1] + 0.5))
tempResult = arange((y_range[0] - 0.5), (y_range[1] + 0.5))
	
===================================================================	
discretize_integrate_1D: 137	
----------------------------	

'\n    Discretize model by integrating numerically the model over the bin.\n    '
from scipy.integrate import quad
tempResult = arange((x_range[0] - 0.5), (x_range[1] + 0.5))
	
===================================================================	
discretize_center_1D: 91	
----------------------------	

'\n    Discretize model by taking the value at the center of the bin.\n    '
tempResult = arange(*x_range)
	
===================================================================	
discretize_linear_1D: 103	
----------------------------	

'\n    Discretize model by performing a linear interpolation.\n    '
tempResult = arange((x_range[0] - 0.5), (x_range[1] + 0.5))
	
===================================================================	
discretize_oversample_2D: 126	
----------------------------	

'\n    Discretize model by taking the average on an oversampled grid.\n    '
tempResult = arange((x_range[0] - (0.5 * (1 - (1 / factor)))), (x_range[1] + (0.5 * (1 + (1 / factor)))), (1.0 / factor))
	
===================================================================	
discretize_oversample_2D: 127	
----------------------------	

'\n    Discretize model by taking the average on an oversampled grid.\n    '
x = numpy.arange((x_range[0] - (0.5 * (1 - (1 / factor)))), (x_range[1] + (0.5 * (1 + (1 / factor)))), (1.0 / factor))
tempResult = arange((y_range[0] - (0.5 * (1 - (1 / factor)))), (y_range[1] + (0.5 * (1 + (1 / factor)))), (1.0 / factor))
	
===================================================================	
discretize_oversample_1D: 119	
----------------------------	

'\n    Discretize model by taking the average on an oversampled grid.\n    '
tempResult = arange((x_range[0] - (0.5 * (1 - (1 / factor)))), (x_range[1] + (0.5 * (1 + (1 / factor)))), (1.0 / factor))
	
===================================================================	
discretize_center_2D: 96	
----------------------------	

'\n    Discretize model by taking the value at the center of the pixel.\n    '
tempResult = arange(*x_range)
	
===================================================================	
discretize_center_2D: 97	
----------------------------	

'\n    Discretize model by taking the value at the center of the pixel.\n    '
x = numpy.arange(*x_range)
tempResult = arange(*y_range)
	
===================================================================	
test_gaussian_eval_2D: 53	
----------------------------	

'\n    Discretize Gaussian with different modes and check\n    if result is at least similar to Gaussian1D.eval()\n    '
model = Gaussian2D(1, 0, 0, 20, 20)
tempResult = arange((- 100), 101)
	
===================================================================	
test_gaussian_eval_2D: 54	
----------------------------	

'\n    Discretize Gaussian with different modes and check\n    if result is at least similar to Gaussian1D.eval()\n    '
model = Gaussian2D(1, 0, 0, 20, 20)
x = numpy.arange((- 100), 101)
tempResult = arange((- 100), 101)
	
===================================================================	
test_gaussian_eval_1D: 34	
----------------------------	

'\n    Discretize Gaussian with different modes and check\n    if result is at least similar to Gaussian1D.eval().\n    '
model = Gaussian1D(1, 0, 20)
tempResult = arange((- 100), 101)
	
===================================================================	
test_discretize_callable_1d: 80	
----------------------------	

'\n    Test discretize when a 1d function is passed.\n    '

def f(x):
    return (x ** 2)
y = discretize_model(f, ((- 5), 6))
tempResult = arange((- 5), 6)
	
===================================================================	
test_angle_from_view: 79	
----------------------------	

tempResult = arange(3.0)
	
===================================================================	
test_angle_from_view: 84	
----------------------------	

q = (numpy.arange(3.0) * units.deg)
a = q.view(Angle)
assert (type(a) is Angle)
assert (a.unit is q.unit)
assert numpy.all((a == q))
tempResult = arange(4)
	
===================================================================	
test_wrap_at: 475	
----------------------------	

a = Angle(([(- 20), 150, 350, 360] * units.deg))
assert numpy.all((a.wrap_at((360 * u.deg)).degree == numpy.array([340.0, 150.0, 350.0, 0.0])))
assert numpy.all((a.wrap_at(Angle(360, unit=u.deg)).degree == numpy.array([340.0, 150.0, 350.0, 0.0])))
assert numpy.all((a.wrap_at('360d').degree == numpy.array([340.0, 150.0, 350.0, 0.0])))
assert numpy.all((a.wrap_at('180d').degree == numpy.array([(- 20.0), 150.0, (- 10.0), 0.0])))
assert numpy.all((a.wrap_at((np.pi * u.rad)).degree == numpy.array([(- 20.0), 150.0, (- 10.0), 0.0])))
a = Angle('190d')
assert (a.wrap_at('180d') == Angle('-170d'))
tempResult = arange((- 1000.0), 1000.0, 0.125)
	
===================================================================	
test_no_data_nonscalar_frames: 115	
----------------------------	

from ..builtin_frames import AltAz
from astropy.time import Time
tempResult = arange(10.0)
	
===================================================================	
test_no_data_nonscalar_frames: 120	
----------------------------	

from ..builtin_frames import AltAz
from astropy.time import Time
a1 = AltAz(obstime=(Time('2012-01-01') + (numpy.arange(10.0) * units.day)), temperature=(numpy.ones((3, 1)) * units.deg_C))
assert (a1.obstime.shape == (3, 10))
assert (a1.temperature.shape == (3, 10))
assert (a1.shape == (3, 10))
with tests.helper.pytest.raises(ValueError) as exc:
    tempResult = arange(10.0)
	
===================================================================	
test_regression_4293: 134	
----------------------------	

'Really just an extra test on FK4 no e, after finding that the units\n    were not always taken correctly.  This test is against explicitly doing\n    the transformations on pp170 of Explanatory Supplement to the Astronomical\n    Almanac (Seidelmann, 2005).\n\n    See https://github.com/astropy/astropy/pull/4293#issuecomment-234973086\n    '
tempResult = arange(0, 359, 45)
	
===================================================================	
test_regression_4293: 134	
----------------------------	

'Really just an extra test on FK4 no e, after finding that the units\n    were not always taken correctly.  This test is against explicitly doing\n    the transformations on pp170 of Explanatory Supplement to the Astronomical\n    Almanac (Seidelmann, 2005).\n\n    See https://github.com/astropy/astropy/pull/4293#issuecomment-234973086\n    '
tempResult = arange((- 80), 81, 40)
	
===================================================================	
test_regression_4926: 146	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestPhysicsSphericalRepresentation.test_getitem: 295	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestPhysicsSphericalRepresentation.test_getitem: 295	
----------------------------	

tempResult = arange(5, 15)
	
===================================================================	
TestSphericalRepresentation.test_getitem_len_iterable: 116	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSphericalRepresentation.test_getitem_len_iterable: 116	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestCartesianRepresentation.test_init_one_array: 347	
----------------------------	

s1 = CartesianRepresentation(x=([1, 2, 3] * units.pc))
assert (s1.x.unit is units.pc)
assert (s1.y.unit is units.pc)
assert (s1.z.unit is units.pc)
assert_allclose(s1.x.value, 1)
assert_allclose(s1.y.value, 2)
assert_allclose(s1.z.value, 3)
tempResult = arange(27.0)
	
===================================================================	
TestUnitSphericalRepresentation.test_getitem: 202	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestUnitSphericalRepresentation.test_getitem: 202	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestCylindricalRepresentation.test_getitem: 563	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestCylindricalRepresentation.test_getitem: 563	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestCartesianRepresentation.test_getitem: 459	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestCartesianRepresentation.test_getitem: 459	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestArithmetic.setup: 18	
----------------------------	

tempResult = arange(0, 12.1, 2)
	
===================================================================	
TestArithmetic.setup: 19	
----------------------------	

self.lon = Longitude(numpy.arange(0, 12.1, 2), units.hourangle)
tempResult = arange((- 90), 91, 30)
	
===================================================================	
TestManipulation.setup: 13	
----------------------------	

tempResult = arange(0, 24, 4)
	
===================================================================	
TestManipulation.setup: 14	
----------------------------	

lon = Longitude(numpy.arange(0, 24, 4), units.hourangle)
tempResult = arange((- 90), 91, 30)
	
===================================================================	
TestManipulation.setup: 13	
----------------------------	

tempResult = arange(0, 24, 4)
	
===================================================================	
TestManipulation.setup: 14	
----------------------------	

lon = Longitude(numpy.arange(0, 24, 4), units.hourangle)
tempResult = arange((- 90), 91, 30)
	
===================================================================	
TestManipulation.setup: 16	
----------------------------	

lon = Longitude(numpy.arange(0, 24, 4), units.hourangle)
lat = Latitude(numpy.arange((- 90), 91, 30), units.deg)
self.s0 = ICRS((lon[:, numpy.newaxis] * numpy.ones(lat.shape)), (lat * numpy.ones(lon.shape)[:, numpy.newaxis]))
tempResult = arange(len(lon))
	
===================================================================	
test_no_copy: 607	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
test_no_copy: 607	
----------------------------	

tempResult = arange(20.0, 30.0)
	
===================================================================	
test_spherical_offsets: 811	
----------------------------	

i00 = SkyCoord((0 * units.arcmin), (0 * units.arcmin), frame='icrs')
i01 = SkyCoord((0 * units.arcmin), (1 * units.arcmin), frame='icrs')
i10 = SkyCoord((1 * units.arcmin), (0 * units.arcmin), frame='icrs')
i11 = SkyCoord((1 * units.arcmin), (1 * units.arcmin), frame='icrs')
i22 = SkyCoord((2 * units.arcmin), (2 * units.arcmin), frame='icrs')
(dra, ddec) = i00.spherical_offsets_to(i01)
assert_allclose(dra, (0 * units.arcmin))
assert_allclose(ddec, (1 * units.arcmin))
(dra, ddec) = i00.spherical_offsets_to(i10)
assert_allclose(dra, (1 * units.arcmin))
assert_allclose(ddec, (0 * units.arcmin))
(dra, ddec) = i10.spherical_offsets_to(i01)
assert_allclose(dra, ((- 1) * units.arcmin))
assert_allclose(ddec, (1 * units.arcmin))
(dra, ddec) = i11.spherical_offsets_to(i22)
assert_allclose(ddec, (1 * units.arcmin))
assert ((0 * units.arcmin) < dra < (1 * units.arcmin))
fk5 = SkyCoord((0 * units.arcmin), (0 * units.arcmin), frame='fk5')
with tests.helper.pytest.raises(ValueError):
    i00.spherical_offsets_to(fk5)
i1deg = ICRS((1 * units.deg), (1 * units.deg))
(dra, ddec) = i00.spherical_offsets_to(i1deg)
assert_allclose(dra, (1 * units.deg))
assert_allclose(ddec, (1 * units.deg))
i00s = SkyCoord((([0] * 4) * units.arcmin), (([0] * 4) * units.arcmin), frame='icrs')
tempResult = arange(4)
	
===================================================================	
test_spherical_offsets: 814	
----------------------------	

i00 = SkyCoord((0 * units.arcmin), (0 * units.arcmin), frame='icrs')
i01 = SkyCoord((0 * units.arcmin), (1 * units.arcmin), frame='icrs')
i10 = SkyCoord((1 * units.arcmin), (0 * units.arcmin), frame='icrs')
i11 = SkyCoord((1 * units.arcmin), (1 * units.arcmin), frame='icrs')
i22 = SkyCoord((2 * units.arcmin), (2 * units.arcmin), frame='icrs')
(dra, ddec) = i00.spherical_offsets_to(i01)
assert_allclose(dra, (0 * units.arcmin))
assert_allclose(ddec, (1 * units.arcmin))
(dra, ddec) = i00.spherical_offsets_to(i10)
assert_allclose(dra, (1 * units.arcmin))
assert_allclose(ddec, (0 * units.arcmin))
(dra, ddec) = i10.spherical_offsets_to(i01)
assert_allclose(dra, ((- 1) * units.arcmin))
assert_allclose(ddec, (1 * units.arcmin))
(dra, ddec) = i11.spherical_offsets_to(i22)
assert_allclose(ddec, (1 * units.arcmin))
assert ((0 * units.arcmin) < dra < (1 * units.arcmin))
fk5 = SkyCoord((0 * units.arcmin), (0 * units.arcmin), frame='fk5')
with tests.helper.pytest.raises(ValueError):
    i00.spherical_offsets_to(fk5)
i1deg = ICRS((1 * units.deg), (1 * units.deg))
(dra, ddec) = i00.spherical_offsets_to(i1deg)
assert_allclose(dra, (1 * units.deg))
assert_allclose(ddec, (1 * units.deg))
i00s = SkyCoord((([0] * 4) * units.arcmin), (([0] * 4) * units.arcmin), frame='icrs')
i01s = SkyCoord((([0] * 4) * units.arcmin), (numpy.arange(4) * units.arcmin), frame='icrs')
(dra, ddec) = i00s.spherical_offsets_to(i01s)
assert_allclose(dra, (0 * units.arcmin))
tempResult = arange(4)
	
===================================================================	
test_guess_from_table: 686	
----------------------------	

from ...table import Table, Column
from ...utils import NumpyRNGContext
tab = Table()
with NumpyRNGContext(987654321):
    tab.add_column(Column(data=numpy.random.rand(1000), unit='deg', name='RA[J2000]'))
    tab.add_column(Column(data=numpy.random.rand(1000), unit='deg', name='DEC[J2000]'))
sc = SkyCoord.guess_from_table(tab)
numpy.testing.assert_array_equal(sc.ra.deg, tab['RA[J2000]'])
numpy.testing.assert_array_equal(sc.dec.deg, tab['DEC[J2000]'])
tab['RA[J2000]'].unit = None
tab['DEC[J2000]'].unit = None
with tests.helper.pytest.raises(units.UnitsError):
    sc2 = SkyCoord.guess_from_table(tab)
sc2 = SkyCoord.guess_from_table(tab, unit=units.deg)
numpy.testing.assert_array_equal(sc.ra.deg, tab['RA[J2000]'])
numpy.testing.assert_array_equal(sc.dec.deg, tab['DEC[J2000]'])
tab.add_column(Column(data=numpy.random.rand(1000), name='RA_J1900'))
with tests.helper.pytest.raises(ValueError) as excinfo:
    sc3 = SkyCoord.guess_from_table(tab, unit=units.deg)
assert (('J1900' in excinfo.value.args[0]) and ('J2000' in excinfo.value.args[0]))
tab.remove_column('RA_J1900')
with tests.helper.pytest.raises(ValueError):
    sc3 = SkyCoord.guess_from_table(tab, ra=tab['RA[J2000]'], unit=units.deg)
oldra = tab['RA[J2000]']
tab.remove_column('RA[J2000]')
sc3 = SkyCoord.guess_from_table(tab, ra=oldra, unit=units.deg)
numpy.testing.assert_array_equal(sc3.ra.deg, oldra)
numpy.testing.assert_array_equal(sc3.dec.deg, tab['DEC[J2000]'])
tempResult = arange(3)
	
===================================================================	
test_guess_from_table: 687	
----------------------------	

from ...table import Table, Column
from ...utils import NumpyRNGContext
tab = Table()
with NumpyRNGContext(987654321):
    tab.add_column(Column(data=numpy.random.rand(1000), unit='deg', name='RA[J2000]'))
    tab.add_column(Column(data=numpy.random.rand(1000), unit='deg', name='DEC[J2000]'))
sc = SkyCoord.guess_from_table(tab)
numpy.testing.assert_array_equal(sc.ra.deg, tab['RA[J2000]'])
numpy.testing.assert_array_equal(sc.dec.deg, tab['DEC[J2000]'])
tab['RA[J2000]'].unit = None
tab['DEC[J2000]'].unit = None
with tests.helper.pytest.raises(units.UnitsError):
    sc2 = SkyCoord.guess_from_table(tab)
sc2 = SkyCoord.guess_from_table(tab, unit=units.deg)
numpy.testing.assert_array_equal(sc.ra.deg, tab['RA[J2000]'])
numpy.testing.assert_array_equal(sc.dec.deg, tab['DEC[J2000]'])
tab.add_column(Column(data=numpy.random.rand(1000), name='RA_J1900'))
with tests.helper.pytest.raises(ValueError) as excinfo:
    sc3 = SkyCoord.guess_from_table(tab, unit=units.deg)
assert (('J1900' in excinfo.value.args[0]) and ('J2000' in excinfo.value.args[0]))
tab.remove_column('RA_J1900')
with tests.helper.pytest.raises(ValueError):
    sc3 = SkyCoord.guess_from_table(tab, ra=tab['RA[J2000]'], unit=units.deg)
oldra = tab['RA[J2000]']
tab.remove_column('RA[J2000]')
sc3 = SkyCoord.guess_from_table(tab, ra=oldra, unit=units.deg)
numpy.testing.assert_array_equal(sc3.ra.deg, oldra)
numpy.testing.assert_array_equal(sc3.dec.deg, tab['DEC[J2000]'])
(x, y, z) = (np.arange(3).reshape(3, 1) * units.pc)
tempResult = arange(2)
	
===================================================================	
test_barycentric_velocity_consistency: 200	
----------------------------	

t = Time('2016-03-20T12:30:00')
(ep, ev) = get_body_barycentric_posvel(body, t, ephemeris='builtin')
(dp, dv) = get_body_barycentric_posvel(body, t, ephemeris='de432s')
assert_quantity_allclose(ep.xyz, dp.xyz, atol=pos_tol)
assert_quantity_allclose(ev.xyz, dv.xyz, atol=vel_tol)
tempResult = arange(8.0)
	
===================================================================	
test_earth_barycentric_velocity_multi_d: 185	
----------------------------	

tempResult = arange(8.0)
	
===================================================================	
DaophotHeader.parse_col_defs: 34	
----------------------------	

'\n        Parse a series of column definition lines like below.  There may be several\n        such blocks in a single file (where continuation characters have already been\n        stripped).\n        #N ID    XCENTER   YCENTER   MAG         MERR          MSKY           NITER\n        #U ##    pixels    pixels    magnitudes  magnitudes    counts         ##\n        #F %-9d  %-10.3f   %-10.3f   %-12.3f     %-14.3f       %-15.7g        %-6d\n        '
line_ids = ('#N', '#U', '#F')
coldef_dict = defaultdict(list)
stripper = (lambda s: s[2:].strip(' \\'))
for defblock in zip(*map(grouped_lines_dict.get, line_ids)):
    for (key, line) in zip(line_ids, map(stripper, defblock)):
        coldef_dict[key].append(line.split())
if self.data.is_multiline:
    (last_names, last_units, last_formats) = list(zip(*map(coldef_dict.get, line_ids)))[(- 1)]
    N_multiline = len(self.data.first_block)
    tempResult = arange(1, (N_multiline + 1))
	
===================================================================	
module: 29	
----------------------------	

'\nThis module tests some of the methods related to the ``ECSV``\nreader/writer.\n\nRequires `pyyaml <http://pyyaml.org/>`_ to be installed.\n'
import os
import copy
import sys
import numpy as np
from ....table import Table, Column
from ....table.table_helpers import simple_table
from ....tests.helper import pytest
from ....extern.six.moves import StringIO
from ..ecsv import DELIMITERS
from ... import ascii
from .... import units as u
try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False
DTYPES = ['bool', 'int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64', 'float16', 'float32', 'float64', 'float128', 'str']
if ((os.name == 'nt') or (sys.maxsize <= (2 ** 32))):
    DTYPES.remove('float128')
T_DTYPES = Table()
for dtype in DTYPES:
    if (dtype == 'bool'):
        data = numpy.array([False, True, False])
    elif (dtype == 'str'):
        data = numpy.array(['ab 0', 'ab, 1', 'ab2'])
    else:
        tempResult = arange(3, dtype=dtype)
	
===================================================================	
test_multidim_input: 94	
----------------------------	

'\n    Multi-dimensional column in input\n    '
tempResult = arange(4)
	
===================================================================	
Section._getdata: 502	
----------------------------	

for (idx, (key, axis)) in enumerate(zip(keys, self.hdu.shape)):
    if isinstance(key, slice):
        ks = range(*key.indices(axis))
        break
    elif isiterable(key):
        tempResult = arange(axis, dtype=int)
	
===================================================================	
TestChecksumFunctions.test_nonstandard_checksum: 44	
----------------------------	

tempResult = arange((10.0 ** 6), dtype=numpy.float64)
	
===================================================================	
TestChecksumFunctions.test_compressed_image_data_float32: 218	
----------------------------	

tempResult = arange(100, dtype='float32')
	
===================================================================	
TestChecksumFunctions.test_groups_hdu_data: 103	
----------------------------	

tempResult = arange(100.0)
	
===================================================================	
TestChecksumFunctions.test_groups_hdu_data: 105	
----------------------------	

imdata = numpy.arange(100.0)
imdata.shape = (10, 1, 1, 2, 5)
tempResult = arange(10)
	
===================================================================	
TestChecksumFunctions.test_writeto_convenience: 262	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestChecksumFunctions.test_hdu_writeto: 269	
----------------------------	

tempResult = arange(100, dtype='int16')
	
===================================================================	
TestChecksumFunctions.test_compressed_image_data_int16: 188	
----------------------------	

tempResult = arange(100, dtype='int16')
	
===================================================================	
TestChecksumFunctions.test_append: 254	
----------------------------	

hdul = io.fits.open(self.data('tb.fits'))
hdul.writeto(self.temp('tmp.fits'), overwrite=True)
tempResult = arange(100)
	
===================================================================	
TestChecksumFunctions.test_image_create: 32	
----------------------------	

tempResult = arange(100, dtype=numpy.int64)
	
===================================================================	
TestChecksumFunctions.test_datasum_only: 291	
----------------------------	

tempResult = arange(100, dtype='int16')
	
===================================================================	
TestCore.test_nonstandard_hdu: 314	
----------------------------	

'\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/157\n\n        Tests that "Nonstandard" HDUs with SIMPLE = F are read and written\n        without prepending a superfluous and unwanted standard primary HDU.\n        '
tempResult = arange(100, dtype=numpy.uint8)
	
===================================================================	
TestFileFunctions.test_write_stringio_discontiguous: 678	
----------------------------	

'\n        Regression test related to\n        https://github.com/astropy/astropy/issues/2794#issuecomment-55441539\n\n        Demonstrates that writing an HDU containing a discontiguous Numpy array\n        should work properly.\n        '
tempResult = arange(100)
	
===================================================================	
TestFileFunctions.test_write_read_gzip_file: 539	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/2794\n\n        Ensure files written through gzip are readable.\n        '
tempResult = arange(100)
	
===================================================================	
TestFileFunctions.test_filename_with_colon: 694	
----------------------------	

'\n        Test reading and writing a file with a colon in the filename.\n\n        Regression test for https://github.com/astropy/astropy/issues/3122\n        '
filename = 'APEXHET.2014-04-01T15:18:01.000.fits'
tempResult = arange(10)
	
===================================================================	
TestDiff.test_identical_files_basic: 380	
----------------------------	

'Test identicality of two simple, extensionless files.'
tempResult = arange(100)
	
===================================================================	
TestDiff.test_identical_files_basic: 390	
----------------------------	

'Test identicality of two simple, extensionless files.'
a = np.arange(100).reshape((10, 10))
hdu = PrimaryHDU(data=a)
hdu.writeto(self.temp('testa.fits'))
hdu.writeto(self.temp('testb.fits'))
diff = FITSDiff(self.temp('testa.fits'), self.temp('testb.fits'))
assert diff.identical
report = diff.report()
assert ('Primary HDU' not in report)
assert ('Extension HDU' not in report)
assert ('No differences found.' in report)
tempResult = arange(10)
	
===================================================================	
TestDiff.test_partially_identical_files2: 415	
----------------------------	

'\n        Test files that have some identical HDUs but one different HDU.\n        '
tempResult = arange(100)
	
===================================================================	
TestDiff.test_different_dimensions: 198	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestDiff.test_different_dimensions: 199	
----------------------------	

ia = np.arange(100).reshape((10, 10))
tempResult = arange(100)
	
===================================================================	
TestDiff.test_trivial_identical_images: 174	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestDiff.test_trivial_identical_images: 175	
----------------------------	

ia = np.arange(100).reshape((10, 10))
tempResult = arange(100)
	
===================================================================	
TestDiff.test_partially_identical_files1: 399	
----------------------------	

'\n        Test files that have some identical HDUs but a different extension\n        count.\n        '
tempResult = arange(100)
	
===================================================================	
TestDiff.test_identical_comp_image_hdus: 189	
----------------------------	

'Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/189\n\n        For this test we mostly just care that comparing to compressed images\n        does not crash, and returns the correct results.  Two compressed images\n        will be considered identical if the decompressed data is the same.\n        Obviously we test whether or not the same compression was used by\n        looking for (or ignoring) header differences.\n        '
tempResult = arange(100.0)
	
===================================================================	
TestDiff.test_different_pixels: 211	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestDiff.test_different_pixels: 212	
----------------------------	

ia = np.arange(100).reshape((10, 10))
tempResult = arange(100)
	
===================================================================	
TestGroupsFunctions.test_duplicate_parameter: 115	
----------------------------	

'\n        Tests support for multiple parameters of the same name, and ensures\n        that the data in duplicate parameters are returned as a single summed\n        value.\n        '
tempResult = arange(100.0)
	
===================================================================	
TestGroupsFunctions.test_duplicate_parameter: 117	
----------------------------	

'\n        Tests support for multiple parameters of the same name, and ensures\n        that the data in duplicate parameters are returned as a single summed\n        value.\n        '
imdata = numpy.arange(100.0)
imdata.shape = (10, 1, 1, 2, 5)
tempResult = arange(10, dtype=numpy.float32)
	
===================================================================	
TestGroupsFunctions.test_create_groupdata: 89	
----------------------------	

'\n        Basic test for creating GroupData from scratch.\n        '
tempResult = arange(100.0)
	
===================================================================	
TestGroupsFunctions.test_create_groupdata: 91	
----------------------------	

'\n        Basic test for creating GroupData from scratch.\n        '
imdata = numpy.arange(100.0)
imdata.shape = (10, 1, 1, 2, 5)
tempResult = arange(10, dtype=numpy.float32)
	
===================================================================	
TestHDUListFunctions.test_file_like_3: 252	
----------------------------	

tmpfile = open(self.temp('tmpfile.fits'), 'wb')
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_open_file_with_bad_header_padding: 336	
----------------------------	

'\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/136\n\n        Open files with nulls for header block padding instead of spaces.\n        '
tempResult = arange(100)
	
===================================================================	
TestHDUListFunctions.test_append_extension_to_empty_list: 68	
----------------------------	

'Tests appending a Simple ImageHDU to an empty HDUList.'
hdul = io.fits.HDUList()
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_insert_extension_to_empty_list: 136	
----------------------------	

'Tests inserting a Simple ImageHDU to an empty HDUList.'
hdul = io.fits.HDUList()
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_file_like_2: 240	
----------------------------	

tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_update_resized_header2: 396	
----------------------------	

'\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/150\n\n        This is similar to test_update_resized_header, but specifically tests a\n        case of multiple consecutive flush() calls on the same HDUList object,\n        where each flush() requires a resize.\n        '
tempResult = arange(100)
	
===================================================================	
TestHDUListFunctions.test_update_resized_header2: 397	
----------------------------	

'\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/150\n\n        This is similar to test_update_resized_header, but specifically tests a\n        case of multiple consecutive flush() calls on the same HDUList object,\n        where each flush() requires a resize.\n        '
data1 = numpy.arange(100)
tempResult = arange(100)
	
===================================================================	
TestHDUListFunctions.test_update_resized_header: 369	
----------------------------	

'\n        Test saving updates to a file where the header is one block smaller\n        than before, and in the case where the heade ris one block larger than\n        before.\n        '
tempResult = arange(100)
	
===================================================================	
TestHDUListFunctions.test_insert_primary_to_empty_list: 126	
----------------------------	

'Tests inserting a Simple PrimaryHDU to an empty HDUList.'
hdul = io.fits.HDUList()
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_file_like: 230	
----------------------------	

'\n        Tests the use of a file like object with no tell or seek methods\n        in HDUList.writeto(), HDULIST.flush() or astropy.io.fits.writeto()\n        '
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_insert_primary_to_non_empty_list: 166	
----------------------------	

'Tests inserting a Simple PrimaryHDU to a non-empty HDUList.'
hdul = io.fits.open(self.data('arange.fits'))
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_append_primary_to_non_empty_list: 98	
----------------------------	

'Tests appending a Simple PrimaryHDU to a non-empty HDUList.'
hdul = io.fits.open(self.data('arange.fits'))
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_replace_mmap_data: 467	
----------------------------	

"Regression test for\n        https://github.com/spacetelescope/PyFITS/issues/25\n\n        Replacing the mmap'd data of one file with mmap'd data from a\n        different file should work.\n        "
tempResult = arange(10)
	
===================================================================	
TestHDUListFunctions.test_update_with_truncated_header: 355	
----------------------------	

"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/148\n\n        Test that saving an update where the header is shorter than the\n        original header doesn't leave a stump from the old header in the file.\n        "
tempResult = arange(100)
	
===================================================================	
TestHDUListFunctions.test_replace_mmap_data_2: 489	
----------------------------	

"Regression test for\n        https://github.com/spacetelescope/PyFITS/issues/25\n\n        Replacing the mmap'd data of one file with mmap'd data from a\n        different file should work.  Like test_replace_mmap_data but with\n        table data instead of image data.\n        "
tempResult = arange(10)
	
===================================================================	
TestHDUListFunctions.test_append_primary_to_empty_list: 58	
----------------------------	

hdul = io.fits.HDUList()
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_insert_groupshdu_to_non_empty_list: 185	
----------------------------	

'Tests inserting a Simple GroupsHDU to an empty HDUList.'
hdul = io.fits.HDUList()
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_append_groupshdu_to_non_empty_list: 118	
----------------------------	

'Tests appending a Simple GroupsHDU to an empty HDUList.'
hdul = io.fits.HDUList()
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHDUListFunctions.test_insert_image_extension_to_primary_in_non_empty_list: 215	
----------------------------	

'\n        Tests inserting a Simple Image ExtensionHDU to a non-empty HDUList\n        as the primary HDU.\n        '
hdul = io.fits.open(self.data('tb.fits'))
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestHeaderFunctions.test_invalid_end_cards: 1112	
----------------------------	

"\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/217\n\n        This tests the case where the END card looks like a normal card like\n        'END = ' and other similar oddities.  As long as a card starts with END\n        and looks like it was intended to be the END card we allow it, but with\n        a warning.\n        "
tempResult = arange(100)
	
===================================================================	
TestHeaderFunctions.test_end_in_comment: 1288	
----------------------------	

'\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/142\n\n        Tests a case where the comment of a card ends with END, and is followed\n        by several blank cards.\n        '
tempResult = arange(100)
	
===================================================================	
TestImageFunctions.test_section_data_cube: 201	
----------------------------	

tempResult = arange(18)
	
===================================================================	
TestImageFunctions.test_invalid_blanks: 453	
----------------------------	

'\n        Test that invalid use of the BLANK keyword leads to an appropriate\n        warning, and that the BLANK keyword is ignored when returning the\n        HDU data.\n\n        Regression test for https://github.com/astropy/astropy/issues/3865\n        '
tempResult = arange(5, dtype=numpy.float64)
	
===================================================================	
TestImageFunctions.test_set_data: 629	
----------------------------	

'\n        Test data assignment - issue #5087\n        '
im = io.fits.ImageHDU()
tempResult = arange(12)
	
===================================================================	
TestImageFunctions.test_section_data_four: 325	
----------------------------	

tempResult = arange(256)
	
===================================================================	
TestCompressedImage.test_scale_back_compressed_uint_assignment: 1016	
----------------------------	

'\n        Extend fix for #4600 to assignment to data\n\n        Identical to test_scale_back_uint_assignment() but uses a compressed\n        image.\n\n        Suggested by:\n        https://github.com/astropy/astropy/pull/4602#issuecomment-208713748\n        '
tempResult = arange(100, 200, dtype=numpy.uint16)
	
===================================================================	
TestImageFunctions.test_invalid_blank: 602	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/2711\n\n        If the BLANK keyword contains an invalid value it should be ignored for\n        any calculations (though a warning should be issued).\n        '
tempResult = arange(100, dtype=numpy.float64)
	
===================================================================	
TestCompressedImage.test_empty: 660	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/2595\n        '
hdu = io.fits.CompImageHDU()
assert (hdu.data is None)
hdu.writeto(self.temp('test.fits'))
with io.fits.open(self.temp('test.fits'), mode='update') as hdul:
    assert (len(hdul) == 2)
    assert isinstance(hdul[1], io.fits.CompImageHDU)
    assert (hdul[1].data is None)
    tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestCompressedImage.test_empty: 664	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/2595\n        '
hdu = io.fits.CompImageHDU()
assert (hdu.data is None)
hdu.writeto(self.temp('test.fits'))
with io.fits.open(self.temp('test.fits'), mode='update') as hdul:
    assert (len(hdul) == 2)
    assert isinstance(hdul[1], io.fits.CompImageHDU)
    assert (hdul[1].data is None)
    hdul[1].data = numpy.arange(100, dtype=numpy.int32)
with io.fits.open(self.temp('test.fits')) as hdul:
    assert (len(hdul) == 2)
    assert isinstance(hdul[1], io.fits.CompImageHDU)
    tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestImageFunctions.test_scaled_image_fromfile: 617	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/2710\n        '
tempResult = arange(100, dtype=numpy.float32)
	
===================================================================	
TestImageFunctions.test_uint_header_consistency: 399	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/2305\n\n        This ensures that an HDU containing unsigned integer data always has\n        the apppriate BZERO value in its header.\n        '
for int_size in (16, 32, 64):
    max_uint = ((2 ** int_size) - 1)
    if (int_size == 64):
        max_uint = numpy.uint64(int_size)
    dtype = 'uint{}'.format(int_size)
    arr = numpy.empty(100, dtype=dtype)
    arr.fill(max_uint)
    tempResult = arange(100, dtype=dtype)
	
===================================================================	
TestImageFunctions.test_scale_back_uint_assignment: 643	
----------------------------	

'\n        Extend fix for #4600 to assignment to data\n\n        Suggested by:\n        https://github.com/astropy/astropy/pull/4602#issuecomment-208713748\n        '
tempResult = arange(100, 200, dtype=numpy.uint16)
	
===================================================================	
TestImageFunctions.test_section_data_square: 180	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestImageFunctions.test_scale_back_with_blanks: 468	
----------------------------	

'\n        Test that when auto-rescaling integer data with "blank" values (where\n        the blanks are replaced by NaN in the float data), that the "BLANK"\n        keyword is removed from the header.\n\n        Further, test that when using the ``scale_back=True`` option the blank\n        values are restored properly.\n\n        Regression test for https://github.com/astropy/astropy/issues/3865\n        '
tempResult = arange(5, dtype=numpy.int32)
	
===================================================================	
TestCompressedImage.test_compression_column_tforms: 835	
----------------------------	

'Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/199'
tempResult = arange(1, 8, dtype=numpy.float32)
	
===================================================================	
TestCompressedImage.test_compression_column_tforms: 835	
----------------------------	

'Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/199'
tempResult = arange(1, 7)
	
===================================================================	
TestImageFunctions.test_scale_bzero_with_int_data: 634	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/4600\n        '
tempResult = arange(100, 200, dtype=numpy.int16)
	
===================================================================	
TestCompressedImage.test_comp_image_hcompress_image_stack: 690	
----------------------------	

'\n        Regression test for https://aeon.stsci.edu/ssb/trac/pyfits/ticket/171\n\n        Tests that data containing more than two dimensions can be\n        compressed with HCOMPRESS_1 so long as the user-supplied tile size can\n        be flattened to two dimensions.\n        '
tempResult = arange(300, dtype=numpy.float32)
	
===================================================================	
TestCompressedImage.test_subtractive_dither_seed: 698	
----------------------------	

'\n        Regression test for https://github.com/spacetelescope/PyFITS/issues/32\n\n        Ensure that when floating point data is compressed with the\n        SUBTRACTIVE_DITHER_1 quantization method that the correct ZDITHER0 seed\n        is added to the header, and that the data can be correctly\n        decompressed.\n        '
tempResult = arange(100.0)
	
===================================================================	
TestCompressedImage.test_duplicate_compression_header_keywords: 996	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/2750\n\n        Tests that the fake header (for the compressed image) can still be read\n        even if the real header contained a duplicate ZTENSION keyword (the\n        issue applies to any keyword specific to the compression convention,\n        however).\n        '
tempResult = arange(100, dtype=numpy.int32)
	
===================================================================	
TestCompressedImage.test_scale_bzero_with_compressed_int_data: 1007	
----------------------------	

'\n        Regression test for https://github.com/astropy/astropy/issues/4600\n        and https://github.com/astropy/astropy/issues/4588\n\n        Identical to test_scale_bzero_with_int_data() but uses a compressed\n        image.\n        '
tempResult = arange(100, 200, dtype=numpy.int16)
	
===================================================================	
TestNonstandardHdus.test_create_fitshdu_from_filename: 18	
----------------------------	

'Regression test on `FitsHDU.fromfile`'
tempResult = arange(100)
	
===================================================================	
TestVLATables.test_copy_vla: 1756	
----------------------------	

'\n        Regression test for https://github.com/spacetelescope/PyFITS/issues/47\n        '
tempResult = arange((n + 1))
	
===================================================================	
TestVLATables.test_copy_vla: 1757	
----------------------------	

'\n        Regression test for https://github.com/spacetelescope/PyFITS/issues/47\n        '
arr1 = [numpy.arange((n + 1)) for n in range(255)]
tempResult = arange(255, (256 + n))
	
===================================================================	
TestVLATables.test_copy_vla: 1758	
----------------------------	

'\n        Regression test for https://github.com/spacetelescope/PyFITS/issues/47\n        '
arr1 = [numpy.arange((n + 1)) for n in range(255)]
arr2 = [numpy.arange(255, (256 + n)) for n in range(255)]
tempResult = arange(255)
	
===================================================================	
TestVLATables.test111111: 1742	
----------------------------	

tempResult = arange(1572)
	
===================================================================	
TestUintFunctions.test_uint_columns: 50	
----------------------------	

'Test basic functionality of tables with columns containing\n        pseudo-unsigned integers.  See\n        https://github.com/astropy/astropy/pull/906\n        '
bits = (8 * int(utype[1]))
if ((platform.architecture()[0] == '64bit') or (bits != 64)):
    bzero = self.utype_map[utype]((2 ** (bits - 1)))
    one = self.utype_map[utype](1)
    tempResult = arange((bits + 1), dtype=self.utype_map[utype])
	
===================================================================	
OrthoPolynomialBase.invlex_coeff: 129	
----------------------------	

coeff = []
tempResult = arange((self.x_degree + 1))
	
===================================================================	
OrthoPolynomialBase.invlex_coeff: 130	
----------------------------	

coeff = []
xvar = numpy.arange((self.x_degree + 1))
tempResult = arange((self.y_degree + 1))
	
===================================================================	
OrthoPolynomialBase._invlex: 120	
----------------------------	

c = []
tempResult = arange((self.x_degree + 1))
	
===================================================================	
OrthoPolynomialBase._invlex: 121	
----------------------------	

c = []
xvar = numpy.arange((self.x_degree + 1))
tempResult = arange((self.y_degree + 1))
	
===================================================================	
Polynomial2D.fit_deriv: 483	
----------------------------	

'\n        Computes the Vandermonde matrix.\n\n        Parameters\n        ----------\n        x : ndarray\n            input\n        y : ndarray\n            input\n        params : throw away parameter\n            parameter list returned by non-linear fitters\n\n        Returns\n        -------\n        result : ndarray\n            The Vandermonde matrix\n        '
if (x.ndim == 2):
    x = x.flatten()
if (y.ndim == 2):
    y = y.flatten()
if (x.size != y.size):
    raise ValueError('Expected x and y to be of equal size')
tempResult = arange((self.degree + 1))
	
===================================================================	
Polynomial2D.fit_deriv: 484	
----------------------------	

'\n        Computes the Vandermonde matrix.\n\n        Parameters\n        ----------\n        x : ndarray\n            input\n        y : ndarray\n            input\n        params : throw away parameter\n            parameter list returned by non-linear fitters\n\n        Returns\n        -------\n        result : ndarray\n            The Vandermonde matrix\n        '
if (x.ndim == 2):
    x = x.flatten()
if (y.ndim == 2):
    y = y.flatten()
if (x.size != y.size):
    raise ValueError('Expected x and y to be of equal size')
designx = (x[:, None] ** numpy.arange((self.degree + 1)))
tempResult = arange(1, (self.degree + 1))
	
===================================================================	
_Tabular.__init__: 40	
----------------------------	

n_models = kwargs.get('n_models', 1)
if (n_models > 1):
    raise NotImplementedError('Only n_models=1 is supported.')
super(_Tabular, self).__init__(**kwargs)
if (lookup_table is not None):
    lookup_table = numpy.asarray(lookup_table)
    if (self.lookup_table.ndim != lookup_table.ndim):
        raise ValueError('lookup_table should be an array with {0} dimensions'.format(self.lookup_table.ndim))
    self.lookup_table = lookup_table
if (points is None):
    tempResult = arange(x, dtype=numpy.float)
	
===================================================================	
TestLinearConstraints.setup_class: 180	
----------------------------	

self.p1 = models.Polynomial1D(4)
self.p1.c0 = 0
self.p1.c1 = 0
self.p1.window = [0.0, 9.0]
tempResult = arange(10)
	
===================================================================	
TestBounds.test_bounds_gauss2d_slsqp: 154	
----------------------------	

tempResult = arange(11)
	
===================================================================	
TestBounds.test_bounds_gauss2d_slsqp: 154	
----------------------------	

tempResult = arange(11)
	
===================================================================	
TestNonLinearConstraints.test_joint_fitter: 55	
----------------------------	

g1 = models.Gaussian1D(10, 14.9, stddev=0.3)
g2 = models.Gaussian1D(10, 13, stddev=0.4)
jf = fitting.JointFitter([g1, g2], {g1: ['amplitude'], g2: ['amplitude']}, [9.8])
tempResult = arange(10, 20, 0.1)
	
===================================================================	
test_fit_with_bound_constraints_estimate_jacobian: 309	
----------------------------	

'\n    Regression test for https://github.com/astropy/astropy/issues/2400\n\n    Checks that bounds constraints are obeyed on a custom model that does not\n    define fit_deriv (and thus its Jacobian must be estimated for non-linear\n    fitting).\n    '

class MyModel(Fittable1DModel):
    a = Parameter(default=1)
    b = Parameter(default=2)

    @staticmethod
    def evaluate(x, a, b):
        return ((a * x) + b)
m_real = MyModel(a=1.5, b=(- 3))
tempResult = arange(100)
	
===================================================================	
TestNonLinearConstraints.setup_class: 24	
----------------------------	

self.g1 = models.Gaussian1D(10, 14.9, stddev=0.3)
self.g2 = models.Gaussian1D(10, 13, stddev=0.4)
tempResult = arange(10, 20, 0.1)
	
===================================================================	
TestBounds.test_bounds_gauss2d_lsq: 135	
----------------------------	

tempResult = arange(11)
	
===================================================================	
TestBounds.test_bounds_gauss2d_lsq: 135	
----------------------------	

tempResult = arange(11)
	
===================================================================	
test_custom_inverse: 138	
----------------------------	

'Test setting a custom inverse on a model.'
p = models.Polynomial1D(1, c0=(- 2), c1=3)
inv = models.Polynomial1D(1, c0=(2.0 / 3.0), c1=(1.0 / 3.0))
with tests.helper.pytest.raises(NotImplementedError):
    p.inverse
p.inverse = inv
tempResult = arange(100)
	
===================================================================	
test_render_model_1d: 201	
----------------------------	

npix = 101
image = numpy.zeros(npix)
tempResult = arange(npix)
	
===================================================================	
test_render_model_1d: 204	
----------------------------	

npix = 101
image = numpy.zeros(npix)
coords = numpy.arange(npix)
model = models.Gaussian1D()
test_pts = [0, 49.1, 49.5, 49.9, 100]
tempResult = arange(5.5, 6.7, 0.2)
	
===================================================================	
TestJointFitter.setup_class: 123	
----------------------------	

'\n        Create 2 gaussian models and some data with noise.\n        Create a fitter for the two models keeping the amplitude parameter\n        common for the two models.\n        '
self.g1 = models.Gaussian1D(10, mean=14.9, stddev=0.3)
self.g2 = models.Gaussian1D(10, mean=13, stddev=0.4)
self.jf = JointFitter([self.g1, self.g2], {self.g1: ['amplitude'], self.g2: ['amplitude']}, [9.8])
tempResult = arange(10, 20, 0.1)
	
===================================================================	
TestICheb2D.test_default_params: 82	
----------------------------	

tempResult = arange(9)
	
===================================================================	
TestICheb2D.test_chebyshev2D_nonlinear_fitting: 96	
----------------------------	

cheb2d = models.Chebyshev2D(2, 2)
tempResult = arange(9)
	
===================================================================	
TestICheb2D.test_chebyshev2D_nonlinear_fitting_with_weights: 106	
----------------------------	

cheb2d = models.Chebyshev2D(2, 2)
tempResult = arange(9)
	
===================================================================	
TestLinearLSQFitter.test_linear_fit_model_set: 171	
----------------------------	

'Tests fitting multiple models simultaneously.'
init_model = models.Polynomial1D(degree=2, c0=[1, 1], n_models=2)
tempResult = arange(10)
	
===================================================================	
TestLinearLSQFitter.test_linear_fit_2d_model_set: 183	
----------------------------	

'Tests fitted multiple 2-D models simultaneously.'
init_model = models.Polynomial2D(degree=2, c0_0=[1, 1], n_models=2)
tempResult = arange(10)
	
===================================================================	
TestLinearLSQFitter.test_linear_fit_2d_model_set: 184	
----------------------------	

'Tests fitted multiple 2-D models simultaneously.'
init_model = models.Polynomial2D(degree=2, c0_0=[1, 1], n_models=2)
x = numpy.arange(10)
tempResult = arange(10)
	
===================================================================	
TestNonLinearFitters.setup_class: 199	
----------------------------	

self.initial_values = [100, 5, 1]
tempResult = arange(0, 10, 0.1)
	
===================================================================	
test_compound_models_with_class_variables: 154	
----------------------------	

models_2d = [models.AiryDisk2D, models.Sersic2D]
models_1d = [models.Sersic1D]
for model_2d in models_2d:

    class CompoundModel2D((models.Const2D + model_2d)):
        pass
    (x, y) = numpy.mgrid[:10, :10]
    f = CompoundModel2D()(x, y)
    assert (f.shape == (10, 10))
for model_1d in models_1d:

    class CompoundModel1D((models.Const1D + model_1d)):
        pass
    tempResult = arange(10)
	
===================================================================	
test_GaussianAbsorption1D: 26	
----------------------------	

g_em = models.Gaussian1D(0.8, 3000, 20)
g_ab = models.GaussianAbsorption1D(0.8, 3000, 20)
tempResult = arange(2900, 3100, 2)
	
===================================================================	
TestSingleInputDoubleOutputSingleModel.test_scalar_parameters_1d_array_input: 429	
----------------------------	

'\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
t = TModel_1_2(1, 10, 1000)
tempResult = arange(5)
	
===================================================================	
TestSingleInputSingleOutputSingleModel.test_scalar_parameters_3d_array_input: 269	
----------------------------	

'\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
t = TModel_1_1(1, 10)
tempResult = arange(12)
	
===================================================================	
TestSingleInputDoubleOutputSingleModel.test_scalar_parameters_3d_array_input: 449	
----------------------------	

'\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
t = TModel_1_2(1, 10, 1000)
tempResult = arange(12)
	
===================================================================	
TestSingleInputSingleOutputSingleModel.test_scalar_parameters_2d_array_input: 261	
----------------------------	

'\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
t = TModel_1_1(1, 10)
tempResult = arange(6)
	
===================================================================	
TestSingleInputSingleOutputTwoModel.test_scalar_parameters_2d_array_input: 344	
----------------------------	

'\n        The dimension of the input should match the number of models unless\n        model_set_axis=False is given, in which case the input is copied across\n        all models.\n        '
t = TModel_1_1([1, 2], [10, 20], n_models=2)
tempResult = arange(6)
	
===================================================================	
TestSingleInputSingleOutputTwoModel.test_scalar_parameters_2d_array_input: 347	
----------------------------	

'\n        The dimension of the input should match the number of models unless\n        model_set_axis=False is given, in which case the input is copied across\n        all models.\n        '
t = TModel_1_1([1, 2], [10, 20], n_models=2)
y1 = t((np.arange(6).reshape(2, 3) * 100))
assert (numpy.shape(y1) == (2, 3))
assert numpy.all((y1 == [[11, 111, 211], [322, 422, 522]]))
tempResult = arange(6)
	
===================================================================	
TestFitting.setup_class: 48	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSingleInputDoubleOutputSingleModel.test_scalar_parameters_2d_array_input: 439	
----------------------------	

'\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
t = TModel_1_2(1, 10, 1000)
tempResult = arange(6)
	
===================================================================	
TestSingleInputSingleOutputTwoModel.test_scalar_parameters_3d_array_input: 354	
----------------------------	

'\n        The dimension of the input should match the number of models unless\n        model_set_axis=False is given, in which case the input is copied across\n        all models.\n        '
t = TModel_1_1([1, 2], [10, 20], n_models=2)
tempResult = arange(12)
	
===================================================================	
TestEvaluation.setup_class: 167	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestInputType.setup_class: 26	
----------------------------	

self.x = 5.3
self.y = 6.7
tempResult = arange(1, 10, 0.1)
	
===================================================================	
TestInputType.setup_class: 27	
----------------------------	

self.x = 5.3
self.y = 6.7
self.x1 = numpy.arange(1, 10, 0.1)
tempResult = arange(1, 10, 0.1)
	
===================================================================	
TestSingleInputSingleOutputTwoModel.test_scalar_parameters_1d_array_input: 330	
----------------------------	

'\n        The dimension of the input should match the number of models unless\n        model_set_axis=False is given, in which case the input is copied across\n        all models.\n        '
t = TModel_1_1([1, 2], [10, 20], n_models=2)
with tests.helper.pytest.raises(ValueError):
    tempResult = arange(5)
	
===================================================================	
TestSingleInputSingleOutputSingleModel.test_scalar_parameters_1d_array_input: 253	
----------------------------	

'\n        Scalar parameters should broadcast with an array input to result in an\n        array output of the same shape as the input.\n        '
t = TModel_1_1(1, 10)
tempResult = arange(5)
	
===================================================================	
test_tabular_interp_1d: 363	
----------------------------	

'\n    Test Tabular1D model.\n    '
tempResult = arange(0, 5)
	
===================================================================	
test_tabular_nd: 396	
----------------------------	

tempResult = arange(24)
	
===================================================================	
Fittable1DModelTester.setup_class: 218	
----------------------------	

self.N = 100
self.M = 100
self.eval_error = 0.0001
self.fit_error = 0.1
self.x = 5.3
self.y = 6.7
tempResult = arange(1, 10, 0.1)
	
===================================================================	
Fittable1DModelTester.setup_class: 219	
----------------------------	

self.N = 100
self.M = 100
self.eval_error = 0.0001
self.fit_error = 0.1
self.x = 5.3
self.y = 6.7
self.x1 = numpy.arange(1, 10, 0.1)
tempResult = arange(1, 10, 0.1)
	
===================================================================	
test_voigt_model: 352	
----------------------------	

'\n    Currently just tests that the model peaks at its origin.\n    Regression test for https://github.com/astropy/astropy/issues/3942\n    '
m = models.Voigt1D(x_0=5, amplitude_L=10, fwhm_L=0.5, fwhm_G=0.9)
tempResult = arange(0, 10, 0.01)
	
===================================================================	
Fittable2DModelTester.setup_class: 103	
----------------------------	

self.N = 100
self.M = 100
self.eval_error = 0.0001
self.fit_error = 0.1
self.x = 5.3
self.y = 6.7
tempResult = arange(1, 10, 0.1)
	
===================================================================	
Fittable2DModelTester.setup_class: 104	
----------------------------	

self.N = 100
self.M = 100
self.eval_error = 0.0001
self.fit_error = 0.1
self.x = 5.3
self.y = 6.7
self.x1 = numpy.arange(1, 10, 0.1)
tempResult = arange(1, 10, 0.1)
	
===================================================================	
test_tabular_interp_2d: 379	
----------------------------	

table = numpy.array([[(- 0.04614432), (- 0.02512547), (- 0.00619557), 0.0144165, 0.0297525], [(- 0.04510594), (- 0.03183369), (- 0.01118008), 0.01201388, 0.02496205], [(- 0.05464094), (- 0.02804499), (- 0.00960086), 0.01134333, 0.02284104], [(- 0.04879338), (- 0.02539565), (- 0.00440462), 0.01795145, 0.02122417], [(- 0.03637372), (- 0.01630025), (- 0.00157902), 0.01649774, 0.01952131]])
tempResult = arange(0, 5)
	
===================================================================	
test_tabular_interp_2d: 379	
----------------------------	

table = numpy.array([[(- 0.04614432), (- 0.02512547), (- 0.00619557), 0.0144165, 0.0297525], [(- 0.04510594), (- 0.03183369), (- 0.01118008), 0.01201388, 0.02496205], [(- 0.05464094), (- 0.02804499), (- 0.00960086), 0.01134333, 0.02284104], [(- 0.04879338), (- 0.02539565), (- 0.00440462), 0.01795145, 0.02122417], [(- 0.03637372), (- 0.01630025), (- 0.00157902), 0.01649774, 0.01952131]])
tempResult = arange(0, 5)
	
===================================================================	
test_tabular_interp_2d: 386	
----------------------------	

table = numpy.array([[(- 0.04614432), (- 0.02512547), (- 0.00619557), 0.0144165, 0.0297525], [(- 0.04510594), (- 0.03183369), (- 0.01118008), 0.01201388, 0.02496205], [(- 0.05464094), (- 0.02804499), (- 0.00960086), 0.01134333, 0.02284104], [(- 0.04879338), (- 0.02539565), (- 0.00440462), 0.01795145, 0.02122417], [(- 0.03637372), (- 0.01630025), (- 0.00157902), 0.01649774, 0.01952131]])
points = (numpy.arange(0, 5), numpy.arange(0, 5))
xnew = numpy.array([0.0, 0.7, 1.4, 2.1, 3.9])
LookupTable = models.tabular_model(2)
model = LookupTable(points, table)
znew = model(xnew, xnew)
result = numpy.array([(- 0.04614432), (- 0.03450009), (- 0.02241028), (- 0.0069727), 0.01938675])
numpy.testing.utils.assert_allclose(znew, result, atol=(10 ** (- 7)))
tempResult = arange(12)
	
===================================================================	
TestMultipleParameterSets.setup_class: 227	
----------------------------	

tempResult = arange(1, 10, 0.1)
	
===================================================================	
test_c_projection_striding: 158	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_arithmetics_data: 19	
----------------------------	

	
===================================================================	
test_arithmetics_data: 19	
----------------------------	

	
===================================================================	
test_arithmetics_data: 19	
----------------------------	

	
===================================================================	
test_arithmetics_data: 19	
----------------------------	

	
===================================================================	
test_arithmetics_data: 19	
----------------------------	

	
===================================================================	
test_arithmetics_data: 19	
----------------------------	

	
===================================================================	
test_arithmetics_data_unit_identical: 48	
----------------------------	

	
===================================================================	
test_arithmetics_data_unit_identical: 48	
----------------------------	

	
===================================================================	
test_arithmetics_data_unit_identical: 48	
----------------------------	

	
===================================================================	
test_arithmetics_data_unit_identical: 48	
----------------------------	

	
===================================================================	
test_arithmetics_data_unit_identical: 48	
----------------------------	

	
===================================================================	
test_arithmetics_data_unit_identical: 48	
----------------------------	

	
===================================================================	
test_slicing_all_npndarray_1d: 58	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_slicing_all_something_wrong: 103	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_slicing_all_npndarray_nd: 74	
----------------------------	

tempResult = arange(1000)
	
===================================================================	
test_slicing_all_npndarray_shape_diff: 91	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_boolean_slicing: 115	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_slicing_only_data: 32	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_init_fake_with_fake: 66	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_init_fake_with_fake: 75	
----------------------------	

uncert = np.arange(5).reshape(5, 1)
fake_uncert1 = UncertClass(uncert)
fake_uncert2 = UncertClass(fake_uncert1)
assert_array_equal(fake_uncert2.array, uncert)
assert (fake_uncert2.array is not uncert)
fake_uncert1 = UncertClass(uncert, copy=False)
fake_uncert2 = UncertClass(fake_uncert1, copy=False)
assert_array_equal(fake_uncert2.array, fake_uncert1.array)
assert (fake_uncert2.array is fake_uncert1.array)
tempResult = arange(5)
	
===================================================================	
test_init_fake_with_StdDevUncertainty: 99	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_init_fake_with_ndarray: 38	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_init_fake_with_quantity: 51	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestBlockReplicate.test_block_size_broadcasting: 234	
----------------------------	

'Test scalar block_size broadcasting.'
tempResult = arange(4)
	
===================================================================	
TestBlockReplicate.test_1d_conserve_sum: 211	
----------------------------	

'Test 1D array with conserve_sum=False.'
tempResult = arange(2)
	
===================================================================	
test_extract_array_1d: 85	
----------------------------	

'In 1d, shape can be int instead of tuple'
tempResult = arange(4)
	
===================================================================	
test_extract_array_1d: 86	
----------------------------	

'In 1d, shape can be int instead of tuple'
assert numpy.all((extract_array(numpy.arange(4), 3, ((- 1),), fill_value=(- 99)) == numpy.array([(- 99), (- 99), 0])))
tempResult = arange(4)
	
===================================================================	
TestBlockReduce.test_2d_trim: 180	
----------------------------	

'\n        Test trimming of 2D array when size is not perfectly divisible\n        by block_size.\n        '
tempResult = arange(15)
	
===================================================================	
TestBlockReduce.test_block_size_broadcasting: 188	
----------------------------	

'Test scalar block_size broadcasting.'
tempResult = arange(16)
	
===================================================================	
TestBlockReduce.test_2d_mean: 172	
----------------------------	

'Test 2D array with func=np.mean.'
tempResult = arange(4)
	
===================================================================	
test_extract_Array_float: 90	
----------------------------	

'integer is at bin center'
tempResult = arange(2.51, 3.49, 0.1)
	
===================================================================	
test_extract_Array_float: 91	
----------------------------	

'integer is at bin center'
for a in numpy.arange(2.51, 3.49, 0.1):
    tempResult = arange(5)
	
===================================================================	
test_extract_array_1d_trim: 95	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_trim: 97	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
assert numpy.all((extract_array(numpy.arange(4), (2,), (0,), mode='trim') == numpy.array([0])))
for i in [1, 2, 3]:
    tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_trim: 98	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
assert numpy.all((extract_array(numpy.arange(4), (2,), (0,), mode='trim') == numpy.array([0])))
for i in [1, 2, 3]:
    assert numpy.all((extract_array(numpy.arange(4), (2,), (i,), mode='trim') == numpy.array([(i - 1), i])))
tempResult = arange(4.0)
	
===================================================================	
test_extract_array_return_pos: 111	
----------------------------	

"Check that the return position is calculated correctly.\n\n    The result will differ by mode. All test here are done in 1d because it's\n    easier to construct correct test cases.\n    "
tempResult = arange(5)
	
===================================================================	
test_extract_array_return_pos: 112	
----------------------------	

"Check that the return position is calculated correctly.\n\n    The result will differ by mode. All test here are done in 1d because it's\n    easier to construct correct test cases.\n    "
large_test_array = numpy.arange(5)
tempResult = arange((- 1), 6)
	
===================================================================	
test_extract_array_return_pos: 118	
----------------------------	

"Check that the return position is calculated correctly.\n\n    The result will differ by mode. All test here are done in 1d because it's\n    easier to construct correct test cases.\n    "
large_test_array = numpy.arange(5)
for i in numpy.arange((- 1), 6):
    (extracted, new_pos) = extract_array(large_test_array, 3, i, mode='partial', return_position=True)
    assert (new_pos == (1,))
for (i, expected) in zip([1.49, 1.51, 3], [1.49, 0.51, 1]):
    (extracted, new_pos) = extract_array(large_test_array, (2,), (i,), mode='strict', return_position=True)
    assert (new_pos == (expected,))
tempResult = arange((- 1), 6)
	
===================================================================	
TestBlockReplicate.test_2d_conserve_sum: 226	
----------------------------	

'Test 2D array with conserve_sum=False.'
tempResult = arange(6)
	
===================================================================	
test_extract_array_1d_even: 65	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_even: 67	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
assert numpy.all((extract_array(numpy.arange(4), (2,), (0,), fill_value=(- 99)) == numpy.array([(- 99), 0])))
for i in [1, 2, 3]:
    tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_even: 68	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    '
assert numpy.all((extract_array(numpy.arange(4), (2,), (0,), fill_value=(- 99)) == numpy.array([(- 99), 0])))
for i in [1, 2, 3]:
    assert numpy.all((extract_array(numpy.arange(4), (2,), (i,)) == numpy.array([(i - 1), i])))
tempResult = arange(4.0)
	
===================================================================	
test_extract_array_wrong_mode: 60	
----------------------------	

'Call extract_array with non-existing mode.'
with tests.helper.pytest.raises(ValueError) as e:
    tempResult = arange(4)
	
===================================================================	
TestBlockReplicate.test_block_size_len: 241	
----------------------------	

'Test block_size length.'
tempResult = arange(5)
	
===================================================================	
test_extract_array_1d_odd: 72	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    The first few lines test the most error-prone part: Extraction of an\n    array on the boundaries.\n    Additional tests (e.g. dtype of return array) are done for the last\n    case only.\n    '
tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_odd: 73	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    The first few lines test the most error-prone part: Extraction of an\n    array on the boundaries.\n    Additional tests (e.g. dtype of return array) are done for the last\n    case only.\n    '
assert numpy.all((extract_array(numpy.arange(4), (3,), ((- 1),), fill_value=(- 99)) == numpy.array([(- 99), (- 99), 0])))
tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_odd: 75	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    The first few lines test the most error-prone part: Extraction of an\n    array on the boundaries.\n    Additional tests (e.g. dtype of return array) are done for the last\n    case only.\n    '
assert numpy.all((extract_array(numpy.arange(4), (3,), ((- 1),), fill_value=(- 99)) == numpy.array([(- 99), (- 99), 0])))
assert numpy.all((extract_array(numpy.arange(4), (3,), (0,), fill_value=(- 99)) == numpy.array([(- 99), 0, 1])))
for i in [1, 2]:
    tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_odd: 76	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    The first few lines test the most error-prone part: Extraction of an\n    array on the boundaries.\n    Additional tests (e.g. dtype of return array) are done for the last\n    case only.\n    '
assert numpy.all((extract_array(numpy.arange(4), (3,), ((- 1),), fill_value=(- 99)) == numpy.array([(- 99), (- 99), 0])))
assert numpy.all((extract_array(numpy.arange(4), (3,), (0,), fill_value=(- 99)) == numpy.array([(- 99), 0, 1])))
for i in [1, 2]:
    assert numpy.all((extract_array(numpy.arange(4), (3,), (i,)) == numpy.array([(i - 1), i, (i + 1)])))
tempResult = arange(4)
	
===================================================================	
test_extract_array_1d_odd: 77	
----------------------------	

'Extract 1 d arrays.\n\n    All dimensions are treated the same, so we can test in 1 dim.\n    The first few lines test the most error-prone part: Extraction of an\n    array on the boundaries.\n    Additional tests (e.g. dtype of return array) are done for the last\n    case only.\n    '
assert numpy.all((extract_array(numpy.arange(4), (3,), ((- 1),), fill_value=(- 99)) == numpy.array([(- 99), (- 99), 0])))
assert numpy.all((extract_array(numpy.arange(4), (3,), (0,), fill_value=(- 99)) == numpy.array([(- 99), 0, 1])))
for i in [1, 2]:
    assert numpy.all((extract_array(numpy.arange(4), (3,), (i,)) == numpy.array([(i - 1), i, (i + 1)])))
assert numpy.all((extract_array(numpy.arange(4), (3,), (3,), fill_value=(- 99)) == numpy.array([2, 3, (- 99)])))
tempResult = arange(4.0)
	
===================================================================	
TestBlockReduce.test_2d: 165	
----------------------------	

'Test 2D array.'
tempResult = arange(4)
	
===================================================================	
TestBlockReduce.test_1d: 150	
----------------------------	

'Test 1D array.'
tempResult = arange(4)
	
===================================================================	
TestCutout2D.setup_class: 248	
----------------------------	

tempResult = arange(20.0)
	
===================================================================	
TestBlockReplicate.test_1d: 204	
----------------------------	

'Test 1D array.'
tempResult = arange(2)
	
===================================================================	
TestBlockReplicate.test_2d: 219	
----------------------------	

'Test 2D array.'
tempResult = arange(2)
	
===================================================================	
TestBlockReduce.test_1d_mean: 157	
----------------------------	

'Test 1D array with func=np.mean.'
tempResult = arange(4)
	
===================================================================	
eqn8: 228	
----------------------------	

tempResult = arange((N + 1), dtype=numpy.float64)
	
===================================================================	
freedman_bin_width: 58	
----------------------------	

'Return the optimal histogram bin width using the Freedman-Diaconis rule\n\n    The Freedman-Diaconis rule is a normal reference rule like Scott\'s\n    rule, but uses rank-based statistics for results which are more robust\n    to deviations from a normal distribution.\n\n    Parameters\n    ----------\n    data : array-like, ndim=1\n        observed (one-dimensional) data\n    return_bins : bool (optional)\n        if True, then return the bin edges\n\n    Returns\n    -------\n    width : float\n        optimal bin width using the Freedman-Diaconis rule\n    bins : ndarray\n        bin edges: returned if ``return_bins`` is True\n\n    Notes\n    -----\n    The optimal bin width is\n\n    .. math::\n        \\Delta_b = \\frac{2(q_{75} - q_{25})}{n^{1/3}}\n\n    where :math:`q_{N}` is the :math:`N` percent quartile of the data, and\n    :math:`n` is the number of data points [1]_.\n\n    References\n    ----------\n    .. [1] D. Freedman & P. Diaconis (1981)\n       "On the histogram as a density estimator: L2 theory".\n       Probability Theory and Related Fields 57 (4): 453-476\n\n    See Also\n    --------\n    knuth_bin_width\n    scott_bin_width\n    bayesian_blocks\n    histogram\n    '
data = numpy.asarray(data)
if (data.ndim != 1):
    raise ValueError('data should be one-dimensional')
n = data.size
if (n < 4):
    raise ValueError('data should have more than three entries')
(v25, v75) = numpy.percentile(data, [25, 75])
dx = ((2 * (v75 - v25)) / (n ** (1 / 3)))
if return_bins:
    (dmin, dmax) = (data.min(), data.max())
    Nbins = max(1, numpy.ceil(((dmax - dmin) / dx)))
    tempResult = arange((Nbins + 1))
	
===================================================================	
scott_bin_width: 40	
----------------------------	

'Return the optimal histogram bin width using Scott\'s rule\n\n    Scott\'s rule is a normal reference rule: it minimizes the integrated\n    mean squared error in the bin approximation under the assumption that the\n    data is approximately Gaussian.\n\n    Parameters\n    ----------\n    data : array-like, ndim=1\n        observed (one-dimensional) data\n    return_bins : bool (optional)\n        if True, then return the bin edges\n\n    Returns\n    -------\n    width : float\n        optimal bin width using Scott\'s rule\n    bins : ndarray\n        bin edges: returned if ``return_bins`` is True\n\n    Notes\n    -----\n    The optimal bin width is\n\n    .. math::\n        \\Delta_b = \\frac{3.5\\sigma}{n^{1/3}}\n\n    where :math:`\\sigma` is the standard deviation of the data, and\n    :math:`n` is the number of data points [1]_.\n\n    References\n    ----------\n    .. [1] Scott, David W. (1979). "On optimal and data-based histograms".\n       Biometricka 66 (3): 605-610\n\n    See Also\n    --------\n    knuth_bin_width\n    freedman_bin_width\n    bayesian_blocks\n    histogram\n    '
data = numpy.asarray(data)
if (data.ndim != 1):
    raise ValueError('data should be one-dimensional')
n = data.size
sigma = numpy.std(data)
dx = ((3.5 * sigma) / (n ** (1 / 3)))
if return_bins:
    Nbins = numpy.ceil(((data.max() - data.min()) / dx))
    Nbins = max(1, Nbins)
    tempResult = arange((Nbins + 1))
	
===================================================================	
LombScargle.autofrequency: 95	
----------------------------	

'Determine a suitable frequency grid for data.\n\n        Note that this assumes the peak width is driven by the observational\n        baseline, which is generally a good assumption when the baseline is\n        much larger than the oscillation period.\n        If you are searching for periods longer than the baseline of your\n        observations, this may not perform well.\n\n        Even with a large baseline, be aware that the maximum frequency\n        returned is based on the concept of "average Nyquist frequency", which\n        may not be useful for irregularly-sampled data. The maximum frequency\n        can be adjusted via the nyquist_factor argument, or through the\n        maximum_frequency argument.\n\n        Parameters\n        ----------\n        samples_per_peak : float (optional, default=5)\n            The approximate number of desired samples across the typical peak\n        nyquist_factor : float (optional, default=5)\n            The multiple of the average nyquist frequency used to choose the\n            maximum frequency if maximum_frequency is not provided.\n        minimum_frequency : float (optional)\n            If specified, then use this minimum frequency rather than one\n            chosen based on the size of the baseline.\n        maximum_frequency : float (optional)\n            If specified, then use this maximum frequency rather than one\n            chosen based on the average nyquist frequency.\n\n        Returns\n        -------\n        frequency : ndarray or Quantity\n            The heuristically-determined optimal frequency bin\n        '
baseline = (self.t.max() - self.t.min())
n_samples = self.t.size
df = ((1.0 / baseline) / samples_per_peak)
if (minimum_frequency is not None):
    f0 = minimum_frequency
else:
    f0 = (0.5 * df)
if (maximum_frequency is not None):
    Nf = int(numpy.ceil(((maximum_frequency - f0) / df)))
else:
    Nf = int((((0.5 * samples_per_peak) * nyquist_factor) * n_samples))
tempResult = arange(Nf)
	
===================================================================	
extirpolate: 39	
----------------------------	

'\n    Extirpolate the values (x, y) onto an integer grid range(N),\n    using lagrange polynomial weights on the M nearest points.\n    Parameters\n    ----------\n    x : array_like\n        array of abscissas\n    y : array_like\n        array of ordinates\n    N : int\n        number of integer bins to use. For best performance, N should be larger\n        than the maximum of x\n    M : int\n        number of adjoining points on which to extirpolate.\n\n    Returns\n    -------\n    yN : ndarray\n         N extirpolated values associated with range(N)\n\n    Example\n    -------\n    >>> rng = np.random.RandomState(0)\n    >>> x = 100 * rng.rand(20)\n    >>> y = np.sin(x)\n    >>> y_hat = extirpolate(x, y)\n    >>> x_hat = np.arange(len(y_hat))\n    >>> f = lambda x: np.sin(x / 10)\n    >>> np.allclose(np.sum(y * f(x)), np.sum(y_hat * f(x_hat)))\n    True\n\n    Notes\n    -----\n    This code is based on the C implementation of spread() presented in\n    Numerical Recipes in C, Second Edition (Press et al. 1989; p.583).\n    '
(x, y) = map(numpy.ravel, numpy.broadcast_arrays(x, y))
if (N is None):
    N = int(((numpy.max(x) + (0.5 * M)) + 1))
result = numpy.zeros(N, dtype=y.dtype)
integers = ((x % 1) == 0)
add_at(result, x[integers].astype(int), y[integers])
(x, y) = (x[(~ integers)], y[(~ integers)])
ilo = numpy.clip((x - (M // 2)).astype(int), 0, (N - M))
tempResult = arange(M)
	
===================================================================	
trig_sum: 67	
----------------------------	

"Compute (approximate) trigonometric sums for a number of frequencies\n    This routine computes weighted sine and cosine sums:\n        S_j = sum_i { h_i * sin(2 pi * f_j * t_i) }\n        C_j = sum_i { h_i * cos(2 pi * f_j * t_i) }\n    Where f_j = freq_factor * (f0 + j * df) for the values j in 1 ... N.\n    The sums can be computed either by a brute force O[N^2] method, or\n    by an FFT-based O[Nlog(N)] method.\n\n    Parameters\n    ----------\n    t : array_like\n        array of input times\n    h : array_like\n        array weights for the sum\n    df : float\n        frequency spacing\n    N : int\n        number of frequency bins to return\n    f0 : float (optional, default=0)\n        The low frequency to use\n    freq_factor : float (optional, default=1)\n        Factor which multiplies the frequency\n    use_fft : bool\n        if True, use the approximate FFT algorithm to compute the result.\n        This uses the FFT with Press & Rybicki's Lagrangian extirpolation.\n    oversampling : int (default = 5)\n        oversampling freq_factor for the approximation; roughly the number of\n        time samples across the highest-frequency sinusoid. This parameter\n        contains the tradeoff between accuracy and speed. Not referenced\n        if use_fft is False.\n    Mfft : int\n        The number of adjacent points to use in the FFT approximation.\n        Not referenced if use_fft is False.\n\n    Returns\n    -------\n    S, C : ndarrays\n        summation arrays for frequencies f = df * np.arange(1, N + 1)\n    "
df *= freq_factor
f0 *= freq_factor
if (df <= 0):
    raise ValueError('df must be positive')
(t, h) = map(numpy.ravel, numpy.broadcast_arrays(t, h))
if use_fft:
    Mfft = int(Mfft)
    if (Mfft <= 0):
        raise ValueError('Mfft must be positive')
    Nfft = bitceil((N * oversampling))
    t0 = t.min()
    if (f0 > 0):
        h = (h * numpy.exp((((2j * numpy.pi) * f0) * (t - t0))))
    tnorm = ((((t - t0) * Nfft) * df) % Nfft)
    grid = extirpolate(tnorm, h, Nfft, Mfft)
    fftgrid = numpy.fft.ifft(grid)[:N]
    if (t0 != 0):
        tempResult = arange(N)
	
===================================================================	
trig_sum: 72	
----------------------------	

"Compute (approximate) trigonometric sums for a number of frequencies\n    This routine computes weighted sine and cosine sums:\n        S_j = sum_i { h_i * sin(2 pi * f_j * t_i) }\n        C_j = sum_i { h_i * cos(2 pi * f_j * t_i) }\n    Where f_j = freq_factor * (f0 + j * df) for the values j in 1 ... N.\n    The sums can be computed either by a brute force O[N^2] method, or\n    by an FFT-based O[Nlog(N)] method.\n\n    Parameters\n    ----------\n    t : array_like\n        array of input times\n    h : array_like\n        array weights for the sum\n    df : float\n        frequency spacing\n    N : int\n        number of frequency bins to return\n    f0 : float (optional, default=0)\n        The low frequency to use\n    freq_factor : float (optional, default=1)\n        Factor which multiplies the frequency\n    use_fft : bool\n        if True, use the approximate FFT algorithm to compute the result.\n        This uses the FFT with Press & Rybicki's Lagrangian extirpolation.\n    oversampling : int (default = 5)\n        oversampling freq_factor for the approximation; roughly the number of\n        time samples across the highest-frequency sinusoid. This parameter\n        contains the tradeoff between accuracy and speed. Not referenced\n        if use_fft is False.\n    Mfft : int\n        The number of adjacent points to use in the FFT approximation.\n        Not referenced if use_fft is False.\n\n    Returns\n    -------\n    S, C : ndarrays\n        summation arrays for frequencies f = df * np.arange(1, N + 1)\n    "
df *= freq_factor
f0 *= freq_factor
if (df <= 0):
    raise ValueError('df must be positive')
(t, h) = map(numpy.ravel, numpy.broadcast_arrays(t, h))
if use_fft:
    Mfft = int(Mfft)
    if (Mfft <= 0):
        raise ValueError('Mfft must be positive')
    Nfft = bitceil((N * oversampling))
    t0 = t.min()
    if (f0 > 0):
        h = (h * numpy.exp((((2j * numpy.pi) * f0) * (t - t0))))
    tnorm = ((((t - t0) * Nfft) * df) % Nfft)
    grid = extirpolate(tnorm, h, Nfft, Mfft)
    fftgrid = numpy.fft.ifft(grid)[:N]
    if (t0 != 0):
        f = (f0 + (df * numpy.arange(N)))
        fftgrid *= numpy.exp((((2j * numpy.pi) * t0) * f))
    C = (Nfft * fftgrid.real)
    S = (Nfft * fftgrid.imag)
else:
    tempResult = arange(N)
	
===================================================================	
test_extirpolate: 28	
----------------------------	

(x, y, f) = extirpolate_data
y_hat = extirpolate(x, y, N, M)
tempResult = arange(len(y_hat))
	
===================================================================	
test_bitceil: 9	
----------------------------	

	
===================================================================	
test_extirpolate_with_integers: 47	
----------------------------	

(x, y, f) = extirpolate_int_data
y_hat = extirpolate(x, y, N, M)
tempResult = arange(len(y_hat))
	
===================================================================	
test_output_shapes: 136	
----------------------------	

(t, y, dy) = data
freq = numpy.asarray(numpy.zeros(shape))
tempResult = arange(1, (freq.size + 1))
	
===================================================================	
test_fast_approximations: 112	
----------------------------	

(t, y, dy) = data
tempResult = arange(40)
	
===================================================================	
test_all_methods: 34	
----------------------------	

if ((method == 'scipy') and (fit_mean or with_errors)):
    return
(t, y, dy) = data
tempResult = arange(40)
	
===================================================================	
test_integer_inputs: 72	
----------------------------	

if ((method == 'scipy') and (fit_mean or with_errors)):
    return
(t, y, dy) = data
t = numpy.floor((100 * t))
t_int = t.astype(int)
y = numpy.floor((100 * y))
y_int = y.astype(int)
dy = numpy.floor((100 * dy))
dy_int = dy.astype('int32')
tempResult = arange(40)
	
===================================================================	
test_nterms_methods: 89	
----------------------------	

(t, y, dy) = data
tempResult = arange(40)
	
===================================================================	
test_fitness_function_results: 86	
----------------------------	

'Test results for several fitness functions'
rng = numpy.random.RandomState(42)
t = rng.randn(100)
edges = bayesian_blocks(t, fitness='events')
assert_allclose(edges, [(- 2.6197451), (- 0.71094865), 0.36866702, 1.85227818])
t[80:] = t[:20]
edges = bayesian_blocks(t, fitness='events', p0=0.01)
assert_allclose(edges, [(- 2.6197451), (- 0.47432431), (- 0.46202823), 1.85227818])
dt = 0.01
tempResult = arange(1000)
	
===================================================================	
test_median_absolute_deviation: 36	
----------------------------	

with NumpyRNGContext(12345):
    randvar = randn(10000)
    mad = funcs.median_absolute_deviation(randvar)
    randvar = randvar.reshape((10, 1000))
    mad = funcs.median_absolute_deviation(randvar, axis=1)
    assert (len(mad) == 10)
    assert (mad.size < randvar.size)
    mad = funcs.median_absolute_deviation(randvar, axis=0)
    assert (len(mad) == 1000)
    assert (mad.size < randvar.size)
    tempResult = arange(((3 * 4) * 5))
	
===================================================================	
test_sigma_clip: 34	
----------------------------	

with NumpyRNGContext(12345):
    randvar = randn(10000)
    filtered_data = sigma_clip(randvar, sigma=1, iters=2)
    assert (sum(filtered_data.mask) > 0)
    assert (sum((~ filtered_data.mask)) < randvar.size)
    filtered_data2 = sigma_clip(randvar, sigma=1, iters=2, stdfunc=numpy.var)
    assert (not numpy.all((filtered_data.mask == filtered_data2.mask)))
    filtered_data3 = sigma_clip(randvar, sigma=1, iters=2, cenfunc=numpy.mean)
    assert (not numpy.all((filtered_data.mask == filtered_data3.mask)))
    filtered_data = sigma_clip(randvar, sigma=3, iters=None)
    assert (filtered_data.data[0] == randvar[0])
    filtered_data.data[0] += 1.0
    assert (filtered_data.data[0] != randvar[0])
    filtered_data = sigma_clip(randvar, sigma=3, iters=None, copy=False)
    assert (filtered_data.data[0] == randvar[0])
    filtered_data.data[0] += 1.0
    assert (filtered_data.data[0] == randvar[0])
    tempResult = arange(5)
	
===================================================================	
test_sigma_clip_scalar_mask: 51	
----------------------------	

'Test that the returned mask is not a scalar.'
tempResult = arange(5)
	
===================================================================	
test_sigma_clipped_stats: 69	
----------------------------	

'Test list data with input mask or mask_value (#3268).'
data = [0, 1]
mask = numpy.array([True, False])
result = sigma_clipped_stats(data, mask=mask)
assert isinstance(result[1], float)
assert (result == (1.0, 1.0, 0.0))
result = sigma_clipped_stats(data, mask_value=0.0)
assert isinstance(result[1], float)
assert (result == (1.0, 1.0, 0.0))
data = [0, 2]
result = sigma_clipped_stats(data)
assert isinstance(result[1], float)
assert (result == (1.0, 1.0, 1.0))
tempResult = arange(10)
	
===================================================================	
Index.__init__: 54	
----------------------------	

from .table import Table, Column
if ((engine is not None) and (not isinstance(engine, type))):
    self.engine = engine.__class__
    self.data = engine
    self.columns = columns
    return
self.engine = (engine or SortedArray)
if (columns is None):
    columns = []
    data = []
    row_index = []
elif (len(columns) == 0):
    raise ValueError('Cannot create index without at least one column')
elif (len(columns) == 1):
    col = columns[0]
    row_index = Column(col.argsort())
    data = Table([col[row_index]])
else:
    num_rows = len(columns[0])
    new_columns = []
    for col in columns:
        if isinstance(col, Time):
            new_columns.append(col.jd)
            remainder = (col - col.__class__(col.jd, format='jd'))
            new_columns.append(remainder.jd)
        else:
            new_columns.append(col)
    tempResult = arange(num_rows)
	
===================================================================	
_hstack: 333	
----------------------------	

"\n    Stack tables horizontally (by columns)\n\n    A ``join_type`` of 'exact' (default) means that the arrays must all\n    have exactly the same number of rows.  If ``join_type`` is 'inner' then\n    the intersection of rows will be the output.  A value of 'outer' means\n    the output will have the union of all rows, with array values being\n    masked where no common values are available.\n\n    Parameters\n    ----------\n    arrays : List of tables\n        Tables to stack by columns (horizontally)\n    join_type : str\n        Join type ('inner' | 'exact' | 'outer'), default is 'outer'\n    uniq_col_name : str or None\n        String generate a unique output column name in case of a conflict.\n        The default is '{col_name}_{table_name}'.\n    table_names : list of str or None\n        Two-element list of table names used when generating unique output\n        column names.  The default is ['1', '2', ..].\n\n    Returns\n    -------\n    stacked_table : `~astropy.table.Table` object\n        New table containing the stacked data from the input tables.\n    "
_col_name_map = col_name_map
if (join_type not in ('inner', 'exact', 'outer')):
    raise ValueError("join_type arg must be either 'inner', 'exact' or 'outer'")
if (table_names is None):
    table_names = ['{0}'.format((ii + 1)) for ii in range(len(arrays))]
if (len(arrays) != len(table_names)):
    raise ValueError('Number of arrays must match number of table_names')
if (len(arrays) == 1):
    return arrays[0]
col_name_map = get_col_name_map(arrays, [], uniq_col_name, table_names)
arr_lens = [len(arr) for arr in arrays]
if (join_type == 'exact'):
    if (len(set(arr_lens)) > 1):
        raise TableMergeError("Inconsistent number of rows in input arrays (use 'inner' or 'outer' join_type to allow non-matching rows)")
    join_type = 'outer'
if (join_type == 'inner'):
    min_arr_len = min(arr_lens)
    if (len(set(arr_lens)) > 1):
        arrays = [arr[:min_arr_len] for arr in arrays]
    arr_lens = [min_arr_len for arr in arrays]
masked = (any((getattr(arr, 'masked', False) for arr in arrays)) or (len(set(arr_lens)) > 1))
n_rows = max(arr_lens)
out = _get_out_class(arrays)(masked=masked)
for (out_name, in_names) in extern.six.iteritems(col_name_map):
    for (name, array, arr_len) in zip(in_names, arrays, arr_lens):
        if (name is None):
            continue
        if (n_rows > arr_len):
            tempResult = arange(n_rows)
	
===================================================================	
TableFormatter._pformat_col_iter: 204	
----------------------------	

'Iterator which yields formatted string representation of column values.\n\n        Parameters\n        ----------\n        max_lines : int\n            Maximum lines of output (header + data rows)\n\n        show_name : bool\n            Include column name (default=True)\n\n        show_unit : bool\n            Include a header row for unit.  Default is to show a row\n            for units only if one or more columns has a defined value\n            for the unit.\n\n        outs : dict\n            Must be a dict which is used to pass back additional values\n            defined within the iterator.\n\n        show_dtype : bool\n            Include column dtype (default=False)\n\n        show_length : bool\n            Include column length at end.  Default is to show this only\n            if the column is not shown completely.\n        '
(max_lines, _) = self._get_pprint_size(max_lines, (- 1))
multidims = getattr(col, 'shape', [0])[1:]
if multidims:
    multidim0 = tuple((0 for n in multidims))
    multidim1 = tuple(((n - 1) for n in multidims))
    trivial_multidims = (numpy.prod(multidims) == 1)
i_dashes = None
i_centers = []
n_header = 0
if show_name:
    i_centers.append(n_header)
    col_name = extern.six.text_type(col.info.name)
    if multidims:
        col_name += ' [{0}]'.format(','.join((extern.six.text_type(n) for n in multidims)))
    n_header += 1
    (yield col_name)
if show_unit:
    i_centers.append(n_header)
    n_header += 1
    (yield extern.six.text_type((col.info.unit or '')))
if show_dtype:
    i_centers.append(n_header)
    n_header += 1
    try:
        dtype = dtype_info_name(col.dtype)
    except AttributeError:
        dtype = 'object'
    (yield extern.six.text_type(dtype))
if (show_unit or show_name or show_dtype):
    i_dashes = n_header
    n_header += 1
    (yield '---')
max_lines -= n_header
n_print2 = (max_lines // 2)
n_rows = len(col)
col_format = (col.info.format or getattr(col.info, 'default_format', None))
pssf = (getattr(col.info, 'possible_string_format_functions', None) or _possible_string_format_functions)
auto_format_func = get_auto_format_func(col.info.name, pssf)
format_key = (col_format, col.info.name)
format_func = _format_funcs.get(format_key, auto_format_func)
if (len(col) > max_lines):
    if (show_length is None):
        show_length = True
    i0 = (n_print2 - (1 if show_length else 0))
    i1 = ((n_rows - n_print2) - (max_lines % 2))
    tempResult = arange(0, (i0 + 1))
	
===================================================================	
TableFormatter._pformat_col_iter: 204	
----------------------------	

'Iterator which yields formatted string representation of column values.\n\n        Parameters\n        ----------\n        max_lines : int\n            Maximum lines of output (header + data rows)\n\n        show_name : bool\n            Include column name (default=True)\n\n        show_unit : bool\n            Include a header row for unit.  Default is to show a row\n            for units only if one or more columns has a defined value\n            for the unit.\n\n        outs : dict\n            Must be a dict which is used to pass back additional values\n            defined within the iterator.\n\n        show_dtype : bool\n            Include column dtype (default=False)\n\n        show_length : bool\n            Include column length at end.  Default is to show this only\n            if the column is not shown completely.\n        '
(max_lines, _) = self._get_pprint_size(max_lines, (- 1))
multidims = getattr(col, 'shape', [0])[1:]
if multidims:
    multidim0 = tuple((0 for n in multidims))
    multidim1 = tuple(((n - 1) for n in multidims))
    trivial_multidims = (numpy.prod(multidims) == 1)
i_dashes = None
i_centers = []
n_header = 0
if show_name:
    i_centers.append(n_header)
    col_name = extern.six.text_type(col.info.name)
    if multidims:
        col_name += ' [{0}]'.format(','.join((extern.six.text_type(n) for n in multidims)))
    n_header += 1
    (yield col_name)
if show_unit:
    i_centers.append(n_header)
    n_header += 1
    (yield extern.six.text_type((col.info.unit or '')))
if show_dtype:
    i_centers.append(n_header)
    n_header += 1
    try:
        dtype = dtype_info_name(col.dtype)
    except AttributeError:
        dtype = 'object'
    (yield extern.six.text_type(dtype))
if (show_unit or show_name or show_dtype):
    i_dashes = n_header
    n_header += 1
    (yield '---')
max_lines -= n_header
n_print2 = (max_lines // 2)
n_rows = len(col)
col_format = (col.info.format or getattr(col.info, 'default_format', None))
pssf = (getattr(col.info, 'possible_string_format_functions', None) or _possible_string_format_functions)
auto_format_func = get_auto_format_func(col.info.name, pssf)
format_key = (col_format, col.info.name)
format_func = _format_funcs.get(format_key, auto_format_func)
if (len(col) > max_lines):
    if (show_length is None):
        show_length = True
    i0 = (n_print2 - (1 if show_length else 0))
    i1 = ((n_rows - n_print2) - (max_lines % 2))
    tempResult = arange((i1 + 1), len(col))
	
===================================================================	
TableFormatter._pformat_col_iter: 207	
----------------------------	

'Iterator which yields formatted string representation of column values.\n\n        Parameters\n        ----------\n        max_lines : int\n            Maximum lines of output (header + data rows)\n\n        show_name : bool\n            Include column name (default=True)\n\n        show_unit : bool\n            Include a header row for unit.  Default is to show a row\n            for units only if one or more columns has a defined value\n            for the unit.\n\n        outs : dict\n            Must be a dict which is used to pass back additional values\n            defined within the iterator.\n\n        show_dtype : bool\n            Include column dtype (default=False)\n\n        show_length : bool\n            Include column length at end.  Default is to show this only\n            if the column is not shown completely.\n        '
(max_lines, _) = self._get_pprint_size(max_lines, (- 1))
multidims = getattr(col, 'shape', [0])[1:]
if multidims:
    multidim0 = tuple((0 for n in multidims))
    multidim1 = tuple(((n - 1) for n in multidims))
    trivial_multidims = (numpy.prod(multidims) == 1)
i_dashes = None
i_centers = []
n_header = 0
if show_name:
    i_centers.append(n_header)
    col_name = extern.six.text_type(col.info.name)
    if multidims:
        col_name += ' [{0}]'.format(','.join((extern.six.text_type(n) for n in multidims)))
    n_header += 1
    (yield col_name)
if show_unit:
    i_centers.append(n_header)
    n_header += 1
    (yield extern.six.text_type((col.info.unit or '')))
if show_dtype:
    i_centers.append(n_header)
    n_header += 1
    try:
        dtype = dtype_info_name(col.dtype)
    except AttributeError:
        dtype = 'object'
    (yield extern.six.text_type(dtype))
if (show_unit or show_name or show_dtype):
    i_dashes = n_header
    n_header += 1
    (yield '---')
max_lines -= n_header
n_print2 = (max_lines // 2)
n_rows = len(col)
col_format = (col.info.format or getattr(col.info, 'default_format', None))
pssf = (getattr(col.info, 'possible_string_format_functions', None) or _possible_string_format_functions)
auto_format_func = get_auto_format_func(col.info.name, pssf)
format_key = (col_format, col.info.name)
format_func = _format_funcs.get(format_key, auto_format_func)
if (len(col) > max_lines):
    if (show_length is None):
        show_length = True
    i0 = (n_print2 - (1 if show_length else 0))
    i1 = ((n_rows - n_print2) - (max_lines % 2))
    ii = numpy.concatenate([numpy.arange(0, (i0 + 1)), numpy.arange((i1 + 1), len(col))])
else:
    i0 = (- 1)
    tempResult = arange(len(col))
	
===================================================================	
SortedArray.sort: 155	
----------------------------	

'\n        Make row order align with key order.\n        '
tempResult = arange(len(self.row_index))
	
===================================================================	
Table._make_index_row_display_table: 519	
----------------------------	

if (index_row_name not in self.columns):
    tempResult = arange(len(self))
	
===================================================================	
simple_table: 47	
----------------------------	

"\n    Return a simple table for testing.\n\n    Example\n    --------\n    ::\n\n      >>> from astropy.table.table_helpers import simple_table\n      >>> print(simple_table(3, 6, masked=True, kinds='ifOS'))\n       a   b     c      d   e   f\n      --- --- -------- --- --- ---\n       -- 1.0 {'c': 2}  --   5 5.0\n        2 2.0       --   e   6  --\n        3  -- {'e': 4}   f  -- 7.0\n\n    Parameters\n    ----------\n    size : int\n        Number of table rows\n    cols : int, default=number of kinds\n        Number of table columns\n    kinds : str\n        String consisting of the column dtype.kinds.  This string\n        will be cycled through to generate the column dtype.\n        The allowed values are 'i', 'f', 'S', 'O'.\n\n    Returns\n    -------\n    out : `Table`\n        New table with appropriate characteristics\n    "
if (cols is None):
    cols = len(kinds)
if (cols > 26):
    raise ValueError('Max 26 columns in SimpleTable')
columns = []
names = [chr((ord('a') + ii)) for ii in range(cols)]
letters = numpy.array([c for c in string.ascii_letters])
for (jj, kind) in zip(range(cols), cycle(kinds)):
    if (kind == 'i'):
        tempResult = arange(1, (size + 1), dtype=numpy.int64)
	
===================================================================	
simple_table: 49	
----------------------------	

"\n    Return a simple table for testing.\n\n    Example\n    --------\n    ::\n\n      >>> from astropy.table.table_helpers import simple_table\n      >>> print(simple_table(3, 6, masked=True, kinds='ifOS'))\n       a   b     c      d   e   f\n      --- --- -------- --- --- ---\n       -- 1.0 {'c': 2}  --   5 5.0\n        2 2.0       --   e   6  --\n        3  -- {'e': 4}   f  -- 7.0\n\n    Parameters\n    ----------\n    size : int\n        Number of table rows\n    cols : int, default=number of kinds\n        Number of table columns\n    kinds : str\n        String consisting of the column dtype.kinds.  This string\n        will be cycled through to generate the column dtype.\n        The allowed values are 'i', 'f', 'S', 'O'.\n\n    Returns\n    -------\n    out : `Table`\n        New table with appropriate characteristics\n    "
if (cols is None):
    cols = len(kinds)
if (cols > 26):
    raise ValueError('Max 26 columns in SimpleTable')
columns = []
names = [chr((ord('a') + ii)) for ii in range(cols)]
letters = numpy.array([c for c in string.ascii_letters])
for (jj, kind) in zip(range(cols), cycle(kinds)):
    if (kind == 'i'):
        data = (numpy.arange(1, (size + 1), dtype=numpy.int64) + jj)
    elif (kind == 'f'):
        tempResult = arange(size, dtype=numpy.float64)
	
===================================================================	
simple_table: 51	
----------------------------	

"\n    Return a simple table for testing.\n\n    Example\n    --------\n    ::\n\n      >>> from astropy.table.table_helpers import simple_table\n      >>> print(simple_table(3, 6, masked=True, kinds='ifOS'))\n       a   b     c      d   e   f\n      --- --- -------- --- --- ---\n       -- 1.0 {'c': 2}  --   5 5.0\n        2 2.0       --   e   6  --\n        3  -- {'e': 4}   f  -- 7.0\n\n    Parameters\n    ----------\n    size : int\n        Number of table rows\n    cols : int, default=number of kinds\n        Number of table columns\n    kinds : str\n        String consisting of the column dtype.kinds.  This string\n        will be cycled through to generate the column dtype.\n        The allowed values are 'i', 'f', 'S', 'O'.\n\n    Returns\n    -------\n    out : `Table`\n        New table with appropriate characteristics\n    "
if (cols is None):
    cols = len(kinds)
if (cols > 26):
    raise ValueError('Max 26 columns in SimpleTable')
columns = []
names = [chr((ord('a') + ii)) for ii in range(cols)]
letters = numpy.array([c for c in string.ascii_letters])
for (jj, kind) in zip(range(cols), cycle(kinds)):
    if (kind == 'i'):
        data = (numpy.arange(1, (size + 1), dtype=numpy.int64) + jj)
    elif (kind == 'f'):
        data = (numpy.arange(size, dtype=numpy.float64) + jj)
    elif (kind == 'S'):
        tempResult = arange(size)
	
===================================================================	
simple_table: 54	
----------------------------	

"\n    Return a simple table for testing.\n\n    Example\n    --------\n    ::\n\n      >>> from astropy.table.table_helpers import simple_table\n      >>> print(simple_table(3, 6, masked=True, kinds='ifOS'))\n       a   b     c      d   e   f\n      --- --- -------- --- --- ---\n       -- 1.0 {'c': 2}  --   5 5.0\n        2 2.0       --   e   6  --\n        3  -- {'e': 4}   f  -- 7.0\n\n    Parameters\n    ----------\n    size : int\n        Number of table rows\n    cols : int, default=number of kinds\n        Number of table columns\n    kinds : str\n        String consisting of the column dtype.kinds.  This string\n        will be cycled through to generate the column dtype.\n        The allowed values are 'i', 'f', 'S', 'O'.\n\n    Returns\n    -------\n    out : `Table`\n        New table with appropriate characteristics\n    "
if (cols is None):
    cols = len(kinds)
if (cols > 26):
    raise ValueError('Max 26 columns in SimpleTable')
columns = []
names = [chr((ord('a') + ii)) for ii in range(cols)]
letters = numpy.array([c for c in string.ascii_letters])
for (jj, kind) in zip(range(cols), cycle(kinds)):
    if (kind == 'i'):
        data = (numpy.arange(1, (size + 1), dtype=numpy.int64) + jj)
    elif (kind == 'f'):
        data = (numpy.arange(size, dtype=numpy.float64) + jj)
    elif (kind == 'S'):
        indices = ((numpy.arange(size) + jj) % len(letters))
        data = letters[indices]
    elif (kind == 'O'):
        tempResult = arange(size)
	
===================================================================	
simple_table: 63	
----------------------------	

"\n    Return a simple table for testing.\n\n    Example\n    --------\n    ::\n\n      >>> from astropy.table.table_helpers import simple_table\n      >>> print(simple_table(3, 6, masked=True, kinds='ifOS'))\n       a   b     c      d   e   f\n      --- --- -------- --- --- ---\n       -- 1.0 {'c': 2}  --   5 5.0\n        2 2.0       --   e   6  --\n        3  -- {'e': 4}   f  -- 7.0\n\n    Parameters\n    ----------\n    size : int\n        Number of table rows\n    cols : int, default=number of kinds\n        Number of table columns\n    kinds : str\n        String consisting of the column dtype.kinds.  This string\n        will be cycled through to generate the column dtype.\n        The allowed values are 'i', 'f', 'S', 'O'.\n\n    Returns\n    -------\n    out : `Table`\n        New table with appropriate characteristics\n    "
if (cols is None):
    cols = len(kinds)
if (cols > 26):
    raise ValueError('Max 26 columns in SimpleTable')
columns = []
names = [chr((ord('a') + ii)) for ii in range(cols)]
letters = numpy.array([c for c in string.ascii_letters])
for (jj, kind) in zip(range(cols), cycle(kinds)):
    if (kind == 'i'):
        data = (numpy.arange(1, (size + 1), dtype=numpy.int64) + jj)
    elif (kind == 'f'):
        data = (numpy.arange(size, dtype=numpy.float64) + jj)
    elif (kind == 'S'):
        indices = ((numpy.arange(size) + jj) % len(letters))
        data = letters[indices]
    elif (kind == 'O'):
        indices = ((numpy.arange(size) + jj) % len(letters))
        vals = letters[indices]
        data = [{val: index} for (val, index) in zip(vals, indices)]
    else:
        raise ValueError('Unknown data kind')
    columns.append(Column(data))
table = Table(columns, names=names, masked=masked)
if masked:
    for (ii, col) in enumerate(table.columns.values()):
        tempResult = arange(size)
	
===================================================================	
TimingTables.__init__: 18	
----------------------------	

self.masked = masked
self.table = Table(masked=self.masked)
numpy.random.seed(12345)
tempResult = arange(size)
	
===================================================================	
TimingTables.__init__: 28	
----------------------------	

self.masked = masked
self.table = Table(masked=self.masked)
numpy.random.seed(12345)
self.table['i'] = numpy.arange(size)
self.table['a'] = numpy.random.random(size)
self.table['b'] = (numpy.random.random(size) > 0.5)
self.table['c'] = numpy.random.random((size, 10))
self.table['d'] = numpy.random.choice(numpy.array(list(string.ascii_letters)), size)
self.extra_row = {'a': 1.2, 'b': True, 'c': numpy.repeat(1, 10), 'd': 'Z'}
self.extra_column = numpy.random.randint(0, 100, size)
self.row_indices = numpy.where((self.table['a'] > 0.9))[0]
self.table_grouped = self.table.group_by('d')
self.other_table = Table(masked=self.masked)
tempResult = arange(1, size, 3)
	
===================================================================	
wide_array: 18	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_wide_array: 34	
----------------------------	

first_row = wide_array[:1].data
tempResult = arange(100)
	
===================================================================	
TestColumn.test_format: 58	
----------------------------	

'Show that the formatted output from str() works'
from ... import conf
with conf.set_temp('max_lines', 8):
    tempResult = arange(2000)
	
===================================================================	
test_mutable_operations: 166	
----------------------------	

'\n    Operations like adding or deleting a row should removing grouping,\n    but adding or removing or renaming a column should retain grouping.\n    '
for masked in (False, True):
    t1 = Table(T1, masked=masked)
    tg = t1.group_by('a')
    tg.add_row((0, 'a', 3.0, 4))
    assert numpy.all((tg.groups.indices == numpy.array([0, len(tg)])))
    assert (tg.groups.keys is None)
    tg = t1.group_by('a')
    tg.remove_row(4)
    assert numpy.all((tg.groups.indices == numpy.array([0, len(tg)])))
    assert (tg.groups.keys is None)
    tg = t1.group_by('a')
    indices = tg.groups.indices.copy()
    tempResult = arange(len(tg))
	
===================================================================	
TestInitFromRows.test_init_with_rows: 383	
----------------------------	

for rows in ([[1, 'a'], [2, 'b']], [(1, 'a'), (2, 'b')], ((1, 'a'), (2, 'b'))):
    t = table_type(rows=rows, names=('a', 'b'))
    assert numpy.all((t['a'] == [1, 2]))
    assert numpy.all((t['b'] == ['a', 'b']))
    assert (t.colnames == ['a', 'b'])
    assert (t['a'].dtype.kind == 'i')
    assert (t['b'].dtype.kind in ('S', 'U'))
    assert t['b'].dtype.str.endswith('1')
tempResult = arange(6)
	
===================================================================	
TestTableColumnsInit.test_init: 17	
----------------------------	

'Test initialisation with lists, tuples, dicts of arrays\n        rather than Columns [regression test for #2647]'
tempResult = arange(10.0)
	
===================================================================	
TestTableColumnsInit.test_init: 18	
----------------------------	

'Test initialisation with lists, tuples, dicts of arrays\n        rather than Columns [regression test for #2647]'
x1 = numpy.arange(10.0)
tempResult = arange(5.0)
	
===================================================================	
TestTableColumnsInit.test_init: 19	
----------------------------	

'Test initialisation with lists, tuples, dicts of arrays\n        rather than Columns [regression test for #2647]'
x1 = numpy.arange(10.0)
x2 = numpy.arange(5.0)
tempResult = arange(7.0)
	
===================================================================	
test_setting_from_masked_column: 358	
----------------------------	

'Test issue in #2997'
mask_b = numpy.array([True, True, False, False])
for select in (mask_b, slice(0, 2)):
    t = Table(masked=True)
    t['a'] = Column([1, 2, 3, 4])
    t['b'] = MaskedColumn([11, 22, 33, 44], mask=mask_b)
    t['c'] = MaskedColumn([111, 222, 333, 444], mask=[True, False, True, False])
    t['b'][select] = t['c'][select]
    assert (t['b'][1] == t[1]['b'])
    assert (t['b'][0] is numpy.ma.masked)
    assert (t['b'][1] == 222)
    assert (t['b'][2] == 33)
    assert (t['b'][3] == 44)
    assert numpy.all((t['b'].mask == t.mask['b']))
    mask_before_add = t.mask.copy()
    tempResult = arange(len(t))
	
===================================================================	
test_ndarray_mixin: 342	
----------------------------	

'\n    Test directly adding a plain structured array into a table instead of the\n    view as an NdarrayMixin.  Once added as an NdarrayMixin then all the previous\n    tests apply.\n    '
a = numpy.array([(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')], dtype=('<i4,' + ('|S1' if extern.six.PY2 else '|U1')))
b = numpy.array([(10, 'aa'), (20, 'bb'), (30, 'cc'), (40, 'dd')], dtype=[('x', 'i4'), ('y', ('S2' if extern.six.PY2 else 'U2'))])
c = numpy.rec.fromrecords([(100, 'raa'), (200, 'rbb'), (300, 'rcc'), (400, 'rdd')], names=['rx', 'ry'])
tempResult = arange(8)
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
from ...tests.helper import pytest
from ... import table
from ...table import Table
from ...table.table_helpers import simple_table
from ...extern.six import PY2
from ...utils import console
tempResult = arange(2000, dtype=numpy.float64)
	
===================================================================	
module: 10	
----------------------------	

import numpy as np
from ...tests.helper import pytest
from ... import table
from ...table import Table
from ...table.table_helpers import simple_table
from ...extern.six import PY2
from ...utils import console
BIG_WIDE_ARR = np.arange(2000, dtype=np.float64).reshape(100, 20)
tempResult = arange(18, dtype=numpy.int64)
	
===================================================================	
TestPprint.test_format0: 69	
----------------------------	

"Try getting screen size but fail to defaults because testing doesn't\n        have access to screen (fcntl.ioctl fails).\n        "
self._setup(table_type)
tempResult = arange(4000, dtype=numpy.float64)
	
===================================================================	
TestFormat.test_column_format_with_threshold: 154	
----------------------------	

from ... import conf
with conf.set_temp('max_lines', 8):
    tempResult = arange(20)
	
===================================================================	
TestFormatWithMaskedElements.test_column_format_with_threshold: 231	
----------------------------	

from ... import conf
with conf.set_temp('max_lines', 8):
    tempResult = arange(20)
	
===================================================================	
TestAddRow.test_insert_table_row: 627	
----------------------------	

'\n        Light testing of Table.insert_row() method.  The deep testing is done via\n        the add_row() tests which calls insert_row(index=len(self), ...), so\n        here just test that the added index parameter is handled correctly.\n        '
self._setup(table_types)
row = (10, 40.0, 'x', [10, 20])
for index in range((- 3), 4):
    tempResult = arange(3)
	
===================================================================	
TestReverse.test_reverse_big: 269	
----------------------------	

tempResult = arange(10000)
	
===================================================================	
TestSetTableColumn.test_set_new_col_existing_table: 144	
----------------------------	

'Create a new column in an existing table using the item access syntax'
self._setup(table_types)
t = table_types.Table([self.a])
t['bb'] = self.b
assert numpy.all((t['bb'] == self.b))
assert (t.colnames == ['a', 'bb'])
assert (t['bb'].meta == self.b.meta)
assert (t['bb'].format == self.b.format)
t['c'] = t['a']
assert numpy.all((t['c'] == t['a']))
assert (t.colnames == ['a', 'bb', 'c'])
assert (t['c'].meta == t['a'].meta)
assert (t['c'].format == t['a'].format)
tempResult = arange(12)
	
===================================================================	
TestSort.test_single_big: 863	
----------------------------	

'Sort a big-ish table with a non-trivial sort order'
tempResult = arange(10000)
	
===================================================================	
Time._advanced_index: 420	
----------------------------	

'Turn argmin, argmax output into an advanced index.\n\n        Argmin, argmax output contains indices along a given axis in an array\n        shaped like the other dimensions.  To use this to get values at the\n        correct location, a list is constructed in which the other axes are\n        indexed sequentially.  For ``keepdims`` is ``True``, the net result is\n        the same as constructing an index grid with ``np.ogrid`` and then\n        replacing the ``axis`` item with ``indices`` with its shaped expanded\n        at ``axis``. For ``keepdims`` is ``False``, the result is the same but\n        with the ``axis`` dimension removed from all list entries.\n\n        For ``axis`` is ``None``, this calls :func:`~numpy.unravel_index`.\n\n        Parameters\n        ----------\n        indices : array\n            Output of argmin or argmax.\n        axis : int or None\n            axis along which argmin or argmax was used.\n        keepdims : bool\n            Whether to construct indices that keep or remove the axis along\n            which argmin or argmax was used.  Default: ``False``.\n\n        Returns\n        -------\n        advanced_index : list of arrays\n            Suitable for use as an advanced index.\n        '
if (axis is None):
    return numpy.unravel_index(indices, self.shape)
ndim = self.ndim
if (axis < 0):
    axis = (axis + ndim)
if (keepdims and (indices.ndim < self.ndim)):
    indices = numpy.expand_dims(indices, axis)
tempResult = arange(s)
	
===================================================================	
TestBasic.test_different_dimensions: 55	
----------------------------	

'Test scalars, vector, and higher-dimensions'
(val, val1) = (2450000.0, 0.125)
t1 = Time(val, val1, format='jd')
assert ((t1.isscalar is True) and (t1.shape == ()))
tempResult = arange(2450000.0, 2450010.0)
	
===================================================================	
TestBasic.test_different_dimensions: 61	
----------------------------	

'Test scalars, vector, and higher-dimensions'
(val, val1) = (2450000.0, 0.125)
t1 = Time(val, val1, format='jd')
assert ((t1.isscalar is True) and (t1.shape == ()))
val = numpy.arange(2450000.0, 2450010.0)
t2 = Time(val, format='jd')
assert ((t2.isscalar is False) and (t2.shape == val.shape))
val2 = 0.0
t3 = Time(val, val2, format='jd')
assert ((t3.isscalar is False) and (t3.shape == val.shape))
tempResult = arange(5.0)
	
===================================================================	
TestBasic.test_init_from_time_objects: 375	
----------------------------	

'Initialize from one or more Time objects'
t1 = Time('2007:001', scale='tai')
t2 = Time(['2007-01-02', '2007-01-03'], scale='utc')
t3 = Time([t1, t2])
assert (len(t3) == 3)
assert (t3.scale == t1.scale)
assert (t3.format == t1.format)
assert numpy.all((t3.value == numpy.concatenate([[t1.yday], t2.tai.yday])))
t3 = Time(t1)
assert t3.isscalar
assert (t3.scale == t1.scale)
assert (t3.format == t1.format)
assert numpy.all((t3.value == t1.value))
t3 = Time(t1, scale='utc')
assert (t3.scale == 'utc')
assert numpy.all((t3.value == t1.utc.value))
t3 = Time([t1, t2], scale='tt')
assert (t3.scale == 'tt')
assert (t3.format == t1.format)
assert numpy.all((t3.value == numpy.concatenate([[t1.tt.yday], t2.tt.yday])))
tempResult = arange(50000.0, 50006.0)
	
===================================================================	
TestBasic.test_init_from_time_objects: 376	
----------------------------	

'Initialize from one or more Time objects'
t1 = Time('2007:001', scale='tai')
t2 = Time(['2007-01-02', '2007-01-03'], scale='utc')
t3 = Time([t1, t2])
assert (len(t3) == 3)
assert (t3.scale == t1.scale)
assert (t3.format == t1.format)
assert numpy.all((t3.value == numpy.concatenate([[t1.yday], t2.tai.yday])))
t3 = Time(t1)
assert t3.isscalar
assert (t3.scale == t1.scale)
assert (t3.format == t1.format)
assert numpy.all((t3.value == t1.value))
t3 = Time(t1, scale='utc')
assert (t3.scale == 'utc')
assert numpy.all((t3.value == t1.utc.value))
t3 = Time([t1, t2], scale='tt')
assert (t3.scale == 'tt')
assert (t3.format == t1.format)
assert numpy.all((t3.value == numpy.concatenate([[t1.tt.yday], t2.tt.yday])))
mjd = numpy.arange(50000.0, 50006.0)
tempResult = arange(0.0, 0.999, 0.2)
	
===================================================================	
test_bool: 653	
----------------------------	

'Any Time object should evaluate to True unless it is empty [#3520].'
tempResult = arange(50000, 50010)
	
===================================================================	
TestVal2.test_val_broadcasts_against_val2: 396	
----------------------------	

tempResult = arange(50000.0, 50007.0)
	
===================================================================	
TestVal2.test_val_broadcasts_against_val2: 397	
----------------------------	

mjd = numpy.arange(50000.0, 50007.0)
tempResult = arange(0.0, 0.999, 0.2)
	
===================================================================	
test_len_size: 660	
----------------------------	

'Check length of Time objects and that scalar ones do not have one.'
tempResult = arange(50000, 50010)
	
===================================================================	
test_len_size: 662	
----------------------------	

'Check length of Time objects and that scalar ones do not have one.'
t = Time(numpy.arange(50000, 50010), format='mjd', scale='utc')
assert ((len(t) == 10) and (t.size == 10))
tempResult = arange(50000, 50010)
	
===================================================================	
TestBasic.test_datetime: 283	
----------------------------	

'\n        Test datetime format, including guessing the format from the input type\n        by not providing the format keyword to Time.\n        '
dt = datetime.datetime(2000, 1, 2, 3, 4, 5, 123456)
dt2 = datetime.datetime(2001, 1, 1)
t = Time(dt, scale='utc', precision=9)
assert (t.iso == '2000-01-02 03:04:05.123456000')
assert (t.datetime == dt)
assert (t.value == dt)
t2 = Time(t.iso, scale='utc')
assert (t2.datetime == dt)
t = Time([dt, dt2], scale='utc')
assert numpy.all((t.value == [dt, dt2]))
t = Time('2000-01-01 01:01:01.123456789', scale='tai')
assert (t.datetime == datetime.datetime(2000, 1, 1, 1, 1, 1, 123457))
tempResult = arange(12)
	
===================================================================	
TestBasic.test_getitem: 86	
----------------------------	

'Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.'
tempResult = arange(50000, 50010)
	
===================================================================	
TestBasic.test_getitem: 111	
----------------------------	

'Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.'
mjd = numpy.arange(50000, 50010)
t = Time(mjd, format='mjd', scale='utc', location=('45d', '50d'))
t1 = t[3]
assert (t1.isscalar is True)
assert (t1._time.jd1 == t._time.jd1[3])
assert (t1.location is t.location)
t1a = Time(mjd[3], format='mjd', scale='utc')
assert (t1a.isscalar is True)
assert numpy.all((t1._time.jd1 == t1a._time.jd1))
t1b = Time(t[3])
assert (t1b.isscalar is True)
assert numpy.all((t1._time.jd1 == t1b._time.jd1))
t2 = t[4:6]
assert (t2.isscalar is False)
assert numpy.all((t2._time.jd1 == t._time.jd1[4:6]))
assert (t2.location is t.location)
t2a = Time(t[4:6])
assert (t2a.isscalar is False)
assert numpy.all((t2a._time.jd1 == t._time.jd1[4:6]))
t2b = Time([t[4], t[5]])
assert (t2b.isscalar is False)
assert numpy.all((t2b._time.jd1 == t._time.jd1[4:6]))
t2c = Time((t[4], t[5]))
assert (t2c.isscalar is False)
assert numpy.all((t2c._time.jd1 == t._time.jd1[4:6]))
tempResult = arange(len(t))
	
===================================================================	
TestBasic.test_getitem: 114	
----------------------------	

'Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.'
mjd = numpy.arange(50000, 50010)
t = Time(mjd, format='mjd', scale='utc', location=('45d', '50d'))
t1 = t[3]
assert (t1.isscalar is True)
assert (t1._time.jd1 == t._time.jd1[3])
assert (t1.location is t.location)
t1a = Time(mjd[3], format='mjd', scale='utc')
assert (t1a.isscalar is True)
assert numpy.all((t1._time.jd1 == t1a._time.jd1))
t1b = Time(t[3])
assert (t1b.isscalar is True)
assert numpy.all((t1._time.jd1 == t1b._time.jd1))
t2 = t[4:6]
assert (t2.isscalar is False)
assert numpy.all((t2._time.jd1 == t._time.jd1[4:6]))
assert (t2.location is t.location)
t2a = Time(t[4:6])
assert (t2a.isscalar is False)
assert numpy.all((t2a._time.jd1 == t._time.jd1[4:6]))
t2b = Time([t[4], t[5]])
assert (t2b.isscalar is False)
assert numpy.all((t2b._time.jd1 == t._time.jd1[4:6]))
t2c = Time((t[4], t[5]))
assert (t2c.isscalar is False)
assert numpy.all((t2c._time.jd1 == t._time.jd1[4:6]))
t.delta_tdb_tt = numpy.arange(len(t))
t3 = t[4:6]
assert numpy.all((t3._delta_tdb_tt == t._delta_tdb_tt[4:6]))
tempResult = arange(len(mjd))
	
===================================================================	
TestBasic.test_getitem: 114	
----------------------------	

'Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.'
mjd = numpy.arange(50000, 50010)
t = Time(mjd, format='mjd', scale='utc', location=('45d', '50d'))
t1 = t[3]
assert (t1.isscalar is True)
assert (t1._time.jd1 == t._time.jd1[3])
assert (t1.location is t.location)
t1a = Time(mjd[3], format='mjd', scale='utc')
assert (t1a.isscalar is True)
assert numpy.all((t1._time.jd1 == t1a._time.jd1))
t1b = Time(t[3])
assert (t1b.isscalar is True)
assert numpy.all((t1._time.jd1 == t1b._time.jd1))
t2 = t[4:6]
assert (t2.isscalar is False)
assert numpy.all((t2._time.jd1 == t._time.jd1[4:6]))
assert (t2.location is t.location)
t2a = Time(t[4:6])
assert (t2a.isscalar is False)
assert numpy.all((t2a._time.jd1 == t._time.jd1[4:6]))
t2b = Time([t[4], t[5]])
assert (t2b.isscalar is False)
assert numpy.all((t2b._time.jd1 == t._time.jd1[4:6]))
t2c = Time((t[4], t[5]))
assert (t2c.isscalar is False)
assert numpy.all((t2c._time.jd1 == t._time.jd1[4:6]))
t.delta_tdb_tt = numpy.arange(len(t))
t3 = t[4:6]
assert numpy.all((t3._delta_tdb_tt == t._delta_tdb_tt[4:6]))
tempResult = arange(len(mjd))
	
===================================================================	
TestBasic.test_getitem: 124	
----------------------------	

'Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.'
mjd = numpy.arange(50000, 50010)
t = Time(mjd, format='mjd', scale='utc', location=('45d', '50d'))
t1 = t[3]
assert (t1.isscalar is True)
assert (t1._time.jd1 == t._time.jd1[3])
assert (t1.location is t.location)
t1a = Time(mjd[3], format='mjd', scale='utc')
assert (t1a.isscalar is True)
assert numpy.all((t1._time.jd1 == t1a._time.jd1))
t1b = Time(t[3])
assert (t1b.isscalar is True)
assert numpy.all((t1._time.jd1 == t1b._time.jd1))
t2 = t[4:6]
assert (t2.isscalar is False)
assert numpy.all((t2._time.jd1 == t._time.jd1[4:6]))
assert (t2.location is t.location)
t2a = Time(t[4:6])
assert (t2a.isscalar is False)
assert numpy.all((t2a._time.jd1 == t._time.jd1[4:6]))
t2b = Time([t[4], t[5]])
assert (t2b.isscalar is False)
assert numpy.all((t2b._time.jd1 == t._time.jd1[4:6]))
t2c = Time((t[4], t[5]))
assert (t2c.isscalar is False)
assert numpy.all((t2c._time.jd1 == t._time.jd1[4:6]))
t.delta_tdb_tt = numpy.arange(len(t))
t3 = t[4:6]
assert numpy.all((t3._delta_tdb_tt == t._delta_tdb_tt[4:6]))
t4 = Time(mjd, format='mjd', scale='utc', location=(numpy.arange(len(mjd)), numpy.arange(len(mjd))))
t5 = t4[3]
assert (t5.location == t4.location[3])
t6 = t4[4:6]
assert numpy.all((t6.location == t4.location[4:6]))
allzeros = numpy.array((0.0, 0.0, 0.0), dtype=t4.location.dtype)
assert (t6.location.view(numpy.ndarray)[(- 1)] != allzeros)
assert (t4.location.view(numpy.ndarray)[5] != allzeros)
t6.location.view(numpy.ndarray)[(- 1)] = allzeros
assert (t4.location.view(numpy.ndarray)[5] == allzeros)
tempResult = arange(0.0, 0.999, 0.2)
	
===================================================================	
TestBasic.test_getitem: 143	
----------------------------	

'Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.'
mjd = numpy.arange(50000, 50010)
t = Time(mjd, format='mjd', scale='utc', location=('45d', '50d'))
t1 = t[3]
assert (t1.isscalar is True)
assert (t1._time.jd1 == t._time.jd1[3])
assert (t1.location is t.location)
t1a = Time(mjd[3], format='mjd', scale='utc')
assert (t1a.isscalar is True)
assert numpy.all((t1._time.jd1 == t1a._time.jd1))
t1b = Time(t[3])
assert (t1b.isscalar is True)
assert numpy.all((t1._time.jd1 == t1b._time.jd1))
t2 = t[4:6]
assert (t2.isscalar is False)
assert numpy.all((t2._time.jd1 == t._time.jd1[4:6]))
assert (t2.location is t.location)
t2a = Time(t[4:6])
assert (t2a.isscalar is False)
assert numpy.all((t2a._time.jd1 == t._time.jd1[4:6]))
t2b = Time([t[4], t[5]])
assert (t2b.isscalar is False)
assert numpy.all((t2b._time.jd1 == t._time.jd1[4:6]))
t2c = Time((t[4], t[5]))
assert (t2c.isscalar is False)
assert numpy.all((t2c._time.jd1 == t._time.jd1[4:6]))
t.delta_tdb_tt = numpy.arange(len(t))
t3 = t[4:6]
assert numpy.all((t3._delta_tdb_tt == t._delta_tdb_tt[4:6]))
t4 = Time(mjd, format='mjd', scale='utc', location=(numpy.arange(len(mjd)), numpy.arange(len(mjd))))
t5 = t4[3]
assert (t5.location == t4.location[3])
t6 = t4[4:6]
assert numpy.all((t6.location == t4.location[4:6]))
allzeros = numpy.array((0.0, 0.0, 0.0), dtype=t4.location.dtype)
assert (t6.location.view(numpy.ndarray)[(- 1)] != allzeros)
assert (t4.location.view(numpy.ndarray)[5] != allzeros)
t6.location.view(numpy.ndarray)[(- 1)] = allzeros
assert (t4.location.view(numpy.ndarray)[5] == allzeros)
frac = numpy.arange(0.0, 0.999, 0.2)
t7 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=('45d', '50d'))
assert (t7[(0, 0)]._time.jd1 == t7._time.jd1[(0, 0)])
assert (t7[(0, 0)].isscalar is True)
assert numpy.all((t7[5]._time.jd1 == t7._time.jd1[5]))
assert numpy.all((t7[5]._time.jd2 == t7._time.jd2[5]))
assert numpy.all((t7[:, 2]._time.jd1 == t7._time.jd1[:, 2]))
assert numpy.all((t7[:, 2]._time.jd2 == t7._time.jd2[:, 2]))
assert numpy.all((t7[:, 0]._time.jd1 == t._time.jd1))
assert numpy.all((t7[:, 0]._time.jd2 == t._time.jd2))
t7_tdb = t7.tdb
assert (t7_tdb[(0, 0)].delta_tdb_tt == t7_tdb.delta_tdb_tt[(0, 0)])
assert numpy.all((t7_tdb[5].delta_tdb_tt == t7_tdb.delta_tdb_tt[5]))
assert numpy.all((t7_tdb[:, 2].delta_tdb_tt == t7_tdb.delta_tdb_tt[:, 2]))
t7.delta_tdb_tt = 0.1
t7_tdb2 = t7.tdb
assert (t7_tdb2[(0, 0)].delta_tdb_tt == 0.1)
assert (t7_tdb2[5].delta_tdb_tt == 0.1)
assert (t7_tdb2[:, 2].delta_tdb_tt == 0.1)
tempResult = arange(len(frac))
	
===================================================================	
TestBasic.test_getitem: 143	
----------------------------	

'Test that Time objects holding arrays are properly subscriptable,\n        set isscalar as appropriate, and also subscript delta_ut1_utc, etc.'
mjd = numpy.arange(50000, 50010)
t = Time(mjd, format='mjd', scale='utc', location=('45d', '50d'))
t1 = t[3]
assert (t1.isscalar is True)
assert (t1._time.jd1 == t._time.jd1[3])
assert (t1.location is t.location)
t1a = Time(mjd[3], format='mjd', scale='utc')
assert (t1a.isscalar is True)
assert numpy.all((t1._time.jd1 == t1a._time.jd1))
t1b = Time(t[3])
assert (t1b.isscalar is True)
assert numpy.all((t1._time.jd1 == t1b._time.jd1))
t2 = t[4:6]
assert (t2.isscalar is False)
assert numpy.all((t2._time.jd1 == t._time.jd1[4:6]))
assert (t2.location is t.location)
t2a = Time(t[4:6])
assert (t2a.isscalar is False)
assert numpy.all((t2a._time.jd1 == t._time.jd1[4:6]))
t2b = Time([t[4], t[5]])
assert (t2b.isscalar is False)
assert numpy.all((t2b._time.jd1 == t._time.jd1[4:6]))
t2c = Time((t[4], t[5]))
assert (t2c.isscalar is False)
assert numpy.all((t2c._time.jd1 == t._time.jd1[4:6]))
t.delta_tdb_tt = numpy.arange(len(t))
t3 = t[4:6]
assert numpy.all((t3._delta_tdb_tt == t._delta_tdb_tt[4:6]))
t4 = Time(mjd, format='mjd', scale='utc', location=(numpy.arange(len(mjd)), numpy.arange(len(mjd))))
t5 = t4[3]
assert (t5.location == t4.location[3])
t6 = t4[4:6]
assert numpy.all((t6.location == t4.location[4:6]))
allzeros = numpy.array((0.0, 0.0, 0.0), dtype=t4.location.dtype)
assert (t6.location.view(numpy.ndarray)[(- 1)] != allzeros)
assert (t4.location.view(numpy.ndarray)[5] != allzeros)
t6.location.view(numpy.ndarray)[(- 1)] = allzeros
assert (t4.location.view(numpy.ndarray)[5] == allzeros)
frac = numpy.arange(0.0, 0.999, 0.2)
t7 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=('45d', '50d'))
assert (t7[(0, 0)]._time.jd1 == t7._time.jd1[(0, 0)])
assert (t7[(0, 0)].isscalar is True)
assert numpy.all((t7[5]._time.jd1 == t7._time.jd1[5]))
assert numpy.all((t7[5]._time.jd2 == t7._time.jd2[5]))
assert numpy.all((t7[:, 2]._time.jd1 == t7._time.jd1[:, 2]))
assert numpy.all((t7[:, 2]._time.jd2 == t7._time.jd2[:, 2]))
assert numpy.all((t7[:, 0]._time.jd1 == t._time.jd1))
assert numpy.all((t7[:, 0]._time.jd2 == t._time.jd2))
t7_tdb = t7.tdb
assert (t7_tdb[(0, 0)].delta_tdb_tt == t7_tdb.delta_tdb_tt[(0, 0)])
assert numpy.all((t7_tdb[5].delta_tdb_tt == t7_tdb.delta_tdb_tt[5]))
assert numpy.all((t7_tdb[:, 2].delta_tdb_tt == t7_tdb.delta_tdb_tt[:, 2]))
t7.delta_tdb_tt = 0.1
t7_tdb2 = t7.tdb
assert (t7_tdb2[(0, 0)].delta_tdb_tt == 0.1)
assert (t7_tdb2[5].delta_tdb_tt == 0.1)
assert (t7_tdb2[:, 2].delta_tdb_tt == 0.1)
tempResult = arange(len(frac))
	
===================================================================	
TestBasic.test_location_array: 219	
----------------------------	

'Check that location arrays are checked for size and used\n        for the corresponding times.  Also checks that erfa\n        can handle array-valued locations, and can broadcast these if needed.\n        '
lat = 19.48125
lon = (- 155.933222)
t = Time((['2006-01-15 21:24:37.5'] * 2), format='iso', scale='utc', precision=6, location=(lon, lat))
assert numpy.all((t.utc.iso == '2006-01-15 21:24:37.500000'))
assert numpy.all((t.tdb.iso[0] == '2006-01-15 21:25:42.684373'))
t2 = Time((['2006-01-15 21:24:37.5'] * 2), format='iso', scale='utc', precision=6, location=(numpy.array([lon, 0]), numpy.array([lat, 0])))
assert numpy.all((t2.utc.iso == '2006-01-15 21:24:37.500000'))
assert (t2.tdb.iso[0] == '2006-01-15 21:25:42.684373')
assert (t2.tdb.iso[1] != '2006-01-15 21:25:42.684373')
with tests.helper.pytest.raises(ValueError):
    Time('2006-01-15 21:24:37.5', format='iso', scale='utc', precision=6, location=(numpy.array([lon, 0]), numpy.array([lat, 0])))
with tests.helper.pytest.raises(ValueError):
    Time((['2006-01-15 21:24:37.5'] * 3), format='iso', scale='utc', precision=6, location=(numpy.array([lon, 0]), numpy.array([lat, 0])))
tempResult = arange(50000.0, 50008.0)
	
===================================================================	
TestTimeComparisons.setup: 11	
----------------------------	

tempResult = arange(49995, 50005)
	
===================================================================	
TestTimeComparisons.setup: 12	
----------------------------	

self.t1 = Time(numpy.arange(49995, 50005), format='mjd', scale='utc')
tempResult = arange(49000, 51000, 200)
	
===================================================================	
TestTimeDelta.setup: 21	
----------------------------	

self.t = Time('2010-01-01', scale='utc')
self.t2 = Time('2010-01-02 00:00:01', scale='utc')
self.t3 = Time('2010-01-03 01:02:03', scale='utc', precision=9, in_subfmt='date_hms', out_subfmt='date_hm', location=(((- 75.0) * units.degree), (30.0 * units.degree), (500 * units.m)))
self.dt = TimeDelta(100.0, format='sec')
tempResult = arange(100, 1000, 100)
	
===================================================================	
TestTimeDelta.test_mul_div: 135	
----------------------------	

for dt in (self.dt, self.dt_array):
    dt2 = ((dt + dt) + dt)
    dt3 = (3.0 * dt)
    assert allclose_jd(dt2.jd, dt3.jd)
    dt4 = (dt3 / 3.0)
    assert allclose_jd(dt4.jd, dt.jd)
tempResult = arange(3)
	
===================================================================	
TestArithmetic.setup: 265	
----------------------------	

tempResult = arange(50000, 50100, 10)
	
===================================================================	
TestManipulation.setup: 14	
----------------------------	

tempResult = arange(50000, 50010)
	
===================================================================	
TestManipulation.setup: 15	
----------------------------	

mjd = numpy.arange(50000, 50010)
tempResult = arange(0.0, 0.999, 0.2)
	
===================================================================	
TestManipulation.setup: 18	
----------------------------	

mjd = numpy.arange(50000, 50010)
frac = numpy.arange(0.0, 0.999, 0.2)
self.t0 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc')
self.t1 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=('45d', '50d'))
tempResult = arange(len(frac))
	
===================================================================	
TestManipulation.setup: 18	
----------------------------	

mjd = numpy.arange(50000, 50010)
frac = numpy.arange(0.0, 0.999, 0.2)
self.t0 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc')
self.t1 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=('45d', '50d'))
tempResult = arange(len(frac))
	
===================================================================	
TestManipulation.setup: 19	
----------------------------	

mjd = numpy.arange(50000, 50010)
frac = numpy.arange(0.0, 0.999, 0.2)
self.t0 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc')
self.t1 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=('45d', '50d'))
self.t2 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=(numpy.arange(len(frac)), numpy.arange(len(frac))))
tempResult = arange(len(frac))
	
===================================================================	
TestManipulation.setup: 19	
----------------------------	

mjd = numpy.arange(50000, 50010)
frac = numpy.arange(0.0, 0.999, 0.2)
self.t0 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc')
self.t1 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=('45d', '50d'))
self.t2 = Time((mjd[:, numpy.newaxis] + frac), format='mjd', scale='utc', location=(numpy.arange(len(frac)), numpy.arange(len(frac))))
tempResult = arange(len(frac))
	
===================================================================	
TestArithmetic.test_argsort: 305	
----------------------------	

assert numpy.all((self.t0.argsort() == numpy.array([2, 0, 1, 4, 3])))
tempResult = arange(2)
	
===================================================================	
TestArithmetic.test_argsort: 306	
----------------------------	

assert numpy.all((self.t0.argsort() == numpy.array([2, 0, 1, 4, 3])))
assert numpy.all((self.t0.argsort(axis=0) == np.arange(2).reshape(2, 1, 1)))
tempResult = arange(5)
	
===================================================================	
TestArithmetic.test_argsort: 308	
----------------------------	

assert numpy.all((self.t0.argsort() == numpy.array([2, 0, 1, 4, 3])))
assert numpy.all((self.t0.argsort(axis=0) == np.arange(2).reshape(2, 1, 1)))
assert numpy.all((self.t0.argsort(axis=1) == np.arange(5).reshape(5, 1)))
assert numpy.all((self.t0.argsort(axis=2) == numpy.array([2, 0, 1, 4, 3])))
tempResult = arange(50)
	
===================================================================	
TestTimeDeltaQuantity.test_valid_quantity_operations2: 155	
----------------------------	

'Check that TimeDelta is treated as a quantity where possible.'
t0 = TimeDelta(100000.0, format='sec')
f = (1.0 / t0)
assert isinstance(f, units.Quantity)
assert (f.unit == (1.0 / units.day))
g = ((10.0 * units.m) / (units.second ** 2))
v = (t0 * g)
assert isinstance(v, units.Quantity)
assert (v.decompose().unit == (units.m / units.second))
q = numpy.log10((t0 / units.second))
assert isinstance(q, units.Quantity)
assert (q.value == numpy.log10(t0.sec))
s = (1.0 * units.m)
v = (s / t0)
assert isinstance(v, units.Quantity)
assert (v.decompose().unit == (units.m / units.second))
tempResult = arange(100000.0, 100012.0)
	
===================================================================	
TestTimeQuantity.test_valid_quantity_operations: 80	
----------------------------	

'Check that adding a time-valued quantity to a Time gives a Time'
t0 = Time(100000.0, format='cxcsec')
q1 = (10.0 * units.second)
t1 = (t0 + q1)
assert isinstance(t1, Time)
assert (t1.value == (t0.value + q1.to(u.second).value))
q2 = (1.0 * units.day)
t2 = (t0 - q2)
assert allclose_sec(t2.value, (t0.value - q2.to(u.second).value))
tempResult = arange(15.0)
	
===================================================================	
TestTimeDeltaQuantity.test_invalid_quantity_broadcast: 169	
----------------------------	

'Check broadcasting rules in interactions with Quantity.'
tempResult = arange(12.0)
	
===================================================================	
TestTimeDeltaQuantity.test_invalid_quantity_broadcast: 171	
----------------------------	

'Check broadcasting rules in interactions with Quantity.'
t0 = TimeDelta(np.arange(12.0).reshape(4, 3), format='sec')
with tests.helper.pytest.raises(ValueError):
    tempResult = arange(4.0)
	
===================================================================	
TestTimeQuantity.test_column_with_and_without_units: 51	
----------------------------	

'Ensure a Column without a unit is treated as an array [#3648]'
tempResult = arange(50000.0, 50010.0)
	
===================================================================	
TestTimeQuantity.test_column_with_and_without_units: 53	
----------------------------	

'Ensure a Column without a unit is treated as an array [#3648]'
a = numpy.arange(50000.0, 50010.0)
ta = Time(a, format='mjd')
tempResult = arange(50000.0, 50010.0)
	
===================================================================	
TestTimeQuantity.test_column_with_and_without_units: 56	
----------------------------	

'Ensure a Column without a unit is treated as an array [#3648]'
a = numpy.arange(50000.0, 50010.0)
ta = Time(a, format='mjd')
c1 = Column(numpy.arange(50000.0, 50010.0), name='mjd')
tc1 = Time(c1, format='mjd')
assert numpy.all((ta == tc1))
tempResult = arange(50000.0, 50010.0)
	
===================================================================	
TestTimeQuantity.test_column_with_and_without_units: 59	
----------------------------	

'Ensure a Column without a unit is treated as an array [#3648]'
a = numpy.arange(50000.0, 50010.0)
ta = Time(a, format='mjd')
c1 = Column(numpy.arange(50000.0, 50010.0), name='mjd')
tc1 = Time(c1, format='mjd')
assert numpy.all((ta == tc1))
c2 = Column(numpy.arange(50000.0, 50010.0), name='mjd', unit='day')
tc2 = Time(c2, format='mjd')
assert numpy.all((ta == tc2))
tempResult = arange(50000.0, 50010.0)
	
===================================================================	
TestTimeDeltaQuantity.test_valid_quantity_operations1: 133	
----------------------------	

'Check adding/substracting/comparing a time-valued quantity works\n        with a TimeDelta.  Addition/subtraction should give TimeDelta'
t0 = TimeDelta(106400.0, format='sec')
q1 = (10.0 * units.second)
t1 = (t0 + q1)
assert isinstance(t1, TimeDelta)
assert (t1.value == (t0.value + q1.to(u.second).value))
q2 = (1.0 * units.day)
t2 = (t0 - q2)
assert allclose_sec(t2.value, (t0.value - q2.to(u.second).value))
assert (t0 > q1)
assert (t0 < (1.0 * units.yr))
tempResult = arange(12.0)
	
===================================================================	
TestLogQuantityViews.setup: 404	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestLogQuantityViews.setup: 405	
----------------------------	

self.lq = units.Magnitude((numpy.arange(10.0) * units.Jy))
tempResult = arange(5.0)
	
===================================================================	
TestLogQuantityArithmetic.test_multiplication_division: 462	
----------------------------	

'Check that multiplication/division with other quantities is only\n        possible when the physical unit is dimensionless, and that this turns\n        the result into a normal quantity.'
tempResult = arange(1.0, 11.0)
	
===================================================================	
TestLogQuantityArithmetic.test_multiplication_division: 472	
----------------------------	

'Check that multiplication/division with other quantities is only\n        possible when the physical unit is dimensionless, and that this turns\n        the result into a normal quantity.'
lq = units.Magnitude((numpy.arange(1.0, 11.0) * units.Jy))
with tests.helper.pytest.raises(units.UnitsError):
    (lq * (1.0 * units.m))
with tests.helper.pytest.raises(units.UnitsError):
    ((1.0 * units.m) * lq)
with tests.helper.pytest.raises(units.UnitsError):
    (lq / lq)
for unit in (units.m, units.mag, units.dex):
    with tests.helper.pytest.raises(units.UnitsError):
        (lq / unit)
tempResult = arange(1, 11.0)
	
===================================================================	
TestLogQuantityMethods.setup: 618	
----------------------------	

tempResult = arange(1.0, 5.0)
	
===================================================================	
TestLogQuantityMethods.setup: 619	
----------------------------	

self.mJy = (np.arange(1.0, 5.0).reshape(2, 2) * units.mag(units.Jy))
tempResult = arange(1.0, 5.5, 0.5)
	
===================================================================	
TestLogQuantityComparisons.test_comparison_to_non_quantities_fails: 582	
----------------------------	

tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogQuantityArithmetic.test_addition_subtraction: 533	
----------------------------	

'Check that addition/subtraction with quantities with magnitude or\n        MagUnit units works, and that it changes the physical units\n        appropriately.'
tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogQuantityArithmetic.test_addition_subtraction_to_normal_units_fails: 521	
----------------------------	

tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogQuantitySlicing.test_item_get_and_set: 434	
----------------------------	

tempResult = arange(1.0, 11.0)
	
===================================================================	
TestLogQuantityArithmetic.test_inplace_addition_subtraction: 560	
----------------------------	

'Check that inplace addition/subtraction with quantities with\n        magnitude or MagUnit units works, and that it changes the physical\n        units appropriately.'
tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogUnitArithmetic.test_raise_to_power: 255	
----------------------------	

'Check that raising LogUnits to some power is only possible when the\n        physical unit is dimensionless, and that conversion is turned off when\n        the resulting logarithmic unit (such as mag**2) is incompatible.'
lu1 = units.mag(units.Jy)
if (power == 0):
    assert ((lu1 ** power) == units.dimensionless_unscaled)
elif (power == 1):
    assert ((lu1 ** power) == lu1)
else:
    with tests.helper.pytest.raises(units.UnitsError):
        (lu1 ** power)
lu2 = units.mag(units.dimensionless_unscaled)
t = (lu2 ** power)
if (power == 0):
    assert (t == units.dimensionless_unscaled)
elif (power == 1):
    assert (t == lu2)
else:
    assert (not isinstance(t, type(lu2)))
    assert (t == (lu2.function_unit ** power))
    t2 = (t ** (1.0 / power))
    assert (t2 == lu2.function_unit)
    with units.set_enabled_equivalencies(units.logarithmic()):
        tempResult = arange(3.0)
	
===================================================================	
TestLogUnitArithmetic.test_raise_to_power: 255	
----------------------------	

'Check that raising LogUnits to some power is only possible when the\n        physical unit is dimensionless, and that conversion is turned off when\n        the resulting logarithmic unit (such as mag**2) is incompatible.'
lu1 = units.mag(units.Jy)
if (power == 0):
    assert ((lu1 ** power) == units.dimensionless_unscaled)
elif (power == 1):
    assert ((lu1 ** power) == lu1)
else:
    with tests.helper.pytest.raises(units.UnitsError):
        (lu1 ** power)
lu2 = units.mag(units.dimensionless_unscaled)
t = (lu2 ** power)
if (power == 0):
    assert (t == units.dimensionless_unscaled)
elif (power == 1):
    assert (t == lu2)
else:
    assert (not isinstance(t, type(lu2)))
    assert (t == (lu2.function_unit ** power))
    t2 = (t ** (1.0 / power))
    assert (t2 == lu2.function_unit)
    with units.set_enabled_equivalencies(units.logarithmic()):
        tempResult = arange(3.0)
	
===================================================================	
TestLogQuantityUfuncs.setup: 660	
----------------------------	

tempResult = arange(1.0, 5.0)
	
===================================================================	
TestLogQuantityUfuncs.setup: 661	
----------------------------	

self.mJy = (np.arange(1.0, 5.0).reshape(2, 2) * units.mag(units.Jy))
tempResult = arange(1.0, 5.5, 0.5)
	
===================================================================	
TestLogQuantityArithmetic.test_raise_to_power: 497	
----------------------------	

'Check that raising LogQuantities to some power is only possible when\n        the physical unit is dimensionless, and that conversion is turned off\n        when the resulting logarithmic unit (say, mag**2) is incompatible.'
tempResult = arange(1.0, 4.0)
	
===================================================================	
TestLogQuantityArithmetic.test_raise_to_power: 505	
----------------------------	

'Check that raising LogQuantities to some power is only possible when\n        the physical unit is dimensionless, and that conversion is turned off\n        when the resulting logarithmic unit (say, mag**2) is incompatible.'
lq = units.Magnitude((numpy.arange(1.0, 4.0) * units.Jy))
if (power == 0):
    assert numpy.all(((lq ** power) == 1.0))
elif (power == 1):
    assert numpy.all(((lq ** power) == lq))
else:
    with tests.helper.pytest.raises(units.UnitsError):
        (lq ** power)
tempResult = arange(10.0)
	
===================================================================	
TestLogQuantitySlicing.test_slice_get_and_set: 447	
----------------------------	

tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogUnitArithmetic.test_multiplication_division: 216	
----------------------------	

'Check that multiplication/division with other units is only\n        possible when the physical unit is dimensionless, and that this\n        turns the unit into a normal one.'
lu1 = units.mag(units.Jy)
with tests.helper.pytest.raises(units.UnitsError):
    (lu1 * units.m)
with tests.helper.pytest.raises(units.UnitsError):
    (units.m * lu1)
with tests.helper.pytest.raises(units.UnitsError):
    (lu1 / lu1)
for unit in (units.dimensionless_unscaled, units.m, units.mag, units.dex):
    with tests.helper.pytest.raises(units.UnitsError):
        (lu1 / unit)
lu2 = units.mag(units.dimensionless_unscaled)
with tests.helper.pytest.raises(units.UnitsError):
    (lu2 * lu1)
with tests.helper.pytest.raises(units.UnitsError):
    (lu2 / lu1)
assert ((lu2 / lu2) == units.dimensionless_unscaled)
tf = (lu2 * units.m)
tr = (units.m * lu2)
for t in (tf, tr):
    assert (not isinstance(t, type(lu2)))
    assert (t == (lu2.function_unit * units.m))
    with units.set_enabled_equivalencies(units.logarithmic()):
        with tests.helper.pytest.raises(units.UnitsError):
            t.to(lu2.physical_unit)
t = (tf / units.cm)
with units.set_enabled_equivalencies(units.logarithmic()):
    assert t.is_equivalent(lu2.function_unit)
    tempResult = arange(3.0)
	
===================================================================	
TestLogUnitArithmetic.test_multiplication_division: 216	
----------------------------	

'Check that multiplication/division with other units is only\n        possible when the physical unit is dimensionless, and that this\n        turns the unit into a normal one.'
lu1 = units.mag(units.Jy)
with tests.helper.pytest.raises(units.UnitsError):
    (lu1 * units.m)
with tests.helper.pytest.raises(units.UnitsError):
    (units.m * lu1)
with tests.helper.pytest.raises(units.UnitsError):
    (lu1 / lu1)
for unit in (units.dimensionless_unscaled, units.m, units.mag, units.dex):
    with tests.helper.pytest.raises(units.UnitsError):
        (lu1 / unit)
lu2 = units.mag(units.dimensionless_unscaled)
with tests.helper.pytest.raises(units.UnitsError):
    (lu2 * lu1)
with tests.helper.pytest.raises(units.UnitsError):
    (lu2 / lu1)
assert ((lu2 / lu2) == units.dimensionless_unscaled)
tf = (lu2 * units.m)
tr = (units.m * lu2)
for t in (tf, tr):
    assert (not isinstance(t, type(lu2)))
    assert (t == (lu2.function_unit * units.m))
    with units.set_enabled_equivalencies(units.logarithmic()):
        with tests.helper.pytest.raises(units.UnitsError):
            t.to(lu2.physical_unit)
t = (tf / units.cm)
with units.set_enabled_equivalencies(units.logarithmic()):
    assert t.is_equivalent(lu2.function_unit)
    tempResult = arange(3.0)
	
===================================================================	
TestLogQuantityCreation.test_subclass_creation: 331	
----------------------------	

'Create LogQuantity subclass objects for some physical units,\n        and basic check on transformations'
tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogQuantityComparisons.test_comparison: 590	
----------------------------	

tempResult = arange(1.0, 4.0)
	
===================================================================	
TestLogQuantityComparisons.test_comparison: 609	
----------------------------	

lq1 = units.Magnitude((numpy.arange(1.0, 4.0) * units.Jy))
lq2 = units.Magnitude((2.0 * units.Jy))
assert numpy.all(((lq1 > lq2) == numpy.array([True, False, False])))
assert numpy.all(((lq1 == lq2) == numpy.array([False, True, False])))
lq3 = units.Dex((2.0 * units.Jy))
assert numpy.all(((lq1 > lq3) == numpy.array([True, False, False])))
assert numpy.all(((lq1 == lq3) == numpy.array([False, True, False])))
lq4 = units.Magnitude((2.0 * units.m))
assert (not (lq1 == lq4))
assert (lq1 != lq4)
with tests.helper.pytest.raises(units.UnitsError):
    (lq1 < lq4)
q5 = (1.5 * units.Jy)
assert numpy.all(((lq1 > q5) == numpy.array([True, False, False])))
assert numpy.all(((q5 < lq1) == numpy.array([True, False, False])))
with tests.helper.pytest.raises(units.UnitsError):
    (lq1 >= (2.0 * units.m))
with tests.helper.pytest.raises(units.UnitsError):
    (lq1 <= (lq1.value * units.mag))
tempResult = arange(1.0, 4.0)
	
===================================================================	
TestLogQuantityArithmetic.test_inplace_addition_subtraction_unit_checks: 547	
----------------------------	

lu1 = units.mag(units.Jy)
tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogQuantityArithmetic.test_inplace_addition_subtraction_unit_checks: 550	
----------------------------	

lu1 = units.mag(units.Jy)
lq1 = units.Magnitude(numpy.arange(1.0, 10.0), lu1)
with tests.helper.pytest.raises(units.UnitsError):
    lq1 += other
tempResult = arange(1.0, 10.0)
	
===================================================================	
TestLogQuantityArithmetic.test_inplace_addition_subtraction_unit_checks: 554	
----------------------------	

lu1 = units.mag(units.Jy)
lq1 = units.Magnitude(numpy.arange(1.0, 10.0), lu1)
with tests.helper.pytest.raises(units.UnitsError):
    lq1 += other
assert numpy.all((lq1.value == numpy.arange(1.0, 10.0)))
assert (lq1.unit == lu1)
with tests.helper.pytest.raises(units.UnitsError):
    lq1 -= other
tempResult = arange(1.0, 10.0)
	
===================================================================	
test_unsupported: 823	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestQuantityCreation.test_copy: 81	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestQuantityCreation.test_non_quantity_with_unit: 148	
----------------------------	

'Test that unit attributes in objects get recognized.'

class MyQuantityLookalike(numpy.ndarray):
    pass
tempResult = arange(3.0)
	
===================================================================	
TestQuantityDisplay.test_uninitialized_unit_format: 530	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
test_quantity_from_table: 853	
----------------------------	

'\n    Checks that units from tables are respected when converted to a Quantity.\n    This also generically checks the use of *anything* with a `unit` attribute\n    passed into Quantity\n    '
from ...table import Table
tempResult = arange(5)
	
===================================================================	
test_quantity_from_table: 853	
----------------------------	

'\n    Checks that units from tables are respected when converted to a Quantity.\n    This also generically checks the use of *anything* with a `unit` attribute\n    passed into Quantity\n    '
from ...table import Table
tempResult = arange(5)
	
===================================================================	
TestSpecificTypeQuantity.test_view: 925	
----------------------------	

tempResult = arange(5.0)
	
===================================================================	
TestSpecificTypeQuantity.test_view: 928	
----------------------------	

l = (np.arange(5.0) * u.km).view(self.Length)
assert (type(l) is self.Length)
with tests.helper.pytest.raises(units.UnitTypeError):
    tempResult = arange(5.0)
	
===================================================================	
TestSpecificTypeQuantity.test_view: 929	
----------------------------	

l = (np.arange(5.0) * u.km).view(self.Length)
assert (type(l) is self.Length)
with tests.helper.pytest.raises(units.UnitTypeError):
    (np.arange(5.0) * u.s).view(self.Length)
tempResult = arange(5.0)
	
===================================================================	
test_quantity_pickelability: 770	
----------------------------	

'\n    Testing pickleability of quantity\n    '
tempResult = arange(10)
	
===================================================================	
TestSpecificTypeQuantity.test_operation_precedence_and_fallback: 937	
----------------------------	

tempResult = arange(5.0)
	
===================================================================	
TestSpecificTypeQuantity.test_creation: 912	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestSpecificTypeQuantity.test_creation: 915	
----------------------------	

l = self.Length((numpy.arange(10.0) * units.km))
assert (type(l) is self.Length)
with tests.helper.pytest.raises(units.UnitTypeError):
    tempResult = arange(10.0)
	
===================================================================	
TestSpecificTypeQuantity.test_creation: 917	
----------------------------	

l = self.Length((numpy.arange(10.0) * units.km))
assert (type(l) is self.Length)
with tests.helper.pytest.raises(units.UnitTypeError):
    self.Length((numpy.arange(10.0) * units.hour))
with tests.helper.pytest.raises(units.UnitTypeError):
    tempResult = arange(10.0)
	
===================================================================	
TestSpecificTypeQuantity.test_creation: 918	
----------------------------	

l = self.Length((numpy.arange(10.0) * units.km))
assert (type(l) is self.Length)
with tests.helper.pytest.raises(units.UnitTypeError):
    self.Length((numpy.arange(10.0) * units.hour))
with tests.helper.pytest.raises(units.UnitTypeError):
    self.Length(numpy.arange(10.0))
tempResult = arange(5.0)
	
===================================================================	
TestSpecificTypeQuantity.test_creation: 922	
----------------------------	

l = self.Length((numpy.arange(10.0) * units.km))
assert (type(l) is self.Length)
with tests.helper.pytest.raises(units.UnitTypeError):
    self.Length((numpy.arange(10.0) * units.hour))
with tests.helper.pytest.raises(units.UnitTypeError):
    self.Length(numpy.arange(10.0))
l2 = self.Length2(numpy.arange(5.0))
assert (type(l2) is self.Length2)
assert (l2._default_unit is self.Length2._default_unit)
with tests.helper.pytest.raises(units.UnitTypeError):
    tempResult = arange(10.0)
	
===================================================================	
TestQuantityCreation.test_order: 112	
----------------------------	

'Test that order is correctly propagated to np.array'
tempResult = arange(10.0)
	
===================================================================	
TestQuantityCreation.test_order: 122	
----------------------------	

'Test that order is correctly propagated to np.array'
ac = numpy.array(numpy.arange(10.0), order='C')
qcc = units.Quantity(ac, units.m, order='C')
assert qcc.flags['C_CONTIGUOUS']
qcf = units.Quantity(ac, units.m, order='F')
assert qcf.flags['F_CONTIGUOUS']
qca = units.Quantity(ac, units.m, order='A')
assert qca.flags['C_CONTIGUOUS']
assert u.Quantity(qcc, order='C').flags['C_CONTIGUOUS']
assert u.Quantity(qcc, order='A').flags['C_CONTIGUOUS']
assert u.Quantity(qcc, order='F').flags['F_CONTIGUOUS']
tempResult = arange(10.0)
	
===================================================================	
TestQuantityCreation.test_ndmin: 135	
----------------------------	

'Test that ndmin is correctly propagated to np.array'
tempResult = arange(10.0)
	
===================================================================	
TestQuantityDisplay.test_repr_latex: 541	
----------------------------	

from ...units.quantity import conf
q2scalar = units.Quantity(150000000000000.0, 'm/s')
assert (self.scalarintq._repr_latex_() == '$1 \\; \\mathrm{m}$')
assert (self.scalarfloatq._repr_latex_() == '$1.3 \\; \\mathrm{m}$')
assert (q2scalar._repr_latex_() == '$1.5 \\times 10^{14} \\; \\mathrm{\\frac{m}{s}}$')
assert (self.arrq._repr_latex_() == '$[1,~2.3,~8.9] \\; \\mathrm{m}$')
tempResult = arange(100)
	
===================================================================	
TestQuantityDisplay.test_repr_latex: 542	
----------------------------	

from ...units.quantity import conf
q2scalar = units.Quantity(150000000000000.0, 'm/s')
assert (self.scalarintq._repr_latex_() == '$1 \\; \\mathrm{m}$')
assert (self.scalarfloatq._repr_latex_() == '$1.3 \\; \\mathrm{m}$')
assert (q2scalar._repr_latex_() == '$1.5 \\times 10^{14} \\; \\mathrm{\\frac{m}{s}}$')
assert (self.arrq._repr_latex_() == '$[1,~2.3,~8.9] \\; \\mathrm{m}$')
qmed = (numpy.arange(100) * units.m)
tempResult = arange(1000)
	
===================================================================	
TestQuantityDisplay.test_repr_latex: 543	
----------------------------	

from ...units.quantity import conf
q2scalar = units.Quantity(150000000000000.0, 'm/s')
assert (self.scalarintq._repr_latex_() == '$1 \\; \\mathrm{m}$')
assert (self.scalarfloatq._repr_latex_() == '$1.3 \\; \\mathrm{m}$')
assert (q2scalar._repr_latex_() == '$1.5 \\times 10^{14} \\; \\mathrm{\\frac{m}{s}}$')
assert (self.arrq._repr_latex_() == '$[1,~2.3,~8.9] \\; \\mathrm{m}$')
qmed = (numpy.arange(100) * units.m)
qbig = (numpy.arange(1000) * units.m)
tempResult = arange(10000)
	
===================================================================	
test_arrays: 588	
----------------------------	

'\n    Test using quantites with array values\n    '
tempResult = arange(10)
	
===================================================================	
test_arrays: 631	
----------------------------	

'\n    Test using quantites with array values\n    '
qsec = units.Quantity(numpy.arange(10), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert (not qsec.isscalar)
assert (len(qsec) == len(qsec.value))
qsecsub25 = qsec[2:5]
assert (qsecsub25.unit == qsec.unit)
assert isinstance(qsecsub25, units.Quantity)
assert (len(qsecsub25) == 3)
qsecnotarray = units.Quantity(10.0, units.second)
assert qsecnotarray.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qsecnotarray)
with tests.helper.pytest.raises(TypeError):
    qsecnotarray[0]
qseclen0array = units.Quantity(numpy.array(10), units.second, dtype=int)
assert qseclen0array.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qseclen0array)
with tests.helper.pytest.raises(TypeError):
    qseclen0array[0]
assert isinstance(qseclen0array.value, int)
a = numpy.array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0), (7.0, 8.0, 9.0)], dtype=[(str('x'), numpy.float), (str('y'), numpy.float), (str('z'), numpy.float)])
qkpc = units.Quantity(a, units.kpc)
assert (not qkpc.isscalar)
qkpc0 = qkpc[0]
assert (qkpc0.value == a[0].item())
assert (qkpc0.unit == qkpc.unit)
assert isinstance(qkpc0, units.Quantity)
assert qkpc0.isscalar
qkpcx = qkpc['x']
assert numpy.all((qkpcx.value == a['x']))
assert (qkpcx.unit == qkpc.unit)
assert isinstance(qkpcx, units.Quantity)
assert (not qkpcx.isscalar)
qkpcx1 = qkpc['x'][1]
assert (qkpcx1.unit == qkpc.unit)
assert isinstance(qkpcx1, units.Quantity)
assert qkpcx1.isscalar
qkpc1x = qkpc[1]['x']
assert qkpc1x.isscalar
assert (qkpc1x == qkpcx1)
qsec = units.Quantity(list(range(10)), units.second)
assert isinstance(qsec.value, numpy.ndarray)
tempResult = arange(10)
	
===================================================================	
test_arrays: 632	
----------------------------	

'\n    Test using quantites with array values\n    '
qsec = units.Quantity(numpy.arange(10), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert (not qsec.isscalar)
assert (len(qsec) == len(qsec.value))
qsecsub25 = qsec[2:5]
assert (qsecsub25.unit == qsec.unit)
assert isinstance(qsecsub25, units.Quantity)
assert (len(qsecsub25) == 3)
qsecnotarray = units.Quantity(10.0, units.second)
assert qsecnotarray.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qsecnotarray)
with tests.helper.pytest.raises(TypeError):
    qsecnotarray[0]
qseclen0array = units.Quantity(numpy.array(10), units.second, dtype=int)
assert qseclen0array.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qseclen0array)
with tests.helper.pytest.raises(TypeError):
    qseclen0array[0]
assert isinstance(qseclen0array.value, int)
a = numpy.array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0), (7.0, 8.0, 9.0)], dtype=[(str('x'), numpy.float), (str('y'), numpy.float), (str('z'), numpy.float)])
qkpc = units.Quantity(a, units.kpc)
assert (not qkpc.isscalar)
qkpc0 = qkpc[0]
assert (qkpc0.value == a[0].item())
assert (qkpc0.unit == qkpc.unit)
assert isinstance(qkpc0, units.Quantity)
assert qkpc0.isscalar
qkpcx = qkpc['x']
assert numpy.all((qkpcx.value == a['x']))
assert (qkpcx.unit == qkpc.unit)
assert isinstance(qkpcx, units.Quantity)
assert (not qkpcx.isscalar)
qkpcx1 = qkpc['x'][1]
assert (qkpcx1.unit == qkpc.unit)
assert isinstance(qkpcx1, units.Quantity)
assert qkpcx1.isscalar
qkpc1x = qkpc[1]['x']
assert qkpc1x.isscalar
assert (qkpc1x == qkpcx1)
qsec = units.Quantity(list(range(10)), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert_array_equal((qsec * 2).value, (numpy.arange(10) * 2))
tempResult = arange(10)
	
===================================================================	
test_arrays: 634	
----------------------------	

'\n    Test using quantites with array values\n    '
qsec = units.Quantity(numpy.arange(10), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert (not qsec.isscalar)
assert (len(qsec) == len(qsec.value))
qsecsub25 = qsec[2:5]
assert (qsecsub25.unit == qsec.unit)
assert isinstance(qsecsub25, units.Quantity)
assert (len(qsecsub25) == 3)
qsecnotarray = units.Quantity(10.0, units.second)
assert qsecnotarray.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qsecnotarray)
with tests.helper.pytest.raises(TypeError):
    qsecnotarray[0]
qseclen0array = units.Quantity(numpy.array(10), units.second, dtype=int)
assert qseclen0array.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qseclen0array)
with tests.helper.pytest.raises(TypeError):
    qseclen0array[0]
assert isinstance(qseclen0array.value, int)
a = numpy.array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0), (7.0, 8.0, 9.0)], dtype=[(str('x'), numpy.float), (str('y'), numpy.float), (str('z'), numpy.float)])
qkpc = units.Quantity(a, units.kpc)
assert (not qkpc.isscalar)
qkpc0 = qkpc[0]
assert (qkpc0.value == a[0].item())
assert (qkpc0.unit == qkpc.unit)
assert isinstance(qkpc0, units.Quantity)
assert qkpc0.isscalar
qkpcx = qkpc['x']
assert numpy.all((qkpcx.value == a['x']))
assert (qkpcx.unit == qkpc.unit)
assert isinstance(qkpcx, units.Quantity)
assert (not qkpcx.isscalar)
qkpcx1 = qkpc['x'][1]
assert (qkpcx1.unit == qkpc.unit)
assert isinstance(qkpcx1, units.Quantity)
assert qkpcx1.isscalar
qkpc1x = qkpc[1]['x']
assert qkpc1x.isscalar
assert (qkpc1x == qkpcx1)
qsec = units.Quantity(list(range(10)), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert_array_equal((qsec * 2).value, (numpy.arange(10) * 2))
assert_array_equal((qsec / 2).value, (numpy.arange(10) / 2))
with tests.helper.pytest.raises(units.UnitsError):
    tempResult = arange(10)
	
===================================================================	
test_arrays: 636	
----------------------------	

'\n    Test using quantites with array values\n    '
qsec = units.Quantity(numpy.arange(10), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert (not qsec.isscalar)
assert (len(qsec) == len(qsec.value))
qsecsub25 = qsec[2:5]
assert (qsecsub25.unit == qsec.unit)
assert isinstance(qsecsub25, units.Quantity)
assert (len(qsecsub25) == 3)
qsecnotarray = units.Quantity(10.0, units.second)
assert qsecnotarray.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qsecnotarray)
with tests.helper.pytest.raises(TypeError):
    qsecnotarray[0]
qseclen0array = units.Quantity(numpy.array(10), units.second, dtype=int)
assert qseclen0array.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qseclen0array)
with tests.helper.pytest.raises(TypeError):
    qseclen0array[0]
assert isinstance(qseclen0array.value, int)
a = numpy.array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0), (7.0, 8.0, 9.0)], dtype=[(str('x'), numpy.float), (str('y'), numpy.float), (str('z'), numpy.float)])
qkpc = units.Quantity(a, units.kpc)
assert (not qkpc.isscalar)
qkpc0 = qkpc[0]
assert (qkpc0.value == a[0].item())
assert (qkpc0.unit == qkpc.unit)
assert isinstance(qkpc0, units.Quantity)
assert qkpc0.isscalar
qkpcx = qkpc['x']
assert numpy.all((qkpcx.value == a['x']))
assert (qkpcx.unit == qkpc.unit)
assert isinstance(qkpcx, units.Quantity)
assert (not qkpcx.isscalar)
qkpcx1 = qkpc['x'][1]
assert (qkpcx1.unit == qkpc.unit)
assert isinstance(qkpcx1, units.Quantity)
assert qkpcx1.isscalar
qkpc1x = qkpc[1]['x']
assert qkpc1x.isscalar
assert (qkpc1x == qkpcx1)
qsec = units.Quantity(list(range(10)), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert_array_equal((qsec * 2).value, (numpy.arange(10) * 2))
assert_array_equal((qsec / 2).value, (numpy.arange(10) / 2))
with tests.helper.pytest.raises(units.UnitsError):
    assert_array_equal((qsec + 2).value, (numpy.arange(10) + 2))
with tests.helper.pytest.raises(units.UnitsError):
    tempResult = arange(10)
	
===================================================================	
test_arrays: 637	
----------------------------	

'\n    Test using quantites with array values\n    '
qsec = units.Quantity(numpy.arange(10), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert (not qsec.isscalar)
assert (len(qsec) == len(qsec.value))
qsecsub25 = qsec[2:5]
assert (qsecsub25.unit == qsec.unit)
assert isinstance(qsecsub25, units.Quantity)
assert (len(qsecsub25) == 3)
qsecnotarray = units.Quantity(10.0, units.second)
assert qsecnotarray.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qsecnotarray)
with tests.helper.pytest.raises(TypeError):
    qsecnotarray[0]
qseclen0array = units.Quantity(numpy.array(10), units.second, dtype=int)
assert qseclen0array.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qseclen0array)
with tests.helper.pytest.raises(TypeError):
    qseclen0array[0]
assert isinstance(qseclen0array.value, int)
a = numpy.array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0), (7.0, 8.0, 9.0)], dtype=[(str('x'), numpy.float), (str('y'), numpy.float), (str('z'), numpy.float)])
qkpc = units.Quantity(a, units.kpc)
assert (not qkpc.isscalar)
qkpc0 = qkpc[0]
assert (qkpc0.value == a[0].item())
assert (qkpc0.unit == qkpc.unit)
assert isinstance(qkpc0, units.Quantity)
assert qkpc0.isscalar
qkpcx = qkpc['x']
assert numpy.all((qkpcx.value == a['x']))
assert (qkpcx.unit == qkpc.unit)
assert isinstance(qkpcx, units.Quantity)
assert (not qkpcx.isscalar)
qkpcx1 = qkpc['x'][1]
assert (qkpcx1.unit == qkpc.unit)
assert isinstance(qkpcx1, units.Quantity)
assert qkpcx1.isscalar
qkpc1x = qkpc[1]['x']
assert qkpc1x.isscalar
assert (qkpc1x == qkpcx1)
qsec = units.Quantity(list(range(10)), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert_array_equal((qsec * 2).value, (numpy.arange(10) * 2))
assert_array_equal((qsec / 2).value, (numpy.arange(10) / 2))
with tests.helper.pytest.raises(units.UnitsError):
    assert_array_equal((qsec + 2).value, (numpy.arange(10) + 2))
with tests.helper.pytest.raises(units.UnitsError):
    assert_array_equal((qsec - 2).value, (numpy.arange(10) + 2))
tempResult = arange(10)
	
===================================================================	
test_arrays: 638	
----------------------------	

'\n    Test using quantites with array values\n    '
qsec = units.Quantity(numpy.arange(10), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert (not qsec.isscalar)
assert (len(qsec) == len(qsec.value))
qsecsub25 = qsec[2:5]
assert (qsecsub25.unit == qsec.unit)
assert isinstance(qsecsub25, units.Quantity)
assert (len(qsecsub25) == 3)
qsecnotarray = units.Quantity(10.0, units.second)
assert qsecnotarray.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qsecnotarray)
with tests.helper.pytest.raises(TypeError):
    qsecnotarray[0]
qseclen0array = units.Quantity(numpy.array(10), units.second, dtype=int)
assert qseclen0array.isscalar
with tests.helper.pytest.raises(TypeError):
    len(qseclen0array)
with tests.helper.pytest.raises(TypeError):
    qseclen0array[0]
assert isinstance(qseclen0array.value, int)
a = numpy.array([(1.0, 2.0, 3.0), (4.0, 5.0, 6.0), (7.0, 8.0, 9.0)], dtype=[(str('x'), numpy.float), (str('y'), numpy.float), (str('z'), numpy.float)])
qkpc = units.Quantity(a, units.kpc)
assert (not qkpc.isscalar)
qkpc0 = qkpc[0]
assert (qkpc0.value == a[0].item())
assert (qkpc0.unit == qkpc.unit)
assert isinstance(qkpc0, units.Quantity)
assert qkpc0.isscalar
qkpcx = qkpc['x']
assert numpy.all((qkpcx.value == a['x']))
assert (qkpcx.unit == qkpc.unit)
assert isinstance(qkpcx, units.Quantity)
assert (not qkpcx.isscalar)
qkpcx1 = qkpc['x'][1]
assert (qkpcx1.unit == qkpc.unit)
assert isinstance(qkpcx1, units.Quantity)
assert qkpcx1.isscalar
qkpc1x = qkpc[1]['x']
assert qkpc1x.isscalar
assert (qkpc1x == qkpcx1)
qsec = units.Quantity(list(range(10)), units.second)
assert isinstance(qsec.value, numpy.ndarray)
assert_array_equal((qsec * 2).value, (numpy.arange(10) * 2))
assert_array_equal((qsec / 2).value, (numpy.arange(10) / 2))
with tests.helper.pytest.raises(units.UnitsError):
    assert_array_equal((qsec + 2).value, (numpy.arange(10) + 2))
with tests.helper.pytest.raises(units.UnitsError):
    assert_array_equal((qsec - 2).value, (numpy.arange(10) + 2))
qsec2 = (numpy.arange(10) * units.second)
tempResult = arange(10)
	
===================================================================	
TestQuantityCreation.test_subok: 104	
----------------------------	

'Test subok can be used to keep class, or to insist on Quantity'

class MyQuantitySubclass(units.Quantity):
    pass
tempResult = arange(10.0)
	
===================================================================	
TestQuantityArrayCopy.test_to_copies: 19	
----------------------------	

tempResult = arange(1.0, 100.0)
	
===================================================================	
TestQuantityReshapeFuncs.test_flatten: 84	
----------------------------	

tempResult = arange(6.0)
	
===================================================================	
TestQuantityStatsFuncs.test_clip_func: 330	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestQuantityArrayCopy.test_si_copies: 28	
----------------------------	

tempResult = arange(100.0)
	
===================================================================	
TestQuantityReshapeFuncs.test_ravel: 77	
----------------------------	

tempResult = arange(6.0)
	
===================================================================	
TestQuantityReshapeFuncs.test_swapaxes: 98	
----------------------------	

tempResult = arange(6.0)
	
===================================================================	
TestQuantityReshapeFuncs.test_transpose: 91	
----------------------------	

tempResult = arange(6.0)
	
===================================================================	
TestQuantityArrayCopy.test_flat: 45	
----------------------------	

tempResult = arange(9.0)
	
===================================================================	
TestQuantityArrayCopy.test_flat: 48	
----------------------------	

q = units.Quantity(np.arange(9.0).reshape(3, 3), 'm/s')
q_flat = q.flat
assert (q_flat[8] == ((8.0 * units.m) / units.s))
tempResult = arange(2.0)
	
===================================================================	
TestQuantityReshapeFuncs.test_reshape: 63	
----------------------------	

tempResult = arange(6.0)
	
===================================================================	
TestArrayConversion.test_slice: 382	
----------------------------	

'Test that setitem changes the unit if needed (or ignores it for\n        values where that is allowed; viz., #2695)'
q2 = ((numpy.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) * units.km) / units.m)
q1 = q2.copy()
q2[(0, 0)] = 10000.0
assert (q2.unit == q1.unit)
assert (q2[(0, 0)].value == 10.0)
q2[0] = ((9.0 * units.Mm) / units.km)
assert all((q2.flatten()[:3].value == numpy.array([9.0, 9.0, 9.0])))
q2[0, :(- 1)] = 8000.0
assert all((q2.flatten()[:3].value == numpy.array([8.0, 8.0, 9.0])))
with tests.helper.pytest.raises(units.UnitsError):
    q2[(1, 1)] = (10 * units.s)
tempResult = arange(10.0)
	
===================================================================	
TestRecArray.test_creation: 472	
----------------------------	

tempResult = arange(12.0)
	
===================================================================	
TestQuantityStatsFuncs.test_clip_meth: 335	
----------------------------	

expected = (numpy.array([3.0, 3.0, 3.0, 3.0, 4.0, 5.0, 6.0, 6.0, 6.0, 6.0]) * units.m)
tempResult = arange(10)
	
===================================================================	
TestQuantityArrayCopy.test_copy_on_creation: 11	
----------------------------	

tempResult = arange(1000.0)
	
===================================================================	
TestQuantityReshapeFuncs.test_squeeze: 70	
----------------------------	

tempResult = arange(6.0)
	
===================================================================	
TestQuantityArrayCopy.test_getitem_is_view: 36	
----------------------------	

'Check that [keys] work, and that, like ndarray, it returns\n        a view, so that changing one changes the other.\n\n        Also test that one can add axes (closes #1422)\n        '
tempResult = arange(100.0)
	
===================================================================	
TestInplaceUfuncs.test_two_argument_ufunc_inplace_1: 506	
----------------------------	

	
===================================================================	
TestInplaceUfuncs.test_two_argument_ufunc_inplace_2: 520	
----------------------------	

	
===================================================================	
TestQuantityMathFuncs.test_float_power_array: 247	
----------------------------	

assert numpy.all((numpy.float_power((numpy.array([1.0, 2.0, 3.0]) * units.m), 3.0) == (numpy.array([1.0, 8.0, 27.0]) * (units.m ** 3))))
tempResult = arange(4.0)
	
===================================================================	
TestQuantityMathFuncs.test_power_array: 242	
----------------------------	

assert numpy.all((numpy.power((numpy.array([1.0, 2.0, 3.0]) * units.m), 3.0) == (numpy.array([1.0, 8.0, 27.0]) * (units.m ** 3))))
tempResult = arange(4.0)
	
===================================================================	
TestQuantityMathFuncs.test_multiply_array: 180	
----------------------------	

tempResult = arange(3.0)
	
===================================================================	
TestQuantityMathFuncs.test_multiply_array: 180	
----------------------------	

tempResult = arange(0, 6.0, 2.0)
	
===================================================================	
TestQuantityMathFuncs.test_modf_array: 330	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestInplaceUfuncs.test_one_argument_ufunc_inplace_2: 462	
----------------------------	

	
===================================================================	
TestInplaceUfuncs.test_one_argument_ufunc_inplace: 448	
----------------------------	

	
===================================================================	
TestQuantityMathFuncs.test_divide_array: 190	
----------------------------	

tempResult = arange(3.0)
	
===================================================================	
TestQuantityMathFuncs.test_divide_array: 190	
----------------------------	

tempResult = arange(3.0)
	
===================================================================	
TestInplaceUfuncs.test_one_argument_two_output_ufunc_inplace: 484	
----------------------------	

	
===================================================================	
TestInplaceUfuncs.test_ufunc_inplace_non_contiguous_data: 551	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
test_matmul: 23	
----------------------------	

tempResult = arange(18)
	
===================================================================	
test_JsonCustomEncoder: 42	
----------------------------	

tempResult = arange(3)
	
===================================================================	
ZScaleInterval.get_limits: 97	
----------------------------	

values = numpy.asarray(values)
values = values[numpy.isfinite(values)]
stride = int(max(1.0, (values.size / self.nsamples)))
samples = values[::stride][:self.nsamples]
samples.sort()
npix = len(samples)
vmin = samples[0]
vmax = samples[(- 1)]
minpix = max(self.min_npixels, int((npix * self.max_reject)))
tempResult = arange(npix)
	
===================================================================	
test_hist_autobin: 44	
----------------------------	

rng = numpy.random.RandomState(rseed)
x = rng.randn(100)
if HAS_SCIPY:
    tempResult = arange((- 3), 3, 10)
	
===================================================================	
test_hist_autobin: 46	
----------------------------	

rng = numpy.random.RandomState(rseed)
x = rng.randn(100)
if HAS_SCIPY:
    bintypes = [10, numpy.arange((- 3), 3, 10), 'knuth', 'scott', 'freedman', 'blocks']
else:
    tempResult = arange((- 3), 3, 10)
	
===================================================================	
module: 15	
----------------------------	

import numpy as np
from numpy import ma
from numpy.testing import assert_allclose
from ...tests.helper import pytest
from ..mpl_normalize import ImageNormalize, simple_norm
from ..interval import ManualInterval
from ..stretch import SqrtStretch
try:
    import matplotlib
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
DATA = numpy.linspace(0.0, 15.0, 6)
tempResult = arange(3)
	
===================================================================	
BaseFormatterLocator._locate_values: 79	
----------------------------	

imin = numpy.ceil((value_min / spacing))
imax = numpy.floor((value_max / spacing))
tempResult = arange(imin, (imax + 1), dtype=int)
	
===================================================================	
predict_search: 82	
----------------------------	

'Predict the run time needed and the number of objects\n    for a Cone Search for the given access URL, position, and\n    radius.\n\n    Run time prediction uses `astropy.utils.timer.RunTimePredictor`.\n    Baseline searches are done with starting and ending radii at\n    0.05 and 0.5 of the given radius, respectively.\n\n    Extrapolation on good data uses least-square straight line fitting,\n    assuming linear increase of search time and number of objects\n    with radius, which might not be accurate for some cases. If\n    there are less than 3 data points in the fit, it fails.\n\n    Warnings (controlled by :py:mod:`warnings`) are given when:\n\n        #. Fitted slope is negative.\n        #. Any of the estimated results is negative.\n        #. Estimated run time exceeds\n           `astropy.utils.data.Conf.remote_timeout`.\n\n    .. note::\n\n        If ``verbose=True``, extra log info will be provided.\n        But unlike :func:`conesearch_timer`, timer info is suppressed.\n\n        If ``plot=True``, plot will be displayed.\n        Plotting uses `matplotlib <http://matplotlib.sourceforge.net/>`_.\n\n        The predicted results are just *rough* estimates.\n\n        Prediction is done using :func:`conesearch`. Prediction for\n        `AsyncConeSearch` is not supported.\n\n    Parameters\n    ----------\n    url : str\n        Cone Search access URL to use.\n\n    args, kwargs : see :func:`conesearch`\n        Extra keyword ``plot`` is allowed and only used by this\n        function and not :func:`conesearch`.\n\n    Returns\n    -------\n    t_est : float\n        Estimated time in seconds needed for the search.\n\n    n_est : int\n        Estimated number of objects the search will yield.\n\n    Raises\n    ------\n    AssertionError\n        If prediction fails.\n\n    ConeSearchError\n        If input parameters are invalid.\n\n    VOSError\n        If VO service request fails.\n\n    '
if (len(args) != 2):
    raise ConeSearchError('conesearch must have exactly 2 arguments')
plot = kwargs.get('plot', False)
if ('plot' in kwargs):
    del kwargs['plot']
(center, radius) = args
sr = _validate_sr(radius)
if (sr <= 0):
    raise ConeSearchError('Search radius must be > 0 degrees')
kwargs['catalog_db'] = url
cs_pred = RunTimePredictor(conesearch, center, **kwargs)
num_datapoints = 10
sr_min = (0.05 * sr)
sr_max = (0.5 * sr)
sr_step = ((1.0 / num_datapoints) * (sr_max - sr_min))
tempResult = arange(sr_min, (sr_max + sr_step), sr_step)
	
===================================================================	
test_wcs_swapping: 41	
----------------------------	

wcs = WCS(naxis=4)
wcs.wcs.pc = numpy.zeros([4, 4])
tempResult = arange(1, 5)
	
===================================================================	
test_wcs_dropping: 17	
----------------------------	

wcs = WCS(naxis=4)
wcs.wcs.pc = numpy.zeros([4, 4])
tempResult = arange(1, 5)
	
===================================================================	
test_pix2world: 84	
----------------------------	

'\n    From github issue #1463\n    '
filename = get_pkg_data_filename('data/sip2.fits')
with catch_warnings(wcs.wcs.FITSFixedWarning) as caught_warnings:
    ww = wcs.WCS(filename)
    assert (len(caught_warnings) == 1)
n = 3
tempResult = arange(n)
	
***************************************************	
scipy_scipy-0.19.0: 640	
===================================================================	
_plot_dendrogram: 541	
----------------------------	

try:
    if (ax is None):
        import matplotlib.pylab
    import matplotlib.patches
    import matplotlib.collections
except ImportError:
    raise ImportError('You must install the matplotlib library to plot the dendrogram. Use no_plot=True to calculate the dendrogram without plotting.')
if (ax is None):
    ax = matplotlib.pylab.gca()
    trigger_redraw = True
else:
    trigger_redraw = False
ivw = (len(ivl) * 10)
dvw = (mh + (mh * 0.05))
tempResult = arange(5, ((len(ivl) * 10) + 5), 10)
	
===================================================================	
cut_tree: 206	
----------------------------	

'\n    Given a linkage matrix Z, return the cut tree.\n\n    Parameters\n    ----------\n    Z : scipy.cluster.linkage array\n        The linkage matrix.\n    n_clusters : array_like, optional\n        Number of clusters in the tree at the cut point.\n    height : array_like, optional\n        The height at which to cut the tree.  Only possible for ultrametric\n        trees.\n\n    Returns\n    -------\n    cutree : array\n        An array indicating group membership at each agglomeration step.  I.e.,\n        for a full cut tree, in the first column each data point is in its own\n        cluster.  At the next step, two nodes are merged.  Finally all singleton\n        and non-singleton clusters are in one group.  If `n_clusters` or\n        `height` is given, the columns correspond to the columns of `n_clusters` or\n        `height`.\n\n    Examples\n    --------\n    >>> from scipy import cluster\n    >>> np.random.seed(23)\n    >>> X = np.random.randn(50, 4)\n    >>> Z = cluster.hierarchy.ward(X)\n    >>> cutree = cluster.hierarchy.cut_tree(Z, n_clusters=[5, 10])\n    >>> cutree[:10]\n    array([[0, 0],\n           [1, 1],\n           [2, 2],\n           [3, 3],\n           [3, 4],\n           [2, 2],\n           [0, 0],\n           [1, 5],\n           [3, 6],\n           [4, 7]])\n\n    '
nobs = num_obs_linkage(Z)
nodes = _order_cluster_tree(Z)
if ((height is not None) and (n_clusters is not None)):
    raise ValueError('At least one of either height or n_clusters must be None')
elif ((height is None) and (n_clusters is None)):
    tempResult = arange(nobs)
	
===================================================================	
cut_tree: 211	
----------------------------	

'\n    Given a linkage matrix Z, return the cut tree.\n\n    Parameters\n    ----------\n    Z : scipy.cluster.linkage array\n        The linkage matrix.\n    n_clusters : array_like, optional\n        Number of clusters in the tree at the cut point.\n    height : array_like, optional\n        The height at which to cut the tree.  Only possible for ultrametric\n        trees.\n\n    Returns\n    -------\n    cutree : array\n        An array indicating group membership at each agglomeration step.  I.e.,\n        for a full cut tree, in the first column each data point is in its own\n        cluster.  At the next step, two nodes are merged.  Finally all singleton\n        and non-singleton clusters are in one group.  If `n_clusters` or\n        `height` is given, the columns correspond to the columns of `n_clusters` or\n        `height`.\n\n    Examples\n    --------\n    >>> from scipy import cluster\n    >>> np.random.seed(23)\n    >>> X = np.random.randn(50, 4)\n    >>> Z = cluster.hierarchy.ward(X)\n    >>> cutree = cluster.hierarchy.cut_tree(Z, n_clusters=[5, 10])\n    >>> cutree[:10]\n    array([[0, 0],\n           [1, 1],\n           [2, 2],\n           [3, 3],\n           [3, 4],\n           [2, 2],\n           [0, 0],\n           [1, 5],\n           [3, 6],\n           [4, 7]])\n\n    '
nobs = num_obs_linkage(Z)
nodes = _order_cluster_tree(Z)
if ((height is not None) and (n_clusters is not None)):
    raise ValueError('At least one of either height or n_clusters must be None')
elif ((height is None) and (n_clusters is None)):
    cols_idx = numpy.arange(nobs)
elif (height is not None):
    heights = numpy.array([x.dist for x in nodes])
    cols_idx = numpy.searchsorted(heights, height)
else:
    tempResult = arange(nobs)
	
===================================================================	
cut_tree: 218	
----------------------------	

'\n    Given a linkage matrix Z, return the cut tree.\n\n    Parameters\n    ----------\n    Z : scipy.cluster.linkage array\n        The linkage matrix.\n    n_clusters : array_like, optional\n        Number of clusters in the tree at the cut point.\n    height : array_like, optional\n        The height at which to cut the tree.  Only possible for ultrametric\n        trees.\n\n    Returns\n    -------\n    cutree : array\n        An array indicating group membership at each agglomeration step.  I.e.,\n        for a full cut tree, in the first column each data point is in its own\n        cluster.  At the next step, two nodes are merged.  Finally all singleton\n        and non-singleton clusters are in one group.  If `n_clusters` or\n        `height` is given, the columns correspond to the columns of `n_clusters` or\n        `height`.\n\n    Examples\n    --------\n    >>> from scipy import cluster\n    >>> np.random.seed(23)\n    >>> X = np.random.randn(50, 4)\n    >>> Z = cluster.hierarchy.ward(X)\n    >>> cutree = cluster.hierarchy.cut_tree(Z, n_clusters=[5, 10])\n    >>> cutree[:10]\n    array([[0, 0],\n           [1, 1],\n           [2, 2],\n           [3, 3],\n           [3, 4],\n           [2, 2],\n           [0, 0],\n           [1, 5],\n           [3, 6],\n           [4, 7]])\n\n    '
nobs = num_obs_linkage(Z)
nodes = _order_cluster_tree(Z)
if ((height is not None) and (n_clusters is not None)):
    raise ValueError('At least one of either height or n_clusters must be None')
elif ((height is None) and (n_clusters is None)):
    cols_idx = numpy.arange(nobs)
elif (height is not None):
    heights = numpy.array([x.dist for x in nodes])
    cols_idx = numpy.searchsorted(heights, height)
else:
    cols_idx = (nobs - numpy.searchsorted(numpy.arange(nobs), n_clusters))
try:
    n_cols = len(cols_idx)
except TypeError:
    n_cols = 1
    cols_idx = numpy.array([cols_idx])
groups = numpy.zeros((n_cols, nobs), dtype=int)
tempResult = arange(nobs)
	
===================================================================	
test_cut_tree: 728	
----------------------------	

numpy.random.seed(23)
nobs = 50
X = numpy.random.randn(nobs, 4)
Z = scipy.cluster.hierarchy.ward(X)
cutree = cut_tree(Z)
tempResult = arange(nobs)
	
===================================================================	
test_cut_tree: 730	
----------------------------	

numpy.random.seed(23)
nobs = 50
X = numpy.random.randn(nobs, 4)
Z = scipy.cluster.hierarchy.ward(X)
cutree = cut_tree(Z)
assert_equal(cutree[:, 0], numpy.arange(nobs))
assert_equal(cutree[:, (- 1)], numpy.zeros(nobs))
tempResult = arange((nobs - 1), (- 1), (- 1))
	
===================================================================	
TestComplex.test_dst_complex: 61	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestComplex.test_dst_complex: 62	
----------------------------	

y = dst((numpy.arange(5) * 1j))
tempResult = arange(5)
	
===================================================================	
TestComplex.test_dst_complex64: 56	
----------------------------	

tempResult = arange(5, dtype=numpy.complex64)
	
===================================================================	
TestComplex.test_dst_complex64: 57	
----------------------------	

y = dst((numpy.arange(5, dtype=numpy.complex64) * 1j))
tempResult = arange(5)
	
===================================================================	
TestComplex.test_idct_complex: 51	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestComplex.test_idct_complex: 52	
----------------------------	

y = idct((numpy.arange(5) * 1j))
tempResult = arange(5)
	
===================================================================	
TestComplex.test_idst_complex: 66	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestComplex.test_idst_complex: 67	
----------------------------	

y = idst((numpy.arange(5) * 1j))
tempResult = arange(5)
	
===================================================================	
TestComplex.test_dct_complex64: 41	
----------------------------	

tempResult = arange(5, dtype=numpy.complex64)
	
===================================================================	
TestComplex.test_dct_complex64: 42	
----------------------------	

y = dct((1j * numpy.arange(5, dtype=numpy.complex64)))
tempResult = arange(5)
	
===================================================================	
TestComplex.test_dct_complex: 46	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestComplex.test_dct_complex: 47	
----------------------------	

y = dct((numpy.arange(5) * 1j))
tempResult = arange(5)
	
===================================================================	
_difftrap: 245	
----------------------------	

"\n    Perform part of the trapezoidal rule to integrate a function.\n    Assume that we had called difftrap with all lower powers-of-2\n    starting with 1.  Calling difftrap only returns the summation\n    of the new ordinates.  It does _not_ multiply by the width\n    of the trapezoids.  This must be performed by the caller.\n        'function' is the function to evaluate (must accept vector arguments).\n        'interval' is a sequence with lower and upper limits\n                   of integration.\n        'numtraps' is the number of trapezoids to use (must be a\n                   power-of-2).\n    "
if (numtraps <= 0):
    raise ValueError('numtraps must be > 0 in difftrap().')
elif (numtraps == 1):
    return (0.5 * (function(interval[0]) + function(interval[1])))
else:
    numtosum = (numtraps / 2)
    h = (float((interval[1] - interval[0])) / numtosum)
    lox = (interval[0] + (0.5 * h))
    tempResult = arange(numtosum)
	
===================================================================	
newton_cotes: 308	
----------------------------	
'\n    Return weights and error coefficient for Newton-Cotes integration.\n\n    Suppose we have (N+1) samples of f at the positions\n    x_0, x_1, ..., x_N.  Then an N-point Newton-Cotes formula for the\n    integral between x_0 and x_N is:\n\n    :math:`\\int_{x_0}^{x_N} f(x)dx = \\Delta x \\sum_{i=0}^{N} a_i f(x_i)\n    + B_N (\\Delta x)^{N+2} f^{N+1} (\\xi)`\n\n    where :math:`\\xi \\in [x_0,x_N]`\n    and :math:`\\Delta x = \\frac{x_N-x_0}{N}` is the average samples spacing.\n\n    If the samples are equally-spaced and N is even, then the error\n    term is :math:`B_N (\\Delta x)^{N+3} f^{N+2}(\\xi)`.\n\n    Parameters\n    ----------\n    rn : int\n        The integer order for equally-spaced data or the relative positions of\n        the samples with the first sample at 0 and the last at N, where N+1 is\n        the length of `rn`.  N is the order of the Newton-Cotes integration.\n    equal : int, optional\n        Set to 1 to enforce equally spaced data.\n\n    Returns\n    -------\n    an : ndarray\n        1-D array of weights to apply to the function at the provided sample\n        positions.\n    B : float\n        Error coefficient.\n\n    Notes\n    -----\n    Normally, the Newton-Cotes rules are used on smaller integration\n    regions and a composite rule is used to return the total integral.\n\n    '
try:
    N = (len(rn) - 1)
    if equal:
        tempResult = arange((N + 1))	
===================================================================	
newton_cotes: 313	
----------------------------	

'\n    Return weights and error coefficient for Newton-Cotes integration.\n\n    Suppose we have (N+1) samples of f at the positions\n    x_0, x_1, ..., x_N.  Then an N-point Newton-Cotes formula for the\n    integral between x_0 and x_N is:\n\n    :math:`\\int_{x_0}^{x_N} f(x)dx = \\Delta x \\sum_{i=0}^{N} a_i f(x_i)\n    + B_N (\\Delta x)^{N+2} f^{N+1} (\\xi)`\n\n    where :math:`\\xi \\in [x_0,x_N]`\n    and :math:`\\Delta x = \\frac{x_N-x_0}{N}` is the average samples spacing.\n\n    If the samples are equally-spaced and N is even, then the error\n    term is :math:`B_N (\\Delta x)^{N+3} f^{N+2}(\\xi)`.\n\n    Parameters\n    ----------\n    rn : int\n        The integer order for equally-spaced data or the relative positions of\n        the samples with the first sample at 0 and the last at N, where N+1 is\n        the length of `rn`.  N is the order of the Newton-Cotes integration.\n    equal : int, optional\n        Set to 1 to enforce equally spaced data.\n\n    Returns\n    -------\n    an : ndarray\n        1-D array of weights to apply to the function at the provided sample\n        positions.\n    B : float\n        Error coefficient.\n\n    Notes\n    -----\n    Normally, the Newton-Cotes rules are used on smaller integration\n    regions and a composite rule is used to return the total integral.\n\n    '
try:
    N = (len(rn) - 1)
    if equal:
        rn = numpy.arange((N + 1))
    elif numpy.all((numpy.diff(rn) == 1)):
        equal = 1
except:
    N = rn
    tempResult = arange((N + 1))
	
===================================================================	
newton_cotes: 323	
----------------------------	

'\n    Return weights and error coefficient for Newton-Cotes integration.\n\n    Suppose we have (N+1) samples of f at the positions\n    x_0, x_1, ..., x_N.  Then an N-point Newton-Cotes formula for the\n    integral between x_0 and x_N is:\n\n    :math:`\\int_{x_0}^{x_N} f(x)dx = \\Delta x \\sum_{i=0}^{N} a_i f(x_i)\n    + B_N (\\Delta x)^{N+2} f^{N+1} (\\xi)`\n\n    where :math:`\\xi \\in [x_0,x_N]`\n    and :math:`\\Delta x = \\frac{x_N-x_0}{N}` is the average samples spacing.\n\n    If the samples are equally-spaced and N is even, then the error\n    term is :math:`B_N (\\Delta x)^{N+3} f^{N+2}(\\xi)`.\n\n    Parameters\n    ----------\n    rn : int\n        The integer order for equally-spaced data or the relative positions of\n        the samples with the first sample at 0 and the last at N, where N+1 is\n        the length of `rn`.  N is the order of the Newton-Cotes integration.\n    equal : int, optional\n        Set to 1 to enforce equally spaced data.\n\n    Returns\n    -------\n    an : ndarray\n        1-D array of weights to apply to the function at the provided sample\n        positions.\n    B : float\n        Error coefficient.\n\n    Notes\n    -----\n    Normally, the Newton-Cotes rules are used on smaller integration\n    regions and a composite rule is used to return the total integral.\n\n    '
try:
    N = (len(rn) - 1)
    if equal:
        rn = numpy.arange((N + 1))
    elif numpy.all((numpy.diff(rn) == 1)):
        equal = 1
except:
    N = rn
    rn = numpy.arange((N + 1))
    equal = 1
if (equal and (N in _builtincoeffs)):
    (na, da, vi, nb, db) = _builtincoeffs[N]
    an = ((na * numpy.array(vi, dtype=float)) / da)
    return (an, (float(nb) / db))
if ((rn[0] != 0) or (rn[(- 1)] != N)):
    raise ValueError('The sample positions must start at 0 and end at N')
yi = (rn / float(N))
ti = ((2 * yi) - 1)
tempResult = arange((N + 1))
	
===================================================================	
compute_jac_indices: 81	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
tempResult = arange(((m - 1) * n))
	
===================================================================	
compute_jac_indices: 82	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
tempResult = arange(n)
	
===================================================================	
compute_jac_indices: 82	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
tempResult = arange((m - 1))
	
===================================================================	
compute_jac_indices: 83	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
j_col = (numpy.tile(numpy.arange(n), (n * (m - 1))) + numpy.repeat((numpy.arange((m - 1)) * n), (n ** 2)))
tempResult = arange(((m - 1) * n), ((m * n) + k))
	
===================================================================	
compute_jac_indices: 84	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
j_col = (numpy.tile(numpy.arange(n), (n * (m - 1))) + numpy.repeat((numpy.arange((m - 1)) * n), (n ** 2)))
i_bc = numpy.repeat(numpy.arange(((m - 1) * n), ((m * n) + k)), n)
tempResult = arange(n)
	
===================================================================	
compute_jac_indices: 85	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
j_col = (numpy.tile(numpy.arange(n), (n * (m - 1))) + numpy.repeat((numpy.arange((m - 1)) * n), (n ** 2)))
i_bc = numpy.repeat(numpy.arange(((m - 1) * n), ((m * n) + k)), n)
j_bc = numpy.tile(numpy.arange(n), (n + k))
tempResult = arange(((m - 1) * n))
	
===================================================================	
compute_jac_indices: 86	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
j_col = (numpy.tile(numpy.arange(n), (n * (m - 1))) + numpy.repeat((numpy.arange((m - 1)) * n), (n ** 2)))
i_bc = numpy.repeat(numpy.arange(((m - 1) * n), ((m * n) + k)), n)
j_bc = numpy.tile(numpy.arange(n), (n + k))
i_p_col = numpy.repeat(numpy.arange(((m - 1) * n)), k)
tempResult = arange((m * n), ((m * n) + k))
	
===================================================================	
compute_jac_indices: 87	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
j_col = (numpy.tile(numpy.arange(n), (n * (m - 1))) + numpy.repeat((numpy.arange((m - 1)) * n), (n ** 2)))
i_bc = numpy.repeat(numpy.arange(((m - 1) * n), ((m * n) + k)), n)
j_bc = numpy.tile(numpy.arange(n), (n + k))
i_p_col = numpy.repeat(numpy.arange(((m - 1) * n)), k)
j_p_col = numpy.tile(numpy.arange((m * n), ((m * n) + k)), ((m - 1) * n))
tempResult = arange(((m - 1) * n), ((m * n) + k))
	
===================================================================	
compute_jac_indices: 88	
----------------------------	

'Compute indices for the collocation system Jacobian construction.\n\n    See `construct_global_jac` for the explanation.\n    '
i_col = numpy.repeat(numpy.arange(((m - 1) * n)), n)
j_col = (numpy.tile(numpy.arange(n), (n * (m - 1))) + numpy.repeat((numpy.arange((m - 1)) * n), (n ** 2)))
i_bc = numpy.repeat(numpy.arange(((m - 1) * n), ((m * n) + k)), n)
j_bc = numpy.tile(numpy.arange(n), (n + k))
i_p_col = numpy.repeat(numpy.arange(((m - 1) * n)), k)
j_p_col = numpy.tile(numpy.arange((m * n), ((m * n) + k)), ((m - 1) * n))
i_p_bc = numpy.repeat(numpy.arange(((m - 1) * n), ((m * n) + k)), k)
tempResult = arange((m * n), ((m * n) + k))
	
===================================================================	
test_banded_ode_solvers: 93	
----------------------------	

t_exact = numpy.linspace(0, 1.0, 5)
a_real = numpy.array([[(- 0.6), 0.1, 0.0, 0.0, 0.0], [0.2, (- 0.5), 0.9, 0.0, 0.0], [0.1, 0.1, (- 0.4), 0.1, 0.0], [0.0, 0.3, (- 0.1), (- 0.9), (- 0.3)], [0.0, 0.0, 0.1, 0.1, (- 0.7)]])
a_real_upper = numpy.triu(a_real)
a_real_lower = numpy.tril(a_real)
a_real_diag = numpy.triu(a_real_lower)
real_matrices = [a_real, a_real_upper, a_real_lower, a_real_diag]
real_solutions = []
for a in real_matrices:
    tempResult = arange(1, (a.shape[0] + 1))
	
===================================================================	
test_banded_ode_solvers: 112	
----------------------------	

t_exact = numpy.linspace(0, 1.0, 5)
a_real = numpy.array([[(- 0.6), 0.1, 0.0, 0.0, 0.0], [0.2, (- 0.5), 0.9, 0.0, 0.0], [0.1, 0.1, (- 0.4), 0.1, 0.0], [0.0, 0.3, (- 0.1), (- 0.9), (- 0.3)], [0.0, 0.0, 0.1, 0.1, (- 0.7)]])
a_real_upper = numpy.triu(a_real)
a_real_lower = numpy.tril(a_real)
a_real_diag = numpy.triu(a_real_lower)
real_matrices = [a_real, a_real_upper, a_real_lower, a_real_diag]
real_solutions = []
for a in real_matrices:
    y0 = numpy.arange(1, (a.shape[0] + 1))
    y_exact = _analytical_solution(a, y0, t_exact)
    real_solutions.append((y0, t_exact, y_exact))

def check_real(idx, solver, meth, use_jac, with_jac, banded):
    a = real_matrices[idx]
    (y0, t_exact, y_exact) = real_solutions[idx]
    (t, y) = _solve_linear_sys(a, y0, tend=t_exact[(- 1)], dt=(t_exact[1] - t_exact[0]), solver=solver, method=meth, use_jac=use_jac, with_jacobian=with_jac, banded=banded)
    assert_allclose(t, t_exact)
    assert_allclose(y, y_exact)
for idx in range(len(real_matrices)):
    p = [['vode', 'lsoda'], ['bdf', 'adams'], [False, True], [False, True], [False, True]]
    for (solver, meth, use_jac, with_jac, banded) in itertools.product(*p):
        (yield (check_real, idx, solver, meth, use_jac, with_jac, banded))
a_complex = (a_real - (0.5j * a_real))
a_complex_diag = numpy.diag(numpy.diag(a_complex))
complex_matrices = [a_complex, a_complex_diag]
complex_solutions = []
for a in complex_matrices:
    tempResult = arange(1, (a.shape[0] + 1))
	
===================================================================	
check_odeint: 37	
----------------------------	

if (jactype == JACTYPE_FULL):
    ml = None
    mu = None
    jacobian = jac
elif (jactype == JACTYPE_BANDED):
    ml = 2
    mu = 1
    jacobian = bjac
else:
    raise ValueError(('invalid jactype: %r' % (jactype,)))
tempResult = arange(1.0, 6.0)
	
===================================================================	
check_odeint: 42	
----------------------------	

if (jactype == JACTYPE_FULL):
    ml = None
    mu = None
    jacobian = jac
elif (jactype == JACTYPE_BANDED):
    ml = 2
    mu = 1
    jacobian = bjac
else:
    raise ValueError(('invalid jactype: %r' % (jactype,)))
y0 = numpy.arange(1.0, 6.0)
rtol = 1e-11
atol = 1e-13
dt = 0.125
nsteps = 64
tempResult = arange((nsteps + 1))
	
===================================================================	
TestCumtrapz.test_y_nd_x_nd: 159	
----------------------------	

tempResult = arange(((3 * 2) * 4))
	
===================================================================	
TestCumtrapz.test_y_nd_x_1d: 172	
----------------------------	

tempResult = arange(((3 * 2) * 4))
	
===================================================================	
TestCumtrapz.test_y_nd_x_1d: 173	
----------------------------	

y = np.arange(((3 * 2) * 4)).reshape(3, 2, 4)
tempResult = arange(4)
	
===================================================================	
TestFixedQuad.test_vector: 21	
----------------------------	

n = 4
tempResult = arange(1, (2 * n))
	
===================================================================	
TestQuadrature.test_romb: 83	
----------------------------	

tempResult = arange(17)
	
===================================================================	
TestQuadrature.test_simps: 137	
----------------------------	

tempResult = arange(17)
	
===================================================================	
TestQuadrature.test_simps: 141	
----------------------------	

y = numpy.arange(17)
assert_equal(simps(y), 128)
assert_equal(simps(y, dx=0.5), 64)
assert_equal(simps(y, x=numpy.linspace(0, 4, 17)), 32)
tempResult = arange(4)
	
===================================================================	
TestQuadrature.test_romb_gh_3731: 86	
----------------------------	

tempResult = arange(((2 ** 4) + 1))
	
===================================================================	
PPoly.antiderivative: 434	
----------------------------	

"\n        Construct a new piecewise polynomial representing the antiderivative.\n\n        Antiderivative is also the indefinite integral of the function,\n        and derivative is its inverse operation.\n\n        Parameters\n        ----------\n        nu : int, optional\n            Order of antiderivative to evaluate. Default is 1, i.e. compute\n            the first integral. If negative, the derivative is returned.\n\n        Returns\n        -------\n        pp : PPoly\n            Piecewise polynomial of order k2 = k + n representing\n            the antiderivative of this polynomial.\n\n        Notes\n        -----\n        The antiderivative returned by this function is continuous and\n        continuously differentiable to order n-1, up to floating point\n        rounding error.\n\n        If antiderivative is computed and ``self.extrapolate='periodic'``,\n        it will be set to False for the returned instance. This is done because\n        the antiderivative is no longer periodic and its correct evaluation\n        outside of the initially given x interval is difficult.\n        "
if (nu <= 0):
    return self.derivative((- nu))
c = numpy.zeros((((self.c.shape[0] + nu), self.c.shape[1]) + self.c.shape[2:]), dtype=self.c.dtype)
c[:(- nu)] = self.c
tempResult = arange(self.c.shape[0], 0, (- 1))
	
===================================================================	
NdPPoly._derivative_inplace: 797	
----------------------------	

'\n        Compute 1D derivative along a selected dimension in-place\n        May result to non-contiguous c array.\n        '
if (nu < 0):
    return self._antiderivative_inplace((- nu), axis)
ndim = len(self.x)
axis = (axis % ndim)
if (nu == 0):
    return
else:
    sl = ([slice(None)] * ndim)
    sl[axis] = slice(None, (- nu), None)
    c2 = self.c[sl]
if (c2.shape[axis] == 0):
    shp = list(c2.shape)
    shp[axis] = 1
    c2 = numpy.zeros(shp, dtype=c2.dtype)
tempResult = arange(c2.shape[axis], 0, (- 1))
	
===================================================================	
PPoly.derivative: 424	
----------------------------	

'\n        Construct a new piecewise polynomial representing the derivative.\n\n        Parameters\n        ----------\n        nu : int, optional\n            Order of derivative to evaluate. Default is 1, i.e. compute the\n            first derivative. If negative, the antiderivative is returned.\n\n        Returns\n        -------\n        pp : PPoly\n            Piecewise polynomial of order k2 = k - n representing the derivative\n            of this polynomial.\n\n        Notes\n        -----\n        Derivatives are evaluated piecewise for each polynomial\n        segment, even if the polynomial is not differentiable at the\n        breakpoints. The polynomial intervals are considered half-open,\n        ``[a, b)``, except for the last interval which is closed\n        ``[a, b]``.\n        '
if (nu < 0):
    return self.antiderivative((- nu))
if (nu == 0):
    c2 = self.c.copy()
else:
    c2 = self.c[:(- nu), :].copy()
if (c2.shape[0] == 0):
    c2 = numpy.zeros(((1,) + c2.shape[1:]), dtype=c2.dtype)
tempResult = arange(c2.shape[0], 0, (- 1))
	
===================================================================	
NdPPoly._antiderivative_inplace: 815	
----------------------------	

'\n        Compute 1D antiderivative along a selected dimension\n        May result to non-contiguous c array.\n        '
if (nu <= 0):
    return self._derivative_inplace((- nu), axis)
ndim = len(self.x)
axis = (axis % ndim)
perm = list(range(ndim))
(perm[0], perm[axis]) = (perm[axis], perm[0])
perm = (perm + list(range(ndim, self.c.ndim)))
c = self.c.transpose(perm)
c2 = numpy.zeros((((c.shape[0] + nu),) + c.shape[1:]), dtype=c.dtype)
c2[:(- nu)] = c
tempResult = arange(c.shape[0], 0, (- 1))
	
===================================================================	
approximate_taylor_polynomial: 180	
----------------------------	

'\n    Estimate the Taylor polynomial of f at x by polynomial fitting.\n\n    Parameters\n    ----------\n    f : callable\n        The function whose Taylor polynomial is sought. Should accept\n        a vector of `x` values.\n    x : scalar\n        The point at which the polynomial is to be evaluated.\n    degree : int\n        The degree of the Taylor polynomial\n    scale : scalar\n        The width of the interval to use to evaluate the Taylor polynomial.\n        Function values spread over a range this wide are used to fit the\n        polynomial. Must be chosen carefully.\n    order : int or None, optional\n        The order of the polynomial to be used in the fitting; `f` will be\n        evaluated ``order+1`` times. If None, use `degree`.\n\n    Returns\n    -------\n    p : poly1d instance\n        The Taylor polynomial (translated to the origin, so that\n        for example p(0)=f(x)).\n\n    Notes\n    -----\n    The appropriate choice of "scale" is a trade-off; too large and the\n    function differs from its Taylor polynomial too much to get a good\n    answer, too small and round-off errors overwhelm the higher-order terms.\n    The algorithm used becomes numerically unstable around order 30 even\n    under ideal circumstances.\n\n    Choosing order somewhat larger than degree may improve the higher-order\n    terms.\n\n    '
if (order is None):
    order = degree
n = (order + 1)
xs = ((scale * numpy.cos(numpy.linspace(0, numpy.pi, n, endpoint=(n % 1)))) + x)
P = KroghInterpolator(xs, f(xs))
d = P.derivatives(x, der=(degree + 1))
tempResult = arange((degree + 1))
	
===================================================================	
TestInterp.test_check_finite: 592	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestBSpline.test_ctor: 26	
----------------------------	

assert_raises((TypeError, ValueError), BSpline, **dict(t=[1, 1j], c=[1.0], k=0))
assert_raises(ValueError, BSpline, **dict(t=[1, numpy.nan], c=[1.0], k=0))
assert_raises(ValueError, BSpline, **dict(t=[1, numpy.inf], c=[1.0], k=0))
assert_raises(ValueError, BSpline, **dict(t=[1, (- 1)], c=[1.0], k=0))
assert_raises(ValueError, BSpline, **dict(t=[[1], [1]], c=[1.0], k=0))
assert_raises(ValueError, BSpline, **dict(t=[0, 1, 2], c=[1], k=0))
assert_raises(ValueError, BSpline, **dict(t=[0, 1, 2, 3, 4], c=[1.0, 1.0], k=2))
assert_raises(ValueError, BSpline, **dict(t=[0.0, 0.0, 1.0, 2.0, 3.0, 4.0], c=[1.0, 1.0, 1.0], k='cubic'))
assert_raises(ValueError, BSpline, **dict(t=[0.0, 0.0, 1.0, 2.0, 3.0, 4.0], c=[1.0, 1.0, 1.0], k=2.5))
assert_raises(ValueError, BSpline, **dict(t=[0.0, 0, 1, 1, 2, 3], c=[1.0, 1, 1], k=2))
(n, k) = (11, 3)
tempResult = arange(((n + k) + 1))
	
===================================================================	
TestLSQ.test_int_xy: 739	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestLSQ.test_int_xy: 740	
----------------------------	

x = np.arange(10).astype(numpy.int_)
tempResult = arange(10)
	
===================================================================	
TestLSQ.test_checkfinite: 752	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestInterop.test_splprep: 408	
----------------------------	

tempResult = arange(15)
	
===================================================================	
TestInterop.test_splprep: 417	
----------------------------	

x = np.arange(15).reshape((3, 5))
(b, u) = splprep(x)
(tck, u1) = scipy.interpolate._fitpack_impl.splprep(x)
assert_allclose(u, u1, atol=1e-15)
assert_allclose(splev(u, b), x, atol=1e-15)
assert_allclose(splev(u, tck), x, atol=1e-15)
((b_f, u_f), _, _, _) = splprep(x, s=0, full_output=True)
assert_allclose(u, u_f, atol=1e-15)
assert_allclose(splev(u_f, b_f), x, atol=1e-15)
tempResult = arange(((3 * 4) * 5))
	
===================================================================	
TestInterp.test_quintic_derivs: 531	
----------------------------	

(k, n) = (5, 7)
tempResult = arange(n)
	
===================================================================	
TestInterp.test_int_xy: 578	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestInterp.test_int_xy: 579	
----------------------------	

x = np.arange(10).astype(numpy.int_)
tempResult = arange(10)
	
===================================================================	
TestUnivariateSpline.test_out_of_range_regression: 66	
----------------------------	

tempResult = arange(5, dtype=float)
	
===================================================================	
TestUnivariateSpline.test_nan: 115	
----------------------------	

tempResult = arange(10, dtype=float)
	
===================================================================	
TestUnivariateSpline.test_lsq_fpchec: 99	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestUnivariateSpline.test_lsq_fpchec: 100	
----------------------------	

xs = (numpy.arange(100) * 1.0)
tempResult = arange(100)
	
===================================================================	
TestCloughTocher2DInterpolator.test_tri_input_rescale: 179	
----------------------------	

x = numpy.array([(0, 0), ((- 5), (- 5)), ((- 5), 5), (5, 5), (2.5, 3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestLinearNDInterpolation.test_tri_input_rescale: 93	
----------------------------	

x = numpy.array([(0, 0), ((- 5), (- 5)), ((- 5), 5), (5, 5), (2.5, 3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestLinearNDInterpolation.test_smoketest: 18	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestLinearNDInterpolation.test_smoketest_rescale: 67	
----------------------------	

x = numpy.array([(0, 0), ((- 5), (- 5)), ((- 5), 5), (5, 5), (2.5, 3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestLinearNDInterpolation.test_tri_input: 37	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestLinearNDInterpolation.test_tripoints_input_rescale: 84	
----------------------------	

x = numpy.array([(0, 0), ((- 5), (- 5)), ((- 5), 5), (5, 5), (2.5, 3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestLinearNDInterpolation.test_smoketest_alternate: 24	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestLinearNDInterpolation.test_complex_smoketest: 30	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestCloughTocher2DInterpolator.test_tri_input: 171	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestCloughTocher2DInterpolator.test_tripoints_input_rescale: 192	
----------------------------	

x = numpy.array([(0, 0), ((- 5), (- 5)), ((- 5), 5), (5, 5), (2.5, 3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestAkima1DInterpolator.test_extend: 492	
----------------------------	

tempResult = arange(0.0, 11.0)
	
===================================================================	
TestInterp1D.test_local_nans: 414	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestInterp1D.setUp: 87	
----------------------------	

tempResult = arange(5.0)
	
===================================================================	
TestInterp1D.setUp: 88	
----------------------------	

self.x5 = numpy.arange(5.0)
tempResult = arange(10.0)
	
===================================================================	
TestInterp1D.setUp: 89	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
tempResult = arange(10.0)
	
===================================================================	
TestInterp1D.setUp: 91	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
tempResult = arange(2.0)
	
===================================================================	
TestInterp1D.setUp: 92	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
self.x2 = numpy.arange(2.0)
tempResult = arange(2.0)
	
===================================================================	
TestInterp1D.setUp: 95	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
self.x2 = numpy.arange(2.0)
self.y2 = numpy.arange(2.0)
self.x1 = numpy.array([0.0])
self.y1 = numpy.array([0.0])
tempResult = arange(20.0)
	
===================================================================	
TestInterp1D.setUp: 96	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
self.x2 = numpy.arange(2.0)
self.y2 = numpy.arange(2.0)
self.x1 = numpy.array([0.0])
self.y1 = numpy.array([0.0])
self.y210 = np.arange(20.0).reshape((2, 10))
tempResult = arange(20.0)
	
===================================================================	
TestInterp1D.setUp: 97	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
self.x2 = numpy.arange(2.0)
self.y2 = numpy.arange(2.0)
self.x1 = numpy.array([0.0])
self.y1 = numpy.array([0.0])
self.y210 = np.arange(20.0).reshape((2, 10))
self.y102 = np.arange(20.0).reshape((10, 2))
tempResult = arange(20.0)
	
===================================================================	
TestInterp1D.setUp: 98	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
self.x2 = numpy.arange(2.0)
self.y2 = numpy.arange(2.0)
self.x1 = numpy.array([0.0])
self.y1 = numpy.array([0.0])
self.y210 = np.arange(20.0).reshape((2, 10))
self.y102 = np.arange(20.0).reshape((10, 2))
self.y225 = np.arange(20.0).reshape((2, 2, 5))
tempResult = arange(10.0)
	
===================================================================	
TestInterp1D.setUp: 99	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
self.x2 = numpy.arange(2.0)
self.y2 = numpy.arange(2.0)
self.x1 = numpy.array([0.0])
self.y1 = numpy.array([0.0])
self.y210 = np.arange(20.0).reshape((2, 10))
self.y102 = np.arange(20.0).reshape((10, 2))
self.y225 = np.arange(20.0).reshape((2, 2, 5))
self.y25 = np.arange(10.0).reshape((2, 5))
tempResult = arange(30.0)
	
===================================================================	
TestInterp1D.setUp: 100	
----------------------------	

self.x5 = numpy.arange(5.0)
self.x10 = numpy.arange(10.0)
self.y10 = numpy.arange(10.0)
self.x25 = self.x10.reshape((2, 5))
self.x2 = numpy.arange(2.0)
self.y2 = numpy.arange(2.0)
self.x1 = numpy.array([0.0])
self.y1 = numpy.array([0.0])
self.y210 = np.arange(20.0).reshape((2, 10))
self.y102 = np.arange(20.0).reshape((10, 2))
self.y225 = np.arange(20.0).reshape((2, 2, 5))
self.y25 = np.arange(10.0).reshape((2, 5))
self.y235 = np.arange(30.0).reshape((2, 3, 5))
tempResult = arange(30.0)
	
===================================================================	
TestInterp1D._bounds_check_int_nan_fill: 225	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestInterp1D._bounds_check_int_nan_fill: 226	
----------------------------	

x = np.arange(10).astype(numpy.int_)
tempResult = arange(10)
	
===================================================================	
TestPPoly.test_descending: 679	
----------------------------	


def binom_matrix(power):
    n = np.arange((power + 1)).reshape((- 1), 1)
    k = numpy.arange((power + 1))
    B = binom(n, k)
    return B[::(- 1), ::(- 1)]
numpy.random.seed(0)
power = 3
for m in [10, 20, 30]:
    x = numpy.sort(numpy.random.uniform(0, 10, (m + 1)))
    ca = numpy.random.uniform((- 2), 2, size=((power + 1), m))
    h = numpy.diff(x)
    tempResult = arange((power + 1))
	
===================================================================	
TestInterp2D.test_interp2d_linear: 63	
----------------------------	

a = numpy.zeros([5, 5])
a[(2, 2)] = 1.0
tempResult = arange(5)
	
===================================================================	
TestRegularGridInterpolator.test_fillvalue_type: 1836	
----------------------------	

values = numpy.ones((10, 20, 30), dtype='>f4')
tempResult = arange(n)
	
===================================================================	
MyValue.__init__: 1847	
----------------------------	

self.ndim = 2
self.shape = shape
tempResult = arange(numpy.prod(shape))
	
===================================================================	
TestInterp1D.test_spline_nans: 423	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestInterpN._sample_2d_data: 1861	
----------------------------	

tempResult = arange(1, 6)
	
===================================================================	
TestInterpN._sample_2d_data: 1863	
----------------------------	

x = numpy.arange(1, 6)
x = numpy.array([0.5, 2.0, 3.0, 4.0, 5.5])
tempResult = arange(1, 6)
	
===================================================================	
TestInterp1D.test_linear_dtypes: 185	
----------------------------	

for dtyp in numpy.sctypes['float']:
    tempResult = arange(8, dtype=dtyp)
	
===================================================================	
TestAkima1DInterpolator.test_eval_3d: 466	
----------------------------	

tempResult = arange(0.0, 11.0)
	
===================================================================	
TestPPoly.binom_matrix: 669	
----------------------------	

tempResult = arange((power + 1))
	
===================================================================	
TestPPoly.binom_matrix: 670	
----------------------------	

n = np.arange((power + 1)).reshape((- 1), 1)
tempResult = arange((power + 1))
	
===================================================================	
TestLagrange.test_lagrange: 440	
----------------------------	

p = poly1d([5, 2, 1, 4, 3])
tempResult = arange(len(p.coeffs))
	
===================================================================	
TestInterp1D._nd_check_shape: 369	
----------------------------	

a = [4, 5, 6, 7]
tempResult = arange(numpy.prod(a))
	
===================================================================	
TestInterp1D._nd_check_shape: 371	
----------------------------	

a = [4, 5, 6, 7]
y = np.arange(np.prod(a)).reshape(*a)
for (n, s) in enumerate(a):
    tempResult = arange(s)
	
===================================================================	
TestInterp1D._nd_check_shape: 374	
----------------------------	

a = [4, 5, 6, 7]
y = np.arange(np.prod(a)).reshape(*a)
for (n, s) in enumerate(a):
    x = numpy.arange(s)
    z = interp1d(x, y, axis=n, kind=kind)
    assert_array_almost_equal(z(x), y, err_msg=kind)
    tempResult = arange(((2 * 3) * 1))
	
===================================================================	
TestAkima1DInterpolator.test_eval: 448	
----------------------------	

tempResult = arange(0.0, 11.0)
	
===================================================================	
TestAkima1DInterpolator.test_eval_2d: 456	
----------------------------	

tempResult = arange(0.0, 11.0)
	
===================================================================	
TestGriddata.test_xi_1d: 100	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
test_nearest_options: 114	
----------------------------	

(npts, nd) = (4, 3)
tempResult = arange((npts * nd))
	
===================================================================	
test_nearest_options: 115	
----------------------------	

(npts, nd) = (4, 3)
x = np.arange((npts * nd)).reshape((npts, nd))
tempResult = arange(npts)
	
===================================================================	
TestGriddata.test_complex_2d: 48	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestGriddata.test_alternative_call: 19	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestGriddata.test_multipoint_2d: 37	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestGriddata.test_multivalue_2d: 28	
----------------------------	

x = numpy.array([(0, 0), ((- 0.5), (- 0.5)), ((- 0.5), 0.5), (0.5, 0.5), (0.25, 0.3)], dtype=numpy.double)
tempResult = arange(x.shape[0], dtype=numpy.double)
	
===================================================================	
TestPCHIP.test_all_zeros: 342	
----------------------------	

tempResult = arange(10)
	
===================================================================	
CheckKrogh.test_shapes_vectorvalue: 190	
----------------------------	

tempResult = arange(3)
	
===================================================================	
CheckKrogh.test_shapes_vectorvalue_derivative: 202	
----------------------------	

tempResult = arange(3)
	
===================================================================	
CheckBarycentric.test_shapes_vectorvalue: 273	
----------------------------	

tempResult = arange(3)
	
===================================================================	
TestPCHIP.test_cast: 318	
----------------------------	

data = numpy.array([[0, 4, 12, 27, 47, 60, 79, 87, 99, 100], [(- 33), (- 33), (- 19), (- 2), 12, 26, 38, 45, 53, 55]])
tempResult = arange(100)
	
===================================================================	
module: 29	
----------------------------	

' Nose test generators\n\nNeed function load / save / roundtrip tests\n\n'
from __future__ import division, print_function, absolute_import
import os
from os.path import join as pjoin, dirname
from glob import glob
from io import BytesIO
from tempfile import mkdtemp
from scipy._lib.six import u, text_type, string_types
import warnings
import shutil
import gzip
from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_raises, run_module_suite, assert_
import numpy as np
from numpy import array
import scipy.sparse as SP
import scipy.io.matlab.byteordercodes as boc
from scipy.io.matlab.miobase import matdims, MatWriteError, MatReadError
from scipy.io.matlab.mio import mat_reader_factory, loadmat, savemat, whosmat
from scipy.io.matlab.mio5 import MatlabObject, MatFile5Writer, MatFile5Reader, MatlabFunction, varmats_from_mat, to_writeable, EmptyStructMarker
from scipy.io.matlab import mio5_params as mio5p
test_data_path = pjoin(dirname(__file__), 'data')

def mlarr(*args, **kwargs):
    'Convenience function to return matlab-compatible 2D array.'
    arr = numpy.array(*args, **kwargs)
    arr.shape = matdims(arr)
    return arr
tempResult = arange(9, dtype=float)
	
===================================================================	
test_write_opposite_endian: 595	
----------------------------	

float_arr = numpy.array([[2.0, 3.0], [3.0, 4.0]])
tempResult = arange(6)
	
===================================================================	
test_mat4_3d: 630	
----------------------------	

stream = BytesIO()
tempResult = arange(24)
	
===================================================================	
test_1d_shape: 353	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_read_opts: 523	
----------------------------	

tempResult = arange(6)
	
===================================================================	
test_round_types: 728	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_miuint32_compromise: 806	
----------------------------	

filename = pjoin(test_data_path, 'miuint32_for_miint32.mat')
res = loadmat(filename)
tempResult = arange(10)
	
===================================================================	
test_varmats_from_mat: 738	
----------------------------	

tempResult = arange(10)
	
===================================================================	
make_simple: 26	
----------------------------	

f = netcdf_file(*args, **kwargs)
f.history = 'Created for a test'
f.createDimension('time', N_EG_ELS)
time = f.createVariable('time', VARTYPE_EG, ('time',))
tempResult = arange(N_EG_ELS)
	
===================================================================	
matrix_balance: 404	
----------------------------	

'\n    A wrapper around LAPACK\'s xGEBAL routine family for matrix balancing.\n\n    The balancing tries to equalize the row and column 1-norms by applying\n    a similarity transformation such that the magnitude variation of the\n    matrix entries is reflected to the scaling matrices.\n\n    Moreover, if enabled, the matrix is first permuted to isolate the upper\n    triangular parts of the matrix and, again if scaling is also enabled,\n    only the remaining subblocks are subjected to scaling.\n\n    The balanced matrix satisfies the following equality\n\n    .. math::\n\n                        B = T^{-1} A T\n\n    The scaling coefficients are approximated to the nearest power of 2\n    to avoid round-off errors.\n\n    Parameters\n    ----------\n    A : (n, n) array_like\n        Square data matrix for the balancing.\n    permute : bool, optional\n        The selector to define whether permutation of A is also performed\n        prior to scaling.\n    scale : bool, optional\n        The selector to turn on and off the scaling. If False, the matrix\n        will not be scaled.\n    separate : bool, optional\n        This switches from returning a full matrix of the transformation\n        to a tuple of two separate 1D permutation and scaling arrays.\n    overwrite_a : bool, optional\n        This is passed to xGEBAL directly. Essentially, overwrites the result\n        to the data. It might increase the space efficiency. See LAPACK manual\n        for details. This is False by default.\n\n    Returns\n    -------\n    B : (n, n) ndarray\n        Balanced matrix\n    T : (n, n) ndarray\n        A possibly permuted diagonal matrix whose nonzero entries are\n        integer powers of 2 to avoid numerical truncation errors.\n    scale, perm : (n,) ndarray\n        If ``separate`` keyword is set to True then instead of the array\n        ``T`` above, the scaling and the permutation vector is given\n        separately without allocating the full array ``T``.\n\n    .. versionadded:: 0.19.0\n\n    Notes\n    -----\n\n    This algorithm is particularly useful for eigenvalue and matrix\n    decompositions and in many cases it is already called by various\n    LAPACK routines.\n\n    The algorithm is based on the well-known technique of [1]_ and has\n    been modified to account for special cases. See [2]_ for details\n    which have been implemented since LAPACK v3.5.0. Before this version\n    there are corner cases where balancing can actually worsen the\n    conditioning. See [3]_ for such examples.\n\n    Examples\n    --------\n    >>> from scipy import linalg\n    >>> x = np.array([[1,2,0], [9,1,0.01], [1,2,10*np.pi]])\n\n    >>> y, permscale = linalg.matrix_balance(x)\n    >>> np.abs(x).sum(axis=0) / np.abs(x).sum(axis=1)\n    array([ 3.66666667,  0.4995005 ,  0.91312162])\n\n    >>> np.abs(y).sum(axis=0) / np.abs(y).sum(axis=1) # 1-norms approx. equal\n    array([ 1.10625   ,  0.90547703,  1.00011878])\n\n    >>> permscale  # only powers of 2 (0.5 == 2^(-1))\n    array([[  0.5,   0. ,   0. ],\n           [  0. ,   1. ,   0. ],\n           [  0. ,   0. ,  16. ]])\n\n    References\n    ----------\n    .. [1] : B.N. Parlett and C. Reinsch, "Balancing a Matrix for\n       Calculation of Eigenvalues and Eigenvectors", Numerische Mathematik,\n       Vol.13(4), 1969, DOI:10.1007/BF02165404\n\n    .. [2] : R. James, J. Langou, B.R. Lowery, "On matrix balancing and\n       eigenvector computation", 2014, Available online:\n       http://arxiv.org/abs/1401.5766\n\n    .. [3] :  D.S. Watkins. A case where balancing is harmful.\n       Electron. Trans. Numer. Anal, Vol.23, 2006.\n\n    '
A = numpy.atleast_2d(_asarray_validated(A, check_finite=True))
if (not numpy.equal(*A.shape)):
    raise ValueError('The data matrix for balancing should be square.')
gebal = get_lapack_funcs('gebal', (A,))
(B, lo, hi, ps, info) = gebal(A, scale=scale, permute=permute, overwrite_a=overwrite_a)
if (info < 0):
    raise ValueError('xGEBAL exited with the internal error "illegal value in argument number {}.". See LAPACK documentation for the xGEBAL error codes.'.format((- info)))
scaling = numpy.ones_like(ps, dtype=float)
scaling[lo:(hi + 1)] = ps[lo:(hi + 1)]
ps = (ps.astype(int, copy=False) - 1)
n = A.shape[0]
tempResult = arange(n)
	
===================================================================	
helmert: 143	
----------------------------	

'\n    Create a Helmert matrix of order `n`.\n\n    This has applications in statistics, compositional or simplicial analysis,\n    and in Aitchison geometry.\n\n    Parameters\n    ----------\n    n : int\n        The size of the array to create.\n    full : bool, optional\n        If True the (n, n) ndarray will be returned.\n        Otherwise the submatrix that does not include the first\n        row will be returned.\n        Default: False.\n\n    Returns\n    -------\n    M : ndarray\n        The Helmert matrix.\n        The shape is (n, n) or (n-1, n) depending on the `full` argument.\n\n    Examples\n    --------\n    >>> from scipy.linalg import helmert\n    >>> helmert(5, full=True)\n    array([[ 0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ],\n           [ 0.70710678, -0.70710678,  0.        ,  0.        ,  0.        ],\n           [ 0.40824829,  0.40824829, -0.81649658,  0.        ,  0.        ],\n           [ 0.28867513,  0.28867513,  0.28867513, -0.8660254 ,  0.        ],\n           [ 0.2236068 ,  0.2236068 ,  0.2236068 ,  0.2236068 , -0.89442719]])\n\n    '
tempResult = arange(n)
	
===================================================================	
helmert: 144	
----------------------------	

'\n    Create a Helmert matrix of order `n`.\n\n    This has applications in statistics, compositional or simplicial analysis,\n    and in Aitchison geometry.\n\n    Parameters\n    ----------\n    n : int\n        The size of the array to create.\n    full : bool, optional\n        If True the (n, n) ndarray will be returned.\n        Otherwise the submatrix that does not include the first\n        row will be returned.\n        Default: False.\n\n    Returns\n    -------\n    M : ndarray\n        The Helmert matrix.\n        The shape is (n, n) or (n-1, n) depending on the `full` argument.\n\n    Examples\n    --------\n    >>> from scipy.linalg import helmert\n    >>> helmert(5, full=True)\n    array([[ 0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ],\n           [ 0.70710678, -0.70710678,  0.        ,  0.        ,  0.        ],\n           [ 0.40824829,  0.40824829, -0.81649658,  0.        ,  0.        ],\n           [ 0.28867513,  0.28867513,  0.28867513, -0.8660254 ,  0.        ],\n           [ 0.2236068 ,  0.2236068 ,  0.2236068 ,  0.2236068 , -0.89442719]])\n\n    '
H = (numpy.tril(numpy.ones((n, n)), (- 1)) - numpy.diag(numpy.arange(n)))
tempResult = arange(n)
	
===================================================================	
helmert: 144	
----------------------------	

'\n    Create a Helmert matrix of order `n`.\n\n    This has applications in statistics, compositional or simplicial analysis,\n    and in Aitchison geometry.\n\n    Parameters\n    ----------\n    n : int\n        The size of the array to create.\n    full : bool, optional\n        If True the (n, n) ndarray will be returned.\n        Otherwise the submatrix that does not include the first\n        row will be returned.\n        Default: False.\n\n    Returns\n    -------\n    M : ndarray\n        The Helmert matrix.\n        The shape is (n, n) or (n-1, n) depending on the `full` argument.\n\n    Examples\n    --------\n    >>> from scipy.linalg import helmert\n    >>> helmert(5, full=True)\n    array([[ 0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ,  0.4472136 ],\n           [ 0.70710678, -0.70710678,  0.        ,  0.        ,  0.        ],\n           [ 0.40824829,  0.40824829, -0.81649658,  0.        ,  0.        ],\n           [ 0.28867513,  0.28867513,  0.28867513, -0.8660254 ,  0.        ],\n           [ 0.2236068 ,  0.2236068 ,  0.2236068 ,  0.2236068 , -0.89442719]])\n\n    '
H = (numpy.tril(numpy.ones((n, n)), (- 1)) - numpy.diag(numpy.arange(n)))
tempResult = arange(1, (n + 1))
	
===================================================================	
hilbert: 155	
----------------------------	

'\n    Create a Hilbert matrix of order `n`.\n\n    Returns the `n` by `n` array with entries `h[i,j] = 1 / (i + j + 1)`.\n\n    Parameters\n    ----------\n    n : int\n        The size of the array to create.\n\n    Returns\n    -------\n    h : (n, n) ndarray\n        The Hilbert matrix.\n\n    See Also\n    --------\n    invhilbert : Compute the inverse of a Hilbert matrix.\n\n    Notes\n    -----\n    .. versionadded:: 0.10.0\n\n    Examples\n    --------\n    >>> from scipy.linalg import hilbert\n    >>> hilbert(3)\n    array([[ 1.        ,  0.5       ,  0.33333333],\n           [ 0.5       ,  0.33333333,  0.25      ],\n           [ 0.33333333,  0.25      ,  0.2       ]])\n\n    '
tempResult = arange(((2 * n) - 1))
	
===================================================================	
tri: 16	
----------------------------	

'\n    Construct (N, M) matrix filled with ones at and below the k-th diagonal.\n\n    The matrix has A[i,j] == 1 for i <= j + k\n\n    Parameters\n    ----------\n    N : int\n        The size of the first dimension of the matrix.\n    M : int or None, optional\n        The size of the second dimension of the matrix. If `M` is None,\n        `M = N` is assumed.\n    k : int, optional\n        Number of subdiagonal below which matrix is filled with ones.\n        `k` = 0 is the main diagonal, `k` < 0 subdiagonal and `k` > 0\n        superdiagonal.\n    dtype : dtype, optional\n        Data type of the matrix.\n\n    Returns\n    -------\n    tri : (N, M) ndarray\n        Tri matrix.\n\n    Examples\n    --------\n    >>> from scipy.linalg import tri\n    >>> tri(3, 5, 2, dtype=int)\n    array([[1, 1, 1, 0, 0],\n           [1, 1, 1, 1, 0],\n           [1, 1, 1, 1, 1]])\n    >>> tri(3, 5, -1, dtype=int)\n    array([[0, 0, 0, 0, 0],\n           [1, 0, 0, 0, 0],\n           [1, 1, 0, 0, 0]])\n\n    '
if (M is None):
    M = N
if isinstance(M, string_types):
    dtype = M
    M = N
tempResult = arange(N)
	
===================================================================	
tri: 16	
----------------------------	

'\n    Construct (N, M) matrix filled with ones at and below the k-th diagonal.\n\n    The matrix has A[i,j] == 1 for i <= j + k\n\n    Parameters\n    ----------\n    N : int\n        The size of the first dimension of the matrix.\n    M : int or None, optional\n        The size of the second dimension of the matrix. If `M` is None,\n        `M = N` is assumed.\n    k : int, optional\n        Number of subdiagonal below which matrix is filled with ones.\n        `k` = 0 is the main diagonal, `k` < 0 subdiagonal and `k` > 0\n        superdiagonal.\n    dtype : dtype, optional\n        Data type of the matrix.\n\n    Returns\n    -------\n    tri : (N, M) ndarray\n        Tri matrix.\n\n    Examples\n    --------\n    >>> from scipy.linalg import tri\n    >>> tri(3, 5, 2, dtype=int)\n    array([[1, 1, 1, 0, 0],\n           [1, 1, 1, 1, 0],\n           [1, 1, 1, 1, 1]])\n    >>> tri(3, 5, -1, dtype=int)\n    array([[0, 0, 0, 0, 0],\n           [1, 0, 0, 0, 0],\n           [1, 1, 0, 0, 0]])\n\n    '
if (M is None):
    M = N
if isinstance(M, string_types):
    dtype = M
    M = N
tempResult = arange(M)
	
===================================================================	
dft: 235	
----------------------------	

'\n    Discrete Fourier transform matrix.\n\n    Create the matrix that computes the discrete Fourier transform of a\n    sequence [1]_.  The n-th primitive root of unity used to generate the\n    matrix is exp(-2*pi*i/n), where i = sqrt(-1).\n\n    Parameters\n    ----------\n    n : int\n        Size the matrix to create.\n    scale : str, optional\n        Must be None, \'sqrtn\', or \'n\'.\n        If `scale` is \'sqrtn\', the matrix is divided by `sqrt(n)`.\n        If `scale` is \'n\', the matrix is divided by `n`.\n        If `scale` is None (the default), the matrix is not normalized, and the\n        return value is simply the Vandermonde matrix of the roots of unity.\n\n    Returns\n    -------\n    m : (n, n) ndarray\n        The DFT matrix.\n\n    Notes\n    -----\n    When `scale` is None, multiplying a vector by the matrix returned by\n    `dft` is mathematically equivalent to (but much less efficient than)\n    the calculation performed by `scipy.fftpack.fft`.\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] "DFT matrix", http://en.wikipedia.org/wiki/DFT_matrix\n\n    Examples\n    --------\n    >>> from scipy.linalg import dft\n    >>> np.set_printoptions(precision=5, suppress=True)\n    >>> x = np.array([1, 2, 3, 0, 3, 2, 1, 0])\n    >>> m = dft(8)\n    >>> m.dot(x)   # Compute the DFT of x\n    array([ 12.+0.j,  -2.-2.j,   0.-4.j,  -2.+2.j,   4.+0.j,  -2.-2.j,\n            -0.+4.j,  -2.+2.j])\n\n    Verify that ``m.dot(x)`` is the same as ``fft(x)``.\n\n    >>> from scipy.fftpack import fft\n    >>> fft(x)     # Same result as m.dot(x)\n    array([ 12.+0.j,  -2.-2.j,   0.-4.j,  -2.+2.j,   4.+0.j,  -2.-2.j,\n             0.+4.j,  -2.+2.j])\n    '
if (scale not in [None, 'sqrtn', 'n']):
    raise ValueError(("scale must be None, 'sqrtn', or 'n'; %r is not valid." % (scale,)))
tempResult = arange(n)
	
===================================================================	
dft: 236	
----------------------------	

'\n    Discrete Fourier transform matrix.\n\n    Create the matrix that computes the discrete Fourier transform of a\n    sequence [1]_.  The n-th primitive root of unity used to generate the\n    matrix is exp(-2*pi*i/n), where i = sqrt(-1).\n\n    Parameters\n    ----------\n    n : int\n        Size the matrix to create.\n    scale : str, optional\n        Must be None, \'sqrtn\', or \'n\'.\n        If `scale` is \'sqrtn\', the matrix is divided by `sqrt(n)`.\n        If `scale` is \'n\', the matrix is divided by `n`.\n        If `scale` is None (the default), the matrix is not normalized, and the\n        return value is simply the Vandermonde matrix of the roots of unity.\n\n    Returns\n    -------\n    m : (n, n) ndarray\n        The DFT matrix.\n\n    Notes\n    -----\n    When `scale` is None, multiplying a vector by the matrix returned by\n    `dft` is mathematically equivalent to (but much less efficient than)\n    the calculation performed by `scipy.fftpack.fft`.\n\n    .. versionadded:: 0.14.0\n\n    References\n    ----------\n    .. [1] "DFT matrix", http://en.wikipedia.org/wiki/DFT_matrix\n\n    Examples\n    --------\n    >>> from scipy.linalg import dft\n    >>> np.set_printoptions(precision=5, suppress=True)\n    >>> x = np.array([1, 2, 3, 0, 3, 2, 1, 0])\n    >>> m = dft(8)\n    >>> m.dot(x)   # Compute the DFT of x\n    array([ 12.+0.j,  -2.-2.j,   0.-4.j,  -2.+2.j,   4.+0.j,  -2.-2.j,\n            -0.+4.j,  -2.+2.j])\n\n    Verify that ``m.dot(x)`` is the same as ``fft(x)``.\n\n    >>> from scipy.fftpack import fft\n    >>> fft(x)     # Same result as m.dot(x)\n    array([ 12.+0.j,  -2.-2.j,   0.-4.j,  -2.+2.j,   4.+0.j,  -2.-2.j,\n             0.+4.j,  -2.+2.j])\n    '
if (scale not in [None, 'sqrtn', 'n']):
    raise ValueError(("scale must be None, 'sqrtn', or 'n'; %r is not valid." % (scale,)))
omegas = np.exp(((((- 2j) * np.pi) * np.arange(n)) / n)).reshape((- 1), 1)
tempResult = arange(n)
	
===================================================================	
invpascal: 228	
----------------------------	

'\n    Returns the inverse of the n x n Pascal matrix.\n\n    The Pascal matrix is a matrix containing the binomial coefficients as\n    its elements.\n\n    Parameters\n    ----------\n    n : int\n        The size of the matrix to create; that is, the result is an n x n\n        matrix.\n    kind : str, optional\n        Must be one of \'symmetric\', \'lower\', or \'upper\'.\n        Default is \'symmetric\'.\n    exact : bool, optional\n        If `exact` is True, the result is either an array of type\n        `numpy.int64` (if `n` <= 35) or an object array of Python integers.\n        If `exact` is False, the coefficients in the matrix are computed using\n        `scipy.special.comb` with `exact=False`.  The result will be a floating\n        point array, and for large `n`, the values in the array will not be the\n        exact coefficients.\n\n    Returns\n    -------\n    invp : (n, n) ndarray\n        The inverse of the Pascal matrix.\n\n    See Also\n    --------\n    pascal\n\n    Notes\n    -----\n\n    .. versionadded:: 0.16.0\n\n    References\n    ----------\n    .. [1] "Pascal matrix",  http://en.wikipedia.org/wiki/Pascal_matrix\n    .. [2] Cohen, A. M., "The inverse of a Pascal matrix", Mathematical\n           Gazette, 59(408), pp. 111-112, 1975.\n\n    Examples\n    --------\n    >>> from scipy.linalg import invpascal, pascal\n    >>> invp = invpascal(5)\n    >>> invp\n    array([[  5, -10,  10,  -5,   1],\n           [-10,  30, -35,  19,  -4],\n           [ 10, -35,  46, -27,   6],\n           [ -5,  19, -27,  17,  -4],\n           [  1,  -4,   6,  -4,   1]])\n\n    >>> p = pascal(5)\n    >>> p.dot(invp)\n    array([[ 1.,  0.,  0.,  0.,  0.],\n           [ 0.,  1.,  0.,  0.,  0.],\n           [ 0.,  0.,  1.,  0.,  0.],\n           [ 0.,  0.,  0.,  1.,  0.],\n           [ 0.,  0.,  0.,  0.,  1.]])\n\n    An example of the use of `kind` and `exact`:\n\n    >>> invpascal(5, kind=\'lower\', exact=False)\n    array([[ 1., -0.,  0., -0.,  0.],\n           [-1.,  1., -0.,  0., -0.],\n           [ 1., -2.,  1., -0.,  0.],\n           [-1.,  3., -3.,  1., -0.],\n           [ 1., -4.,  6., -4.,  1.]])\n\n    '
from scipy.special import comb
if (kind not in ['symmetric', 'lower', 'upper']):
    raise ValueError("'kind' must be 'symmetric', 'lower' or 'upper'.")
if (kind == 'symmetric'):
    if exact:
        if (n > 34):
            dt = object
        else:
            dt = numpy.int64
    else:
        dt = numpy.float64
    invp = numpy.empty((n, n), dtype=dt)
    for i in range(n):
        for j in range(0, (i + 1)):
            v = 0
            for k in range((n - i)):
                v += (comb((i + k), k, exact=exact) * comb((i + k), ((i + k) - j), exact=exact))
            invp[(i, j)] = (((- 1) ** (i - j)) * v)
            if (i != j):
                invp[(j, i)] = invp[(i, j)]
else:
    invp = pascal(n, kind=kind, exact=exact)
    if (invp.dtype == numpy.uint64):
        invp = invp.view(numpy.int64)
    tempResult = arange(n)
	
===================================================================	
_get_array: 28	
----------------------------	

'\n    Get a test array of given shape and data type.\n    Returned NxN matrices are posdef, and 2xN are banded-posdef.\n\n    '
if ((len(shape) == 2) and (shape[0] == 2)):
    x = numpy.zeros(shape, dtype=dtype)
    x[0, 1:] = (- 1)
    x[1] = 2
    return x
elif ((len(shape) == 2) and (shape[0] == shape[1])):
    x = numpy.zeros(shape, dtype=dtype)
    tempResult = arange(shape[0])
	
===================================================================	
TestSolve.test_singularity: 389	
----------------------------	

a = numpy.array([[1, 0, 0, 0, 0, 0, 1, 0, 1], [1, 1, 1, 0, 0, 0, 1, 0, 1], [0, 1, 1, 0, 0, 0, 1, 0, 1], [1, 0, 1, 1, 1, 1, 0, 0, 0], [1, 0, 1, 1, 1, 1, 0, 0, 0], [1, 0, 1, 1, 1, 1, 0, 0, 0], [1, 0, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]])
tempResult = arange(9)
	
===================================================================	
TestMatrixNorms.test_keepdims_kwd: 944	
----------------------------	

tempResult = arange(120, dtype='d')
	
===================================================================	
TestSolveCirculant.test_basic3: 1000	
----------------------------	

c = numpy.array([1, 2, (- 3), (- 5)])
tempResult = arange(24)
	
===================================================================	
TestSolveCirculant.test_complex: 1007	
----------------------------	

c = numpy.array([(1 + 2j), (- 3), 4j, 5])
tempResult = arange(8)
	
===================================================================	
TestSolve.test_transposed_keyword: 413	
----------------------------	

tempResult = arange(9)
	
===================================================================	
TestSolveCirculant.test_basic2: 993	
----------------------------	

c = numpy.array([1, 2, (- 3), (- 5)])
tempResult = arange(12)
	
===================================================================	
TestMatrix_Balance.test_separate: 1072	
----------------------------	

(_, (y, z)) = matrix_balance(numpy.array([[1000, 1], [1000, 0]]), separate=1)
assert_equal(int(numpy.diff(numpy.log2(y))), 5)
tempResult = arange(2)
	
===================================================================	
TestSolve.test_pos_and_sym: 381	
----------------------------	

tempResult = arange(1, 10)
	
===================================================================	
TestFBLAS2Simple.test_syr_her: 206	
----------------------------	

tempResult = arange(1, 5, dtype='d')
	
===================================================================	
TestFBLAS2Simple.test_syr_her: 210	
----------------------------	

x = numpy.arange(1, 5, dtype='d')
resx = numpy.triu((x[:, numpy.newaxis] * x))
resx_reverse = numpy.triu((x[::(- 1), numpy.newaxis] * x[::(- 1)]))
y = numpy.linspace(0, 8.5, 17, endpoint=False)
tempResult = arange(1, 9, dtype='d')
	
===================================================================	
TestFBLAS2Simple.test_syr2: 281	
----------------------------	

tempResult = arange(1, 5, dtype='d')
	
===================================================================	
TestFBLAS2Simple.test_syr2: 282	
----------------------------	

x = numpy.arange(1, 5, dtype='d')
tempResult = arange(5, 9, dtype='d')
	
===================================================================	
TestFBLAS2Simple.test_her2: 314	
----------------------------	

tempResult = arange(1, 9, dtype='d')
	
===================================================================	
TestFBLAS2Simple.test_her2: 315	
----------------------------	

x = np.arange(1, 9, dtype='d').view('D')
tempResult = arange(9, 17, dtype='d')
	
===================================================================	
test_dgemm.test_shapes: 30	
----------------------------	

tempResult = arange(6, dtype='d')
	
===================================================================	
test_dgemm.test_shapes: 31	
----------------------------	

a = np.arange(6, dtype='d').reshape((3, 2))
tempResult = arange((- 6), 2, dtype='d')
	
===================================================================	
test_dgemm.test_transposes: 9	
----------------------------	

tempResult = arange(12, dtype='d')
	
===================================================================	
test_dgemm.test_transposes: 10	
----------------------------	

a = np.arange(12, dtype='d').reshape((3, 4))[:2, :2]
tempResult = arange(1, 13, dtype='d')
	
===================================================================	
test_lapack_misaligned: 1827	
----------------------------	

M = numpy.eye(10, dtype=float)
tempResult = arange(100)
	
===================================================================	
test_lapack_misaligned: 1829	
----------------------------	

M = numpy.eye(10, dtype=float)
R = numpy.arange(100)
R.shape = (10, 10)
tempResult = arange(20000, dtype=numpy.uint8)
	
===================================================================	
TestEig.test_not_square_error: 249	
----------------------------	

'Check that passing a non-square array raises a ValueError.'
tempResult = arange(6)
	
===================================================================	
TestEig.test_shape_mismatch: 255	
----------------------------	

'Check that passing arrays of with different shapes raises a ValueError.'
A = identity(2)
tempResult = arange(9.0)
	
===================================================================	
BaseQRupdate.test_integer_input: 1389	
----------------------------	

tempResult = arange(16)
	
===================================================================	
test_symmetry: 129	
----------------------------	

tempResult = arange(9)
	
===================================================================	
test_unstable: 85	
----------------------------	

random = numpy.random.RandomState(1234)
n = 100
tempResult = arange(n)
	
===================================================================	
TestPILUtil.test_bytescale_cscale_lowhigh: 67	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestPILUtil.test_bytescale_low_greaterthan_high: 88	
----------------------------	

with assert_raises(ValueError):
    tempResult = arange(3)
	
===================================================================	
TestPILUtil.test_bytescale_high_greaterthan_255: 96	
----------------------------	

with assert_raises(ValueError):
    tempResult = arange(3)
	
===================================================================	
TestPILUtil.test_bytescale_low_equals_high: 99	
----------------------------	

tempResult = arange(3)
	
===================================================================	
TestPILUtil.test_bytescale_low_lessthan_0: 92	
----------------------------	

with assert_raises(ValueError):
    tempResult = arange(3)
	
===================================================================	
test_imread_4bit: 181	
----------------------------	

filename = os.path.join(datapath, 'data', 'pattern4bit.png')
with open(filename, 'rb') as f:
    im = scipy.misc.imread(f)
assert_equal(im.dtype, numpy.uint8)
tempResult = arange(12)
	
===================================================================	
test_imread_4bit: 181	
----------------------------	

filename = os.path.join(datapath, 'data', 'pattern4bit.png')
with open(filename, 'rb') as f:
    im = scipy.misc.imread(f)
assert_equal(im.dtype, numpy.uint8)
tempResult = arange(31)
	
===================================================================	
_select: 210	
----------------------------	

'Returns min, max, or both, plus their positions (if requested), and\n    median.'
input = numpy.asanyarray(input)
find_positions = (find_min_positions or find_max_positions)
positions = None
if find_positions:
    tempResult = arange(input.size)
	
===================================================================	
_select: 275	
----------------------------	

'Returns min, max, or both, plus their positions (if requested), and\n    median.'
input = numpy.asanyarray(input)
find_positions = (find_min_positions or find_max_positions)
positions = None
if find_positions:
    positions = numpy.arange(input.size).reshape(input.shape)

def single_group(vals, positions):
    result = []
    if find_min:
        result += [vals.min()]
    if find_min_positions:
        result += [positions[(vals == vals.min())][0]]
    if find_max:
        result += [vals.max()]
    if find_max_positions:
        result += [positions[(vals == vals.max())][0]]
    if find_median:
        result += [numpy.median(vals)]
    return result
if (labels is None):
    return single_group(input, positions)
(input, labels) = numpy.broadcast_arrays(input, labels)
if (index is None):
    mask = (labels > 0)
    masked_positions = None
    if find_positions:
        masked_positions = positions[mask]
    return single_group(input[mask], masked_positions)
if numpy.isscalar(index):
    mask = (labels == index)
    masked_positions = None
    if find_positions:
        masked_positions = positions[mask]
    return single_group(input[mask], masked_positions)
if ((not _safely_castable_to_int(labels.dtype)) or (labels.min() < 0) or (labels.max() > labels.size)):
    (unique_labels, labels) = numpy.unique(labels, return_inverse=True)
    idxs = numpy.searchsorted(unique_labels, index)
    idxs[(idxs >= unique_labels.size)] = 0
    found = (unique_labels[idxs] == index)
else:
    idxs = numpy.asanyarray(index, numpy.int).copy()
    found = ((idxs >= 0) & (idxs <= labels.max()))
idxs[(~ found)] = (labels.max() + 1)
if find_median:
    order = numpy.lexsort((input.ravel(), labels.ravel()))
else:
    order = input.ravel().argsort()
input = input.ravel()[order]
labels = labels.ravel()[order]
if find_positions:
    positions = positions.ravel()[order]
result = []
if find_min:
    mins = numpy.zeros((labels.max() + 2), input.dtype)
    mins[labels[::(- 1)]] = input[::(- 1)]
    result += [mins[idxs]]
if find_min_positions:
    minpos = numpy.zeros((labels.max() + 2), int)
    minpos[labels[::(- 1)]] = positions[::(- 1)]
    result += [minpos[idxs]]
if find_max:
    maxs = numpy.zeros((labels.max() + 2), input.dtype)
    maxs[labels] = input
    result += [maxs[idxs]]
if find_max_positions:
    maxpos = numpy.zeros((labels.max() + 2), int)
    maxpos[labels] = positions
    result += [maxpos[idxs]]
if find_median:
    tempResult = arange(len(labels))
	
===================================================================	
labeled_comprehension: 72	
----------------------------	

'\n    Roughly equivalent to [func(input[labels == i]) for i in index].\n\n    Sequentially applies an arbitrary function (that works on array_like input)\n    to subsets of an n-D image array specified by `labels` and `index`.\n    The option exists to provide the function with positional parameters as the\n    second argument.\n\n    Parameters\n    ----------\n    input : array_like\n        Data from which to select `labels` to process.\n    labels : array_like or None\n        Labels to objects in `input`.\n        If not None, array must be same shape as `input`.\n        If None, `func` is applied to raveled `input`.\n    index : int, sequence of ints or None\n        Subset of `labels` to which to apply `func`.\n        If a scalar, a single value is returned.\n        If None, `func` is applied to all non-zero values of `labels`.\n    func : callable\n        Python function to apply to `labels` from `input`.\n    out_dtype : dtype\n        Dtype to use for `result`.\n    default : int, float or None\n        Default return value when a element of `index` does not exist\n        in `labels`.\n    pass_positions : bool, optional\n        If True, pass linear indices to `func` as a second argument.\n        Default is False.\n\n    Returns\n    -------\n    result : ndarray\n        Result of applying `func` to each of `labels` to `input` in `index`.\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2, 0, 0],\n    ...               [5, 3, 0, 4],\n    ...               [0, 0, 0, 7],\n    ...               [9, 3, 0, 0]])\n    >>> from scipy import ndimage\n    >>> lbl, nlbl = ndimage.label(a)\n    >>> lbls = np.arange(1, nlbl+1)\n    >>> ndimage.labeled_comprehension(a, lbl, lbls, np.mean, float, 0)\n    array([ 2.75,  5.5 ,  6.  ])\n\n    Falling back to `default`:\n\n    >>> lbls = np.arange(1, nlbl+2)\n    >>> ndimage.labeled_comprehension(a, lbl, lbls, np.mean, float, -1)\n    array([ 2.75,  5.5 ,  6.  , -1.  ])\n\n    Passing positions:\n\n    >>> def fn(val, pos):\n    ...     print("fn says: %s : %s" % (val, pos))\n    ...     return (val.sum()) if (pos.sum() % 2 == 0) else (-val.sum())\n    ...\n    >>> ndimage.labeled_comprehension(a, lbl, lbls, fn, float, 0, True)\n    fn says: [1 2 5 3] : [0 1 4 5]\n    fn says: [4 7] : [ 7 11]\n    fn says: [9 3] : [12 13]\n    array([ 11.,  11., -12.,   0.])\n\n    '
as_scalar = numpy.isscalar(index)
input = numpy.asarray(input)
if pass_positions:
    tempResult = arange(input.size)
	
===================================================================	
distance_transform_cdt: 380	
----------------------------	

"\n    Distance transform for chamfer type of transforms.\n\n    Parameters\n    ----------\n    input : array_like\n        Input\n    metric : {'chessboard', 'taxicab'}, optional\n        The `metric` determines the type of chamfering that is done. If the\n        `metric` is equal to 'taxicab' a structure is generated using\n        generate_binary_structure with a squared distance equal to 1. If\n        the `metric` is equal to 'chessboard', a `metric` is generated\n        using generate_binary_structure with a squared distance equal to\n        the dimensionality of the array. These choices correspond to the\n        common interpretations of the 'taxicab' and the 'chessboard'\n        distance metrics in two dimensions.\n\n        The default for `metric` is 'chessboard'.\n    return_distances, return_indices : bool, optional\n        The `return_distances`, and `return_indices` flags can be used to\n        indicate if the distance transform, the feature transform, or both\n        must be returned.\n\n        If the feature transform is returned (``return_indices=True``),\n        the index of the closest background element is returned along\n        the first axis of the result.\n\n        The `return_distances` default is True, and the\n        `return_indices` default is False.\n    distances, indices : ndarrays of int32, optional\n        The `distances` and `indices` arguments can be used to give optional\n        output arrays that must be the same shape as `input`.\n\n    "
if ((not return_distances) and (not return_indices)):
    msg = 'at least one of distances/indices must be specified'
    raise RuntimeError(msg)
ft_inplace = isinstance(indices, numpy.ndarray)
dt_inplace = isinstance(distances, numpy.ndarray)
input = numpy.asarray(input)
if (metric in ['taxicab', 'cityblock', 'manhattan']):
    rank = input.ndim
    metric = generate_binary_structure(rank, 1)
elif (metric == 'chessboard'):
    rank = input.ndim
    metric = generate_binary_structure(rank, rank)
else:
    try:
        metric = numpy.asarray(metric)
    except:
        raise RuntimeError('invalid metric provided')
    for s in metric.shape:
        if (s != 3):
            raise RuntimeError('metric sizes must be equal to 3')
if (not metric.flags.contiguous):
    metric = metric.copy()
if dt_inplace:
    if (distances.dtype.type != numpy.int32):
        raise RuntimeError('distances must be of int32 type')
    if (distances.shape != input.shape):
        raise RuntimeError('distances has wrong shape')
    dt = distances
    dt[...] = numpy.where(input, (- 1), 0).astype(numpy.int32)
else:
    dt = numpy.where(input, (- 1), 0).astype(numpy.int32)
rank = dt.ndim
if return_indices:
    sz = numpy.product(dt.shape, axis=0)
    tempResult = arange(sz, dtype=numpy.int32)
	
===================================================================	
check11: 58	
----------------------------	

func = TRANSFORM_FUNCTIONS[j]
tempResult = arange(12)
	
===================================================================	
TestThreading.test_correlate1d: 170	
----------------------------	

d = numpy.random.randn(5000)
os = numpy.empty((4, d.size))
ot = numpy.empty_like(os)
tempResult = arange(5)
	
===================================================================	
TestThreading.test_correlate1d: 171	
----------------------------	

d = numpy.random.randn(5000)
os = numpy.empty((4, d.size))
ot = numpy.empty_like(os)
self.check_func_serial(4, scipy.ndimage.correlate1d, (d, numpy.arange(5)), os)
tempResult = arange(5)
	
===================================================================	
test_gh_5430: 31	
----------------------------	

sigma = numpy.int32(1)
out = scipy.ndimage._ni_support._normalize_sequence(sigma, 1)
assert_equal(out, [sigma])
sigma = numpy.int64(1)
out = scipy.ndimage._ni_support._normalize_sequence(sigma, 1)
assert_equal(out, [sigma])
sigma = 1
out = scipy.ndimage._ni_support._normalize_sequence(sigma, 1)
assert_equal(out, [sigma])
sigma = [1, 1]
out = scipy.ndimage._ni_support._normalize_sequence(sigma, 2)
assert_equal(out, sigma)
x = numpy.random.normal(size=(256, 256))
perlin = numpy.zeros_like(x)
tempResult = arange(6)
	
===================================================================	
test_ticket_701: 10	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_minmaximum_filter1d: 211	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_histogram01: 776	
----------------------------	

expected = numpy.ones(10)
tempResult = arange(10)
	
===================================================================	
TestNdimage.test_generic_filter01: 824	
----------------------------	

filter_ = numpy.array([[1.0, 2.0], [3.0, 4.0]])
footprint = numpy.array([[1, 0], [0, 1]])
cf = numpy.array([1.0, 4.0])

def _filter_func(buffer, weights, total=1.0):
    weights = (cf / total)
    return (buffer * weights).sum()
for type in self.types:
    tempResult = arange(12, dtype=type)
	
===================================================================	
TestNdimage.test_gauss04: 349	
----------------------------	

tempResult = arange((100 * 100))
	
===================================================================	
TestNdimage.test_fourier_shift_real01: 983	
----------------------------	

for shape in [(32, 16), (31, 15)]:
    for dtype in [numpy.float32, numpy.float64]:
        tempResult = arange((shape[0] * shape[1]), dtype=dtype)
	
===================================================================	
TestNdimage.test_zoom2: 1573	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestNdimage.test_gauss05: 358	
----------------------------	

tempResult = arange((100 * 100))
	
===================================================================	
TestNdimage.test_fourier_shift_complex01: 996	
----------------------------	

for shape in [(32, 16), (31, 15)]:
    for type in [numpy.complex64, numpy.complex128]:
        tempResult = arange((shape[0] * shape[1]), dtype=type)
	
===================================================================	
TestNdimage.test_gauss06: 367	
----------------------------	

tempResult = arange((100 * 100))
	
===================================================================	
TestNdimage.test_generic_filter1d01: 809	
----------------------------	

weights = numpy.array([1.1, 2.2, 3.3])

def _filter_func(input, output, fltr, total):
    fltr = (fltr / total)
    for ii in range((input.shape[0] - 2)):
        output[ii] = (input[ii] * fltr[0])
        output[ii] += (input[(ii + 1)] * fltr[1])
        output[ii] += (input[(ii + 2)] * fltr[2])
for type in self.types:
    tempResult = arange(12, dtype=type)
	
===================================================================	
TestNdimage.test_gauss03: 340	
----------------------------	

tempResult = arange((100 * 100))
	
===================================================================	
test_zoom_output_shape: 17	
----------------------------	

'Ticket #643'
tempResult = arange(12)
	
===================================================================	
test_byte_order_median: 9	
----------------------------	

'Regression test for #413: median_filter does not handle bytes orders.'
tempResult = arange(9, dtype='<f4')
	
===================================================================	
test_byte_order_median: 11	
----------------------------	

'Regression test for #413: median_filter does not handle bytes orders.'
a = np.arange(9, dtype='<f4').reshape(3, 3)
ref = scipy.ndimage.filters.median_filter(a, (3, 3))
tempResult = arange(9, dtype='>f4')
	
===================================================================	
polynomial: 67	
----------------------------	

"\n    Factory function for a general polynomial model.\n\n    Parameters\n    ----------\n    order : int or sequence\n        If an integer, it becomes the order of the polynomial to fit. If\n        a sequence of numbers, then these are the explicit powers in the\n        polynomial.\n        A constant term (power 0) is always included, so don't include 0.\n        Thus, polynomial(n) is equivalent to polynomial(range(1, n+1)).\n\n    Returns\n    -------\n    polynomial : Model instance\n        Model instance.\n\n    "
powers = numpy.asarray(order)
if (powers.shape == ()):
    tempResult = arange(1, (powers + 1))
	
===================================================================	
BroydenTridiagonal.__init__: 69	
----------------------------	

numpy.random.seed(0)
self.n = n
self.x0 = (- numpy.ones(n))
self.lb = numpy.linspace((- 2), (- 1.5), n)
self.ub = numpy.linspace((- 0.8), 0.0, n)
self.lb += (0.1 * numpy.random.randn(n))
self.ub += (0.1 * numpy.random.randn(n))
self.x0 += (0.1 * numpy.random.randn(n))
self.x0 = make_strictly_feasible(self.x0, self.lb, self.ub)
if (mode == 'sparse'):
    self.sparsity = lil_matrix((n, n), dtype=int)
    tempResult = arange(n)
	
===================================================================	
BroydenTridiagonal.__init__: 71	
----------------------------	

numpy.random.seed(0)
self.n = n
self.x0 = (- numpy.ones(n))
self.lb = numpy.linspace((- 2), (- 1.5), n)
self.ub = numpy.linspace((- 0.8), 0.0, n)
self.lb += (0.1 * numpy.random.randn(n))
self.ub += (0.1 * numpy.random.randn(n))
self.x0 += (0.1 * numpy.random.randn(n))
self.x0 = make_strictly_feasible(self.x0, self.lb, self.ub)
if (mode == 'sparse'):
    self.sparsity = lil_matrix((n, n), dtype=int)
    i = numpy.arange(n)
    self.sparsity[(i, i)] = 1
    tempResult = arange(1, n)
	
===================================================================	
BroydenTridiagonal.__init__: 73	
----------------------------	

numpy.random.seed(0)
self.n = n
self.x0 = (- numpy.ones(n))
self.lb = numpy.linspace((- 2), (- 1.5), n)
self.ub = numpy.linspace((- 0.8), 0.0, n)
self.lb += (0.1 * numpy.random.randn(n))
self.ub += (0.1 * numpy.random.randn(n))
self.x0 += (0.1 * numpy.random.randn(n))
self.x0 = make_strictly_feasible(self.x0, self.lb, self.ub)
if (mode == 'sparse'):
    self.sparsity = lil_matrix((n, n), dtype=int)
    i = numpy.arange(n)
    self.sparsity[(i, i)] = 1
    i = numpy.arange(1, n)
    self.sparsity[(i, (i - 1))] = 1
    tempResult = arange((n - 1))
	
===================================================================	
BroydenTridiagonal._jac: 92	
----------------------------	

J = lil_matrix((self.n, self.n))
tempResult = arange(self.n)
	
===================================================================	
BroydenTridiagonal._jac: 94	
----------------------------	

J = lil_matrix((self.n, self.n))
i = numpy.arange(self.n)
J[(i, i)] = (3 - (2 * x))
tempResult = arange(1, self.n)
	
===================================================================	
BroydenTridiagonal._jac: 96	
----------------------------	

J = lil_matrix((self.n, self.n))
i = numpy.arange(self.n)
J[(i, i)] = (3 - (2 * x))
i = numpy.arange(1, self.n)
J[(i, (i - 1))] = (- 1)
tempResult = arange((self.n - 1))
	
===================================================================	
test_enzo_example_c_with_infeasibility: 219	
----------------------------	

m = 50
c = (- numpy.ones(m))
tempResult = arange(m)
	
===================================================================	
test_enzo_example_c_with_unboundedness: 210	
----------------------------	

m = 50
c = (- numpy.ones(m))
tempResult = arange(m)
	
===================================================================	
test_enzo_example_c_with_degeneracy: 201	
----------------------------	

m = 20
c = (- numpy.ones(m))
tempResult = arange(1, (m + 1))
	
===================================================================	
TestCurveFit.test_maxfev_and_bounds: 354	
----------------------------	

tempResult = arange(0, 10)
	
===================================================================	
Get no callers of function numpy.arange at line 400 col 16.	
===================================================================	
TestDifferentialEvolutionSolver.setUp: 18	
----------------------------	

self.old_seterr = numpy.seterr(invalid='raise')
self.limits = numpy.array([[0.0, 0.0], [2.0, 2.0]])
self.bounds = [(0.0, 2.0), (0.0, 2.0)]
self.dummy_solver = DifferentialEvolutionSolver(self.quadratic, [(0, 100)])
self.dummy_solver2 = DifferentialEvolutionSolver(self.quadratic, [(0, 1)], popsize=7, mutation=0.5)
tempResult = arange(0.1, 0.8, 0.1)
	
===================================================================	
TestDifferentialEvolutionSolver.test_select_samples: 159	
----------------------------	

tempResult = arange(12.0, dtype='float64')
	
===================================================================	
test_group_columns: 14	
----------------------------	

structure = [[1, 1, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0], [0, 1, 1, 1, 0, 0], [0, 0, 1, 1, 1, 0], [0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0]]
for transform in [numpy.asarray, csr_matrix, csc_matrix, lil_matrix]:
    A = transform(structure)
    tempResult = arange(6)
	
===================================================================	
TestApproxDerivativeSparse.test_all: 317	
----------------------------	

A = self.structure(self.n)
tempResult = arange(self.n)
	
===================================================================	
TestApproxDerivativeSparse.__init__: 281	
----------------------------	

numpy.random.seed(0)
self.n = 50
tempResult = arange(self.n)
	
===================================================================	
TestApproxDerivativeSparse.__init__: 282	
----------------------------	

numpy.random.seed(0)
self.n = 50
self.lb = ((- 0.1) * (1 + numpy.arange(self.n)))
tempResult = arange(self.n)
	
===================================================================	
TestApproxDerivativeSparse.test_equivalence: 337	
----------------------------	

structure = numpy.ones((self.n, self.n), dtype=int)
tempResult = arange(self.n)
	
===================================================================	
F_1: 75	
----------------------------	

g = numpy.zeros([n])
tempResult = arange(2, (n + 1))
	
===================================================================	
Get no callers of function numpy.arange at line 53 col 8.	
===================================================================	
Get no callers of function numpy.arange at line 54 col 37.	
===================================================================	
Get no callers of function numpy.arange at line 56 col 9.	
===================================================================	
F_6: 112	
----------------------------	

c = 0.9
tempResult = arange(1, (n + 1))
	
===================================================================	
F_9: 138	
----------------------------	

g = numpy.zeros([n])
tempResult = arange(2, n)
	
===================================================================	
F_2: 87	
----------------------------	

g = numpy.zeros([n])
tempResult = arange(2, (n + 1))
	
===================================================================	
cheb1ap: 964	
----------------------------	

"\n    Return (z,p,k) for Nth-order Chebyshev type I analog lowpass filter.\n\n    The returned filter prototype has `rp` decibels of ripple in the passband.\n\n    The filter's angular (e.g. rad/s) cutoff frequency is normalized to 1,\n    defined as the point at which the gain first drops below ``-rp``.\n\n    See Also\n    --------\n    cheby1 : Filter design function using this prototype\n\n    "
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
elif (N == 0):
    return (numpy.array([]), numpy.array([]), (10 ** ((- rp) / 20)))
z = numpy.array([])
eps = numpy.sqrt(((10 ** (0.1 * rp)) - 1.0))
mu = ((1.0 / N) * arcsinh((1 / eps)))
tempResult = arange(((- N) + 1), N, 2)
	
===================================================================	
buttap: 950	
----------------------------	

'Return (z,p,k) for analog prototype of Nth-order Butterworth filter.\n\n    The filter will have an angular (e.g. rad/s) cutoff frequency of 1.\n\n    See Also\n    --------\n    butter : Filter design function using this prototype\n\n    '
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
z = numpy.array([])
tempResult = arange(((- N) + 1), N, 2)
	
===================================================================	
group_delay: 128	
----------------------------	

'Compute the group delay of a digital filter.\n\n    The group delay measures by how many samples amplitude envelopes of\n    various spectral components of a signal are delayed by a filter.\n    It is formally defined as the derivative of continuous (unwrapped) phase::\n\n               d        jw\n     D(w) = - -- arg H(e)\n              dw\n\n    Parameters\n    ----------\n    system : tuple of array_like (b, a)\n        Numerator and denominator coefficients of a filter transfer function.\n    w : {None, int, array-like}, optional\n        If None (default), then compute at 512 frequencies equally spaced\n        around the unit circle.\n        If a single integer, then compute at that many frequencies.\n        If array, compute the delay at the frequencies given\n        (in radians/sample).\n    whole : bool, optional\n        Normally, frequencies are computed from 0 to the Nyquist frequency,\n        pi radians/sample (upper-half of unit-circle).  If `whole` is True,\n        compute frequencies from 0 to ``2*pi`` radians/sample.\n\n    Returns\n    -------\n    w : ndarray\n        The normalized frequencies at which the group delay was computed,\n        in radians/sample.\n    gd : ndarray\n        The group delay.\n\n    Notes\n    -----\n    The similar function in MATLAB is called `grpdelay`.\n\n    If the transfer function :math:`H(z)` has zeros or poles on the unit\n    circle, the group delay at corresponding frequencies is undefined.\n    When such a case arises the warning is raised and the group delay\n    is set to 0 at those frequencies.\n\n    For the details of numerical computation of the group delay refer to [1]_.\n\n    .. versionadded: 0.16.0\n\n    See Also\n    --------\n    freqz : Frequency response of a digital filter\n\n    References\n    ----------\n    .. [1] Richard G. Lyons, "Understanding Digital Signal Processing,\n           3rd edition", p. 830.\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> b, a = signal.iirdesign(0.1, 0.3, 5, 50, ftype=\'cheby1\')\n    >>> w, gd = signal.group_delay((b, a))\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.title(\'Digital filter group delay\')\n    >>> plt.plot(w, gd)\n    >>> plt.ylabel(\'Group delay [samples]\')\n    >>> plt.xlabel(\'Frequency [rad/sample]\')\n    >>> plt.show()\n\n    '
if (w is None):
    w = 512
if isinstance(w, int):
    if whole:
        w = numpy.linspace(0, (2 * pi), w, endpoint=False)
    else:
        w = numpy.linspace(0, pi, w, endpoint=False)
w = numpy.atleast_1d(w)
(b, a) = map(numpy.atleast_1d, system)
c = numpy.convolve(b, a[::(- 1)])
tempResult = arange(c.size)
	
===================================================================	
cheb2ap: 981	
----------------------------	

"\n    Return (z,p,k) for Nth-order Chebyshev type I analog lowpass filter.\n\n    The returned filter prototype has `rs` decibels of ripple in the stopband.\n\n    The filter's angular (e.g. rad/s) cutoff frequency is normalized to 1,\n    defined as the point at which the gain first reaches ``-rs``.\n\n    See Also\n    --------\n    cheby2 : Filter design function using this prototype\n\n    "
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
elif (N == 0):
    return (numpy.array([]), numpy.array([]), 1)
de = (1.0 / sqrt(((10 ** (0.1 * rs)) - 1)))
mu = (arcsinh((1.0 / de)) / N)
if (N % 2):
    tempResult = arange(((- N) + 1), 0, 2)
	
===================================================================	
cheb2ap: 981	
----------------------------	

"\n    Return (z,p,k) for Nth-order Chebyshev type I analog lowpass filter.\n\n    The returned filter prototype has `rs` decibels of ripple in the stopband.\n\n    The filter's angular (e.g. rad/s) cutoff frequency is normalized to 1,\n    defined as the point at which the gain first reaches ``-rs``.\n\n    See Also\n    --------\n    cheby2 : Filter design function using this prototype\n\n    "
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
elif (N == 0):
    return (numpy.array([]), numpy.array([]), 1)
de = (1.0 / sqrt(((10 ** (0.1 * rs)) - 1)))
mu = (arcsinh((1.0 / de)) / N)
if (N % 2):
    tempResult = arange(2, N, 2)
	
===================================================================	
cheb2ap: 983	
----------------------------	

"\n    Return (z,p,k) for Nth-order Chebyshev type I analog lowpass filter.\n\n    The returned filter prototype has `rs` decibels of ripple in the stopband.\n\n    The filter's angular (e.g. rad/s) cutoff frequency is normalized to 1,\n    defined as the point at which the gain first reaches ``-rs``.\n\n    See Also\n    --------\n    cheby2 : Filter design function using this prototype\n\n    "
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
elif (N == 0):
    return (numpy.array([]), numpy.array([]), 1)
de = (1.0 / sqrt(((10 ** (0.1 * rs)) - 1)))
mu = (arcsinh((1.0 / de)) / N)
if (N % 2):
    m = numpy.concatenate((numpy.arange(((- N) + 1), 0, 2), numpy.arange(2, N, 2)))
else:
    tempResult = arange(((- N) + 1), N, 2)
	
===================================================================	
cheb2ap: 985	
----------------------------	

"\n    Return (z,p,k) for Nth-order Chebyshev type I analog lowpass filter.\n\n    The returned filter prototype has `rs` decibels of ripple in the stopband.\n\n    The filter's angular (e.g. rad/s) cutoff frequency is normalized to 1,\n    defined as the point at which the gain first reaches ``-rs``.\n\n    See Also\n    --------\n    cheby2 : Filter design function using this prototype\n\n    "
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
elif (N == 0):
    return (numpy.array([]), numpy.array([]), 1)
de = (1.0 / sqrt(((10 ** (0.1 * rs)) - 1)))
mu = (arcsinh((1.0 / de)) / N)
if (N % 2):
    m = numpy.concatenate((numpy.arange(((- N) + 1), 0, 2), numpy.arange(2, N, 2)))
else:
    m = numpy.arange(((- N) + 1), N, 2)
z = (- conjugate((1j / sin(((m * pi) / (2.0 * N))))))
tempResult = arange(((- N) + 1), N, 2)
	
===================================================================	
lp2lp: 415	
----------------------------	

"\n    Transform a lowpass filter prototype to a different frequency.\n\n    Return an analog low-pass filter with cutoff frequency `wo`\n    from an analog low-pass filter prototype with unity cutoff frequency, in\n    transfer function ('ba') representation.\n\n    "
(a, b) = map(atleast_1d, (a, b))
try:
    wo = float(wo)
except TypeError:
    wo = float(wo[0])
d = len(a)
n = len(b)
M = max((d, n))
tempResult = arange((M - 1), (- 1), (- 1))
	
===================================================================	
lp2hp: 432	
----------------------------	

"\n    Transform a lowpass filter prototype to a highpass filter.\n\n    Return an analog high-pass filter with cutoff frequency `wo`\n    from an analog low-pass filter prototype with unity cutoff frequency, in\n    transfer function ('ba') representation.\n\n    "
(a, b) = map(atleast_1d, (a, b))
try:
    wo = float(wo)
except TypeError:
    wo = float(wo[0])
d = len(a)
n = len(b)
if (wo != 1):
    tempResult = arange(max((d, n)))
	
===================================================================	
ellipap: 1037	
----------------------------	

'Return (z,p,k) of Nth-order elliptic analog lowpass filter.\n\n    The filter is a normalized prototype that has `rp` decibels of ripple\n    in the passband and a stopband `rs` decibels down.\n\n    The filter\'s angular (e.g. rad/s) cutoff frequency is normalized to 1,\n    defined as the point at which the gain first drops below ``-rp``.\n\n    See Also\n    --------\n    ellip : Filter design function using this prototype\n\n    References\n    ----------\n    .. [1] Lutova, Tosic, and Evans, "Filter Design for Signal Processing",\n           Chapters 5 and 12.\n\n    '
if (abs(int(N)) != N):
    raise ValueError('Filter order must be a nonnegative integer')
elif (N == 0):
    return (numpy.array([]), numpy.array([]), (10 ** ((- rp) / 20)))
elif (N == 1):
    p = (- sqrt((1.0 / ((10 ** (0.1 * rp)) - 1.0))))
    k = (- p)
    z = []
    return (asarray(z), asarray(p), k)
eps = numpy.sqrt(((10 ** (0.1 * rp)) - 1))
ck1 = (eps / numpy.sqrt(((10 ** (0.1 * rs)) - 1)))
ck1p = numpy.sqrt((1 - (ck1 * ck1)))
if (ck1p == 1):
    raise ValueError('Cannot design a filter with given rp and rs specifications.')
val = scipy.special.ellipk([(ck1 * ck1), (ck1p * ck1p)])
if (abs((1 - (ck1p * ck1p))) < EPSILON):
    krat = 0
else:
    krat = ((N * val[0]) / val[1])
m = scipy.optimize.fmin(_kratio, [0.5], args=(krat,), maxfun=250, maxiter=250, disp=0)
if ((m < 0) or (m > 1)):
    m = scipy.optimize.fminbound(_kratio, 0, 1, args=(krat,), maxfun=250, maxiter=250, disp=0)
capk = scipy.special.ellipk(m)
tempResult = arange((1 - (N % 2)), N, 2)
	
===================================================================	
_campos_zeros: 1093	
----------------------------	

'\n    Return approximate zero locations of Bessel polynomials y_n(x) for order\n    `n` using polynomial fit (Campos-Calderon 2011)\n    '
if (n == 1):
    return asarray([((- 1) + 0j)])
s = npp_polyval(n, [0, 0, 2, 0, (- 3), 1])
b3 = (npp_polyval(n, [16, (- 8)]) / s)
b2 = (npp_polyval(n, [(- 24), (- 12), 12]) / s)
b1 = (npp_polyval(n, [8, 24, (- 12), (- 2)]) / s)
b0 = (npp_polyval(n, [0, (- 6), 0, 5, (- 1)]) / s)
r = npp_polyval(n, [0, 0, 2, 1])
a1 = (npp_polyval(n, [(- 6), (- 6)]) / r)
a2 = (6 / r)
tempResult = arange(1, (n + 1))
	
===================================================================	
minimum_phase: 217	
----------------------------	

'Convert a linear-phase FIR filter to minimum phase\n\n    Parameters\n    ----------\n    h : array\n        Linear-phase FIR filter coefficients.\n    method : {\'hilbert\', \'homomorphic\'}\n        The method to use:\n\n            \'homomorphic\' (default)\n                This method [4]_ [5]_ works best with filters with an\n                odd number of taps, and the resulting minimum phase filter\n                will have a magnitude response that approximates the square\n                root of the the original filter\'s magnitude response.\n\n            \'hilbert\'\n                This method [1]_ is designed to be used with equiripple\n                filters (e.g., from `remez`) with unity or zero gain\n                regions.\n\n    n_fft : int\n        The number of points to use for the FFT. Should be at least a\n        few times larger than the signal length (see Notes).\n\n    Returns\n    -------\n    h_minimum : array\n        The minimum-phase version of the filter, with length\n        ``(length(h) + 1) // 2``.\n\n    See Also\n    --------\n    firwin\n    firwin2\n    remez\n\n    Notes\n    -----\n    Both the Hilbert [1]_ or homomorphic [4]_ [5]_ methods require selection\n    of an FFT length to estimate the complex cepstrum of the filter.\n\n    In the case of the Hilbert method, the deviation from the ideal\n    spectrum ``epsilon`` is related to the number of stopband zeros\n    ``n_stop`` and FFT length ``n_fft`` as::\n\n        epsilon = 2. * n_stop / n_fft\n\n    For example, with 100 stopband zeros and a FFT length of 2048,\n    ``epsilon = 0.0976``. If we conservatively assume that the number of\n    stopband zeros is one less than the filter length, we can take the FFT\n    length to be the next power of 2 that satisfies ``epsilon=0.01`` as::\n\n        n_fft = 2 ** int(np.ceil(np.log2(2 * (len(h) - 1) / 0.01)))\n\n    This gives reasonable results for both the Hilbert and homomorphic\n    methods, and gives the value used when ``n_fft=None``.\n\n    Alternative implementations exist for creating minimum-phase filters,\n    including zero inversion [2]_ and spectral factorization [3]_ [4]_.\n    For more information, see:\n\n        http://dspguru.com/dsp/howtos/how-to-design-minimum-phase-fir-filters\n\n    Examples\n    --------\n    Create an optimal linear-phase filter, then convert it to minimum phase:\n\n    >>> from scipy.signal import remez, minimum_phase, freqz, group_delay\n    >>> import matplotlib.pyplot as plt\n    >>> freq = [0, 0.2, 0.3, 1.0]\n    >>> desired = [1, 0]\n    >>> h_linear = remez(151, freq, desired, Hz=2.)\n\n    Convert it to minimum phase:\n\n    >>> h_min_hom = minimum_phase(h_linear, method=\'homomorphic\')\n    >>> h_min_hil = minimum_phase(h_linear, method=\'hilbert\')\n\n    Compare the three filters:\n\n    >>> fig, axs = plt.subplots(4, figsize=(4, 8))\n    >>> for h, style, color in zip((h_linear, h_min_hom, h_min_hil),\n    ...                            (\'-\', \'-\', \'--\'), (\'k\', \'r\', \'c\')):\n    ...     w, H = freqz(h)\n    ...     w, gd = group_delay((h, 1))\n    ...     w /= np.pi\n    ...     axs[0].plot(h, color=color, linestyle=style)\n    ...     axs[1].plot(w, np.abs(H), color=color, linestyle=style)\n    ...     axs[2].plot(w, 20 * np.log10(np.abs(H)), color=color, linestyle=style)\n    ...     axs[3].plot(w, gd, color=color, linestyle=style)\n    >>> for ax in axs:\n    ...     ax.grid(True, color=\'0.5\')\n    ...     ax.fill_between(freq[1:3], *ax.get_ylim(), color=\'#ffeeaa\', zorder=1)\n    >>> axs[0].set(xlim=[0, len(h_linear) - 1], ylabel=\'Amplitude\', xlabel=\'Samples\')\n    >>> axs[1].legend([\'Linear\', \'Min-Hom\', \'Min-Hil\'], title=\'Phase\')\n    >>> for ax, ylim in zip(axs[1:], ([0, 1.1], [-150, 10], [-60, 60])):\n    ...     ax.set(xlim=[0, 1], ylim=ylim, xlabel=\'Frequency\')\n    >>> axs[1].set(ylabel=\'Magnitude\')\n    >>> axs[2].set(ylabel=\'Magnitude (dB)\')\n    >>> axs[3].set(ylabel=\'Group delay\')\n    >>> plt.tight_layout()\n\n    References\n    ----------\n    .. [1] N. Damera-Venkata and B. L. Evans, "Optimal design of real and\n           complex minimum phase digital FIR filters," Acoustics, Speech,\n           and Signal Processing, 1999. Proceedings., 1999 IEEE International\n           Conference on, Phoenix, AZ, 1999, pp. 1145-1148 vol.3.\n           doi: 10.1109/ICASSP.1999.756179\n    .. [2] X. Chen and T. W. Parks, "Design of optimal minimum phase FIR\n           filters by direct factorization," Signal Processing,\n           vol. 10, no. 4, pp. 369–383, Jun. 1986.\n    .. [3] T. Saramaki, "Finite Impulse Response Filter Design," in\n           Handbook for Digital Signal Processing, chapter 4,\n           New York: Wiley-Interscience, 1993.\n    .. [4] J. S. Lim, Advanced Topics in Signal Processing.\n           Englewood Cliffs, N.J.: Prentice Hall, 1988.\n    .. [5] A. V. Oppenheim, R. W. Schafer, and J. R. Buck,\n           "Discrete-Time Signal Processing," 2nd edition.\n           Upper Saddle River, N.J.: Prentice Hall, 1999.\n    '
h = numpy.asarray(h)
if numpy.iscomplexobj(h):
    raise ValueError('Complex filters not supported')
if ((h.ndim != 1) or (h.size <= 2)):
    raise ValueError('h must be 1D and at least 2 samples long')
n_half = (len(h) // 2)
if (not numpy.allclose(h[(- n_half):][::(- 1)], h[:n_half])):
    warnings.warn('h does not appear to by symmetric, conversion may fail', RuntimeWarning)
if ((not isinstance(method, string_types)) or (method not in ('homomorphic', 'hilbert'))):
    raise ValueError(('method must be "homomorphic" or "hilbert", got %r' % (method,)))
if (n_fft is None):
    n_fft = (2 ** int(numpy.ceil(numpy.log2(((2 * (len(h) - 1)) / 0.01)))))
n_fft = int(n_fft)
if (n_fft < len(h)):
    raise ValueError(('n_fft must be at least len(h)==%s' % len(h)))
if (method == 'hilbert'):
    tempResult = arange(n_fft)
	
===================================================================	
firwin: 59	
----------------------------	

'\n    FIR filter design using the window method.\n\n    This function computes the coefficients of a finite impulse response\n    filter.  The filter will have linear phase; it will be Type I if\n    `numtaps` is odd and Type II if `numtaps` is even.\n\n    Type II filters always have zero response at the Nyquist rate, so a\n    ValueError exception is raised if firwin is called with `numtaps` even and\n    having a passband whose right end is at the Nyquist rate.\n\n    Parameters\n    ----------\n    numtaps : int\n        Length of the filter (number of coefficients, i.e. the filter\n        order + 1).  `numtaps` must be even if a passband includes the\n        Nyquist frequency.\n    cutoff : float or 1D array_like\n        Cutoff frequency of filter (expressed in the same units as `nyq`)\n        OR an array of cutoff frequencies (that is, band edges). In the\n        latter case, the frequencies in `cutoff` should be positive and\n        monotonically increasing between 0 and `nyq`.  The values 0 and\n        `nyq` must not be included in `cutoff`.\n    width : float or None, optional\n        If `width` is not None, then assume it is the approximate width\n        of the transition region (expressed in the same units as `nyq`)\n        for use in Kaiser FIR filter design.  In this case, the `window`\n        argument is ignored.\n    window : string or tuple of string and parameter values, optional\n        Desired window to use. See `scipy.signal.get_window` for a list\n        of windows and required parameters.\n    pass_zero : bool, optional\n        If True, the gain at the frequency 0 (i.e. the "DC gain") is 1.\n        Otherwise the DC gain is 0.\n    scale : bool, optional\n        Set to True to scale the coefficients so that the frequency\n        response is exactly unity at a certain frequency.\n        That frequency is either:\n\n        - 0 (DC) if the first passband starts at 0 (i.e. pass_zero\n          is True)\n        - `nyq` (the Nyquist rate) if the first passband ends at\n          `nyq` (i.e the filter is a single band highpass filter);\n          center of first passband otherwise\n\n    nyq : float, optional\n        Nyquist frequency.  Each frequency in `cutoff` must be between 0\n        and `nyq`.\n\n    Returns\n    -------\n    h : (numtaps,) ndarray\n        Coefficients of length `numtaps` FIR filter.\n\n    Raises\n    ------\n    ValueError\n        If any value in `cutoff` is less than or equal to 0 or greater\n        than or equal to `nyq`, if the values in `cutoff` are not strictly\n        monotonically increasing, or if `numtaps` is even but a passband\n        includes the Nyquist frequency.\n\n    See also\n    --------\n    firwin2\n    firls\n    minimum_phase\n    remez\n\n    Examples\n    --------\n    Low-pass from 0 to f:\n\n    >>> from scipy import signal\n    >>> numtaps = 3\n    >>> f = 0.1\n    >>> signal.firwin(numtaps, f)\n    array([ 0.06799017,  0.86401967,  0.06799017])\n\n    Use a specific window function:\n\n    >>> signal.firwin(numtaps, f, window=\'nuttall\')\n    array([  3.56607041e-04,   9.99286786e-01,   3.56607041e-04])\n\n    High-pass (\'stop\' from 0 to f):\n\n    >>> signal.firwin(numtaps, f, pass_zero=False)\n    array([-0.00859313,  0.98281375, -0.00859313])\n\n    Band-pass:\n\n    >>> f1, f2 = 0.1, 0.2\n    >>> signal.firwin(numtaps, [f1, f2], pass_zero=False)\n    array([ 0.06301614,  0.88770441,  0.06301614])\n\n    Band-stop:\n\n    >>> signal.firwin(numtaps, [f1, f2])\n    array([-0.00801395,  1.0160279 , -0.00801395])\n\n    Multi-band (passbands are [0, f1], [f2, f3] and [f4, 1]):\n\n    >>> f3, f4 = 0.3, 0.4\n    >>> signal.firwin(numtaps, [f1, f2, f3, f4])\n    array([-0.01376344,  1.02752689, -0.01376344])\n\n    Multi-band (passbands are [f1, f2] and [f3,f4]):\n\n    >>> signal.firwin(numtaps, [f1, f2, f3, f4], pass_zero=False)\n    array([ 0.04890915,  0.91284326,  0.04890915])\n\n    '
cutoff = (numpy.atleast_1d(cutoff) / float(nyq))
if (cutoff.ndim > 1):
    raise ValueError('The cutoff argument must be at most one-dimensional.')
if (cutoff.size == 0):
    raise ValueError('At least one cutoff frequency must be given.')
if ((cutoff.min() <= 0) or (cutoff.max() >= 1)):
    raise ValueError('Invalid cutoff frequency: frequencies must be greater than 0 and less than nyq.')
if numpy.any((numpy.diff(cutoff) <= 0)):
    raise ValueError('Invalid cutoff frequencies: the frequencies must be strictly increasing.')
if (width is not None):
    atten = kaiser_atten(numtaps, (float(width) / nyq))
    beta = kaiser_beta(atten)
    window = ('kaiser', beta)
pass_nyquist = (bool((cutoff.size & 1)) ^ pass_zero)
if (pass_nyquist and ((numtaps % 2) == 0)):
    raise ValueError('A filter with an even number of coefficients must have zero response at the Nyquist rate.')
cutoff = numpy.hstack((([0.0] * pass_zero), cutoff, ([1.0] * pass_nyquist)))
bands = cutoff.reshape((- 1), 2)
alpha = (0.5 * (numtaps - 1))
tempResult = arange(0, numtaps)
	
===================================================================	
firls: 174	
----------------------------	

"\n    FIR filter design using least-squares error minimization.\n\n    Calculate the filter coefficients for the linear-phase finite\n    impulse response (FIR) filter which has the best approximation\n    to the desired frequency response described by `bands` and\n    `desired` in the least squares sense (i.e., the integral of the\n    weighted mean-squared error within the specified bands is\n    minimized).\n\n    Parameters\n    ----------\n    numtaps : int\n        The number of taps in the FIR filter.  `numtaps` must be odd.\n    bands : array_like\n        A monotonic nondecreasing sequence containing the band edges in\n        Hz. All elements must be non-negative and less than or equal to\n        the Nyquist frequency given by `nyq`.\n    desired : array_like\n        A sequence the same size as `bands` containing the desired gain\n        at the start and end point of each band.\n    weight : array_like, optional\n        A relative weighting to give to each band region when solving\n        the least squares problem. `weight` has to be half the size of\n        `bands`.\n    nyq : float, optional\n        Nyquist frequency. Each frequency in `bands` must be between 0\n        and `nyq` (inclusive).\n\n    Returns\n    -------\n    coeffs : ndarray\n        Coefficients of the optimal (in a least squares sense) FIR filter.\n\n    See also\n    --------\n    firwin\n    firwin2\n    minimum_phase\n    remez\n\n    Notes\n    -----\n    This implementation follows the algorithm given in [1]_.\n    As noted there, least squares design has multiple advantages:\n\n        1. Optimal in a least-squares sense.\n        2. Simple, non-iterative method.\n        3. The general solution can obtained by solving a linear\n           system of equations.\n        4. Allows the use of a frequency dependent weighting function.\n\n    This function constructs a Type I linear phase FIR filter, which\n    contains an odd number of `coeffs` satisfying for :math:`n < numtaps`:\n\n    .. math:: coeffs(n) = coeffs(numtaps - 1 - n)\n\n    The odd number of coefficients and filter symmetry avoid boundary\n    conditions that could otherwise occur at the Nyquist and 0 frequencies\n    (e.g., for Type II, III, or IV variants).\n\n    .. versionadded:: 0.18\n\n    References\n    ----------\n    .. [1] Ivan Selesnick, Linear-Phase Fir Filter Design By Least Squares.\n           OpenStax CNX. Aug 9, 2005.\n           http://cnx.org/contents/eb1ecb35-03a9-4610-ba87-41cd771c95f2@7\n\n    Examples\n    --------\n    We want to construct a band-pass filter. Note that the behavior in the\n    frequency ranges between our stop bands and pass bands is unspecified,\n    and thus may overshoot depending on the parameters of our filter:\n\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n    >>> fig, axs = plt.subplots(2)\n    >>> nyq = 5.  # Hz\n    >>> desired = (0, 0, 1, 1, 0, 0)\n    >>> for bi, bands in enumerate(((0, 1, 2, 3, 4, 5), (0, 1, 2, 4, 4.5, 5))):\n    ...     fir_firls = signal.firls(73, bands, desired, nyq=nyq)\n    ...     fir_remez = signal.remez(73, bands, desired[::2], Hz=2 * nyq)\n    ...     fir_firwin2 = signal.firwin2(73, bands, desired, nyq=nyq)\n    ...     hs = list()\n    ...     ax = axs[bi]\n    ...     for fir in (fir_firls, fir_remez, fir_firwin2):\n    ...         freq, response = signal.freqz(fir)\n    ...         hs.append(ax.semilogy(nyq*freq/(np.pi), np.abs(response))[0])\n    ...     for band, gains in zip(zip(bands[::2], bands[1::2]), zip(desired[::2], desired[1::2])):\n    ...         ax.semilogy(band, np.maximum(gains, 1e-7), 'k--', linewidth=2)\n    ...     if bi == 0:\n    ...         ax.legend(hs, ('firls', 'remez', 'firwin2'), loc='lower center', frameon=False)\n    ...     else:\n    ...         ax.set_xlabel('Frequency (Hz)')\n    ...     ax.grid(True)\n    ...     ax.set(title='Band-pass %d-%d Hz' % bands[2:4], ylabel='Magnitude')\n    ...\n    >>> fig.tight_layout()\n    >>> plt.show()\n\n    "
numtaps = int(numtaps)
if (((numtaps % 2) == 0) or (numtaps < 1)):
    raise ValueError('numtaps must be odd and >= 1')
M = ((numtaps - 1) // 2)
nyq = float(nyq)
if (nyq <= 0):
    raise ValueError(('nyq must be positive, got %s <= 0.' % nyq))
bands = (np.asarray(bands).flatten() / nyq)
if ((len(bands) % 2) != 0):
    raise ValueError('bands must contain frequency pairs.')
bands.shape = ((- 1), 2)
desired = np.asarray(desired).flatten()
if (bands.size != desired.size):
    raise ValueError(('desired must have one entry per frequency, got %s gains for %s frequencies.' % (desired.size, bands.size)))
desired.shape = ((- 1), 2)
if ((np.diff(bands) <= 0).any() or (np.diff(bands[:, 0]) < 0).any()):
    raise ValueError('bands must be monotonically nondecreasing and have width > 0.')
if (bands[:(- 1), 1] > bands[1:, 0]).any():
    raise ValueError('bands must not overlap.')
if (desired < 0).any():
    raise ValueError('desired must be non-negative.')
if (weight is None):
    weight = numpy.ones(len(desired))
weight = np.asarray(weight).flatten()
if (len(weight) != len(desired)):
    raise ValueError(('weight must be the same size as the number of band pairs (%s).' % (len(bands),)))
if (weight < 0).any():
    raise ValueError('weight must be non-negative.')
tempResult = arange(numtaps)
	
===================================================================	
_YT_loop: 926	
----------------------------	

'\n    Algorithm "YT" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    http://drum.lib.umd.edu/handle/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    '
nb_real = poles[np.isreal(poles)].shape[0]
hnb = (nb_real // 2)
if (nb_real > 0):
    update_order = [[nb_real], [1]]
else:
    update_order = [[], []]
tempResult = arange((nb_real + 1), (len(poles) + 1), 2)
	
===================================================================	
_YT_loop: 927	
----------------------------	

'\n    Algorithm "YT" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    http://drum.lib.umd.edu/handle/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    '
nb_real = poles[np.isreal(poles)].shape[0]
hnb = (nb_real // 2)
if (nb_real > 0):
    update_order = [[nb_real], [1]]
else:
    update_order = [[], []]
r_comp = numpy.arange((nb_real + 1), (len(poles) + 1), 2)
tempResult = arange(1, (hnb + (nb_real % 2)))
	
===================================================================	
_YT_loop: 932	
----------------------------	

'\n    Algorithm "YT" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    http://drum.lib.umd.edu/handle/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    '
nb_real = poles[np.isreal(poles)].shape[0]
hnb = (nb_real // 2)
if (nb_real > 0):
    update_order = [[nb_real], [1]]
else:
    update_order = [[], []]
r_comp = numpy.arange((nb_real + 1), (len(poles) + 1), 2)
r_p = numpy.arange(1, (hnb + (nb_real % 2)))
update_order[0].extend((2 * r_p))
update_order[1].extend(((2 * r_p) + 1))
update_order[0].extend(r_comp)
update_order[1].extend((r_comp + 1))
tempResult = arange(1, (hnb + 1))
	
===================================================================	
_YT_loop: 940	
----------------------------	

'\n    Algorithm "YT" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    http://drum.lib.umd.edu/handle/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    '
nb_real = poles[np.isreal(poles)].shape[0]
hnb = (nb_real // 2)
if (nb_real > 0):
    update_order = [[nb_real], [1]]
else:
    update_order = [[], []]
r_comp = numpy.arange((nb_real + 1), (len(poles) + 1), 2)
r_p = numpy.arange(1, (hnb + (nb_real % 2)))
update_order[0].extend((2 * r_p))
update_order[1].extend(((2 * r_p) + 1))
update_order[0].extend(r_comp)
update_order[1].extend((r_comp + 1))
r_p = numpy.arange(1, (hnb + 1))
update_order[0].extend(((2 * r_p) - 1))
update_order[1].extend((2 * r_p))
if ((hnb == 0) and numpy.isreal(poles[0])):
    update_order[0].append(1)
    update_order[1].append(1)
update_order[0].extend(r_comp)
update_order[1].extend((r_comp + 1))
tempResult = arange(2, (hnb + (nb_real % 2)))
	
===================================================================	
_YT_loop: 950	
----------------------------	

'\n    Algorithm "YT" Tits, Yang. Globally Convergent\n    Algorithms for Robust Pole Assignment by State Feedback\n    http://drum.lib.umd.edu/handle/1903/5598\n    The poles P have to be sorted accordingly to section 6.2 page 20\n\n    '
nb_real = poles[np.isreal(poles)].shape[0]
hnb = (nb_real // 2)
if (nb_real > 0):
    update_order = [[nb_real], [1]]
else:
    update_order = [[], []]
r_comp = numpy.arange((nb_real + 1), (len(poles) + 1), 2)
r_p = numpy.arange(1, (hnb + (nb_real % 2)))
update_order[0].extend((2 * r_p))
update_order[1].extend(((2 * r_p) + 1))
update_order[0].extend(r_comp)
update_order[1].extend((r_comp + 1))
r_p = numpy.arange(1, (hnb + 1))
update_order[0].extend(((2 * r_p) - 1))
update_order[1].extend((2 * r_p))
if ((hnb == 0) and numpy.isreal(poles[0])):
    update_order[0].append(1)
    update_order[1].append(1)
update_order[0].extend(r_comp)
update_order[1].extend((r_comp + 1))
r_j = numpy.arange(2, (hnb + (nb_real % 2)))
for j in r_j:
    for i in range(1, (hnb + 1)):
        update_order[0].append(i)
        update_order[1].append((i + j))
if ((hnb == 0) and numpy.isreal(poles[0])):
    update_order[0].append(1)
    update_order[1].append(1)
update_order[0].extend(r_comp)
update_order[1].extend((r_comp + 1))
tempResult = arange(2, (hnb + (nb_real % 2)))
	
===================================================================	
istft: 179	
----------------------------	

'\n    Perform the inverse Short Time Fourier transform (iSTFT).\n\n    Parameters\n    ----------\n    Zxx : array_like\n        STFT of the signal to be reconstructed. If a purely real array\n        is passed, it will be cast to a complex data type.\n    fs : float, optional\n        Sampling frequency of the time series. Defaults to 1.0.\n    window : str or tuple or array_like, optional\n        Desired window to use. See `get_window` for a list of windows\n        and required parameters. If `window` is array_like it will be\n        used directly as the window and its length must be `nperseg`.\n        Defaults to a Hann window. Must match the window used to\n        generate the STFT for faithful inversion.\n    nperseg : int, optional\n        Number of data points corresponding to each STFT segment. This\n        parameter must be specified if the number of data points per\n        segment is odd, or if the STFT was padded via ``nfft >\n        nperseg``. If `None`, the value depends on the shape of\n        `Zxx` and `input_onesided`. If `input_onesided` is True,\n        ``nperseg=2*(Zxx.shape[freq_axis] - 1)``. Otherwise,\n        ``nperseg=Zxx.shape[freq_axis]``. Defaults to `None`.\n    noverlap : int, optional\n        Number of points to overlap between segments. If `None`, half\n        of the segment length. Defaults to `None`. When specified, the\n        COLA constraint must be met (see Notes below), and should match\n        the parameter used to generate the STFT. Defaults to `None`.\n    nfft : int, optional\n        Number of FFT points corresponding to each STFT segment. This\n        parameter must be specified if the STFT was padded via ``nfft >\n        nperseg``. If `None`, the default values are the same as for\n        `nperseg`, detailed above, with one exception: if\n        `input_onesided` is True and\n        ``nperseg==2*Zxx.shape[freq_axis] - 1``, `nfft` also takes on\n        that value. This case allows the proper inversion of an\n        odd-length unpadded STFT using ``nfft=None``. Defaults to\n        `None`.\n    input_onesided : bool, optional\n        If `True`, interpret the input array as one-sided FFTs, such\n        as is returned by `stft` with ``return_onesided=True`` and\n        `numpy.fft.rfft`. If `False`, interpret the input as a a\n        two-sided FFT. Defaults to `True`.\n    boundary : bool, optional\n        Specifies whether the input signal was extended at its\n        boundaries by supplying a non-`None` ``boundary`` argument to\n        `stft`. Defaults to `True`.\n    time_axis : int, optional\n        Where the time segments of the STFT is located; the default is\n        the last axis (i.e. ``axis=-1``).\n    freq_axis : int, optional\n        Where the frequency axis of the STFT is located; the default is\n        the penultimate axis (i.e. ``axis=-2``).\n\n    Returns\n    -------\n    t : ndarray\n        Array of output data times.\n    x : ndarray\n        iSTFT of `Zxx`.\n\n    See Also\n    --------\n    stft: Short Time Fourier Transform\n    check_COLA: Check whether the Constant OverLap Add (COLA) constraint\n                is met\n\n    Notes\n    -----\n    In order to enable inversion of an STFT via the inverse STFT with\n    `istft`, the signal windowing must obey the constraint of "Constant\n    OverLap Add" (COLA). This ensures that every point in the input data\n    is equally weighted, thereby avoiding aliasing and allowing full\n    reconstruction. Whether a choice of `window`, `nperseg`, and\n    `noverlap` satisfy this constraint can be tested with\n    `check_COLA`, by using ``nperseg = Zxx.shape[freq_axis]``.\n\n    An STFT which has been modified (via masking or otherwise) is not\n    guaranteed to correspond to a exactly realizible signal. This\n    function implements the iSTFT via the least-squares esimation\n    algorithm detailed in [2]_, which produces a signal that minimizes\n    the mean squared error between the STFT of the returned signal and\n    the modified STFT.\n\n    .. versionadded:: 0.19.0\n\n    References\n    ----------\n    .. [1] Oppenheim, Alan V., Ronald W. Schafer, John R. Buck\n           "Discrete-Time Signal Processing", Prentice Hall, 1999.\n    .. [2] Daniel W. Griffin, Jae S. Limdt "Signal Estimation from\n           Modified Short Fourier Transform", IEEE 1984,\n           10.1109/TASSP.1984.1164317\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    Generate a test signal, a 2 Vrms sine wave at 50Hz corrupted by\n    0.001 V**2/Hz of white noise sampled at 1024 Hz.\n\n    >>> fs = 1024\n    >>> N = 10*fs\n    >>> nperseg = 512\n    >>> amp = 2 * np.sqrt(2)\n    >>> noise_power = 0.001 * fs / 2\n    >>> time = np.arange(N) / float(fs)\n    >>> carrier = amp * np.sin(2*np.pi*50*time)\n    >>> noise = np.random.normal(scale=np.sqrt(noise_power),\n    ...                          size=time.shape)\n    >>> x = carrier + noise\n\n    Compute the STFT, and plot its magnitude\n\n    >>> f, t, Zxx = signal.stft(x, fs=fs, nperseg=nperseg)\n    >>> plt.figure()\n    >>> plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=amp)\n    >>> plt.ylim([f[1], f[-1]])\n    >>> plt.title(\'STFT Magnitude\')\n    >>> plt.ylabel(\'Frequency [Hz]\')\n    >>> plt.xlabel(\'Time [sec]\')\n    >>> plt.yscale(\'log\')\n    >>> plt.show()\n\n    Zero the components that are 10% or less of the carrier magnitude,\n    then convert back to a time series via inverse STFT\n\n    >>> Zxx = np.where(np.abs(Zxx) >= amp/10, Zxx, 0)\n    >>> _, xrec = signal.istft(Zxx, fs)\n\n    Compare the cleaned signal with the original and true carrier signals.\n\n    >>> plt.figure()\n    >>> plt.plot(time, x, time, xrec, time, carrier)\n    >>> plt.xlim([2, 2.1])\n    >>> plt.xlabel(\'Time [sec]\')\n    >>> plt.ylabel(\'Signal\')\n    >>> plt.legend([\'Carrier + Noise\', \'Filtered via STFT\', \'True Carrier\'])\n    >>> plt.show()\n\n    Note that the cleaned signal does not start as abruptly as the original,\n    since some of the coefficients of the transient were also removed:\n\n    >>> plt.figure()\n    >>> plt.plot(time, x, time, xrec, time, carrier)\n    >>> plt.xlim([0, 0.1])\n    >>> plt.xlabel(\'Time [sec]\')\n    >>> plt.ylabel(\'Signal\')\n    >>> plt.legend([\'Carrier + Noise\', \'Filtered via STFT\', \'True Carrier\'])\n    >>> plt.show()\n\n    '
Zxx = (numpy.asarray(Zxx) + 0j)
freq_axis = int(freq_axis)
time_axis = int(time_axis)
if (Zxx.ndim < 2):
    raise ValueError('Input stft must be at least 2d!')
if (freq_axis == time_axis):
    raise ValueError('Must specify differing time and frequency axes!')
nseg = Zxx.shape[time_axis]
if input_onesided:
    n_default = (2 * (Zxx.shape[freq_axis] - 1))
else:
    n_default = Zxx.shape[freq_axis]
if (nperseg is None):
    nperseg = n_default
else:
    nperseg = int(nperseg)
    if (nperseg < 1):
        raise ValueError('nperseg must be a positive integer')
if (nfft is None):
    if (input_onesided and (nperseg == (n_default + 1))):
        nfft = nperseg
    else:
        nfft = n_default
elif (nfft < nperseg):
    raise ValueError('nfft must be greater than or equal to nperseg.')
else:
    nfft = int(nfft)
if (noverlap is None):
    noverlap = (nperseg // 2)
else:
    noverlap = int(noverlap)
if (noverlap >= nperseg):
    raise ValueError('noverlap must be less than nperseg.')
nstep = (nperseg - noverlap)
if (not check_COLA(window, nperseg, noverlap)):
    raise ValueError('Window, STFT shape and noverlap do not satisfy the COLA constraint.')
if ((time_axis != (Zxx.ndim - 1)) or (freq_axis != (Zxx.ndim - 2))):
    if (freq_axis < 0):
        freq_axis = (Zxx.ndim + freq_axis)
    if (time_axis < 0):
        time = (Zxx.ndim + time_axis)
    zouter = list(range(Zxx.ndim))
    for ax in sorted([time_axis, freq_axis], reverse=True):
        zouter.pop(ax)
    Zxx = numpy.transpose(Zxx, (zouter + [freq_axis, time_axis]))
if (isinstance(window, string_types) or (type(window) is tuple)):
    win = get_window(window, nperseg)
else:
    win = numpy.asarray(window)
    if (len(win.shape) != 1):
        raise ValueError('window must be 1-D')
    if (win.shape[0] != nperseg):
        raise ValueError('window must have length of {0}'.format(nperseg))
if input_onesided:
    ifunc = numpy.fft.irfft
else:
    ifunc = scipy.fftpack.ifft
xsubs = ifunc(Zxx, axis=(- 2), n=nfft)[..., :nperseg, :]
outputlength = (nperseg + ((nseg - 1) * nstep))
x = numpy.zeros((list(Zxx.shape[:(- 2)]) + [outputlength]), dtype=xsubs.dtype)
norm = numpy.zeros(outputlength, dtype=xsubs.dtype)
if (numpy.result_type(win, xsubs) != xsubs.dtype):
    win = win.astype(xsubs.dtype)
xsubs *= win.sum()
for ii in range(nseg):
    x[..., (ii * nstep):((ii * nstep) + nperseg)] += (xsubs[(..., ii)] * win)
    norm[..., (ii * nstep):((ii * nstep) + nperseg)] += (win ** 2)
x /= numpy.where((norm > 1e-10), norm, 1.0)
if boundary:
    x = x[..., (nperseg // 2):(- (nperseg // 2))]
if input_onesided:
    x = x.real
if (x.ndim > 1):
    if (time_axis != (Zxx.ndim - 1)):
        if (freq_axis < time_axis):
            time_axis -= 1
        x = numpy.rollaxis(x, (- 1), time_axis)
tempResult = arange(x.shape[0])
	
===================================================================	
_spectral_helper: 322	
----------------------------	

'\n    Calculate various forms of windowed FFTs for PSD, CSD, etc.\n\n    This is a helper function that implements the commonality between\n    the stft, psd, csd, and spectrogram functions. It is not designed to\n    be called externally. The windows are not averaged over; the result\n    from each window is returned.\n\n    Parameters\n    ---------\n    x : array_like\n        Array or sequence containing the data to be analyzed.\n    y : array_like\n        Array or sequence containing the data to be analyzed. If this is\n        the same object in memory as `x` (i.e. ``_spectral_helper(x,\n        x, ...)``), the extra computations are spared.\n    fs : float, optional\n        Sampling frequency of the time series. Defaults to 1.0.\n    window : str or tuple or array_like, optional\n        Desired window to use. See `get_window` for a list of windows\n        and required parameters. If `window` is array_like it will be\n        used directly as the window and its length must be `nperseg`.\n        Defaults to \'hann\'.\n    nperseg : int, optional\n        Length of each segment. Defaults to None, but if window is str or\n        tuple, is set to 256, and if window is array_like, is set to the\n        length of the window.\n    noverlap : int, optional\n        Number of points to overlap between segments. If `None`,\n        ``noverlap = nperseg // 2``. Defaults to `None`.\n    nfft : int, optional\n        Length of the FFT used, if a zero padded FFT is desired. If\n        `None`, the FFT length is `nperseg`. Defaults to `None`.\n    detrend : str or function or `False`, optional\n        Specifies how to detrend each segment. If `detrend` is a\n        string, it is passed as the `type` argument to the `detrend`\n        function. If it is a function, it takes a segment and returns a\n        detrended segment. If `detrend` is `False`, no detrending is\n        done. Defaults to \'constant\'.\n    return_onesided : bool, optional\n        If `True`, return a one-sided spectrum for real data. If\n        `False` return a two-sided spectrum. Note that for complex\n        data, a two-sided spectrum is always returned.\n    scaling : { \'density\', \'spectrum\' }, optional\n        Selects between computing the cross spectral density (\'density\')\n        where `Pxy` has units of V**2/Hz and computing the cross\n        spectrum (\'spectrum\') where `Pxy` has units of V**2, if `x`\n        and `y` are measured in V and `fs` is measured in Hz.\n        Defaults to \'density\'\n    axis : int, optional\n        Axis along which the FFTs are computed; the default is over the\n        last axis (i.e. ``axis=-1``).\n    mode: str {\'psd\', \'stft\'}, optional\n        Defines what kind of return values are expected. Defaults to\n        \'psd\'.\n    boundary : str or None, optional\n        Specifies whether the input signal is extended at both ends, and\n        how to generate the new values, in order to center the first\n        windowed segment on the first input point. This has the benefit\n        of enabling reconstruction of the first input point when the\n        employed window function starts at zero. Valid options are\n        ``[\'even\', \'odd\', \'constant\', \'zeros\', None]``. Defaults to\n        `None`.\n    padded : bool, optional\n        Specifies whether the input signal is zero-padded at the end to\n        make the signal fit exactly into an integer number of window\n        segments, so that all of the signal is included in the output.\n        Defaults to `False`. Padding occurs after boundary extension, if\n        `boundary` is not `None`, and `padded` is `True`.\n    Returns\n    -------\n    freqs : ndarray\n        Array of sample frequencies.\n    t : ndarray\n        Array of times corresponding to each data segment\n    result : ndarray\n        Array of output data, contents dependant on *mode* kwarg.\n\n    References\n    ----------\n    .. [1] Stack Overflow, "Rolling window for 1D arrays in Numpy?",\n           http://stackoverflow.com/a/6811241\n    .. [2] Stack Overflow, "Using strides for an efficient moving\n           average filter", http://stackoverflow.com/a/4947453\n\n    Notes\n    -----\n    Adapted from matplotlib.mlab\n\n    .. versionadded:: 0.16.0\n    '
if (mode not in ['psd', 'stft']):
    raise ValueError(("Unknown value for mode %s, must be one of: {'psd', 'stft'}" % mode))
boundary_funcs = {'even': even_ext, 'odd': odd_ext, 'constant': const_ext, 'zeros': zero_ext, None: None}
if (boundary not in boundary_funcs):
    raise ValueError("Unknown boundary option '{0}', must be one of: {1}".format(boundary, list(boundary_funcs.keys())))
same_data = (y is x)
if ((not same_data) and (mode != 'psd')):
    raise ValueError("x and y must be equal if mode is 'stft'")
axis = int(axis)
x = numpy.asarray(x)
if (not same_data):
    y = numpy.asarray(y)
    outdtype = numpy.result_type(x, y, numpy.complex64)
else:
    outdtype = numpy.result_type(x, numpy.complex64)
if (not same_data):
    xouter = list(x.shape)
    youter = list(y.shape)
    xouter.pop(axis)
    youter.pop(axis)
    try:
        outershape = np.broadcast(np.empty(xouter), np.empty(youter)).shape
    except ValueError:
        raise ValueError('x and y cannot be broadcast together.')
if same_data:
    if (x.size == 0):
        return (numpy.empty(x.shape), numpy.empty(x.shape), numpy.empty(x.shape))
elif ((x.size == 0) or (y.size == 0)):
    outshape = (outershape + (min([x.shape[axis], y.shape[axis]]),))
    emptyout = numpy.rollaxis(numpy.empty(outshape), (- 1), axis)
    return (emptyout, emptyout, emptyout)
if (x.ndim > 1):
    if (axis != (- 1)):
        x = numpy.rollaxis(x, axis, len(x.shape))
        if ((not same_data) and (y.ndim > 1)):
            y = numpy.rollaxis(y, axis, len(y.shape))
if (not same_data):
    if (x.shape[(- 1)] != y.shape[(- 1)]):
        if (x.shape[(- 1)] < y.shape[(- 1)]):
            pad_shape = list(x.shape)
            pad_shape[(- 1)] = (y.shape[(- 1)] - x.shape[(- 1)])
            x = numpy.concatenate((x, numpy.zeros(pad_shape)), (- 1))
        else:
            pad_shape = list(y.shape)
            pad_shape[(- 1)] = (x.shape[(- 1)] - y.shape[(- 1)])
            y = numpy.concatenate((y, numpy.zeros(pad_shape)), (- 1))
if (nperseg is not None):
    nperseg = int(nperseg)
    if (nperseg < 1):
        raise ValueError('nperseg must be a positive integer')
(win, nperseg) = _triage_segments(window, nperseg, input_length=x.shape[(- 1)])
if (nfft is None):
    nfft = nperseg
elif (nfft < nperseg):
    raise ValueError('nfft must be greater than or equal to nperseg.')
else:
    nfft = int(nfft)
if (noverlap is None):
    noverlap = (nperseg // 2)
else:
    noverlap = int(noverlap)
if (noverlap >= nperseg):
    raise ValueError('noverlap must be less than nperseg.')
nstep = (nperseg - noverlap)
if (boundary is not None):
    ext_func = boundary_funcs[boundary]
    x = ext_func(x, (nperseg // 2), axis=(- 1))
    if (not same_data):
        y = ext_func(y, (nperseg // 2), axis=(- 1))
if padded:
    nadd = (((- (x.shape[(- 1)] - nperseg)) % nstep) % nperseg)
    zeros_shape = (list(x.shape[:(- 1)]) + [nadd])
    x = numpy.concatenate((x, numpy.zeros(zeros_shape)), axis=(- 1))
    if (not same_data):
        zeros_shape = (list(y.shape[:(- 1)]) + [nadd])
        y = numpy.concatenate((y, numpy.zeros(zeros_shape)), axis=(- 1))
if (not detrend):

    def detrend_func(d):
        return d
elif (not hasattr(detrend, '__call__')):

    def detrend_func(d):
        return signaltools.detrend(d, type=detrend, axis=(- 1))
elif (axis != (- 1)):

    def detrend_func(d):
        d = numpy.rollaxis(d, (- 1), axis)
        d = detrend(d)
        return numpy.rollaxis(d, axis, len(d.shape))
else:
    detrend_func = detrend
if (numpy.result_type(win, numpy.complex64) != outdtype):
    win = win.astype(outdtype)
if (scaling == 'density'):
    scale = (1.0 / (fs * (win * win).sum()))
elif (scaling == 'spectrum'):
    scale = (1.0 / (win.sum() ** 2))
else:
    raise ValueError(('Unknown scaling: %r' % scaling))
if (mode == 'stft'):
    scale = numpy.sqrt(scale)
if return_onesided:
    if numpy.iscomplexobj(x):
        sides = 'twosided'
        warnings.warn('Input data is complex, switching to return_onesided=False')
    else:
        sides = 'onesided'
        if (not same_data):
            if numpy.iscomplexobj(y):
                sides = 'twosided'
                warnings.warn('Input data is complex, switching to return_onesided=False')
else:
    sides = 'twosided'
if (sides == 'twosided'):
    freqs = scipy.fftpack.fftfreq(nfft, (1 / fs))
elif (sides == 'onesided'):
    freqs = numpy.fft.rfftfreq(nfft, (1 / fs))
result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft, sides)
if (not same_data):
    result_y = _fft_helper(y, win, detrend_func, nperseg, noverlap, nfft, sides)
    result = (numpy.conjugate(result) * result_y)
elif (mode == 'psd'):
    result = (numpy.conjugate(result) * result)
result *= scale
if ((sides == 'onesided') and (mode == 'psd')):
    if (nfft % 2):
        result[..., 1:] *= 2
    else:
        result[..., 1:(- 1)] *= 2
tempResult = arange((nperseg / 2), ((x.shape[(- 1)] - (nperseg / 2)) + 1), (nperseg - noverlap))
	
===================================================================	
ricker: 128	
----------------------------	

'\n    Return a Ricker wavelet, also known as the "Mexican hat wavelet".\n\n    It models the function:\n\n        ``A (1 - x^2/a^2) exp(-x^2/2 a^2)``,\n\n    where ``A = 2/sqrt(3a)pi^1/4``.\n\n    Parameters\n    ----------\n    points : int\n        Number of points in `vector`.\n        Will be centered around 0.\n    a : scalar\n        Width parameter of the wavelet.\n\n    Returns\n    -------\n    vector : (N,) ndarray\n        Array of length `points` in shape of ricker curve.\n\n    Examples\n    --------\n    >>> from scipy import signal\n    >>> import matplotlib.pyplot as plt\n\n    >>> points = 100\n    >>> a = 4.0\n    >>> vec2 = signal.ricker(points, a)\n    >>> print(len(vec2))\n    100\n    >>> plt.plot(vec2)\n    >>> plt.show()\n\n    '
A = (2 / (numpy.sqrt((3 * a)) * (numpy.pi ** 0.25)))
wsq = (a ** 2)
tempResult = arange(0, points)
	
===================================================================	
cascade: 80	
----------------------------	

'\n    Return (x, phi, psi) at dyadic points ``K/2**J`` from filter coefficients.\n\n    Parameters\n    ----------\n    hk : array_like\n        Coefficients of low-pass filter.\n    J : int, optional\n        Values will be computed at grid points ``K/2**J``. Default is 7.\n\n    Returns\n    -------\n    x : ndarray\n        The dyadic points ``K/2**J`` for ``K=0...N * (2**J)-1`` where\n        ``len(hk) = len(gk) = N+1``.\n    phi : ndarray\n        The scaling function ``phi(x)`` at `x`:\n        ``phi(x) = sum(hk * phi(2x-k))``, where k is from 0 to N.\n    psi : ndarray, optional\n        The wavelet function ``psi(x)`` at `x`:\n        ``phi(x) = sum(gk * phi(2x-k))``, where k is from 0 to N.\n        `psi` is only returned if `gk` is not None.\n\n    Notes\n    -----\n    The algorithm uses the vector cascade algorithm described by Strang and\n    Nguyen in "Wavelets and Filter Banks".  It builds a dictionary of values\n    and slices for quick reuse.  Then inserts vectors into final vector at the\n    end.\n\n    '
N = (len(hk) - 1)
if (J > (30 - numpy.log2((N + 1)))):
    raise ValueError('Too many levels.')
if (J < 1):
    raise ValueError('Too few levels.')
(nn, kk) = numpy.ogrid[:N, :N]
s2 = numpy.sqrt(2)
thk = numpy.r_[(hk, 0)]
gk = qmf(hk)
tgk = numpy.r_[(gk, 0)]
indx1 = numpy.clip(((2 * nn) - kk), (- 1), (N + 1))
indx2 = numpy.clip((((2 * nn) - kk) + 1), (- 1), (N + 1))
m = numpy.zeros((2, 2, N, N), 'd')
m[(0, 0)] = numpy.take(thk, indx1, 0)
m[(0, 1)] = numpy.take(thk, indx2, 0)
m[(1, 0)] = numpy.take(tgk, indx1, 0)
m[(1, 1)] = numpy.take(tgk, indx2, 0)
m *= s2
tempResult = arange(0, (N * (1 << J)), dtype=float)
	
===================================================================	
triang: 54	
----------------------------	

'Return a triangular window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    See Also\n    --------\n    bartlett : A triangular window that touches zero\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.triang(51)\n    >>> plt.plot(window)\n    >>> plt.title("Triangular window")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the triangular window")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(1, (((M + 1) // 2) + 1))
	
===================================================================	
cosine: 252	
----------------------------	

'Return a window with a simple cosine shape.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n\n    .. versionadded:: 0.13.0\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.cosine(51)\n    >>> plt.plot(window)\n    >>> plt.title("Cosine window")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the cosine window")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n    >>> plt.show()\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(0, M)
	
===================================================================	
barthann: 162	
----------------------------	

'Return a modified Bartlett-Hann window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.barthann(51)\n    >>> plt.plot(window)\n    >>> plt.title("Bartlett-Hann window")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the Bartlett-Hann window")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(0, M)
	
===================================================================	
exponential: 264	
----------------------------	

'Return an exponential (or Poisson) window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    center : float, optional\n        Parameter defining the center location of the window function.\n        The default value if not given is ``center = (M-1) / 2``.  This\n        parameter must take its default value for symmetric windows.\n    tau : float, optional\n        Parameter defining the decay.  For ``center = 0`` use\n        ``tau = -(M-1) / ln(x)`` if ``x`` is the fraction of the window\n        remaining at the end.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n    The Exponential window is defined as\n\n    .. math::  w(n) = e^{-|n-center| / \\tau}\n\n    References\n    ----------\n    S. Gade and H. Herlufsen, "Windows to FFT analysis (Part I)",\n    Technical Review 3, Bruel & Kjaer, 1987.\n\n    Examples\n    --------\n    Plot the symmetric window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> M = 51\n    >>> tau = 3.0\n    >>> window = signal.exponential(M, tau=tau)\n    >>> plt.plot(window)\n    >>> plt.title("Exponential Window (tau=3.0)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -35, 0])\n    >>> plt.title("Frequency response of the Exponential window (tau=3.0)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    This function can also generate non-symmetric windows:\n\n    >>> tau2 = -(M-1) / np.log(0.01)\n    >>> window2 = signal.exponential(M, 0, tau2, False)\n    >>> plt.figure()\n    >>> plt.plot(window2)\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n    '
if (sym and (center is not None)):
    raise ValueError('If sym==True, center must be None.')
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
if (center is None):
    center = ((M - 1) / 2)
tempResult = arange(0, M)
	
===================================================================	
bartlett: 124	
----------------------------	

'\n    Return a Bartlett window.\n\n    The Bartlett window is very similar to a triangular window, except\n    that the end points are at zero.  It is often used in signal\n    processing for tapering a signal, without generating too much\n    ripple in the frequency domain.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The triangular window, with the first and last samples equal to zero\n        and the maximum value normalized to 1 (though the value 1 does not\n        appear if `M` is even and `sym` is True).\n\n    See Also\n    --------\n    triang : A triangular window that does not touch zero at the ends\n\n    Notes\n    -----\n    The Bartlett window is defined as\n\n    .. math:: w(n) = \\frac{2}{M-1} \\left(\n              \\frac{M-1}{2} - \\left|n - \\frac{M-1}{2}\\right|\n              \\right)\n\n    Most references to the Bartlett window come from the signal\n    processing literature, where it is used as one of many windowing\n    functions for smoothing values.  Note that convolution with this\n    window produces linear interpolation.  It is also known as an\n    apodization (which means"removing the foot", i.e. smoothing\n    discontinuities at the beginning and end of the sampled signal) or\n    tapering function. The Fourier transform of the Bartlett is the product\n    of two sinc functions.\n    Note the excellent discussion in Kanasewich. [2]_\n\n    References\n    ----------\n    .. [1] M.S. Bartlett, "Periodogram Analysis and Continuous Spectra",\n           Biometrika 37, 1-16, 1950.\n    .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics",\n           The University of Alberta Press, 1975, pp. 109-110.\n    .. [3] A.V. Oppenheim and R.W. Schafer, "Discrete-Time Signal\n           Processing", Prentice-Hall, 1999, pp. 468-471.\n    .. [4] Wikipedia, "Window function",\n           http://en.wikipedia.org/wiki/Window_function\n    .. [5] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,\n           "Numerical Recipes", Cambridge University Press, 1986, page 429.\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.bartlett(51)\n    >>> plt.plot(window)\n    >>> plt.title("Bartlett window")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the Bartlett window")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(0, M)
	
===================================================================	
general_gaussian: 200	
----------------------------	

'Return a window with a generalized Gaussian shape.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    p : float\n        Shape parameter.  p = 1 is identical to `gaussian`, p = 0.5 is\n        the same shape as the Laplace distribution.\n    sig : float\n        The standard deviation, sigma.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n    The generalized Gaussian window is defined as\n\n    .. math::  w(n) = e^{ -\\frac{1}{2}\\left|\\frac{n}{\\sigma}\\right|^{2p} }\n\n    the half-power point is at\n\n    .. math::  (2 \\log(2))^{1/(2 p)} \\sigma\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.general_gaussian(51, p=1.5, sig=7)\n    >>> plt.plot(window)\n    >>> plt.title(r"Generalized Gaussian window (p=1.5, $\\sigma$=7)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title(r"Freq. resp. of the gen. Gaussian "\n    ...           "window (p=1.5, $\\sigma$=7)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(0, M)
	
===================================================================	
gaussian: 190	
----------------------------	

'Return a Gaussian window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    std : float\n        The standard deviation, sigma.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n    The Gaussian window is defined as\n\n    .. math::  w(n) = e^{ -\\frac{1}{2}\\left(\\frac{n}{\\sigma}\\right)^2 }\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.gaussian(51, std=7)\n    >>> plt.plot(window)\n    >>> plt.title(r"Gaussian window ($\\sigma$=7)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title(r"Frequency response of the Gaussian window ($\\sigma$=7)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(0, M)
	
===================================================================	
slepian: 239	
----------------------------	

'Return a digital Slepian (DPSS) window.\n\n    Used to maximize the energy concentration in the main lobe.  Also called\n    the digital prolate spheroidal sequence (DPSS).\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    width : float\n        Bandwidth\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value always normalized to 1\n\n    References\n    ----------\n    .. [1] D. Slepian & H. O. Pollak: "Prolate spheroidal wave functions,\n           Fourier analysis and uncertainty-I," Bell Syst. Tech. J., vol.40,\n           pp.43-63, 1961. https://archive.org/details/bstj40-1-43\n    .. [2] H. J. Landau & H. O. Pollak: "Prolate spheroidal wave functions,\n           Fourier analysis and uncertainty-II," Bell Syst. Tech. J. , vol.40,\n           pp.65-83, 1961. https://archive.org/details/bstj40-1-65\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.slepian(51, width=0.3)\n    >>> plt.plot(window)\n    >>> plt.title("Slepian (DPSS) window (BW=0.3)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the Slepian window (BW=0.3)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
width = (width / 2)
width = (width / 2)
tempResult = arange(M, dtype='d')
	
===================================================================	
tukey: 146	
----------------------------	

'Return a Tukey window, also known as a tapered cosine window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    alpha : float, optional\n        Shape parameter of the Tukey window, representing the fraction of the\n        window inside the cosine tapered region.\n        If zero, the Tukey window is equivalent to a rectangular window.\n        If one, the Tukey window is equivalent to a Hann window.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    References\n    ----------\n    .. [1] Harris, Fredric J. (Jan 1978). "On the use of Windows for Harmonic\n           Analysis with the Discrete Fourier Transform". Proceedings of the\n           IEEE 66 (1): 51-83. :doi:`10.1109/PROC.1978.10837`\n    .. [2] Wikipedia, "Window function",\n           http://en.wikipedia.org/wiki/Window_function#Tukey_window\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.tukey(51)\n    >>> plt.plot(window)\n    >>> plt.title("Tukey window")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n    >>> plt.ylim([0, 1.1])\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the Tukey window")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
if (alpha <= 0):
    return numpy.ones(M, 'd')
elif (alpha >= 1.0):
    return hann(M, sym=sym)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(0, M)
	
===================================================================	
parzen: 68	
----------------------------	

'Return a Parzen window.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    References\n    ----------\n    .. [1] E. Parzen, "Mathematical Considerations in the Estimation of\n           Spectra", Technometrics,  Vol. 3, No. 2 (May, 1961), pp. 167-190\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.parzen(51)\n    >>> plt.plot(window)\n    >>> plt.title("Parzen window")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title("Frequency response of the Parzen window")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(((- (M - 1)) / 2.0), (((M - 1) / 2.0) + 0.5), 1.0)
	
===================================================================	
kaiser: 180	
----------------------------	

'Return a Kaiser window.\n\n    The Kaiser window is a taper formed by using a Bessel function.\n\n    Parameters\n    ----------\n    M : int\n        Number of points in the output window. If zero or less, an empty\n        array is returned.\n    beta : float\n        Shape parameter, determines trade-off between main-lobe width and\n        side lobe level. As beta gets large, the window narrows.\n    sym : bool, optional\n        When True (default), generates a symmetric window, for use in filter\n        design.\n        When False, generates a periodic window, for use in spectral analysis.\n\n    Returns\n    -------\n    w : ndarray\n        The window, with the maximum value normalized to 1 (though the value 1\n        does not appear if `M` is even and `sym` is True).\n\n    Notes\n    -----\n    The Kaiser window is defined as\n\n    .. math::  w(n) = I_0\\left( \\beta \\sqrt{1-\\frac{4n^2}{(M-1)^2}}\n               \\right)/I_0(\\beta)\n\n    with\n\n    .. math:: \\quad -\\frac{M-1}{2} \\leq n \\leq \\frac{M-1}{2},\n\n    where :math:`I_0` is the modified zeroth-order Bessel function.\n\n    The Kaiser was named for Jim Kaiser, who discovered a simple approximation\n    to the DPSS window based on Bessel functions.\n    The Kaiser window is a very good approximation to the Digital Prolate\n    Spheroidal Sequence, or Slepian window, which is the transform which\n    maximizes the energy in the main lobe of the window relative to total\n    energy.\n\n    The Kaiser can approximate other windows by varying the beta parameter.\n    (Some literature uses alpha = beta/pi.) [4]_\n\n    ====  =======================\n    beta  Window shape\n    ====  =======================\n    0     Rectangular\n    5     Similar to a Hamming\n    6     Similar to a Hann\n    8.6   Similar to a Blackman\n    ====  =======================\n\n    A beta value of 14 is probably a good starting point. Note that as beta\n    gets large, the window narrows, and so the number of samples needs to be\n    large enough to sample the increasingly narrow spike, otherwise NaNs will\n    be returned.\n\n    Most references to the Kaiser window come from the signal processing\n    literature, where it is used as one of many windowing functions for\n    smoothing values.  It is also known as an apodization (which means\n    "removing the foot", i.e. smoothing discontinuities at the beginning\n    and end of the sampled signal) or tapering function.\n\n    References\n    ----------\n    .. [1] J. F. Kaiser, "Digital Filters" - Ch 7 in "Systems analysis by\n           digital computer", Editors: F.F. Kuo and J.F. Kaiser, p 218-285.\n           John Wiley and Sons, New York, (1966).\n    .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics", The\n           University of Alberta Press, 1975, pp. 177-178.\n    .. [3] Wikipedia, "Window function",\n           http://en.wikipedia.org/wiki/Window_function\n    .. [4] F. J. Harris, "On the use of windows for harmonic analysis with the\n           discrete Fourier transform," Proceedings of the IEEE, vol. 66,\n           no. 1, pp. 51-83, Jan. 1978. :doi:`10.1109/PROC.1978.10837`.\n\n    Examples\n    --------\n    Plot the window and its frequency response:\n\n    >>> from scipy import signal\n    >>> from scipy.fftpack import fft, fftshift\n    >>> import matplotlib.pyplot as plt\n\n    >>> window = signal.kaiser(51, beta=14)\n    >>> plt.plot(window)\n    >>> plt.title(r"Kaiser window ($\\beta$=14)")\n    >>> plt.ylabel("Amplitude")\n    >>> plt.xlabel("Sample")\n\n    >>> plt.figure()\n    >>> A = fft(window, 2048) / (len(window)/2.0)\n    >>> freq = np.linspace(-0.5, 0.5, len(A))\n    >>> response = 20 * np.log10(np.abs(fftshift(A / abs(A).max())))\n    >>> plt.plot(freq, response)\n    >>> plt.axis([-0.5, 0.5, -120, 0])\n    >>> plt.title(r"Frequency response of the Kaiser window ($\\beta$=14)")\n    >>> plt.ylabel("Normalized magnitude [dB]")\n    >>> plt.xlabel("Normalized frequency [cycles per sample]")\n\n    '
if _len_guards(M):
    return numpy.ones(M)
(M, needs_trunc) = _extend(M, sym)
tempResult = arange(0, M)
	
===================================================================	
_boolrelextrema: 15	
----------------------------	

"\n    Calculate the relative extrema of `data`.\n\n    Relative extrema are calculated by finding locations where\n    ``comparator(data[n], data[n+1:n+order+1])`` is True.\n\n    Parameters\n    ----------\n    data : ndarray\n        Array in which to find the relative extrema.\n    comparator : callable\n        Function to use to compare two data points.\n        Should take two arrays as arguments.\n    axis : int, optional\n        Axis over which to select from `data`.  Default is 0.\n    order : int, optional\n        How many points on each side to use for the comparison\n        to consider ``comparator(n,n+x)`` to be True.\n    mode : str, optional\n        How the edges of the vector are treated.  'wrap' (wrap around) or\n        'clip' (treat overflow as the same as the last (or first) element).\n        Default 'clip'.  See numpy.take\n\n    Returns\n    -------\n    extrema : ndarray\n        Boolean array of the same shape as `data` that is True at an extrema,\n        False otherwise.\n\n    See also\n    --------\n    argrelmax, argrelmin\n\n    Examples\n    --------\n    >>> testdata = np.array([1,2,3,2,1])\n    >>> _boolrelextrema(testdata, np.greater, axis=0)\n    array([False, False,  True, False, False], dtype=bool)\n\n    "
if ((int(order) != order) or (order < 1)):
    raise ValueError('Order must be an int >= 1')
datalen = data.shape[axis]
tempResult = arange(0, datalen)
	
===================================================================	
_identify_ridge_lines: 51	
----------------------------	

'\n    Identify ridges in the 2-D matrix.\n\n    Expect that the width of the wavelet feature increases with increasing row\n    number.\n\n    Parameters\n    ----------\n    matr : 2-D ndarray\n        Matrix in which to identify ridge lines.\n    max_distances : 1-D sequence\n        At each row, a ridge line is only connected\n        if the relative max at row[n] is within\n        `max_distances`[n] from the relative max at row[n+1].\n    gap_thresh : int\n        If a relative maximum is not found within `max_distances`,\n        there will be a gap. A ridge line is discontinued if\n        there are more than `gap_thresh` points without connecting\n        a new relative maximum.\n\n    Returns\n    -------\n    ridge_lines : tuple\n        Tuple of 2 1-D sequences. `ridge_lines`[ii][0] are the rows of the\n        ii-th ridge-line, `ridge_lines`[ii][1] are the columns. Empty if none\n        found.  Each ridge-line will be sorted by row (increasing), but the\n        order of the ridge lines is not specified.\n\n    References\n    ----------\n    Bioinformatics (2006) 22 (17): 2059-2065.\n    :doi:`10.1093/bioinformatics/btl355`\n    http://bioinformatics.oxfordjournals.org/content/22/17/2059.long\n\n    Examples\n    --------\n    >>> data = np.random.rand(5,5)\n    >>> ridge_lines = _identify_ridge_lines(data, 1, 1)\n\n    Notes\n    -----\n    This function is intended to be used in conjunction with `cwt`\n    as part of `find_peaks_cwt`.\n\n    '
if (len(max_distances) < matr.shape[0]):
    raise ValueError('Max_distances must have at least as many rows as matr')
all_max_cols = _boolrelextrema(matr, numpy.greater, axis=1, order=1)
has_relmax = numpy.where(all_max_cols.any(axis=1))[0]
if (len(has_relmax) == 0):
    return []
start_row = has_relmax[(- 1)]
ridge_lines = [[[start_row], [col], 0] for col in numpy.where(all_max_cols[start_row])[0]]
final_lines = []
tempResult = arange((start_row - 1), (- 1), (- 1))
	
===================================================================	
_identify_ridge_lines: 52	
----------------------------	

'\n    Identify ridges in the 2-D matrix.\n\n    Expect that the width of the wavelet feature increases with increasing row\n    number.\n\n    Parameters\n    ----------\n    matr : 2-D ndarray\n        Matrix in which to identify ridge lines.\n    max_distances : 1-D sequence\n        At each row, a ridge line is only connected\n        if the relative max at row[n] is within\n        `max_distances`[n] from the relative max at row[n+1].\n    gap_thresh : int\n        If a relative maximum is not found within `max_distances`,\n        there will be a gap. A ridge line is discontinued if\n        there are more than `gap_thresh` points without connecting\n        a new relative maximum.\n\n    Returns\n    -------\n    ridge_lines : tuple\n        Tuple of 2 1-D sequences. `ridge_lines`[ii][0] are the rows of the\n        ii-th ridge-line, `ridge_lines`[ii][1] are the columns. Empty if none\n        found.  Each ridge-line will be sorted by row (increasing), but the\n        order of the ridge lines is not specified.\n\n    References\n    ----------\n    Bioinformatics (2006) 22 (17): 2059-2065.\n    :doi:`10.1093/bioinformatics/btl355`\n    http://bioinformatics.oxfordjournals.org/content/22/17/2059.long\n\n    Examples\n    --------\n    >>> data = np.random.rand(5,5)\n    >>> ridge_lines = _identify_ridge_lines(data, 1, 1)\n\n    Notes\n    -----\n    This function is intended to be used in conjunction with `cwt`\n    as part of `find_peaks_cwt`.\n\n    '
if (len(max_distances) < matr.shape[0]):
    raise ValueError('Max_distances must have at least as many rows as matr')
all_max_cols = _boolrelextrema(matr, numpy.greater, axis=1, order=1)
has_relmax = numpy.where(all_max_cols.any(axis=1))[0]
if (len(has_relmax) == 0):
    return []
start_row = has_relmax[(- 1)]
ridge_lines = [[[start_row], [col], 0] for col in numpy.where(all_max_cols[start_row])[0]]
final_lines = []
rows = numpy.arange((start_row - 1), (- 1), (- 1))
tempResult = arange(0, matr.shape[1])
	
===================================================================	
_polyder: 43	
----------------------------	

"Differentiate polynomials represented with coefficients.\n\n    p must be a 1D or 2D array.  In the 2D case, each column gives\n    the coefficients of a polynomial; the first row holds the coefficients\n    associated with the highest power.  m must be a nonnegative integer.\n    (numpy.polyder doesn't handle the 2D case.)\n    "
if (m == 0):
    result = p
else:
    n = len(p)
    if (n <= m):
        result = numpy.zeros_like(p[:1, ...])
    else:
        dp = p[:(- m)].copy()
        for k in range(m):
            tempResult = arange(((n - k) - 1), ((m - k) - 1), (- 1))
	
===================================================================	
_fit_edge: 58	
----------------------------	

'\n    Given an n-d array `x` and the specification of a slice of `x` from\n    `window_start` to `window_stop` along `axis`, create an interpolating\n    polynomial of each 1-d slice, and evaluate that polynomial in the slice\n    from `interp_start` to `interp_stop`.  Put the result into the\n    corresponding slice of `y`.\n    '
x_edge = axis_slice(x, start=window_start, stop=window_stop, axis=axis)
if ((axis == 0) or (axis == (- x.ndim))):
    xx_edge = x_edge
    swapped = False
else:
    xx_edge = x_edge.swapaxes(axis, 0)
    swapped = True
xx_edge = xx_edge.reshape(xx_edge.shape[0], (- 1))
tempResult = arange(0, (window_stop - window_start))
	
===================================================================	
_fit_edge: 61	
----------------------------	

'\n    Given an n-d array `x` and the specification of a slice of `x` from\n    `window_start` to `window_stop` along `axis`, create an interpolating\n    polynomial of each 1-d slice, and evaluate that polynomial in the slice\n    from `interp_start` to `interp_stop`.  Put the result into the\n    corresponding slice of `y`.\n    '
x_edge = axis_slice(x, start=window_start, stop=window_stop, axis=axis)
if ((axis == 0) or (axis == (- x.ndim))):
    xx_edge = x_edge
    swapped = False
else:
    xx_edge = x_edge.swapaxes(axis, 0)
    swapped = True
xx_edge = xx_edge.reshape(xx_edge.shape[0], (- 1))
poly_coeffs = numpy.polyfit(numpy.arange(0, (window_stop - window_start)), xx_edge, polyorder)
if (deriv > 0):
    poly_coeffs = _polyder(poly_coeffs, deriv)
tempResult = arange((interp_start - window_start), (interp_stop - window_start))
	
===================================================================	
savgol_coeffs: 22	
----------------------------	

"Compute the coefficients for a 1-d Savitzky-Golay FIR filter.\n\n    Parameters\n    ----------\n    window_length : int\n        The length of the filter window (i.e. the number of coefficients).\n        `window_length` must be an odd positive integer.\n    polyorder : int\n        The order of the polynomial used to fit the samples.\n        `polyorder` must be less than `window_length`.\n    deriv : int, optional\n        The order of the derivative to compute.  This must be a\n        nonnegative integer.  The default is 0, which means to filter\n        the data without differentiating.\n    delta : float, optional\n        The spacing of the samples to which the filter will be applied.\n        This is only used if deriv > 0.\n    pos : int or None, optional\n        If pos is not None, it specifies evaluation position within the\n        window.  The default is the middle of the window.\n    use : str, optional\n        Either 'conv' or 'dot'.  This argument chooses the order of the\n        coefficients.  The default is 'conv', which means that the\n        coefficients are ordered to be used in a convolution.  With\n        use='dot', the order is reversed, so the filter is applied by\n        dotting the coefficients with the data set.\n\n    Returns\n    -------\n    coeffs : 1-d ndarray\n        The filter coefficients.\n\n    References\n    ----------\n    A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of Data by\n    Simplified Least Squares Procedures. Analytical Chemistry, 1964, 36 (8),\n    pp 1627-1639.\n\n    See Also\n    --------\n    savgol_filter\n\n    Notes\n    -----\n\n    .. versionadded:: 0.14.0\n\n    Examples\n    --------\n    >>> from scipy.signal import savgol_coeffs\n    >>> savgol_coeffs(5, 2)\n    array([-0.08571429,  0.34285714,  0.48571429,  0.34285714, -0.08571429])\n    >>> savgol_coeffs(5, 2, deriv=1)\n    array([  2.00000000e-01,   1.00000000e-01,   2.00607895e-16,\n            -1.00000000e-01,  -2.00000000e-01])\n\n    Note that use='dot' simply reverses the coefficients.\n\n    >>> savgol_coeffs(5, 2, pos=3)\n    array([ 0.25714286,  0.37142857,  0.34285714,  0.17142857, -0.14285714])\n    >>> savgol_coeffs(5, 2, pos=3, use='dot')\n    array([-0.14285714,  0.17142857,  0.34285714,  0.37142857,  0.25714286])\n\n    `x` contains data from the parabola x = t**2, sampled at\n    t = -1, 0, 1, 2, 3.  `c` holds the coefficients that will compute the\n    derivative at the last position.  When dotted with `x` the result should\n    be 6.\n\n    >>> x = np.array([1, 0, 1, 4, 9])\n    >>> c = savgol_coeffs(5, 2, pos=4, deriv=1, use='dot')\n    >>> c.dot(x)\n    6.0000000000000018\n    "
if (polyorder >= window_length):
    raise ValueError('polyorder must be less than window_length.')
(halflen, rem) = divmod(window_length, 2)
if (rem == 0):
    raise ValueError('window_length must be odd.')
if (pos is None):
    pos = halflen
if (not (0 <= pos < window_length)):
    raise ValueError('pos must be nonnegative and less than window_length.')
if (use not in ['conv', 'dot']):
    raise ValueError("`use` must be 'conv' or 'dot'")
tempResult = arange((- pos), (window_length - pos), dtype=float)
	
===================================================================	
savgol_coeffs: 25	
----------------------------	

"Compute the coefficients for a 1-d Savitzky-Golay FIR filter.\n\n    Parameters\n    ----------\n    window_length : int\n        The length of the filter window (i.e. the number of coefficients).\n        `window_length` must be an odd positive integer.\n    polyorder : int\n        The order of the polynomial used to fit the samples.\n        `polyorder` must be less than `window_length`.\n    deriv : int, optional\n        The order of the derivative to compute.  This must be a\n        nonnegative integer.  The default is 0, which means to filter\n        the data without differentiating.\n    delta : float, optional\n        The spacing of the samples to which the filter will be applied.\n        This is only used if deriv > 0.\n    pos : int or None, optional\n        If pos is not None, it specifies evaluation position within the\n        window.  The default is the middle of the window.\n    use : str, optional\n        Either 'conv' or 'dot'.  This argument chooses the order of the\n        coefficients.  The default is 'conv', which means that the\n        coefficients are ordered to be used in a convolution.  With\n        use='dot', the order is reversed, so the filter is applied by\n        dotting the coefficients with the data set.\n\n    Returns\n    -------\n    coeffs : 1-d ndarray\n        The filter coefficients.\n\n    References\n    ----------\n    A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of Data by\n    Simplified Least Squares Procedures. Analytical Chemistry, 1964, 36 (8),\n    pp 1627-1639.\n\n    See Also\n    --------\n    savgol_filter\n\n    Notes\n    -----\n\n    .. versionadded:: 0.14.0\n\n    Examples\n    --------\n    >>> from scipy.signal import savgol_coeffs\n    >>> savgol_coeffs(5, 2)\n    array([-0.08571429,  0.34285714,  0.48571429,  0.34285714, -0.08571429])\n    >>> savgol_coeffs(5, 2, deriv=1)\n    array([  2.00000000e-01,   1.00000000e-01,   2.00607895e-16,\n            -1.00000000e-01,  -2.00000000e-01])\n\n    Note that use='dot' simply reverses the coefficients.\n\n    >>> savgol_coeffs(5, 2, pos=3)\n    array([ 0.25714286,  0.37142857,  0.34285714,  0.17142857, -0.14285714])\n    >>> savgol_coeffs(5, 2, pos=3, use='dot')\n    array([-0.14285714,  0.17142857,  0.34285714,  0.37142857,  0.25714286])\n\n    `x` contains data from the parabola x = t**2, sampled at\n    t = -1, 0, 1, 2, 3.  `c` holds the coefficients that will compute the\n    derivative at the last position.  When dotted with `x` the result should\n    be 6.\n\n    >>> x = np.array([1, 0, 1, 4, 9])\n    >>> c = savgol_coeffs(5, 2, pos=4, deriv=1, use='dot')\n    >>> c.dot(x)\n    6.0000000000000018\n    "
if (polyorder >= window_length):
    raise ValueError('polyorder must be less than window_length.')
(halflen, rem) = divmod(window_length, 2)
if (rem == 0):
    raise ValueError('window_length must be odd.')
if (pos is None):
    pos = halflen
if (not (0 <= pos < window_length)):
    raise ValueError('pos must be nonnegative and less than window_length.')
if (use not in ['conv', 'dot']):
    raise ValueError("`use` must be 'conv' or 'dot'")
x = numpy.arange((- pos), (window_length - pos), dtype=float)
if (use == 'conv'):
    x = x[::(- 1)]
tempResult = arange((polyorder + 1))
	
===================================================================	
TestArrayTools.test_axis_slice: 10	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestArrayTools.test_axis_reverse: 25	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestDLTI.test_more_step_and_impulse: 149	
----------------------------	

lambda1 = 0.5
lambda2 = 0.75
a = numpy.array([[lambda1, 0.0], [0.0, lambda2]])
b = numpy.array([[1.0, 0.0], [0.0, 1.0]])
c = numpy.array([[1.0, 1.0]])
d = numpy.array([[0.0, 0.0]])
n = 10
(ts, ys) = dstep((a, b, c, d, 1), n=n)
tempResult = arange(n)
	
===================================================================	
TestDLTI.test_more_step_and_impulse: 150	
----------------------------	

lambda1 = 0.5
lambda2 = 0.75
a = numpy.array([[lambda1, 0.0], [0.0, lambda2]])
b = numpy.array([[1.0, 0.0], [0.0, 1.0]])
c = numpy.array([[1.0, 1.0]])
d = numpy.array([[0.0, 0.0]])
n = 10
(ts, ys) = dstep((a, b, c, d, 1), n=n)
stp0 = ((1.0 / (1 - lambda1)) * (1.0 - (lambda1 ** numpy.arange(n))))
tempResult = arange(n)
	
===================================================================	
TestDLTI.test_more_step_and_impulse: 155	
----------------------------	

lambda1 = 0.5
lambda2 = 0.75
a = numpy.array([[lambda1, 0.0], [0.0, lambda2]])
b = numpy.array([[1.0, 0.0], [0.0, 1.0]])
c = numpy.array([[1.0, 1.0]])
d = numpy.array([[0.0, 0.0]])
n = 10
(ts, ys) = dstep((a, b, c, d, 1), n=n)
stp0 = ((1.0 / (1 - lambda1)) * (1.0 - (lambda1 ** numpy.arange(n))))
stp1 = ((1.0 / (1 - lambda2)) * (1.0 - (lambda2 ** numpy.arange(n))))
assert_allclose(ys[0][:, 0], stp0)
assert_allclose(ys[1][:, 0], stp1)
x0 = numpy.array([1.0, 1.0])
(ti, yi) = dimpulse((a, b, c, d, 1), n=n, x0=x0)
tempResult = arange((- 1), (n + 1))
	
===================================================================	
TestDLTI.test_dlsim_trivial: 108	
----------------------------	

a = numpy.array([[0.0]])
b = numpy.array([[0.0]])
c = numpy.array([[0.0]])
d = numpy.array([[0.0]])
n = 5
u = np.zeros(n).reshape((- 1), 1)
(tout, yout, xout) = dlsim((a, b, c, d, 1), u)
tempResult = arange(float(n))
	
===================================================================	
TestDLTI.test_dlsim_simple2d: 135	
----------------------------	

lambda1 = 0.5
lambda2 = 0.25
a = numpy.array([[lambda1, 0.0], [0.0, lambda2]])
b = numpy.array([[0.0], [0.0]])
c = numpy.array([[1.0, 0.0], [0.0, 1.0]])
d = numpy.array([[0.0], [0.0]])
n = 5
u = np.zeros(n).reshape((- 1), 1)
(tout, yout, xout) = dlsim((a, b, c, d, 1), u, x0=1)
tempResult = arange(float(n))
	
===================================================================	
TestDLTI.test_dlsim_simple2d: 136	
----------------------------	

lambda1 = 0.5
lambda2 = 0.25
a = numpy.array([[lambda1, 0.0], [0.0, lambda2]])
b = numpy.array([[0.0], [0.0]])
c = numpy.array([[1.0, 0.0], [0.0, 1.0]])
d = numpy.array([[0.0], [0.0]])
n = 5
u = np.zeros(n).reshape((- 1), 1)
(tout, yout, xout) = dlsim((a, b, c, d, 1), u, x0=1)
assert_array_equal(tout, numpy.arange(float(n)))
tempResult = arange(float(n))
	
===================================================================	
TestDLTI.test_dlsim_simple1d: 120	
----------------------------	

a = numpy.array([[0.5]])
b = numpy.array([[0.0]])
c = numpy.array([[1.0]])
d = numpy.array([[0.0]])
n = 5
u = np.zeros(n).reshape((- 1), 1)
(tout, yout, xout) = dlsim((a, b, c, d, 1), u, x0=1)
tempResult = arange(float(n))
	
===================================================================	
TestDLTI.test_dlsim_simple1d: 121	
----------------------------	

a = numpy.array([[0.5]])
b = numpy.array([[0.0]])
c = numpy.array([[1.0]])
d = numpy.array([[0.0]])
n = 5
u = np.zeros(n).reshape((- 1), 1)
(tout, yout, xout) = dlsim((a, b, c, d, 1), u, x0=1)
assert_array_equal(tout, numpy.arange(float(n)))
tempResult = arange(float(n))
	
===================================================================	
TestGroupDelay.test_identity_filter: 1416	
----------------------------	

(w, gd) = group_delay((1, 1))
tempResult = arange(512)
	
===================================================================	
TestGroupDelay.test_identity_filter: 1419	
----------------------------	

(w, gd) = group_delay((1, 1))
assert_array_almost_equal(w, ((pi * numpy.arange(512)) / 512))
assert_array_almost_equal(gd, numpy.zeros(512))
(w, gd) = group_delay((1, 1), whole=True)
tempResult = arange(512)
	
===================================================================	
TestFreqz_zpk.test_basic_whole: 470	
----------------------------	

(w, h) = freqz_zpk([0.5], [0.5], 1.0, worN=8, whole=True)
tempResult = arange(8.0)
	
===================================================================	
TestIIRFilter.test_symmetry: 1385	
----------------------------	

tempResult = arange(1, 26)
	
===================================================================	
TestFreqz_zpk.test_basic: 465	
----------------------------	

(w, h) = freqz_zpk([0.5], [0.5], 1.0, worN=8)
tempResult = arange(8.0)
	
===================================================================	
TestFreqz.plot: 344	
----------------------------	

tempResult = arange(8.0)
	
===================================================================	
TestFreqz.test_basic_whole: 338	
----------------------------	

(w, h) = freqz([1.0], worN=8, whole=True)
tempResult = arange(8.0)
	
===================================================================	
TestFreqz.test_basic: 333	
----------------------------	

(w, h) = freqz([1.0], worN=8)
tempResult = arange(8.0)
	
===================================================================	
TestFirwin.check_response: 35	
----------------------------	

N = len(h)
alpha = (0.5 * (N - 1))
tempResult = arange(0, N)
	
===================================================================	
TestFirwin2.test04: 214	
----------------------------	

'Test firwin2 when window=None.'
ntaps = 5
freq = [0.0, 0.5, 0.5, 1.0]
gain = [1.0, 1.0, 0.0, 0.0]
taps = firwin2(ntaps, freq, gain, window=None, nfreqs=8193)
alpha = (0.5 * (ntaps - 1))
tempResult = arange(0, ntaps)
	
===================================================================	
TestMinimumPhase.test_bad_args: 325	
----------------------------	

assert_raises(ValueError, minimum_phase, [1.0])
assert_raises(ValueError, minimum_phase, [1.0, 1.0])
assert_raises(ValueError, minimum_phase, (numpy.ones(10) * 1j))
assert_raises(ValueError, minimum_phase, 'foo')
assert_raises(ValueError, minimum_phase, numpy.ones(10), n_fft=8)
assert_raises(ValueError, minimum_phase, numpy.ones(10), method='foo')
with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    tempResult = arange(3)
	
===================================================================	
TestFindPeaks.test_find_peaks_nopeak: 222	
----------------------------	

"\n        Verify that no peak is found in\n        data that's just noise.\n        "
noise_amp = 1.0
num_points = 100
numpy.random.seed(181819141)
test_data = ((numpy.random.rand(num_points) - 0.5) * (2 * noise_amp))
tempResult = arange(10, 50)
	
===================================================================	
TestArgrel.test_2d_gaussians: 182	
----------------------------	

sigmas = [1.0, 2.0, 10.0]
(test_data, act_locs) = _gen_gaussians_even(sigmas, 100)
rot_factor = 20
tempResult = arange(0, len(test_data))
	
===================================================================	
TestFindPeaks.test_find_peaks_withnoise: 206	
----------------------------	

'\n        Verify that peak locations are (approximately) found\n        for a series of gaussians with added noise.\n        '
sigmas = [5.0, 3.0, 10.0, 20.0, 10.0, 50.0]
num_points = 500
(test_data, act_locs) = _gen_gaussians_even(sigmas, num_points)
tempResult = arange(0.1, max(sigmas))
	
===================================================================	
_gen_gaussians: 10	
----------------------------	

tempResult = arange(0, total_length)
	
===================================================================	
TestFindPeaks.test_find_peaks_exact: 197	
----------------------------	

'\n        Generate a series of gaussians and attempt to find the peak locations.\n        '
sigmas = [5.0, 3.0, 10.0, 20.0, 10.0, 50.0]
num_points = 500
(test_data, act_locs) = _gen_gaussians_even(sigmas, num_points)
tempResult = arange(0.1, max(sigmas))
	
===================================================================	
alt_sg_coeffs: 22	
----------------------------	

'This is an alternative implementation of the SG coefficients.\n\n    It uses numpy.polyfit and numpy.polyval.  The results should be\n    equivalent to those of savgol_coeffs(), but this implementation\n    is slower.\n\n    window_length should be odd.\n\n    '
if (pos is None):
    pos = (window_length // 2)
tempResult = arange(window_length)
	
===================================================================	
TestFiltFilt.test_gust_scalars: 1085	
----------------------------	

if (self.filtfilt_kind != 'tf'):
    raise SkipTest('gust only implemented for TF systems')
tempResult = arange(12)
	
===================================================================	
TestFiltFilt.test_axis: 1058	
----------------------------	

tempResult = arange(((10.0 * 11.0) * 12.0))
	
===================================================================	
TestCorrelate2d.test_invalid_shapes: 995	
----------------------------	

tempResult = arange(1, 7)
	
===================================================================	
TestCorrelate2d.test_invalid_shapes: 996	
----------------------------	

a = np.arange(1, 7).reshape((2, 3))
tempResult = arange((- 6), 0)
	
===================================================================	
TestDecimate._test_phaseshift: 1236	
----------------------------	

rate = 120
rates_to = [15, 20, 30, 40]
t_tot = int(100)
tempResult = arange(((rate * t_tot) + 1))
	
===================================================================	
TestDecimate._test_phaseshift: 1241	
----------------------------	

rate = 120
rates_to = [15, 20, 30, 40]
t_tot = int(100)
t = (numpy.arange(((rate * t_tot) + 1)) / float(rate))
freqs = ((numpy.array(rates_to) * 0.8) / 2)
d = (numpy.exp(((((1j * 2) * numpy.pi) * freqs[:, numpy.newaxis]) * t)) * scipy.signal.tukey(t.size, 0.1))
for rate_to in rates_to:
    q = (rate // rate_to)
    tempResult = arange(((rate_to * t_tot) + 1))
	
===================================================================	
_TestCorrelateReal.test_invalid_shapes: 909	
----------------------------	

tempResult = arange(1, 7)
	
===================================================================	
_TestCorrelateReal.test_invalid_shapes: 910	
----------------------------	

a = np.arange(1, 7).reshape((2, 3))
tempResult = arange((- 6), 0)
	
===================================================================	
TestCorrelate2d.test_consistency_correlate_funcs: 985	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestConvolve2d.test_consistency_convolve_funcs: 222	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestDecimate.test_basic_IIR: 1204	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestFFTConvolve.test_invalid_shapes: 332	
----------------------------	

tempResult = arange(1, 7)
	
===================================================================	
TestFFTConvolve.test_invalid_shapes: 333	
----------------------------	

a = np.arange(1, 7).reshape((2, 3))
tempResult = arange((- 6), 0)
	
===================================================================	
_TestConvolve2d.test_invalid_shapes: 191	
----------------------------	

tempResult = arange(1, 7)
	
===================================================================	
_TestConvolve2d.test_invalid_shapes: 192	
----------------------------	

a = np.arange(1, 7).reshape((2, 3))
tempResult = arange((- 6), 0)
	
===================================================================	
TestDecimate.test_bad_args: 1199	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestHilbert.test_hilbert_axisN: 1291	
----------------------------	

tempResult = arange(18)
	
===================================================================	
TestResample._test_data: 389	
----------------------------	

rate = 100
rates_to = [49, 50, 51, 99, 100, 101, 199, 200, 201]
tempResult = arange(rate)
	
===================================================================	
TestResample._test_data: 393	
----------------------------	

rate = 100
rates_to = [49, 50, 51, 99, 100, 101, 199, 200, 201]
t = (numpy.arange(rate) / float(rate))
freqs = numpy.array((1.0, 10.0, 40.0))[:, numpy.newaxis]
x = (numpy.sin((((2 * numpy.pi) * freqs) * t)) * hann(rate))
for rate_to in rates_to:
    tempResult = arange(rate_to)
	
===================================================================	
TestResample._test_data: 421	
----------------------------	

rate = 100
rates_to = [49, 50, 51, 99, 100, 101, 199, 200, 201]
t = (numpy.arange(rate) / float(rate))
freqs = numpy.array((1.0, 10.0, 40.0))[:, numpy.newaxis]
x = (numpy.sin((((2 * numpy.pi) * freqs) * t)) * hann(rate))
for rate_to in rates_to:
    t_to = (numpy.arange(rate_to) / float(rate_to))
    y_tos = (numpy.sin((((2 * numpy.pi) * freqs) * t_to)) * hann(rate_to))
    if (method == 'fft'):
        y_resamps = scipy.signal.resample(x, rate_to, axis=(- 1))
    else:
        if (ext and (rate_to != rate)):
            g = gcd(rate_to, rate)
            up = (rate_to // g)
            down = (rate // g)
            max_rate = max(up, down)
            f_c = (1.0 / max_rate)
            half_len = (10 * max_rate)
            window = scipy.signal.firwin(((2 * half_len) + 1), f_c, window=('kaiser', 5.0))
            polyargs = {'window': window}
        else:
            polyargs = {}
        y_resamps = scipy.signal.resample_poly(x, rate_to, rate, axis=(- 1), **polyargs)
    for (y_to, y_resamp, freq) in zip(y_tos, y_resamps, freqs):
        if (freq >= (0.5 * rate_to)):
            y_to.fill(0.0)
            assert_allclose(y_resamp, y_to, atol=0.001)
        else:
            assert_array_equal(y_to.shape, y_resamp.shape)
            corr = numpy.corrcoef(y_to, y_resamp)[(0, 1)]
            assert_((corr > 0.99), msg=(corr, rate, rate_to))
rng = numpy.random.RandomState(0)
x = (hann(rate) * numpy.cumsum(rng.randn(rate)))
for rate_to in rates_to:
    tempResult = arange(rate_to)
	
===================================================================	
TestHilbert.test_hilbert_theoretical: 1272	
----------------------------	

decimal = 14
pi = numpy.pi
tempResult = arange(0, (2 * pi), (pi / 256))
	
===================================================================	
TestHilbert.test_hilbert_theoretical: 1284	
----------------------------	

decimal = 14
pi = numpy.pi
t = numpy.arange(0, (2 * pi), (pi / 256))
a0 = numpy.sin(t)
a1 = numpy.cos(t)
a2 = numpy.sin((2 * t))
a3 = numpy.cos((2 * t))
a = numpy.vstack([a0, a1, a2, a3])
h = hilbert(a)
h_abs = numpy.abs(h)
h_angle = numpy.angle(h)
h_real = numpy.real(h)
assert_almost_equal(h_real, a, decimal)
assert_almost_equal(h_abs, numpy.ones(a.shape), decimal)
tempResult = arange(((- pi) / 2), (pi / 2), (pi / 256))
	
===================================================================	
TestHilbert.test_hilbert_theoretical: 1285	
----------------------------	

decimal = 14
pi = numpy.pi
t = numpy.arange(0, (2 * pi), (pi / 256))
a0 = numpy.sin(t)
a1 = numpy.cos(t)
a2 = numpy.sin((2 * t))
a3 = numpy.cos((2 * t))
a = numpy.vstack([a0, a1, a2, a3])
h = hilbert(a)
h_abs = numpy.abs(h)
h_angle = numpy.angle(h)
h_real = numpy.real(h)
assert_almost_equal(h_real, a, decimal)
assert_almost_equal(h_abs, numpy.ones(a.shape), decimal)
assert_almost_equal(h_angle[0, :256], numpy.arange(((- pi) / 2), (pi / 2), (pi / 256)), decimal)
tempResult = arange(0, pi, (pi / 256))
	
===================================================================	
TestHilbert.test_hilbert_theoretical: 1286	
----------------------------	

decimal = 14
pi = numpy.pi
t = numpy.arange(0, (2 * pi), (pi / 256))
a0 = numpy.sin(t)
a1 = numpy.cos(t)
a2 = numpy.sin((2 * t))
a3 = numpy.cos((2 * t))
a = numpy.vstack([a0, a1, a2, a3])
h = hilbert(a)
h_abs = numpy.abs(h)
h_angle = numpy.angle(h)
h_real = numpy.real(h)
assert_almost_equal(h_real, a, decimal)
assert_almost_equal(h_abs, numpy.ones(a.shape), decimal)
assert_almost_equal(h_angle[0, :256], numpy.arange(((- pi) / 2), (pi / 2), (pi / 256)), decimal)
assert_almost_equal(h_angle[1, :256], numpy.arange(0, pi, (pi / 256)), decimal)
tempResult = arange(((- pi) / 2), (pi / 2), (pi / 128))
	
===================================================================	
TestHilbert.test_hilbert_theoretical: 1287	
----------------------------	

decimal = 14
pi = numpy.pi
t = numpy.arange(0, (2 * pi), (pi / 256))
a0 = numpy.sin(t)
a1 = numpy.cos(t)
a2 = numpy.sin((2 * t))
a3 = numpy.cos((2 * t))
a = numpy.vstack([a0, a1, a2, a3])
h = hilbert(a)
h_abs = numpy.abs(h)
h_angle = numpy.angle(h)
h_real = numpy.real(h)
assert_almost_equal(h_real, a, decimal)
assert_almost_equal(h_abs, numpy.ones(a.shape), decimal)
assert_almost_equal(h_angle[0, :256], numpy.arange(((- pi) / 2), (pi / 2), (pi / 256)), decimal)
assert_almost_equal(h_angle[1, :256], numpy.arange(0, pi, (pi / 256)), decimal)
assert_almost_equal(h_angle[2, :128], numpy.arange(((- pi) / 2), (pi / 2), (pi / 128)), decimal)
tempResult = arange(0, pi, (pi / 128))
	
===================================================================	
TestFiltFilt.test_acoeff: 1069	
----------------------------	

if (self.filtfilt_kind != 'tf'):
    return
tempResult = arange(10)
	
===================================================================	
TestFiltFilt.test_acoeff: 1070	
----------------------------	

if (self.filtfilt_kind != 'tf'):
    return
out = scipy.signal.filtfilt([0.5, 0.5], 1, numpy.arange(10))
tempResult = arange(10)
	
===================================================================	
TestHilbert.test_bad_args: 1266	
----------------------------	

x = numpy.array([(1.0 + 0j)])
assert_raises(ValueError, hilbert, x)
tempResult = arange(8.0)
	
===================================================================	
TestMedFilt.test_none: 351	
----------------------------	

try:
    scipy.signal.medfilt(None)
except:
    pass
tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestDecimate.test_basic_FIR: 1209	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestConvolve.test_invalid_shapes: 102	
----------------------------	

tempResult = arange(1, 7)
	
===================================================================	
TestConvolve.test_invalid_shapes: 103	
----------------------------	

a = np.arange(1, 7).reshape((2, 3))
tempResult = arange((- 6), 0)
	
===================================================================	
TestHilbert2.test_bad_args: 1306	
----------------------------	

x = numpy.array([[(1.0 + 0j)]])
assert_raises(ValueError, hilbert2, x)
tempResult = arange(24)
	
===================================================================	
TestHilbert2.test_bad_args: 1308	
----------------------------	

x = numpy.array([[(1.0 + 0j)]])
assert_raises(ValueError, hilbert2, x)
x = np.arange(24).reshape(2, 3, 4)
assert_raises(ValueError, hilbert2, x)
tempResult = arange(16)
	
===================================================================	
TestCSpline1DEval.test_complex: 461	
----------------------------	

tempResult = arange(2)
	
===================================================================	
TestFiltFilt.test_basic: 1033	
----------------------------	

zpk = tf2zpk([1, 2, 3], [1, 2, 3])
tempResult = arange(12)
	
===================================================================	
TestResample.test_basic: 367	
----------------------------	

tempResult = arange(128)
	
===================================================================	
TestResample.test_basic: 373	
----------------------------	

sig = numpy.arange(128)
num = 256
win = scipy.signal.get_window(('kaiser', 8.0), 160)
assert_raises(ValueError, scipy.signal.resample, sig, num, window=win)
assert_raises(ValueError, scipy.signal.resample_poly, sig, 'yo', 1)
assert_raises(ValueError, scipy.signal.resample_poly, sig, 1, 0)
tempResult = arange(160)
	
===================================================================	
_TestLinearFilter.test_bad_size_zi: 668	
----------------------------	

tempResult = arange(6)
	
===================================================================	
_TestLinearFilter.test_bad_size_zi: 686	
----------------------------	

x1 = numpy.arange(6)
self.base_bad_size_zi([1], [1], x1, (- 1), [1])
self.base_bad_size_zi([1, 1], [1], x1, (- 1), [0, 1])
self.base_bad_size_zi([1, 1], [1], x1, (- 1), [[0]])
self.base_bad_size_zi([1, 1], [1], x1, (- 1), [0, 1, 2])
self.base_bad_size_zi([1, 1, 1], [1], x1, (- 1), [[0]])
self.base_bad_size_zi([1, 1, 1], [1], x1, (- 1), [0, 1, 2])
self.base_bad_size_zi([1], [1, 1], x1, (- 1), [0, 1])
self.base_bad_size_zi([1], [1, 1], x1, (- 1), [[0]])
self.base_bad_size_zi([1], [1, 1], x1, (- 1), [0, 1, 2])
self.base_bad_size_zi([1, 1, 1], [1, 1], x1, (- 1), [0])
self.base_bad_size_zi([1, 1, 1], [1, 1], x1, (- 1), [[0], [1]])
self.base_bad_size_zi([1, 1, 1], [1, 1], x1, (- 1), [0, 1, 2])
self.base_bad_size_zi([1, 1, 1], [1, 1], x1, (- 1), [0, 1, 2, 3])
self.base_bad_size_zi([1, 1], [1, 1, 1], x1, (- 1), [0])
self.base_bad_size_zi([1, 1], [1, 1, 1], x1, (- 1), [[0], [1]])
self.base_bad_size_zi([1, 1], [1, 1, 1], x1, (- 1), [0, 1, 2])
self.base_bad_size_zi([1, 1], [1, 1, 1], x1, (- 1), [0, 1, 2, 3])
tempResult = arange(12)
	
===================================================================	
TestSTFT.test_roundtrip_boundary_extension: 973	
----------------------------	

numpy.random.seed(1234)
settings = [('boxcar', 100, 10, 0), ('boxcar', 100, 10, 9)]
for (window, N, nperseg, noverlap) in settings:
    tempResult = arange(N)
	
===================================================================	
TestWelch.test_window_long_or_nd: 384	
----------------------------	

with warnings.catch_warnings():
    warnings.simplefilter('ignore', UserWarning)
    assert_raises(ValueError, welch, numpy.zeros(4), 1, numpy.array([1, 1, 1, 1, 1]))
    tempResult = arange(6)
	
===================================================================	
TestWelch.test_integer_onesided_odd: 264	
----------------------------	

x = numpy.zeros(16, dtype=int)
x[0] = 1
x[8] = 1
(f, p) = welch(x, nperseg=9)
tempResult = arange(5.0)
	
===================================================================	
TestWelch.test_no_detrending: 295	
----------------------------	

tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestCSD.test_real_onesided_odd: 503	
----------------------------	

x = numpy.zeros(16)
x[0] = 1
x[8] = 1
(f, p) = csd(x, x, nperseg=9)
tempResult = arange(5.0)
	
===================================================================	
TestPeriodogram.test_real_onesided_odd: 27	
----------------------------	

x = numpy.zeros(15)
x[0] = 1
(f, p) = periodogram(x)
tempResult = arange(8.0)
	
===================================================================	
TestWelch.test_real_onesided_odd_32: 414	
----------------------------	

x = numpy.zeros(16, 'f')
x[0] = 1
x[8] = 1
(f, p) = welch(x, nperseg=9)
tempResult = arange(5.0)
	
===================================================================	
TestSTFT.test_roundtrip_float32: 940	
----------------------------	

numpy.random.seed(1234)
settings = [('hann', 1024, 256, 128)]
for (window, N, nperseg, noverlap) in settings:
    tempResult = arange(N)
	
===================================================================	
TestWelch.test_window_correction: 461	
----------------------------	

A = 20
fs = 10000.0
nperseg = int((fs // 10))
fsig = 300
ii = int(((fsig * nperseg) // fs))
tempResult = arange(fs)
	
===================================================================	
TestWelch.test_nd_axis_0: 329	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
TestCSD.test_nd_axis_m1: 595	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
TestWelch.test_nd_axis_m1: 320	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
TestCSD.test_real_onesided_odd_32: 704	
----------------------------	

x = numpy.zeros(16, 'f')
x[0] = 1
x[8] = 1
(f, p) = csd(x, x, nperseg=9)
tempResult = arange(5.0)
	
===================================================================	
TestCSD.test_detrend_external: 577	
----------------------------	

tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestSTFT.test_roundtrip_padded_FFT: 1000	
----------------------------	

numpy.random.seed(1234)
settings = [('hann', 1024, 256, 128, 512), ('hann', 1024, 256, 128, 501), ('boxcar', 100, 10, 0, 33), (('tukey', 0.5), 1152, 256, 64, 1024)]
for (window, N, nperseg, noverlap, nfft) in settings:
    tempResult = arange(N)
	
===================================================================	
TestCSD.test_detrend_linear: 565	
----------------------------	

tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestWelch.test_real_onesided_odd: 228	
----------------------------	

x = numpy.zeros(16)
x[0] = 1
x[8] = 1
(f, p) = welch(x, nperseg=9)
tempResult = arange(5.0)
	
===================================================================	
TestCSD.test_no_detrending: 570	
----------------------------	

tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestWelch.test_detrend_external_nd_0: 313	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
TestWelch.test_detrend_external: 302	
----------------------------	

tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestPeriodogram.test_integer_odd: 65	
----------------------------	

x = numpy.zeros(15, dtype=int)
x[0] = 1
(f, p) = periodogram(x)
tempResult = arange(8.0)
	
===================================================================	
TestPeriodogram.test_real_onesided_odd_32: 185	
----------------------------	

x = numpy.zeros(15, 'f')
x[0] = 1
(f, p) = periodogram(x)
tempResult = arange(8.0)
	
===================================================================	
TestCSD.test_detrend_external_nd_m1: 582	
----------------------------	

tempResult = arange(40, dtype=numpy.float64)
	
===================================================================	
TestWelch.test_detrend_external_nd_m1: 307	
----------------------------	

tempResult = arange(40, dtype=numpy.float64)
	
===================================================================	
TestCSD.test_window_long_or_nd: 674	
----------------------------	

with warnings.catch_warnings():
    warnings.simplefilter('ignore', UserWarning)
    assert_raises(ValueError, csd, numpy.zeros(4), numpy.ones(4), 1, numpy.array([1, 1, 1, 1, 1]))
    tempResult = arange(6)
	
===================================================================	
TestSTFT.test_roundtrip_padded_signal: 988	
----------------------------	

numpy.random.seed(1234)
settings = [('boxcar', 101, 10, 0), ('hann', 1000, 256, 128)]
for (window, N, nperseg, noverlap) in settings:
    tempResult = arange(N)
	
===================================================================	
TestSTFT.test_roundtrip_complex: 954	
----------------------------	

numpy.random.seed(1234)
settings = [('boxcar', 100, 10, 0), ('boxcar', 100, 10, 9), ('bartlett', 101, 51, 26), ('hann', 1024, 256, 128), (('tukey', 0.5), 1152, 256, 64), ('hann', 1024, 256, 255)]
for (window, N, nperseg, noverlap) in settings:
    tempResult = arange(N)
	
===================================================================	
TestCSD.test_detrend_external_nd_0: 588	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
TestWelch.test_detrend_linear: 290	
----------------------------	

tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestSTFT.test_roundtrip_real: 927	
----------------------------	

numpy.random.seed(1234)
settings = [('boxcar', 100, 10, 0), ('boxcar', 100, 10, 9), ('bartlett', 101, 51, 26), ('hann', 1024, 256, 128), (('tukey', 0.5), 1152, 256, 64), ('hann', 1024, 256, 255)]
for (window, N, nperseg, noverlap) in settings:
    tempResult = arange(N)
	
===================================================================	
TestCSD.test_integer_onesided_odd: 539	
----------------------------	

x = numpy.zeros(16, dtype=int)
x[0] = 1
x[8] = 1
(f, p) = csd(x, x, nperseg=9)
tempResult = arange(5.0)
	
===================================================================	
TestCSD.test_nd_axis_0: 604	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
UpFIRDnCase.__call__: 33	
----------------------------	

self.scrub(numpy.ones(1, self.x_dtype))
self.scrub(numpy.ones(10, self.x_dtype))
x = self.rng.randn(10).astype(self.x_dtype)
if (self.x_dtype in (numpy.complex64, numpy.complex128)):
    x += (1j * self.rng.randn(10))
self.scrub(x)
tempResult = arange(10)
	
===================================================================	
TestWavelets.test_cwt: 90	
----------------------------	

widths = [1.0]
delta_wavelet = (lambda s, t: numpy.array([1]))
len_data = 100
tempResult = arange(0, len_data)
	
===================================================================	
TestWavelets.test_ricker: 74	
----------------------------	

w = scipy.signal.wavelets.ricker(1.0, 1)
expected = (2 / (numpy.sqrt((3 * 1.0)) * (numpy.pi ** 0.25)))
assert_array_equal(w, expected)
lengths = [5, 11, 15, 51, 101]
for length in lengths:
    w = scipy.signal.wavelets.ricker(length, 1.0)
    assert_((len(w) == length))
    max_loc = numpy.argmax(w)
    assert_((max_loc == (length // 2)))
points = 100
w = scipy.signal.wavelets.ricker(points, 2.0)
tempResult = arange(0, (points // 2))
	
===================================================================	
TestWavelets.test_ricker: 80	
----------------------------	

w = scipy.signal.wavelets.ricker(1.0, 1)
expected = (2 / (numpy.sqrt((3 * 1.0)) * (numpy.pi ** 0.25)))
assert_array_equal(w, expected)
lengths = [5, 11, 15, 51, 101]
for length in lengths:
    w = scipy.signal.wavelets.ricker(length, 1.0)
    assert_((len(w) == length))
    max_loc = numpy.argmax(w)
    assert_((max_loc == (length // 2)))
points = 100
w = scipy.signal.wavelets.ricker(points, 2.0)
half_vec = numpy.arange(0, (points // 2))
assert_array_almost_equal(w[half_vec], w[(- (half_vec + 1))])
aas = [5, 10, 15, 20, 30]
points = 99
for a in aas:
    w = scipy.signal.wavelets.ricker(points, a)
    tempResult = arange(0, points)
	
===================================================================	
TestGetWindow.test_array_as_window: 215	
----------------------------	

osfactor = 128
tempResult = arange(128)
	
===================================================================	
bsr_matrix.tocoo: 228	
----------------------------	

'Convert this matrix to COOrdinate format.\n\n        When copy=False the data array will be shared between\n        this matrix and the resultant coo_matrix.\n        '
(M, N) = self.shape
(R, C) = self.blocksize
indptr_diff = numpy.diff(self.indptr)
if (indptr_diff.dtype.itemsize > np.dtype(np.intp).itemsize):
    indptr_diff_limited = indptr_diff.astype(numpy.intp)
    if numpy.any((indptr_diff_limited != indptr_diff)):
        raise ValueError('Matrix too big to convert')
    indptr_diff = indptr_diff_limited
tempResult = arange((M // R))
	
===================================================================	
bsr_matrix.tocoo: 230	
----------------------------	

'Convert this matrix to COOrdinate format.\n\n        When copy=False the data array will be shared between\n        this matrix and the resultant coo_matrix.\n        '
(M, N) = self.shape
(R, C) = self.blocksize
indptr_diff = numpy.diff(self.indptr)
if (indptr_diff.dtype.itemsize > np.dtype(np.intp).itemsize):
    indptr_diff_limited = indptr_diff.astype(numpy.intp)
    if numpy.any((indptr_diff_limited != indptr_diff)):
        raise ValueError('Matrix too big to convert')
    indptr_diff = indptr_diff_limited
row = (R * np.arange((M // R))).repeat(indptr_diff)
row = row.repeat((R * C)).reshape((- 1), R, C)
tempResult = arange(R)
	
===================================================================	
bsr_matrix.tocoo: 233	
----------------------------	

'Convert this matrix to COOrdinate format.\n\n        When copy=False the data array will be shared between\n        this matrix and the resultant coo_matrix.\n        '
(M, N) = self.shape
(R, C) = self.blocksize
indptr_diff = numpy.diff(self.indptr)
if (indptr_diff.dtype.itemsize > np.dtype(np.intp).itemsize):
    indptr_diff_limited = indptr_diff.astype(numpy.intp)
    if numpy.any((indptr_diff_limited != indptr_diff)):
        raise ValueError('Matrix too big to convert')
    indptr_diff = indptr_diff_limited
row = (R * np.arange((M // R))).repeat(indptr_diff)
row = row.repeat((R * C)).reshape((- 1), R, C)
row += numpy.tile(np.arange(R).reshape((- 1), 1), (1, C))
row = row.reshape((- 1))
col = (C * self.indices).repeat((R * C)).reshape((- 1), R, C)
tempResult = arange(C)
	
===================================================================	
_cs_matrix.__setitem__: 442	
----------------------------	

(i, j) = self._unpack_index(index)
(i, j) = self._index_to_arrays(i, j)
if isspmatrix(x):
    broadcast_row = ((x.shape[0] == 1) and (i.shape[0] != 1))
    broadcast_col = ((x.shape[1] == 1) and (i.shape[1] != 1))
    if (not ((broadcast_row or (x.shape[0] == i.shape[0])) and (broadcast_col or (x.shape[1] == i.shape[1])))):
        raise ValueError('shape mismatch in assignment')
    (ci, cj) = self._swap((i.ravel(), j.ravel()))
    self._zero_many(ci, cj)
    x = x.tocoo()
    (r, c) = (x.row, x.col)
    x = numpy.asarray(x.data, dtype=self.dtype)
    if broadcast_row:
        tempResult = arange(i.shape[0])
	
===================================================================	
_cs_matrix.__setitem__: 447	
----------------------------	

(i, j) = self._unpack_index(index)
(i, j) = self._index_to_arrays(i, j)
if isspmatrix(x):
    broadcast_row = ((x.shape[0] == 1) and (i.shape[0] != 1))
    broadcast_col = ((x.shape[1] == 1) and (i.shape[1] != 1))
    if (not ((broadcast_row or (x.shape[0] == i.shape[0])) and (broadcast_col or (x.shape[1] == i.shape[1])))):
        raise ValueError('shape mismatch in assignment')
    (ci, cj) = self._swap((i.ravel(), j.ravel()))
    self._zero_many(ci, cj)
    x = x.tocoo()
    (r, c) = (x.row, x.col)
    x = numpy.asarray(x.data, dtype=self.dtype)
    if broadcast_row:
        r = numpy.repeat(numpy.arange(i.shape[0]), len(r))
        c = numpy.tile(c, i.shape[0])
        x = numpy.tile(x, i.shape[0])
    if broadcast_col:
        r = numpy.repeat(r, i.shape[1])
        tempResult = arange(i.shape[1])
	
===================================================================	
_cs_matrix.multiply: 315	
----------------------------	

'Point-wise multiplication by another matrix, vector, or\n        scalar.\n        '
if isscalarlike(other):
    return self._mul_scalar(other)
if isspmatrix(other):
    if (self.shape == other.shape):
        other = self.__class__(other)
        return self._binopt(other, '_elmul_')
    elif (other.shape == (1, 1)):
        return self._mul_scalar(other.toarray()[(0, 0)])
    elif (self.shape == (1, 1)):
        return other._mul_scalar(self.toarray()[(0, 0)])
    elif ((self.shape[1] == 1) and (other.shape[0] == 1)):
        return self._mul_sparse_matrix(other.tocsc())
    elif ((self.shape[0] == 1) and (other.shape[1] == 1)):
        return other._mul_sparse_matrix(self.tocsc())
    elif ((other.shape[0] == 1) and (self.shape[1] == other.shape[1])):
        other = dia_matrix((other.toarray().ravel(), [0]), shape=(other.shape[1], other.shape[1]))
        return self._mul_sparse_matrix(other)
    elif ((self.shape[0] == 1) and (self.shape[1] == other.shape[1])):
        copy = dia_matrix((self.toarray().ravel(), [0]), shape=(self.shape[1], self.shape[1]))
        return other._mul_sparse_matrix(copy)
    elif ((other.shape[1] == 1) and (self.shape[0] == other.shape[0])):
        other = dia_matrix((other.toarray().ravel(), [0]), shape=(other.shape[0], other.shape[0]))
        return other._mul_sparse_matrix(self)
    elif ((self.shape[1] == 1) and (self.shape[0] == other.shape[0])):
        copy = dia_matrix((self.toarray().ravel(), [0]), shape=(self.shape[0], self.shape[0]))
        return copy._mul_sparse_matrix(other)
    else:
        raise ValueError('inconsistent shapes')
other = numpy.atleast_2d(other)
if (other.ndim != 2):
    return numpy.multiply(self.toarray(), other)
if (other.size == 1):
    return self._mul_scalar(other.flat[0])
elif (self.shape == (1, 1)):
    return numpy.multiply(self.toarray()[(0, 0)], other)
from .coo import coo_matrix
ret = self.tocoo()
if (self.shape == other.shape):
    data = numpy.multiply(ret.data, other[(ret.row, ret.col)])
elif (self.shape[0] == 1):
    if (other.shape[1] == 1):
        data = numpy.multiply(ret.data, other)
    elif (other.shape[1] == self.shape[1]):
        data = numpy.multiply(ret.data, other[:, ret.col])
    else:
        raise ValueError('inconsistent shapes')
    tempResult = arange(other.shape[0])
	
===================================================================	
_cs_matrix.multiply: 326	
----------------------------	

'Point-wise multiplication by another matrix, vector, or\n        scalar.\n        '
if isscalarlike(other):
    return self._mul_scalar(other)
if isspmatrix(other):
    if (self.shape == other.shape):
        other = self.__class__(other)
        return self._binopt(other, '_elmul_')
    elif (other.shape == (1, 1)):
        return self._mul_scalar(other.toarray()[(0, 0)])
    elif (self.shape == (1, 1)):
        return other._mul_scalar(self.toarray()[(0, 0)])
    elif ((self.shape[1] == 1) and (other.shape[0] == 1)):
        return self._mul_sparse_matrix(other.tocsc())
    elif ((self.shape[0] == 1) and (other.shape[1] == 1)):
        return other._mul_sparse_matrix(self.tocsc())
    elif ((other.shape[0] == 1) and (self.shape[1] == other.shape[1])):
        other = dia_matrix((other.toarray().ravel(), [0]), shape=(other.shape[1], other.shape[1]))
        return self._mul_sparse_matrix(other)
    elif ((self.shape[0] == 1) and (self.shape[1] == other.shape[1])):
        copy = dia_matrix((self.toarray().ravel(), [0]), shape=(self.shape[1], self.shape[1]))
        return other._mul_sparse_matrix(copy)
    elif ((other.shape[1] == 1) and (self.shape[0] == other.shape[0])):
        other = dia_matrix((other.toarray().ravel(), [0]), shape=(other.shape[0], other.shape[0]))
        return other._mul_sparse_matrix(self)
    elif ((self.shape[1] == 1) and (self.shape[0] == other.shape[0])):
        copy = dia_matrix((self.toarray().ravel(), [0]), shape=(self.shape[0], self.shape[0]))
        return copy._mul_sparse_matrix(other)
    else:
        raise ValueError('inconsistent shapes')
other = numpy.atleast_2d(other)
if (other.ndim != 2):
    return numpy.multiply(self.toarray(), other)
if (other.size == 1):
    return self._mul_scalar(other.flat[0])
elif (self.shape == (1, 1)):
    return numpy.multiply(self.toarray()[(0, 0)], other)
from .coo import coo_matrix
ret = self.tocoo()
if (self.shape == other.shape):
    data = numpy.multiply(ret.data, other[(ret.row, ret.col)])
elif (self.shape[0] == 1):
    if (other.shape[1] == 1):
        data = numpy.multiply(ret.data, other)
    elif (other.shape[1] == self.shape[1]):
        data = numpy.multiply(ret.data, other[:, ret.col])
    else:
        raise ValueError('inconsistent shapes')
    row = numpy.repeat(numpy.arange(other.shape[0]), len(ret.row))
    col = numpy.tile(ret.col, other.shape[0])
    return coo_matrix((data.view(np.ndarray).ravel(), (row, col)), shape=(other.shape[0], self.shape[1]), copy=False)
elif (self.shape[1] == 1):
    if (other.shape[0] == 1):
        data = numpy.multiply(ret.data[:, None], other)
    elif (other.shape[0] == self.shape[0]):
        data = numpy.multiply(ret.data[:, None], other[ret.row])
    else:
        raise ValueError('inconsistent shapes')
    row = numpy.repeat(ret.row, other.shape[1])
    tempResult = arange(other.shape[1])
	
===================================================================	
_cs_matrix._setdiag: 471	
----------------------------	

if (0 in self.shape):
    return
(M, N) = self.shape
broadcast = (values.ndim == 0)
if (k < 0):
    if broadcast:
        max_index = min((M + k), N)
    else:
        max_index = min((M + k), N, len(values))
    tempResult = arange(max_index, dtype=self.indices.dtype)
	
===================================================================	
_cs_matrix._setdiag: 472	
----------------------------	

if (0 in self.shape):
    return
(M, N) = self.shape
broadcast = (values.ndim == 0)
if (k < 0):
    if broadcast:
        max_index = min((M + k), N)
    else:
        max_index = min((M + k), N, len(values))
    i = numpy.arange(max_index, dtype=self.indices.dtype)
    tempResult = arange(max_index, dtype=self.indices.dtype)
	
===================================================================	
_cs_matrix._setdiag: 479	
----------------------------	

if (0 in self.shape):
    return
(M, N) = self.shape
broadcast = (values.ndim == 0)
if (k < 0):
    if broadcast:
        max_index = min((M + k), N)
    else:
        max_index = min((M + k), N, len(values))
    i = numpy.arange(max_index, dtype=self.indices.dtype)
    j = numpy.arange(max_index, dtype=self.indices.dtype)
    i -= k
else:
    if broadcast:
        max_index = min(M, (N - k))
    else:
        max_index = min(M, (N - k), len(values))
    tempResult = arange(max_index, dtype=self.indices.dtype)
	
===================================================================	
_cs_matrix._setdiag: 480	
----------------------------	

if (0 in self.shape):
    return
(M, N) = self.shape
broadcast = (values.ndim == 0)
if (k < 0):
    if broadcast:
        max_index = min((M + k), N)
    else:
        max_index = min((M + k), N, len(values))
    i = numpy.arange(max_index, dtype=self.indices.dtype)
    j = numpy.arange(max_index, dtype=self.indices.dtype)
    i -= k
else:
    if broadcast:
        max_index = min(M, (N - k))
    else:
        max_index = min(M, (N - k), len(values))
    i = numpy.arange(max_index, dtype=self.indices.dtype)
    tempResult = arange(max_index, dtype=self.indices.dtype)
	
===================================================================	
eye: 68	
----------------------------	

'Sparse matrix with ones on diagonal\n\n    Returns a sparse (m x n) matrix where the k-th diagonal\n    is all ones and everything else is zeros.\n\n    Parameters\n    ----------\n    m : int\n        Number of rows in the matrix.\n    n : int, optional\n        Number of columns. Default: `m`.\n    k : int, optional\n        Diagonal to place ones on. Default: 0 (main diagonal).\n    dtype : dtype, optional\n        Data type of the matrix.\n    format : str, optional\n        Sparse format of the result, e.g. format="csr", etc.\n\n    Examples\n    --------\n    >>> from scipy import sparse\n    >>> sparse.eye(3).toarray()\n    array([[ 1.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  1.]])\n    >>> sparse.eye(3, dtype=np.int8)\n    <3x3 sparse matrix of type \'<type \'numpy.int8\'>\'\n        with 3 stored elements (1 diagonals) in DIAgonal format>\n\n    '
if (n is None):
    n = m
(m, n) = (int(m), int(n))
if ((m == n) and (k == 0)):
    if (format in ['csr', 'csc']):
        idx_dtype = get_index_dtype(maxval=n)
        tempResult = arange((n + 1), dtype=idx_dtype)
	
===================================================================	
eye: 69	
----------------------------	

'Sparse matrix with ones on diagonal\n\n    Returns a sparse (m x n) matrix where the k-th diagonal\n    is all ones and everything else is zeros.\n\n    Parameters\n    ----------\n    m : int\n        Number of rows in the matrix.\n    n : int, optional\n        Number of columns. Default: `m`.\n    k : int, optional\n        Diagonal to place ones on. Default: 0 (main diagonal).\n    dtype : dtype, optional\n        Data type of the matrix.\n    format : str, optional\n        Sparse format of the result, e.g. format="csr", etc.\n\n    Examples\n    --------\n    >>> from scipy import sparse\n    >>> sparse.eye(3).toarray()\n    array([[ 1.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  1.]])\n    >>> sparse.eye(3, dtype=np.int8)\n    <3x3 sparse matrix of type \'<type \'numpy.int8\'>\'\n        with 3 stored elements (1 diagonals) in DIAgonal format>\n\n    '
if (n is None):
    n = m
(m, n) = (int(m), int(n))
if ((m == n) and (k == 0)):
    if (format in ['csr', 'csc']):
        idx_dtype = get_index_dtype(maxval=n)
        indptr = numpy.arange((n + 1), dtype=idx_dtype)
        tempResult = arange(n, dtype=idx_dtype)
	
===================================================================	
eye: 75	
----------------------------	

'Sparse matrix with ones on diagonal\n\n    Returns a sparse (m x n) matrix where the k-th diagonal\n    is all ones and everything else is zeros.\n\n    Parameters\n    ----------\n    m : int\n        Number of rows in the matrix.\n    n : int, optional\n        Number of columns. Default: `m`.\n    k : int, optional\n        Diagonal to place ones on. Default: 0 (main diagonal).\n    dtype : dtype, optional\n        Data type of the matrix.\n    format : str, optional\n        Sparse format of the result, e.g. format="csr", etc.\n\n    Examples\n    --------\n    >>> from scipy import sparse\n    >>> sparse.eye(3).toarray()\n    array([[ 1.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  1.]])\n    >>> sparse.eye(3, dtype=np.int8)\n    <3x3 sparse matrix of type \'<type \'numpy.int8\'>\'\n        with 3 stored elements (1 diagonals) in DIAgonal format>\n\n    '
if (n is None):
    n = m
(m, n) = (int(m), int(n))
if ((m == n) and (k == 0)):
    if (format in ['csr', 'csc']):
        idx_dtype = get_index_dtype(maxval=n)
        indptr = numpy.arange((n + 1), dtype=idx_dtype)
        indices = numpy.arange(n, dtype=idx_dtype)
        data = numpy.ones(n, dtype=dtype)
        cls = {'csr': csr_matrix, 'csc': csc_matrix}[format]
        return cls((data, indices, indptr), (n, n))
    elif (format == 'coo'):
        idx_dtype = get_index_dtype(maxval=n)
        tempResult = arange(n, dtype=idx_dtype)
	
===================================================================	
eye: 76	
----------------------------	

'Sparse matrix with ones on diagonal\n\n    Returns a sparse (m x n) matrix where the k-th diagonal\n    is all ones and everything else is zeros.\n\n    Parameters\n    ----------\n    m : int\n        Number of rows in the matrix.\n    n : int, optional\n        Number of columns. Default: `m`.\n    k : int, optional\n        Diagonal to place ones on. Default: 0 (main diagonal).\n    dtype : dtype, optional\n        Data type of the matrix.\n    format : str, optional\n        Sparse format of the result, e.g. format="csr", etc.\n\n    Examples\n    --------\n    >>> from scipy import sparse\n    >>> sparse.eye(3).toarray()\n    array([[ 1.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  1.]])\n    >>> sparse.eye(3, dtype=np.int8)\n    <3x3 sparse matrix of type \'<type \'numpy.int8\'>\'\n        with 3 stored elements (1 diagonals) in DIAgonal format>\n\n    '
if (n is None):
    n = m
(m, n) = (int(m), int(n))
if ((m == n) and (k == 0)):
    if (format in ['csr', 'csc']):
        idx_dtype = get_index_dtype(maxval=n)
        indptr = numpy.arange((n + 1), dtype=idx_dtype)
        indices = numpy.arange(n, dtype=idx_dtype)
        data = numpy.ones(n, dtype=dtype)
        cls = {'csr': csr_matrix, 'csc': csc_matrix}[format]
        return cls((data, indices, indptr), (n, n))
    elif (format == 'coo'):
        idx_dtype = get_index_dtype(maxval=n)
        row = numpy.arange(n, dtype=idx_dtype)
        tempResult = arange(n, dtype=idx_dtype)
	
===================================================================	
coo_matrix._setdiag: 216	
----------------------------	

(M, N) = self.shape
if (values.ndim and (not len(values))):
    return
idx_dtype = self.row.dtype
full_keep = ((self.col - self.row) != k)
if (k < 0):
    max_index = min((M + k), N)
    if values.ndim:
        max_index = min(max_index, len(values))
    keep = numpy.logical_or(full_keep, (self.col >= max_index))
    tempResult = arange((- k), ((- k) + max_index), dtype=idx_dtype)
	
===================================================================	
coo_matrix._setdiag: 217	
----------------------------	

(M, N) = self.shape
if (values.ndim and (not len(values))):
    return
idx_dtype = self.row.dtype
full_keep = ((self.col - self.row) != k)
if (k < 0):
    max_index = min((M + k), N)
    if values.ndim:
        max_index = min(max_index, len(values))
    keep = numpy.logical_or(full_keep, (self.col >= max_index))
    new_row = numpy.arange((- k), ((- k) + max_index), dtype=idx_dtype)
    tempResult = arange(max_index, dtype=idx_dtype)
	
===================================================================	
coo_matrix._setdiag: 223	
----------------------------	

(M, N) = self.shape
if (values.ndim and (not len(values))):
    return
idx_dtype = self.row.dtype
full_keep = ((self.col - self.row) != k)
if (k < 0):
    max_index = min((M + k), N)
    if values.ndim:
        max_index = min(max_index, len(values))
    keep = numpy.logical_or(full_keep, (self.col >= max_index))
    new_row = numpy.arange((- k), ((- k) + max_index), dtype=idx_dtype)
    new_col = numpy.arange(max_index, dtype=idx_dtype)
else:
    max_index = min(M, (N - k))
    if values.ndim:
        max_index = min(max_index, len(values))
    keep = numpy.logical_or(full_keep, (self.row >= max_index))
    tempResult = arange(max_index, dtype=idx_dtype)
	
===================================================================	
coo_matrix._setdiag: 224	
----------------------------	

(M, N) = self.shape
if (values.ndim and (not len(values))):
    return
idx_dtype = self.row.dtype
full_keep = ((self.col - self.row) != k)
if (k < 0):
    max_index = min((M + k), N)
    if values.ndim:
        max_index = min(max_index, len(values))
    keep = numpy.logical_or(full_keep, (self.col >= max_index))
    new_row = numpy.arange((- k), ((- k) + max_index), dtype=idx_dtype)
    new_col = numpy.arange(max_index, dtype=idx_dtype)
else:
    max_index = min(M, (N - k))
    if values.ndim:
        max_index = min(max_index, len(values))
    keep = numpy.logical_or(full_keep, (self.row >= max_index))
    new_row = numpy.arange(max_index, dtype=idx_dtype)
    tempResult = arange(k, (k + max_index), dtype=idx_dtype)
	
===================================================================	
csr_matrix.extractor: 115	
----------------------------	

'Return a sparse matrix P so that P*self implements\n            slicing of the form self[[1,2,3],:]\n            '
indices = asindices(indices)
(min_indx, max_indx) = check_bounds(indices, N)
if (min_indx < 0):
    indices = indices.copy()
    indices[(indices < 0)] += N
tempResult = arange((len(indices) + 1), dtype=indices.dtype)
	
===================================================================	
dia_matrix.tocsc: 205	
----------------------------	

from .csc import csc_matrix
if (self.nnz == 0):
    return csc_matrix(self.shape, dtype=self.dtype)
(num_rows, num_cols) = self.shape
(num_offsets, offset_len) = self.data.shape
tempResult = arange(offset_len)
	
===================================================================	
dia_matrix._data_mask: 77	
----------------------------	

'Returns a mask of the same shape as self.data, where\n        mask[i,j] is True when data[i,j] corresponds to a stored element.'
(num_rows, num_cols) = self.shape
tempResult = arange(self.data.shape[1])
	
===================================================================	
dia_matrix.tocoo: 223	
----------------------------	

(num_rows, num_cols) = self.shape
(num_offsets, offset_len) = self.data.shape
tempResult = arange(offset_len)
	
===================================================================	
dia_matrix.transpose: 183	
----------------------------	

if (axes is not None):
    raise ValueError("Sparse matrices do not support an 'axes' parameter because swapping dimensions is the only logical permutation.")
(num_rows, num_cols) = self.shape
max_dim = max(self.shape)
offsets = (- self.offsets)
tempResult = arange(len(offsets), dtype=numpy.intc)
	
===================================================================	
dia_matrix.transpose: 184	
----------------------------	

if (axes is not None):
    raise ValueError("Sparse matrices do not support an 'axes' parameter because swapping dimensions is the only logical permutation.")
(num_rows, num_cols) = self.shape
max_dim = max(self.shape)
offsets = (- self.offsets)
r = numpy.arange(len(offsets), dtype=numpy.intc)[:, None]
tempResult = arange(num_rows, dtype=numpy.intc)
	
===================================================================	
IndexMixin._slicetoarange: 151	
----------------------------	

' Given a slice object, use numpy arange to change it to a 1D\n        array.\n        '
(start, stop, step) = j.indices(shape)
tempResult = arange(start, stop, step)
	
===================================================================	
test_graph_maximum_bipartite_matching: 32	
----------------------------	

A = diags(numpy.ones(25), offsets=0, format='csc')
rand_perm = numpy.random.permutation(25)
rand_perm2 = numpy.random.permutation(25)
tempResult = arange(25)
	
===================================================================	
test_graph_maximum_bipartite_matching: 37	
----------------------------	

A = diags(numpy.ones(25), offsets=0, format='csc')
rand_perm = numpy.random.permutation(25)
rand_perm2 = numpy.random.permutation(25)
Rrow = numpy.arange(25)
Rcol = rand_perm
Rdata = numpy.ones(25, dtype=int)
Rmat = coo_matrix((Rdata, (Rrow, Rcol))).tocsc()
Crow = rand_perm2
tempResult = arange(25)
	
===================================================================	
test_graph_maximum_bipartite_matching: 42	
----------------------------	

A = diags(numpy.ones(25), offsets=0, format='csc')
rand_perm = numpy.random.permutation(25)
rand_perm2 = numpy.random.permutation(25)
Rrow = numpy.arange(25)
Rcol = rand_perm
Rdata = numpy.ones(25, dtype=int)
Rmat = coo_matrix((Rdata, (Rrow, Rcol))).tocsc()
Crow = rand_perm2
Ccol = numpy.arange(25)
Cdata = numpy.ones(25, dtype=int)
Cmat = coo_matrix((Cdata, (Crow, Ccol))).tocsc()
B = ((Rmat * A) * Cmat)
perm = maximum_bipartite_matching(B, perm_type='row')
tempResult = arange(25)
	
===================================================================	
test_graph_maximum_bipartite_matching: 49	
----------------------------	

A = diags(numpy.ones(25), offsets=0, format='csc')
rand_perm = numpy.random.permutation(25)
rand_perm2 = numpy.random.permutation(25)
Rrow = numpy.arange(25)
Rcol = rand_perm
Rdata = numpy.ones(25, dtype=int)
Rmat = coo_matrix((Rdata, (Rrow, Rcol))).tocsc()
Crow = rand_perm2
Ccol = numpy.arange(25)
Cdata = numpy.ones(25, dtype=int)
Cmat = coo_matrix((Cdata, (Crow, Ccol))).tocsc()
B = ((Rmat * A) * Cmat)
perm = maximum_bipartite_matching(B, perm_type='row')
Rrow = numpy.arange(25)
Rcol = perm
Rdata = numpy.ones(25, dtype=int)
Rmat = coo_matrix((Rdata, (Rrow, Rcol))).tocsc()
C1 = (Rmat * B)
perm2 = maximum_bipartite_matching(B, perm_type='column')
Crow = perm2
tempResult = arange(25)
	
===================================================================	
test_graph_maximum_bipartite_matching: 58	
----------------------------	

A = diags(numpy.ones(25), offsets=0, format='csc')
rand_perm = numpy.random.permutation(25)
rand_perm2 = numpy.random.permutation(25)
Rrow = numpy.arange(25)
Rcol = rand_perm
Rdata = numpy.ones(25, dtype=int)
Rmat = coo_matrix((Rdata, (Rrow, Rcol))).tocsc()
Crow = rand_perm2
Ccol = numpy.arange(25)
Cdata = numpy.ones(25, dtype=int)
Cmat = coo_matrix((Cdata, (Crow, Ccol))).tocsc()
B = ((Rmat * A) * Cmat)
perm = maximum_bipartite_matching(B, perm_type='row')
Rrow = numpy.arange(25)
Rcol = perm
Rdata = numpy.ones(25, dtype=int)
Rmat = coo_matrix((Rdata, (Rrow, Rcol))).tocsc()
C1 = (Rmat * B)
perm2 = maximum_bipartite_matching(B, perm_type='column')
Crow = perm2
Ccol = numpy.arange(25)
Cdata = numpy.ones(25, dtype=int)
Cmat = coo_matrix((Cdata, (Crow, Ccol))).tocsc()
C2 = (B * Cmat)
assert_equal(any((C1.diagonal() == 0)), False)
assert_equal(any((C2.diagonal() == 0)), False)
B.indices = B.indices.astype('int64')
B.indptr = B.indptr.astype('int64')
perm = maximum_bipartite_matching(B, perm_type='row')
tempResult = arange(25)
	
===================================================================	
test_shortest_path_indices: 50	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_minimum_spanning_tree: 27	
----------------------------	

graph = [[0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 8, 5], [0, 0, 8, 0, 1], [0, 0, 5, 1, 0]]
graph = numpy.asarray(graph)
expected = [[0, 1, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 5], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]]
expected = numpy.asarray(expected)
csgraph = csr_matrix(graph)
mintree = minimum_spanning_tree(csgraph)
numpy.testing.assert_array_equal(mintree.todense(), expected, 'Incorrect spanning tree found.')
numpy.testing.assert_array_equal(csgraph.todense(), graph, 'Original graph was modified.')
mintree = minimum_spanning_tree(csgraph, overwrite=True)
numpy.testing.assert_array_equal(mintree.todense(), expected, 'Graph was not properly modified to contain MST.')
numpy.random.seed(1234)
for N in (5, 10, 15, 20):
    graph = (3 + numpy.random.random((N, N)))
    csgraph = csr_matrix(graph)
    mintree = minimum_spanning_tree(csgraph)
    assert_((mintree.nnz < N))
    tempResult = arange((N - 1))
	
===================================================================	
TestSplu.check11: 342	
----------------------------	

A = self.A.astype(dtype)
if complex_2:
    A = (A + (1j * A.T))
n = A.shape[0]
lu = splu(A)
Pc = numpy.zeros((n, n))
tempResult = arange(n)
	
===================================================================	
TestSplu.check11: 344	
----------------------------	

A = self.A.astype(dtype)
if complex_2:
    A = (A + (1j * A.T))
n = A.shape[0]
lu = splu(A)
Pc = numpy.zeros((n, n))
Pc[(numpy.arange(n), lu.perm_c)] = 1
Pr = numpy.zeros((n, n))
tempResult = arange(n)
	
===================================================================	
TestSpsolveTriangular.test_singular: 380	
----------------------------	

n = 5
A = csr_matrix((n, n))
tempResult = arange(n)
	
===================================================================	
TestLinsolve.test_singular_gh_3312: 37	
----------------------------	

ij = numpy.array([(17, 0), (17, 6), (17, 12), (10, 13)], dtype=numpy.int32)
v = numpy.array([0.284213, 0.94933781, 0.15767017, 0.38797296])
A = csc_matrix((v, ij.T), shape=(20, 20))
tempResult = arange(20)
	
===================================================================	
TestSplu.test_superlu_dlamch_i386_nan: 325	
----------------------------	

n = 8
tempResult = arange(n)
	
===================================================================	
TestLinsolve.test_gssv_badinput: 143	
----------------------------	

N = 10
d = (arange(N) + 1.0)
A = spdiags((d, (2 * d), d[::(- 1)]), ((- 3), 0, 5), N, N)
for spmatrix in (csc_matrix, csr_matrix):
    A = spmatrix(A)
    tempResult = arange(N)
	
===================================================================	
generate_matrix: 43	
----------------------------	

M = numpy.random.random((N, N))
if complex:
    M = (M + (1j * numpy.random.random((N, N))))
if hermitian:
    if pos_definite:
        if sparse:
            tempResult = arange(N)
	
===================================================================	
test_regression_arpackng_1315: 609	
----------------------------	

for dtype in [numpy.float32, numpy.float64]:
    numpy.random.seed(1234)
    tempResult = arange(1, (1000 + 1))
	
===================================================================	
MikotaPair: 24	
----------------------------	

tempResult = arange(1, (n + 1))
	
===================================================================	
MikotaPair: 26	
----------------------------	

x = numpy.arange(1, (n + 1))
B = diag((1.0 / x))
tempResult = arange((n - 1), 0, (- 1))
	
===================================================================	
MikotaPair: 27	
----------------------------	

x = numpy.arange(1, (n + 1))
B = diag((1.0 / x))
y = numpy.arange((n - 1), 0, (- 1))
tempResult = arange(((2 * n) - 1), 0, (- 2))
	
===================================================================	
_check_fiedler: 96	
----------------------------	

numpy.random.seed(1234)
col = numpy.zeros(n)
col[1] = 1
A = toeplitz(col)
D = numpy.diag(A.sum(axis=1))
L = (D - A)
tempResult = arange(n)
	
===================================================================	
_check_fiedler: 98	
----------------------------	

numpy.random.seed(1234)
col = numpy.zeros(n)
col[1] = 1
A = toeplitz(col)
D = numpy.diag(A.sum(axis=1))
L = (D - A)
tmp = ((numpy.pi * numpy.arange(n)) / n)
analytic_w = (2 * (1 - numpy.cos(tmp)))
tempResult = arange(n)
	
===================================================================	
test_diagonal: 73	
----------------------------	

numpy.random.seed(1234)
n = 100
m = 4
tempResult = arange(1, (n + 1), dtype=float)
	
===================================================================	
test_diagonal: 81	
----------------------------	

numpy.random.seed(1234)
n = 100
m = 4
vals = numpy.arange(1, (n + 1), dtype=float)
A = scipy.sparse.diags([vals], [0], (n, n))
B = scipy.sparse.eye(n)
M = scipy.sparse.diags([numpy.reciprocal(vals)], [0], (n, n))
X = numpy.random.rand(n, m)
m_excluded = 3
Y = numpy.eye(n, m_excluded)
(eigs, vecs) = lobpcg(A, X, B, M=M, Y=Y, tol=0.0001, maxiter=40, largest=False)
tempResult = arange((1 + m_excluded), ((1 + m_excluded) + m))
	
===================================================================	
test_gmres_basic: 169	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 171	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 173	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
tempResult = arange(5)
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 175	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(numpy.arange(5)), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
tempResult = arange(5)
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 177	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(numpy.arange(5)), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
A = scipy.sparse.diags(((- 1j) * numpy.arange(5)), format='csr', dtype=complex)
B = numpy.ones(5, dtype=int)
tempResult = arange(5)
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 179	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(numpy.arange(5)), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
A = scipy.sparse.diags(((- 1j) * numpy.arange(5)), format='csr', dtype=complex)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(((- 1j) * numpy.arange(5))), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
tempResult = arange(5)
	
===================================================================	
TestExpmActionInterval.test_sparse_expm_multiply_interval_dtypes: 181	
----------------------------	

A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(numpy.arange(5)), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
A = scipy.sparse.diags(((- 1j) * numpy.arange(5)), format='csr', dtype=complex)
B = numpy.ones(5, dtype=int)
Aexpm = scipy.sparse.diags(numpy.exp(((- 1j) * numpy.arange(5))), format='csr')
assert_allclose(expm_multiply(A, B, 0, 1)[(- 1)], Aexpm.dot(B))
A = scipy.sparse.diags(numpy.arange(5), format='csr', dtype=int)
B = (1j * numpy.ones(5, dtype=complex))
tempResult = arange(5)
	
===================================================================	
test_attributes: 195	
----------------------------	

tempResult = arange(16)
	
===================================================================	
test_identity: 191	
----------------------------	

ident = scipy.sparse.linalg.interface.IdentityOperator((3, 3))
assert_equal((ident * [1, 2, 3]), [1, 2, 3])
tempResult = arange(9)
	
===================================================================	
test_identity: 191	
----------------------------	

ident = scipy.sparse.linalg.interface.IdentityOperator((3, 3))
assert_equal((ident * [1, 2, 3]), [1, 2, 3])
tempResult = arange(9)
	
===================================================================	
TestVsNumpyNorm: 63	
----------------------------	

_sparse_types = (scipy.sparse.bsr_matrix, scipy.sparse.coo_matrix, scipy.sparse.csc_matrix, scipy.sparse.csr_matrix, scipy.sparse.dia_matrix, scipy.sparse.dok_matrix, scipy.sparse.lil_matrix)
tempResult = arange(9)
	
===================================================================	
TestNorm.setUp: 14	
----------------------------	

tempResult = arange(9)
	
===================================================================	
_TestCommon.test_iterator: 1371	
----------------------------	

tempResult = arange(50)
	
===================================================================	
TestBSR.test_constructor4: 3176	
----------------------------	

n = 8
data = numpy.ones((n, n, 1), dtype=numpy.int8)
indptr = numpy.array([0, n], dtype=numpy.int32)
tempResult = arange(n, dtype=numpy.int32)
	
===================================================================	
_TestCommon.test_setdiag_comprehensive: 585	
----------------------------	


def dense_setdiag(a, v, k):
    v = numpy.asarray(v)
    if (k >= 0):
        n = min(a.shape[0], (a.shape[1] - k))
        if (v.ndim != 0):
            n = min(n, len(v))
            v = v[:n]
        i = numpy.arange(0, n)
        j = numpy.arange(k, (k + n))
        a[(i, j)] = v
    elif (k < 0):
        dense_setdiag(a.T, v, (- k))

def check_setdiag(a, b, k):
    for r in [(- 1), len(numpy.diag(a, k)), 2, 30]:
        if (r < 0):
            v = int(numpy.random.randint(1, 20, size=1))
        else:
            v = numpy.random.randint(1, 20, size=r)
        dense_setdiag(a, v, k)
        b.setdiag(v, k)
        d = numpy.diag(a, k)
        if (np.asarray(v).ndim == 0):
            assert_array_equal(d, v, err_msg=('%s %d' % (msg, r)))
        else:
            n = min(len(d), len(v))
            assert_array_equal(d[:n], v[:n], err_msg=('%s %d' % (msg, r)))
        assert_array_equal(b.A, a, err_msg=('%s %d' % (msg, r)))
numpy.random.seed(1234)
shapes = [(0, 5), (5, 0), (1, 5), (5, 1), (5, 5)]
for dtype in [numpy.int8, numpy.float64]:
    for (m, n) in shapes:
        tempResult = arange(((- m) + 1), (n - 1))
	
===================================================================	
_TestCommon.check11111111111111111111111: 1440	
----------------------------	

if (not HAS_NUMPY_UFUNC):
    if (name == 'sign'):
        raise nose.SkipTest('sign conflicts with comparison op support on Numpy without __numpy_ufunc__')
    if (self.spmatrix in (dok_matrix, lil_matrix)):
        raise nose.SkipTest('Unary ops not implemented for dok/lil with Numpy without __numpy_ufunc__')
ufunc = getattr(np, name)
tempResult = arange(20)
	
===================================================================	
_TestFancyIndexingAssign.test_fancy_indexing_set: 2165	
----------------------------	

(n, m) = (5, 10)

def _test_set_slice(i, j):
    A = self.spmatrix((n, m))
    with check_remains_sorted(A):
        A[(i, j)] = 1
    B = asmatrix(numpy.zeros((n, m)))
    B[(i, j)] = 1
    assert_array_almost_equal(A.todense(), B)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', category=SparseEfficiencyWarning)
    tempResult = arange(3)
	
===================================================================	
_TestFancyIndexingAssign.test_fancy_indexing_set: 2167	
----------------------------	

(n, m) = (5, 10)

def _test_set_slice(i, j):
    A = self.spmatrix((n, m))
    with check_remains_sorted(A):
        A[(i, j)] = 1
    B = asmatrix(numpy.zeros((n, m)))
    B[(i, j)] = 1
    assert_array_almost_equal(A.todense(), B)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', category=SparseEfficiencyWarning)
    for (i, j) in [((2, 3, 4), slice(None, 10, 4)), (numpy.arange(3), slice(5, (- 2))), (slice(2, 5), slice(5, (- 2)))]:
        _test_set_slice(i, j)
    tempResult = arange(3)
	
===================================================================	
_TestFancyIndexingAssign.test_fancy_indexing_set: 2167	
----------------------------	

(n, m) = (5, 10)

def _test_set_slice(i, j):
    A = self.spmatrix((n, m))
    with check_remains_sorted(A):
        A[(i, j)] = 1
    B = asmatrix(numpy.zeros((n, m)))
    B[(i, j)] = 1
    assert_array_almost_equal(A.todense(), B)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', category=SparseEfficiencyWarning)
    for (i, j) in [((2, 3, 4), slice(None, 10, 4)), (numpy.arange(3), slice(5, (- 2))), (slice(2, 5), slice(5, (- 2)))]:
        _test_set_slice(i, j)
    tempResult = arange(3)
	
===================================================================	
_TestMinMax.test_minmax_axis: 2428	
----------------------------	

tempResult = arange(50)
	
===================================================================	
_TestMinMax.test_minmax_axis: 2438	
----------------------------	

D = numpy.matrix(np.arange(50).reshape(5, 10))
D[1, :] = 0
D[:, 9] = 0
D[(3, 3)] = 0
D[(2, 2)] = (- 1)
X = self.spmatrix(D)
axes = [(- 2), (- 1), 0, 1]
for axis in axes:
    assert_array_equal(X.max(axis=axis).A, D.max(axis=axis).A)
    assert_array_equal(X.min(axis=axis).A, D.min(axis=axis).A)
tempResult = arange(1, 51)
	
===================================================================	
TestCSR.test_ufuncs: 2637	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestCSC.test_ufuncs: 2801	
----------------------------	

tempResult = arange(21)
	
===================================================================	
_TestMinMax.test_minmax: 2394	
----------------------------	

for dtype in [numpy.float32, numpy.float64, numpy.int32, numpy.int64, numpy.complex128]:
    tempResult = arange(20, dtype=dtype)
	
===================================================================	
_TestMinMax.test_minmax: 2408	
----------------------------	

for dtype in [numpy.float32, numpy.float64, numpy.int32, numpy.int64, numpy.complex128]:
    D = np.arange(20, dtype=dtype).reshape(5, 4)
    X = self.spmatrix(D)
    assert_equal(X.min(), 0)
    assert_equal(X.max(), 19)
    assert_equal(X.min().dtype, dtype)
    assert_equal(X.max().dtype, dtype)
    D *= (- 1)
    X = self.spmatrix(D)
    assert_equal(X.min(), (- 19))
    assert_equal(X.max(), 0)
    D += 5
    X = self.spmatrix(D)
    assert_equal(X.min(), (- 14))
    assert_equal(X.max(), 5)
tempResult = arange(1, 10)
	
===================================================================	
_TestMinMax.test_minmax: 2417	
----------------------------	

for dtype in [numpy.float32, numpy.float64, numpy.int32, numpy.int64, numpy.complex128]:
    D = np.arange(20, dtype=dtype).reshape(5, 4)
    X = self.spmatrix(D)
    assert_equal(X.min(), 0)
    assert_equal(X.max(), 19)
    assert_equal(X.min().dtype, dtype)
    assert_equal(X.max().dtype, dtype)
    D *= (- 1)
    X = self.spmatrix(D)
    assert_equal(X.min(), (- 19))
    assert_equal(X.max(), 0)
    D += 5
    X = self.spmatrix(D)
    assert_equal(X.min(), (- 14))
    assert_equal(X.max(), 5)
X = self.spmatrix(np.arange(1, 10).reshape(3, 3))
assert_equal(X.min(), 1)
assert_equal(X.min().dtype, X.dtype)
X = (- X)
assert_equal(X.max(), (- 1))
Z = self.spmatrix(numpy.zeros(1))
assert_equal(Z.min(), 0)
assert_equal(Z.max(), 0)
assert_equal(Z.max().dtype, Z.dtype)
tempResult = arange(20, dtype=float)
	
===================================================================	
_TestCommon.dense_setdiag: 560	
----------------------------	

v = numpy.asarray(v)
if (k >= 0):
    n = min(a.shape[0], (a.shape[1] - k))
    if (v.ndim != 0):
        n = min(n, len(v))
        v = v[:n]
    tempResult = arange(0, n)
	
===================================================================	
_TestCommon.dense_setdiag: 561	
----------------------------	

v = numpy.asarray(v)
if (k >= 0):
    n = min(a.shape[0], (a.shape[1] - k))
    if (v.ndim != 0):
        n = min(n, len(v))
        v = v[:n]
    i = numpy.arange(0, n)
    tempResult = arange(k, (k + n))
	
===================================================================	
TestConstructUtils.test_diags_vs_diag: 126	
----------------------------	

numpy.random.seed(1234)
for n_diags in [1, 2, 3, 4, 5, 10]:
    n = ((1 + (n_diags // 2)) + numpy.random.randint(0, 10))
    tempResult = arange(((- n) + 1), (n - 1))
	
===================================================================	
TestInt32Overflow.check1: 126	
----------------------------	

n = self.n
data = numpy.ones((n, n, 1), dtype=numpy.int8)
indptr = numpy.array([0, n], dtype=numpy.int32)
tempResult = arange(n, dtype=numpy.int32)
	
===================================================================	
test_csr_matmat_int64_overflow: 178	
----------------------------	

n = 3037000500
assert ((n ** 2) > np.iinfo(np.int64).max)
check_free_memory((((n * ((8 * 2) + 1)) * 3) / 1000000.0))
data = numpy.ones((n,), dtype=numpy.int8)
tempResult = arange((n + 1), dtype=numpy.int64)
	
===================================================================	
test_endianness: 213	
----------------------------	

d = numpy.ones((3, 4))
offsets = [(- 1), 0, 1]
a = dia_matrix((d.astype('<f8'), offsets), (4, 4))
b = dia_matrix((d.astype('>f8'), offsets), (4, 4))
tempResult = arange(4)
	
===================================================================	
TestInt32Overflow.test_dia_matvec: 95	
----------------------------	

n = self.n
data = numpy.ones((n, n), dtype=numpy.int8)
tempResult = arange(n)
	
===================================================================	
TestSparseUtils.test_ismatrix: 62	
----------------------------	

assert_equal(scipy.sparse.sputils.ismatrix(((),)), True)
assert_equal(scipy.sparse.sputils.ismatrix([[1], [2]]), True)
tempResult = arange(3)
	
===================================================================	
TestSparseUtils.test_ismatrix: 64	
----------------------------	

assert_equal(scipy.sparse.sputils.ismatrix(((),)), True)
assert_equal(scipy.sparse.sputils.ismatrix([[1], [2]]), True)
assert_equal(scipy.sparse.sputils.ismatrix(numpy.arange(3)[None]), True)
assert_equal(scipy.sparse.sputils.ismatrix([1, 2]), False)
tempResult = arange(3)
	
===================================================================	
KDTree.count_neighbors: 449	
----------------------------	

'\n        Count how many nearby pairs can be formed.\n\n        Count the number of pairs (x1,x2) can be formed, with x1 drawn\n        from self and x2 drawn from `other`, and where\n        ``distance(x1, x2, p) <= r``.\n        This is the "two-point correlation" described in Gray and Moore 2000,\n        "N-body problems in statistical learning", and the code here is based\n        on their algorithm.\n\n        Parameters\n        ----------\n        other : KDTree instance\n            The other tree to draw points from.\n        r : float or one-dimensional array of floats\n            The radius to produce a count for. Multiple radii are searched with\n            a single tree traversal.\n        p : float, 1<=p<=infinity, optional\n            Which Minkowski p-norm to use\n\n        Returns\n        -------\n        result : int or 1-D array of ints\n            The number of pairs. Note that this is internally stored in a numpy\n            int, and so may overflow if very large (2e9).\n\n        '

def traverse(node1, rect1, node2, rect2, idx):
    min_r = rect1.min_distance_rectangle(rect2, p)
    max_r = rect1.max_distance_rectangle(rect2, p)
    c_greater = (r[idx] > max_r)
    result[idx[c_greater]] += (node1.children * node2.children)
    idx = idx[((min_r <= r[idx]) & (r[idx] <= max_r))]
    if (len(idx) == 0):
        return
    if isinstance(node1, KDTree.leafnode):
        if isinstance(node2, KDTree.leafnode):
            ds = minkowski_distance(self.data[node1.idx][:, np.newaxis, :], other.data[node2.idx][np.newaxis, :, :], p).ravel()
            ds.sort()
            result[idx] += numpy.searchsorted(ds, r[idx], side='right')
        else:
            (less, greater) = rect2.split(node2.split_dim, node2.split)
            traverse(node1, rect1, node2.less, less, idx)
            traverse(node1, rect1, node2.greater, greater, idx)
    elif isinstance(node2, KDTree.leafnode):
        (less, greater) = rect1.split(node1.split_dim, node1.split)
        traverse(node1.less, less, node2, rect2, idx)
        traverse(node1.greater, greater, node2, rect2, idx)
    else:
        (less1, greater1) = rect1.split(node1.split_dim, node1.split)
        (less2, greater2) = rect2.split(node2.split_dim, node2.split)
        traverse(node1.less, less1, node2.less, less2, idx)
        traverse(node1.less, less1, node2.greater, greater2, idx)
        traverse(node1.greater, greater1, node2.less, less2, idx)
        traverse(node1.greater, greater1, node2.greater, greater2, idx)
R1 = Rectangle(self.maxes, self.mins)
R2 = Rectangle(other.maxes, other.mins)
if (numpy.shape(r) == ()):
    r = numpy.array([r])
    result = numpy.zeros(1, dtype=int)
    tempResult = arange(1)
	
===================================================================	
KDTree.count_neighbors: 455	
----------------------------	

'\n        Count how many nearby pairs can be formed.\n\n        Count the number of pairs (x1,x2) can be formed, with x1 drawn\n        from self and x2 drawn from `other`, and where\n        ``distance(x1, x2, p) <= r``.\n        This is the "two-point correlation" described in Gray and Moore 2000,\n        "N-body problems in statistical learning", and the code here is based\n        on their algorithm.\n\n        Parameters\n        ----------\n        other : KDTree instance\n            The other tree to draw points from.\n        r : float or one-dimensional array of floats\n            The radius to produce a count for. Multiple radii are searched with\n            a single tree traversal.\n        p : float, 1<=p<=infinity, optional\n            Which Minkowski p-norm to use\n\n        Returns\n        -------\n        result : int or 1-D array of ints\n            The number of pairs. Note that this is internally stored in a numpy\n            int, and so may overflow if very large (2e9).\n\n        '

def traverse(node1, rect1, node2, rect2, idx):
    min_r = rect1.min_distance_rectangle(rect2, p)
    max_r = rect1.max_distance_rectangle(rect2, p)
    c_greater = (r[idx] > max_r)
    result[idx[c_greater]] += (node1.children * node2.children)
    idx = idx[((min_r <= r[idx]) & (r[idx] <= max_r))]
    if (len(idx) == 0):
        return
    if isinstance(node1, KDTree.leafnode):
        if isinstance(node2, KDTree.leafnode):
            ds = minkowski_distance(self.data[node1.idx][:, np.newaxis, :], other.data[node2.idx][np.newaxis, :, :], p).ravel()
            ds.sort()
            result[idx] += numpy.searchsorted(ds, r[idx], side='right')
        else:
            (less, greater) = rect2.split(node2.split_dim, node2.split)
            traverse(node1, rect1, node2.less, less, idx)
            traverse(node1, rect1, node2.greater, greater, idx)
    elif isinstance(node2, KDTree.leafnode):
        (less, greater) = rect1.split(node1.split_dim, node1.split)
        traverse(node1.less, less, node2, rect2, idx)
        traverse(node1.greater, greater, node2, rect2, idx)
    else:
        (less1, greater1) = rect1.split(node1.split_dim, node1.split)
        (less2, greater2) = rect2.split(node2.split_dim, node2.split)
        traverse(node1.less, less1, node2.less, less2, idx)
        traverse(node1.less, less1, node2.greater, greater2, idx)
        traverse(node1.greater, greater1, node2.less, less2, idx)
        traverse(node1.greater, greater1, node2.greater, greater2, idx)
R1 = Rectangle(self.maxes, self.mins)
R2 = Rectangle(other.maxes, other.mins)
if (numpy.shape(r) == ()):
    r = numpy.array([r])
    result = numpy.zeros(1, dtype=int)
    traverse(self.tree, R1, other.tree, R2, numpy.arange(1))
    return result[0]
elif (len(numpy.shape(r)) == 1):
    r = numpy.asarray(r)
    (n,) = r.shape
    result = numpy.zeros(n, dtype=int)
    tempResult = arange(n)
	
===================================================================	
KDTree.__init__: 82	
----------------------------	

self.data = numpy.asarray(data)
(self.n, self.m) = numpy.shape(self.data)
self.leafsize = int(leafsize)
if (self.leafsize < 1):
    raise ValueError('leafsize must be at least 1')
self.maxes = numpy.amax(self.data, axis=0)
self.mins = numpy.amin(self.data, axis=0)
tempResult = arange(self.n)
	
===================================================================	
KDTree.__build: 143	
----------------------------	

if (len(idx) <= self.leafsize):
    return KDTree.leafnode(idx)
else:
    data = self.data[idx]
    d = numpy.argmax((maxes - mins))
    maxval = maxes[d]
    minval = mins[d]
    if (maxval == minval):
        return KDTree.leafnode(idx)
    data = data[:, d]
    split = ((maxval + minval) / 2)
    less_idx = numpy.nonzero((data <= split))[0]
    greater_idx = numpy.nonzero((data > split))[0]
    if (len(less_idx) == 0):
        split = numpy.amin(data)
        less_idx = numpy.nonzero((data <= split))[0]
        greater_idx = numpy.nonzero((data > split))[0]
    if (len(greater_idx) == 0):
        split = numpy.amax(data)
        less_idx = numpy.nonzero((data < split))[0]
        greater_idx = numpy.nonzero((data >= split))[0]
    if (len(less_idx) == 0):
        if (not numpy.all((data == data[0]))):
            raise ValueError(('Troublesome data array: %s' % data))
        split = data[0]
        tempResult = arange((len(data) - 1))
	
===================================================================	
SphericalVoronoi._calc_vertices_regions: 58	
----------------------------	

'\n        Calculates the Voronoi vertices and regions of the generators stored\n        in self.points. The vertices will be stored in self.vertices and the\n        regions in self.regions.\n\n        This algorithm was discussed at PyData London 2015 by\n        Tyler Reddy, Ross Hemsley and Nikolai Nowaczyk\n        '
self._tri = scipy.spatial.ConvexHull(self.points)
tetrahedrons = self._tri.points[self._tri.simplices]
tetrahedrons = numpy.insert(tetrahedrons, 3, numpy.array([self.center]), axis=1)
circumcenters = calc_circumcenters(tetrahedrons)
self.vertices = project_to_sphere(circumcenters, self.center, self.radius)
tempResult = arange(self._tri.simplices.shape[0])
	
===================================================================	
test__validate_vector: 1292	
----------------------------	

x = [1, 2, 3]
y = _validate_vector(x)
assert_array_equal(y, x)
y = _validate_vector(x, dtype=numpy.float64)
assert_array_equal(y, x)
assert_equal(y.dtype, numpy.float64)
x = [1]
y = _validate_vector(x)
assert_equal(y.ndim, 1)
assert_equal(y, x)
x = 1
y = _validate_vector(x)
assert_equal(y.ndim, 1)
assert_equal(y, [x])
tempResult = arange(5)
	
===================================================================	
TestCdist.test_cdist_wminkowski_int_weights: 81	
----------------------------	

eps = 1e-07
X1 = eo['cdist-X1']
X2 = eo['cdist-X2']
tempResult = arange(X1.shape[1])
	
===================================================================	
TestPdist.test_pdist_wminkowski_int_weights: 510	
----------------------------	

x = numpy.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 1.0, 1.0]])
tempResult = arange(3)
	
===================================================================	
test_euclideans: 1198	
----------------------------	

x1 = numpy.array([1, 1, 1])
x2 = numpy.array([0, 0, 0])
assert_almost_equal(sqeuclidean(x1, x2), 3.0, decimal=14)
assert_almost_equal(euclidean(x1, x2), numpy.sqrt(3), decimal=14)
assert_almost_equal(euclidean(x1[numpy.newaxis, :], x2[numpy.newaxis, :]), numpy.sqrt(3), decimal=14)
assert_almost_equal(sqeuclidean(x1[numpy.newaxis, :], x2[numpy.newaxis, :]), 3.0, decimal=14)
assert_almost_equal(sqeuclidean(x1[:, numpy.newaxis], x2[:, numpy.newaxis]), 3.0, decimal=14)
tempResult = arange(4)
	
===================================================================	
test_ckdtree_count_neighbous_multiple_r: 1028	
----------------------------	

n = 2000
m = 2
numpy.random.seed(1234)
data = numpy.random.normal(size=(n, m))
kdtree = cKDTree(data, leafsize=1)
r0 = [0, 0.01, 0.01, 0.02, 0.05]
tempResult = arange(len(r0))
	
===================================================================	
simulate_periodic_box: 948	
----------------------------	

dd = []
ii = []
tempResult = arange((3 ** data.shape[1]))
	
===================================================================	
TestDelaunay.test_nd_simplex: 268	
----------------------------	

for nd in xrange(2, 8):
    points = numpy.zeros(((nd + 1), nd))
    for j in xrange(nd):
        points[(j, j)] = 1.0
    points[(- 1), :] = 1.0
    tri = scipy.spatial.qhull.Delaunay(points)
    tri.vertices.sort()
    tempResult = arange((nd + 1), dtype=int)
	
===================================================================	
TestDelaunay.check: 329	
----------------------------	

(chunks, opts) = INCREMENTAL_DATASETS[name]
points = numpy.concatenate(chunks, axis=0)
obj = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
for chunk in chunks[1:]:
    obj.add_points(chunk)
obj2 = scipy.spatial.qhull.Delaunay(points)
obj3 = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
if (len(chunks) > 1):
    obj3.add_points(numpy.concatenate(chunks[1:], axis=0), restart=True)
if name.startswith('pathological'):
    tempResult = arange(points.shape[0])
	
===================================================================	
TestDelaunay.check: 330	
----------------------------	

(chunks, opts) = INCREMENTAL_DATASETS[name]
points = numpy.concatenate(chunks, axis=0)
obj = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
for chunk in chunks[1:]:
    obj.add_points(chunk)
obj2 = scipy.spatial.qhull.Delaunay(points)
obj3 = scipy.spatial.qhull.Delaunay(chunks[0], incremental=True, qhull_options=opts)
if (len(chunks) > 1):
    obj3.add_points(numpy.concatenate(chunks[1:], axis=0), restart=True)
if name.startswith('pathological'):
    assert_array_equal(numpy.unique(obj.simplices.ravel()), numpy.arange(points.shape[0]))
    tempResult = arange(points.shape[0])
	
===================================================================	
TestDelaunay.test_joggle: 299	
----------------------------	

points = numpy.random.rand(10, 2)
points = numpy.r_[(points, points)]
tri = scipy.spatial.qhull.Delaunay(points, qhull_options='QJ Qbb Pp')
tempResult = arange(len(points))
	
===================================================================	
_gen_roots_and_weights: 52	
----------------------------	

'[x,w] = gen_roots_and_weights(n,an_func,sqrt_bn_func,mu)\n\n    Returns the roots (x) of an nth order orthogonal polynomial,\n    and weights (w) to use in appropriate Gaussian quadrature with that\n    orthogonal polynomial.\n\n    The polynomials have the recurrence relation\n          P_n+1(x) = (x - A_n) P_n(x) - B_n P_n-1(x)\n\n    an_func(n)          should return A_n\n    sqrt_bn_func(n)     should return sqrt(B_n)\n    mu ( = h_0 )        is the integral of the weight over the orthogonal\n                        interval\n    '
tempResult = arange(n, dtype='d')
	
===================================================================	
roots_chebyt: 429	
----------------------------	

'Gauss-Chebyshev (first kind) quadrature.\n\n    Computes the sample points and weights for Gauss-Chebyshev quadrature.\n    The sample points are the roots of the n-th degree Chebyshev polynomial of\n    the first kind, :math:`T_n(x)`.  These sample points and weights correctly\n    integrate polynomials of degree :math:`2n - 1` or less over the interval\n    :math:`[-1, 1]` with weight function :math:`f(x) = 1/\\sqrt{1 - x^2}`.\n\n    Parameters\n    ----------\n    n : int\n        quadrature order\n    mu : bool, optional\n        If True, return the sum of the weights, optional.\n\n    Returns\n    -------\n    x : ndarray\n        Sample points\n    w : ndarray\n        Weights\n    mu : float\n        Sum of the weights\n\n    See Also\n    --------\n    scipy.integrate.quadrature\n    scipy.integrate.fixed_quad\n    numpy.polynomial.chebyshev.chebgauss\n    '
m = int(n)
if ((n < 1) or (n != m)):
    raise ValueError('n must be a positive integer.')
tempResult = arange(((2 * m) - 1), 0, (- 2))
	
===================================================================	
roots_chebyu: 456	
----------------------------	

'Gauss-Chebyshev (second kind) quadrature.\n\n    Computes the sample points and weights for Gauss-Chebyshev quadrature.\n    The sample points are the roots of the n-th degree Chebyshev polynomial of\n    the second kind, :math:`U_n(x)`.  These sample points and weights correctly\n    integrate polynomials of degree :math:`2n - 1` or less over the interval\n    :math:`[-1, 1]` with weight function :math:`f(x) = \\sqrt{1 - x^2}`.\n\n    Parameters\n    ----------\n    n : int\n        quadrature order\n    mu : bool, optional\n        If True, return the sum of the weights, optional.\n\n    Returns\n    -------\n    x : ndarray\n        Sample points\n    w : ndarray\n        Weights\n    mu : float\n        Sum of the weights\n\n    See Also\n    --------\n    scipy.integrate.quadrature\n    scipy.integrate.fixed_quad\n    '
m = int(n)
if ((n < 1) or (n != m)):
    raise ValueError('n must be a positive integer.')
tempResult = arange(m, 0, (- 1))
	
===================================================================	
IntArg.values: 85	
----------------------------	

v1 = Arg(self.a, self.b).values(max((1 + (n // 2)), (n - 5))).astype(int)
tempResult = arange((- 5), 5)
	
===================================================================	
TestCephes.test_binom_2: 38	
----------------------------	

numpy.random.seed(1234)
n = numpy.r_[numpy.logspace(1, 300, 20)]
tempResult = arange(0, 102)
	
===================================================================	
TestCephes.test_diric: 185	
----------------------------	

n_odd = [1, 5, 25]
x = np.array(((2 * np.pi) + 5e-05)).astype(numpy.float32)
assert_almost_equal(scipy.special.diric(x, n_odd), 1.0, decimal=7)
x = np.array(((2 * np.pi) + 1e-09)).astype(numpy.float64)
assert_almost_equal(scipy.special.diric(x, n_odd), 1.0, decimal=15)
x = np.array(((2 * np.pi) + 1e-15)).astype(numpy.float64)
assert_almost_equal(scipy.special.diric(x, n_odd), 1.0, decimal=15)
if hasattr(np, 'float128'):
    x = np.array(((2 * np.pi) + 1e-12)).astype(numpy.float128)
    assert_almost_equal(scipy.special.diric(x, n_odd), 1.0, decimal=19)
n_even = [2, 4, 24]
x = np.array(((2 * np.pi) + 1e-09)).astype(numpy.float64)
assert_almost_equal(scipy.special.diric(x, n_even), (- 1.0), decimal=15)
tempResult = arange((0.2 * numpy.pi), (1.0 * numpy.pi), (0.2 * numpy.pi))
	
===================================================================	
TestFactorialFunctions.test_factorial: 1408	
----------------------------	

assert_array_almost_equal(scipy.special.factorial(0), 1)
assert_array_almost_equal(scipy.special.factorial(1), 1)
assert_array_almost_equal(scipy.special.factorial(2), 2)
assert_array_almost_equal([6.0, 24.0, 120.0], scipy.special.factorial([3, 4, 5], exact=False))
assert_array_almost_equal(scipy.special.factorial([[5, 3], [4, 3]]), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(0, exact=True), 1)
assert_equal(scipy.special.factorial(1, exact=True), 1)
assert_equal(scipy.special.factorial(2, exact=True), 2)
assert_equal(scipy.special.factorial(5, exact=True), 120)
assert_equal(scipy.special.factorial(15, exact=True), 1307674368000)
assert_equal(scipy.special.factorial([7, 4, 15, 10], exact=True), [5040, 24, 1307674368000, 3628800])
assert_equal(scipy.special.factorial([[5, 3], [4, 3]], True), [[120, 6], [24, 6]])
tempResult = arange((- 3), 22)
	
===================================================================	
TestFactorialFunctions.test_factorial: 1408	
----------------------------	

assert_array_almost_equal(scipy.special.factorial(0), 1)
assert_array_almost_equal(scipy.special.factorial(1), 1)
assert_array_almost_equal(scipy.special.factorial(2), 2)
assert_array_almost_equal([6.0, 24.0, 120.0], scipy.special.factorial([3, 4, 5], exact=False))
assert_array_almost_equal(scipy.special.factorial([[5, 3], [4, 3]]), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(0, exact=True), 1)
assert_equal(scipy.special.factorial(1, exact=True), 1)
assert_equal(scipy.special.factorial(2, exact=True), 2)
assert_equal(scipy.special.factorial(5, exact=True), 120)
assert_equal(scipy.special.factorial(15, exact=True), 1307674368000)
assert_equal(scipy.special.factorial([7, 4, 15, 10], exact=True), [5040, 24, 1307674368000, 3628800])
assert_equal(scipy.special.factorial([[5, 3], [4, 3]], True), [[120, 6], [24, 6]])
tempResult = arange((- 3), 22)
	
===================================================================	
TestFactorialFunctions.test_factorial: 1409	
----------------------------	

assert_array_almost_equal(scipy.special.factorial(0), 1)
assert_array_almost_equal(scipy.special.factorial(1), 1)
assert_array_almost_equal(scipy.special.factorial(2), 2)
assert_array_almost_equal([6.0, 24.0, 120.0], scipy.special.factorial([3, 4, 5], exact=False))
assert_array_almost_equal(scipy.special.factorial([[5, 3], [4, 3]]), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(0, exact=True), 1)
assert_equal(scipy.special.factorial(1, exact=True), 1)
assert_equal(scipy.special.factorial(2, exact=True), 2)
assert_equal(scipy.special.factorial(5, exact=True), 120)
assert_equal(scipy.special.factorial(15, exact=True), 1307674368000)
assert_equal(scipy.special.factorial([7, 4, 15, 10], exact=True), [5040, 24, 1307674368000, 3628800])
assert_equal(scipy.special.factorial([[5, 3], [4, 3]], True), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(numpy.arange((- 3), 22), True), scipy.special.factorial(numpy.arange((- 3), 22), False))
tempResult = arange((- 3), 15)
	
===================================================================	
TestFactorialFunctions.test_factorial: 1409	
----------------------------	

assert_array_almost_equal(scipy.special.factorial(0), 1)
assert_array_almost_equal(scipy.special.factorial(1), 1)
assert_array_almost_equal(scipy.special.factorial(2), 2)
assert_array_almost_equal([6.0, 24.0, 120.0], scipy.special.factorial([3, 4, 5], exact=False))
assert_array_almost_equal(scipy.special.factorial([[5, 3], [4, 3]]), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(0, exact=True), 1)
assert_equal(scipy.special.factorial(1, exact=True), 1)
assert_equal(scipy.special.factorial(2, exact=True), 2)
assert_equal(scipy.special.factorial(5, exact=True), 120)
assert_equal(scipy.special.factorial(15, exact=True), 1307674368000)
assert_equal(scipy.special.factorial([7, 4, 15, 10], exact=True), [5040, 24, 1307674368000, 3628800])
assert_equal(scipy.special.factorial([[5, 3], [4, 3]], True), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(numpy.arange((- 3), 22), True), scipy.special.factorial(numpy.arange((- 3), 22), False))
tempResult = arange((- 3), 15)
	
===================================================================	
TestFactorialFunctions.test_factorial: 1410	
----------------------------	

assert_array_almost_equal(scipy.special.factorial(0), 1)
assert_array_almost_equal(scipy.special.factorial(1), 1)
assert_array_almost_equal(scipy.special.factorial(2), 2)
assert_array_almost_equal([6.0, 24.0, 120.0], scipy.special.factorial([3, 4, 5], exact=False))
assert_array_almost_equal(scipy.special.factorial([[5, 3], [4, 3]]), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(0, exact=True), 1)
assert_equal(scipy.special.factorial(1, exact=True), 1)
assert_equal(scipy.special.factorial(2, exact=True), 2)
assert_equal(scipy.special.factorial(5, exact=True), 120)
assert_equal(scipy.special.factorial(15, exact=True), 1307674368000)
assert_equal(scipy.special.factorial([7, 4, 15, 10], exact=True), [5040, 24, 1307674368000, 3628800])
assert_equal(scipy.special.factorial([[5, 3], [4, 3]], True), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(numpy.arange((- 3), 22), True), scipy.special.factorial(numpy.arange((- 3), 22), False))
assert_equal(scipy.special.factorial(numpy.arange((- 3), 15), True), scipy.special.factorial(numpy.arange((- 3), 15), False))
tempResult = arange((- 3), 5)
	
===================================================================	
TestFactorialFunctions.test_factorial: 1410	
----------------------------	

assert_array_almost_equal(scipy.special.factorial(0), 1)
assert_array_almost_equal(scipy.special.factorial(1), 1)
assert_array_almost_equal(scipy.special.factorial(2), 2)
assert_array_almost_equal([6.0, 24.0, 120.0], scipy.special.factorial([3, 4, 5], exact=False))
assert_array_almost_equal(scipy.special.factorial([[5, 3], [4, 3]]), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(0, exact=True), 1)
assert_equal(scipy.special.factorial(1, exact=True), 1)
assert_equal(scipy.special.factorial(2, exact=True), 2)
assert_equal(scipy.special.factorial(5, exact=True), 120)
assert_equal(scipy.special.factorial(15, exact=True), 1307674368000)
assert_equal(scipy.special.factorial([7, 4, 15, 10], exact=True), [5040, 24, 1307674368000, 3628800])
assert_equal(scipy.special.factorial([[5, 3], [4, 3]], True), [[120, 6], [24, 6]])
assert_equal(scipy.special.factorial(numpy.arange((- 3), 22), True), scipy.special.factorial(numpy.arange((- 3), 22), False))
assert_equal(scipy.special.factorial(numpy.arange((- 3), 15), True), scipy.special.factorial(numpy.arange((- 3), 15), False))
tempResult = arange((- 3), 5)
	
===================================================================	
TestCephes.test_binom: 30	
----------------------------	

n = numpy.array([0.264, 4, 5.2, 17])
k = numpy.array([2, 0.4, 7, 3.3])
nk = np.array(np.broadcast_arrays(n[:, None], k[None, :])).reshape(2, (- 1)).T
rknown = numpy.array([[(- 0.097152), 0.9263051596159367, 0.01858423645695389, (- 0.007581020651518199)], [6, 2.0214389119675666, 0, 2.9827344527963846], [10.92, 2.22993515861399, (- 0.00585728), 10.468891352063146], [136, 3.5252179590758828, 19448, 1024.5526916174495]])
assert_func_equal(scipy.special._ufuncs.binom, rknown.ravel(), nk, rtol=1e-13)
numpy.random.seed(1234)
tempResult = arange((- 7), 30)
	
===================================================================	
TestCephes.test_binom: 31	
----------------------------	

n = numpy.array([0.264, 4, 5.2, 17])
k = numpy.array([2, 0.4, 7, 3.3])
nk = np.array(np.broadcast_arrays(n[:, None], k[None, :])).reshape(2, (- 1)).T
rknown = numpy.array([[(- 0.097152), 0.9263051596159367, 0.01858423645695389, (- 0.007581020651518199)], [6, 2.0214389119675666, 0, 2.9827344527963846], [10.92, 2.22993515861399, (- 0.00585728), 10.468891352063146], [136, 3.5252179590758828, 19448, 1024.5526916174495]])
assert_func_equal(scipy.special._ufuncs.binom, rknown.ravel(), nk, rtol=1e-13)
numpy.random.seed(1234)
n = numpy.r_[(numpy.arange((- 7), 30), ((1000 * numpy.random.rand(30)) - 500))]
tempResult = arange(0, 102)
	
===================================================================	
TestCephes.test_mathieu_cem: 503	
----------------------------	

assert_equal(scipy.special._ufuncs.mathieu_cem(1, 0, 0), (1.0, 0.0))

@numpy.vectorize
def ce_smallq(m, q, z):
    z *= (numpy.pi / 180)
    if (m == 0):
        return ((2 ** (- 0.5)) * (1 - ((0.5 * q) * cos((2 * z)))))
    elif (m == 1):
        return (cos(z) - ((q / 8) * cos((3 * z))))
    elif (m == 2):
        return (cos((2 * z)) - (q * ((cos((4 * z)) / 12) - (1 / 4))))
    else:
        return (cos((m * z)) - (q * ((cos(((m + 2) * z)) / (4 * (m + 1))) - (cos(((m - 2) * z)) / (4 * (m - 1))))))
tempResult = arange(0, 100)
	
===================================================================	
TestCephes.test_diric_broadcasting: 190	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestCephes.test_mathieu_modsem2: 541	
----------------------------	

scipy.special._ufuncs.mathieu_modsem2(1, 1, 1)
tempResult = arange(1, 4)
	
===================================================================	
TestCephes.test_mathieu_modcem2: 528	
----------------------------	

scipy.special._ufuncs.mathieu_modcem2(1, 1, 1)
tempResult = arange(0, 4)
	
===================================================================	
Get no callers of function numpy.arange at line 55 col 12.	
===================================================================	
Get no callers of function numpy.arange at line 56 col 12.	
===================================================================	
TestCephes.test_mathieu_sem: 519	
----------------------------	

assert_equal(scipy.special._ufuncs.mathieu_sem(1, 0, 0), (0.0, 1.0))

@numpy.vectorize
def se_smallq(m, q, z):
    z *= (numpy.pi / 180)
    if (m == 1):
        return (sin(z) - ((q / 8) * sin((3 * z))))
    elif (m == 2):
        return (sin((2 * z)) - ((q * sin((4 * z))) / 12))
    else:
        return (sin((m * z)) - (q * ((sin(((m + 2) * z)) / (4 * (m + 1))) - (sin(((m - 2) * z)) / (4 * (m - 1))))))
tempResult = arange(1, 100)
	
===================================================================	
test_logsumexp_b: 34	
----------------------------	

tempResult = arange(200)
	
===================================================================	
test_logsumexp_b: 35	
----------------------------	

a = numpy.arange(200)
tempResult = arange(200, 0, (- 1))
	
===================================================================	
test_logsumexp: 8	
----------------------------	

tempResult = arange(200)
	
===================================================================	
test_sinpi_zeros: 311	
----------------------------	

eps = np.finfo(float).eps
dx = numpy.r_[((- numpy.logspace(0, (- 13), 3)), 0, numpy.logspace((- 13), 0, 3))]
dy = dx.copy()
(dx, dy) = numpy.meshgrid(dx, dy)
dz = (dx + (1j * dy))
tempResult = arange((- 100), 100, 1)
	
===================================================================	
test_beta: 171	
----------------------------	

numpy.random.seed(1234)
tempResult = arange((- 10), 11, 1)
	
===================================================================	
test_beta: 171	
----------------------------	

numpy.random.seed(1234)
tempResult = arange((- 10), 11, 1)
	
===================================================================	
test_cospi_zeros: 326	
----------------------------	

eps = np.finfo(float).eps
dx = numpy.r_[((- numpy.logspace(0, (- 13), 3)), 0, numpy.logspace((- 13), 0, 3))]
dy = dx.copy()
(dx, dy) = numpy.meshgrid(dx, dy)
dz = (dx + (1j * dy))
tempResult = arange((- 100), 100, 1)
	
===================================================================	
test_rgamma_zeros: 216	
----------------------------	

dx = numpy.r_[((- numpy.logspace((- 1), (- 13), 3)), 0, numpy.logspace((- 13), (- 1), 3))]
dy = dx.copy()
(dx, dy) = numpy.meshgrid(dx, dy)
dz = (dx + (1j * dy))
tempResult = arange(0, (- 170), (- 1))
	
===================================================================	
verify_gauss_quad: 241	
----------------------------	

(x, w, mu) = root_func(N, True)
tempResult = arange(N)
	
===================================================================	
TestRecurrence.check_poly: 121	
----------------------------	

numpy.random.seed(1234)
dataset = []
tempResult = arange(nn)
	
===================================================================	
test_eval_chebyt: 9	
----------------------------	

tempResult = arange(0, 10000, 7)
	
===================================================================	
TestPolys.check_poly: 34	
----------------------------	

numpy.random.seed(1234)
dataset = []
tempResult = arange(nn)
	
===================================================================	
test_multigammaln_array_arg: 33	
----------------------------	

numpy.random.seed(1234)
tempResult = arange(10.0, 18.0)
	
===================================================================	
test_half_integer_real_part: 18	
----------------------------	

tempResult = arange((- 100), 101)
	
===================================================================	
test_integer_real_part: 8	
----------------------------	

tempResult = arange((- 100), 101)
	
===================================================================	
_calc_uniform_order_statistic_medians: 97	
----------------------------	

'\n    Approximations of uniform order statistic medians.\n\n    Parameters\n    ----------\n    n : int\n        Sample size.\n\n    Returns\n    -------\n    v : 1d float array\n        Approximations of the order statistic medians.\n\n    References\n    ----------\n    .. [1] James J. Filliben, "The Probability Plot Correlation Coefficient\n           Test for Normality", Technometrics, Vol. 17, pp. 111-117, 1975.\n\n    Examples\n    --------\n    Order statistics of the uniform distribution on the unit interval\n    are marginally distributed according to beta distributions.\n    The expectations of these order statistic are evenly spaced across\n    the interval, but the distributions are skewed in a way that\n    pushes the medians slightly towards the endpoints of the unit interval:\n\n    >>> n = 4\n    >>> k = np.arange(1, n+1)\n    >>> from scipy.stats import beta\n    >>> a = k\n    >>> b = n-k+1\n    >>> beta.mean(a, b)\n    array([ 0.2,  0.4,  0.6,  0.8])\n    >>> beta.median(a, b)\n    array([ 0.15910358,  0.38572757,  0.61427243,  0.84089642])\n\n    The Filliben approximation uses the exact medians of the smallest\n    and greatest order statistics, and the remaining medians are approximated\n    by points spread evenly across a sub-interval of the unit interval:\n\n    >>> from scipy.morestats import _calc_uniform_order_statistic_medians\n    >>> _calc_uniform_order_statistic_medians(n)\n    array([ 0.15910358,  0.38545246,  0.61454754,  0.84089642])\n\n    This plot shows the skewed distributions of the order statistics\n    of a sample of size four from a uniform distribution on the unit interval:\n\n    >>> import matplotlib.pyplot as plt\n    >>> x = np.linspace(0.0, 1.0, num=50, endpoint=True)\n    >>> pdfs = [beta.pdf(x, a[i], b[i]) for i in range(n)]\n    >>> plt.figure()\n    >>> plt.plot(x, pdfs[0], x, pdfs[1], x, pdfs[2], x, pdfs[3])\n\n    '
v = numpy.zeros(n, dtype=numpy.float64)
v[(- 1)] = (0.5 ** (1.0 / n))
v[0] = (1 - v[(- 1)])
tempResult = arange(2, n)
	
===================================================================	
binom_test: 602	
----------------------------	

"\n    Perform a test that the probability of success is p.\n\n    This is an exact, two-sided test of the null hypothesis\n    that the probability of success in a Bernoulli experiment\n    is `p`.\n\n    Parameters\n    ----------\n    x : integer or array_like\n        the number of successes, or if x has length 2, it is the\n        number of successes and the number of failures.\n    n : integer\n        the number of trials.  This is ignored if x gives both the\n        number of successes and failures\n    p : float, optional\n        The hypothesized probability of success.  0 <= p <= 1. The\n        default value is p = 0.5\n    alternative : {'two-sided', 'greater', 'less'}, optional\n        Indicates the alternative hypothesis. The default value is\n        'two-sided'.\n\n    Returns\n    -------\n    p-value : float\n        The p-value of the hypothesis test\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Binomial_test\n\n    "
x = atleast_1d(x).astype(numpy.integer)
if (len(x) == 2):
    n = (x[1] + x[0])
    x = x[0]
elif (len(x) == 1):
    x = x[0]
    if ((n is None) or (n < x)):
        raise ValueError('n must be >= x')
    n = numpy.int_(n)
else:
    raise ValueError('Incorrect length for x.')
if ((p > 1.0) or (p < 0.0)):
    raise ValueError('p must be in range [0,1]')
if (alternative not in ('two-sided', 'less', 'greater')):
    raise ValueError("alternative not recognized\nshould be 'two-sided', 'less' or 'greater'")
if (alternative == 'less'):
    pval = distributions.binom.cdf(x, n, p)
    return pval
if (alternative == 'greater'):
    pval = distributions.binom.sf((x - 1), n, p)
    return pval
d = distributions.binom.pmf(x, n, p)
rerr = (1 + 1e-07)
if (x == (p * n)):
    pval = 1.0
elif (x < (p * n)):
    tempResult = arange(numpy.ceil((p * n)), (n + 1))
	
===================================================================	
binom_test: 606	
----------------------------	

"\n    Perform a test that the probability of success is p.\n\n    This is an exact, two-sided test of the null hypothesis\n    that the probability of success in a Bernoulli experiment\n    is `p`.\n\n    Parameters\n    ----------\n    x : integer or array_like\n        the number of successes, or if x has length 2, it is the\n        number of successes and the number of failures.\n    n : integer\n        the number of trials.  This is ignored if x gives both the\n        number of successes and failures\n    p : float, optional\n        The hypothesized probability of success.  0 <= p <= 1. The\n        default value is p = 0.5\n    alternative : {'two-sided', 'greater', 'less'}, optional\n        Indicates the alternative hypothesis. The default value is\n        'two-sided'.\n\n    Returns\n    -------\n    p-value : float\n        The p-value of the hypothesis test\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Binomial_test\n\n    "
x = atleast_1d(x).astype(numpy.integer)
if (len(x) == 2):
    n = (x[1] + x[0])
    x = x[0]
elif (len(x) == 1):
    x = x[0]
    if ((n is None) or (n < x)):
        raise ValueError('n must be >= x')
    n = numpy.int_(n)
else:
    raise ValueError('Incorrect length for x.')
if ((p > 1.0) or (p < 0.0)):
    raise ValueError('p must be in range [0,1]')
if (alternative not in ('two-sided', 'less', 'greater')):
    raise ValueError("alternative not recognized\nshould be 'two-sided', 'less' or 'greater'")
if (alternative == 'less'):
    pval = distributions.binom.cdf(x, n, p)
    return pval
if (alternative == 'greater'):
    pval = distributions.binom.sf((x - 1), n, p)
    return pval
d = distributions.binom.pmf(x, n, p)
rerr = (1 + 1e-07)
if (x == (p * n)):
    pval = 1.0
elif (x < (p * n)):
    i = numpy.arange(numpy.ceil((p * n)), (n + 1))
    y = numpy.sum((distributions.binom.pmf(i, n, p) <= (d * rerr)), axis=0)
    pval = (distributions.binom.cdf(x, n, p) + distributions.binom.sf((n - y), n, p))
else:
    tempResult = arange((numpy.floor((p * n)) + 1))
	
===================================================================	
plotting_positions: 970	
----------------------------	

"\n    Returns plotting positions (or empirical percentile points) for the data.\n\n    Plotting positions are defined as ``(i-alpha)/(n+1-alpha-beta)``, where:\n        - i is the rank order statistics\n        - n is the number of unmasked values along the given axis\n        - `alpha` and `beta` are two parameters.\n\n    Typical values for `alpha` and `beta` are:\n        - (0,1)    : ``p(k) = k/n``, linear interpolation of cdf (R, type 4)\n        - (.5,.5)  : ``p(k) = (k-1/2.)/n``, piecewise linear function\n          (R, type 5)\n        - (0,0)    : ``p(k) = k/(n+1)``, Weibull (R type 6)\n        - (1,1)    : ``p(k) = (k-1)/(n-1)``, in this case,\n          ``p(k) = mode[F(x[k])]``. That's R default (R type 7)\n        - (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``, then\n          ``p(k) ~ median[F(x[k])]``.\n          The resulting quantile estimates are approximately median-unbiased\n          regardless of the distribution of x. (R type 8)\n        - (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``, Blom.\n          The resulting quantile estimates are approximately unbiased\n          if x is normally distributed (R type 9)\n        - (.4,.4)  : approximately quantile unbiased (Cunnane)\n        - (.35,.35): APL, used with PWM\n        - (.3175, .3175): used in scipy.stats.probplot\n\n    Parameters\n    ----------\n    data : array_like\n        Input data, as a sequence or array of dimension at most 2.\n    alpha : float, optional\n        Plotting positions parameter. Default is 0.4.\n    beta : float, optional\n        Plotting positions parameter. Default is 0.4.\n\n    Returns\n    -------\n    positions : MaskedArray\n        The calculated plotting positions.\n\n    "
data = ma.array(data, copy=False).reshape(1, (- 1))
n = data.count()
plpos = numpy.empty(data.size, dtype=float)
plpos[n:] = 0
tempResult = arange(1, (n + 1))
	
===================================================================	
sen_seasonal_slopes: 372	
----------------------------	

x = numpy.ma.array(x, subok=True, copy=False, ndmin=2)
(n, _) = x.shape
tempResult = arange(1, (n - i))
	
===================================================================	
_rank1d: 95	
----------------------------	

n = data.count()
rk = numpy.empty(data.size, dtype=float)
idx = data.argsort()
tempResult = arange(1, (n + 1))
	
===================================================================	
_hd_1D: 25	
----------------------------	

'Computes the HD quantiles for a 1D array. Returns nan for invalid data.'
xsorted = numpy.squeeze(numpy.sort(data.compressed().view(ndarray)))
n = xsorted.size
hd = numpy.empty((2, len(prob)), float_)
if (n < 2):
    hd.flat = numpy.nan
    if var:
        return hd
    return hd[0]
tempResult = arange((n + 1))
	
===================================================================	
_mjci_1D: 102	
----------------------------	

data = numpy.sort(data.compressed())
n = data.size
prob = ((np.array(p) * n) + 0.5).astype(int_)
betacdf = scipy.stats.distributions.beta.cdf
mj = numpy.empty(len(prob), float_)
tempResult = arange(1, (n + 1), dtype=float_)
	
===================================================================	
_hdsd_1D: 64	
----------------------------	

'Computes the std error for 1D arrays.'
xsorted = numpy.sort(data.compressed())
n = len(xsorted)
hdsd = numpy.empty(len(prob), float_)
if (n < 2):
    hdsd.flat = numpy.nan
tempResult = arange(n)
	
===================================================================	
rankdata: 1451	
----------------------------	

'\n    rankdata(a, method=\'average\')\n\n    Assign ranks to data, dealing with ties appropriately.\n\n    Ranks begin at 1.  The `method` argument controls how ranks are assigned\n    to equal values.  See [1]_ for further discussion of ranking methods.\n\n    Parameters\n    ----------\n    a : array_like\n        The array of values to be ranked.  The array is first flattened.\n    method : str, optional\n        The method used to assign ranks to tied elements.\n        The options are \'average\', \'min\', \'max\', \'dense\' and \'ordinal\'.\n\n        \'average\':\n            The average of the ranks that would have been assigned to\n            all the tied values is assigned to each value.\n        \'min\':\n            The minimum of the ranks that would have been assigned to all\n            the tied values is assigned to each value.  (This is also\n            referred to as "competition" ranking.)\n        \'max\':\n            The maximum of the ranks that would have been assigned to all\n            the tied values is assigned to each value.\n        \'dense\':\n            Like \'min\', but the rank of the next highest element is assigned\n            the rank immediately after those assigned to the tied elements.\n        \'ordinal\':\n            All values are given a distinct rank, corresponding to the order\n            that the values occur in `a`.\n\n        The default is \'average\'.\n\n    Returns\n    -------\n    ranks : ndarray\n         An array of length equal to the size of `a`, containing rank\n         scores.\n\n    References\n    ----------\n    .. [1] "Ranking", http://en.wikipedia.org/wiki/Ranking\n\n    Examples\n    --------\n    >>> from scipy.stats import rankdata\n    >>> rankdata([0, 2, 3, 2])\n    array([ 1. ,  2.5,  4. ,  2.5])\n    >>> rankdata([0, 2, 3, 2], method=\'min\')\n    array([ 1,  2,  4,  2])\n    >>> rankdata([0, 2, 3, 2], method=\'max\')\n    array([ 1,  3,  4,  3])\n    >>> rankdata([0, 2, 3, 2], method=\'dense\')\n    array([ 1,  2,  3,  2])\n    >>> rankdata([0, 2, 3, 2], method=\'ordinal\')\n    array([ 1,  2,  4,  3])\n    '
if (method not in ('average', 'min', 'max', 'dense', 'ordinal')):
    raise ValueError('unknown method "{0}"'.format(method))
arr = numpy.ravel(numpy.asarray(a))
algo = ('mergesort' if (method == 'ordinal') else 'quicksort')
sorter = numpy.argsort(arr, kind=algo)
inv = numpy.empty(sorter.size, dtype=numpy.intp)
tempResult = arange(sorter.size, dtype=numpy.intp)
	
===================================================================	
weightedtau: 979	
----------------------------	

'\n    Computes a weighted version of Kendall\'s :math:`\\tau`.\n\n    The weighted :math:`\\tau` is a weighted version of Kendall\'s\n    :math:`\\tau` in which exchanges of high weight are more influential than\n    exchanges of low weight. The default parameters compute the additive\n    hyperbolic version of the index, :math:`\\tau_\\mathrm h`, which has\n    been shown to provide the best balance between important and\n    unimportant elements [1]_.\n\n    The weighting is defined by means of a rank array, which assigns a\n    nonnegative rank to each element, and a weigher function, which\n    assigns a weight based from the rank to each element. The weight of an\n    exchange is then the sum or the product of the weights of the ranks of\n    the exchanged elements. The default parameters compute\n    :math:`\\tau_\\mathrm h`: an exchange between elements with rank\n    :math:`r` and :math:`s` (starting from zero) has weight\n    :math:`1/(r+1) + 1/(s+1)`.\n\n    Specifying a rank array is meaningful only if you have in mind an\n    external criterion of importance. If, as it usually happens, you do\n    not have in mind a specific rank, the weighted :math:`\\tau` is\n    defined by averaging the values obtained using the decreasing\n    lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the\n    behavior with default parameters.\n\n    Note that if you are computing the weighted :math:`\\tau` on arrays of\n    ranks, rather than of scores (i.e., a larger value implies a lower\n    rank) you must negate the ranks, so that elements of higher rank are\n    associated with a larger value.\n\n    Parameters\n    ----------\n    x, y : array_like\n        Arrays of scores, of the same shape. If arrays are not 1-D, they will\n        be flattened to 1-D.\n    rank: array_like of ints or bool, optional\n        A nonnegative rank assigned to each element. If it is None, the\n        decreasing lexicographical rank by (`x`, `y`) will be used: elements of\n        higher rank will be those with larger `x`-values, using `y`-values to\n        break ties (in particular, swapping `x` and `y` will give a different\n        result). If it is False, the element indices will be used\n        directly as ranks. The default is True, in which case this\n        function returns the average of the values obtained using the\n        decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).\n    weigher : callable, optional\n        The weigher function. Must map nonnegative integers (zero\n        representing the most important element) to a nonnegative weight.\n        The default, None, provides hyperbolic weighing, that is,\n        rank :math:`r` is mapped to weight :math:`1/(r+1)`.\n    additive : bool, optional\n        If True, the weight of an exchange is computed by adding the\n        weights of the ranks of the exchanged elements; otherwise, the weights\n        are multiplied. The default is True.\n\n    Returns\n    -------\n    correlation : float\n       The weighted :math:`\\tau` correlation index.\n    pvalue : float\n       Presently ``np.nan``, as the null statistics is unknown (even in the\n       additive hyperbolic case).\n\n    See also\n    --------\n    kendalltau : Calculates Kendall\'s tau.\n    spearmanr : Calculates a Spearman rank-order correlation coefficient.\n    theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n\n    Notes\n    -----\n    This function uses an :math:`O(n \\log n)`, mergesort-based algorithm\n    [1]_ that is a weighted extension of Knight\'s algorithm for Kendall\'s\n    :math:`\\tau` [2]_. It can compute Shieh\'s weighted :math:`\\tau` [3]_\n    between rankings without ties (i.e., permutations) by setting\n    `additive` and `rank` to False, as the definition given in [1]_ is a\n    generalization of Shieh\'s.\n\n    NaNs are considered the smallest possible score.\n\n    .. versionadded:: 0.19.0\n\n    References\n    ----------\n    .. [1] Sebastiano Vigna, "A weighted correlation index for rankings with\n           ties", Proceedings of the 24th international conference on World\n           Wide Web, pp. 1166-1176, ACM, 2015.\n    .. [2] W.R. Knight, "A Computer Method for Calculating Kendall\'s Tau with\n           Ungrouped Data", Journal of the American Statistical Association,\n           Vol. 61, No. 314, Part 1, pp. 436-439, 1966.\n    .. [3] Grace S. Shieh. "A weighted Kendall\'s tau statistic", Statistics &\n           Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> tau, p_value = stats.weightedtau(x, y)\n    >>> tau\n    -0.56694968153682723\n    >>> p_value\n    nan\n    >>> tau, p_value = stats.weightedtau(x, y, additive=False)\n    >>> tau\n    -0.62205716951801038\n\n    NaNs are considered the smallest possible score:\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, np.nan]\n    >>> tau, _ = stats.weightedtau(x, y)\n    >>> tau\n    -0.56694968153682723\n\n    This is exactly Kendall\'s tau:\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> tau, _ = stats.weightedtau(x, y, weigher=lambda x: 1)\n    >>> tau\n    -0.47140452079103173\n\n    >>> x = [12, 2, 1, 12, 2]\n    >>> y = [1, 4, 7, 1, 0]\n    >>> stats.weightedtau(x, y, rank=None)\n    WeightedTauResult(correlation=-0.4157652301037516, pvalue=nan)\n    >>> stats.weightedtau(y, x, rank=None)\n    WeightedTauResult(correlation=-0.71813413296990281, pvalue=nan)\n\n    '
x = np.asarray(x).ravel()
y = np.asarray(y).ravel()
if (x.size != y.size):
    raise ValueError(('All inputs to `weightedtau` must be of the same size, found x-size %s and y-size %s' % (x.size, y.size)))
if (not x.size):
    return WeightedTauResult(numpy.nan, numpy.nan)
if numpy.isnan(numpy.min(x)):
    x = _toint64(x)
if numpy.isnan(numpy.min(y)):
    y = _toint64(y)
if (x.dtype != y.dtype):
    if (x.dtype != numpy.int64):
        x = _toint64(x)
    if (y.dtype != numpy.int64):
        y = _toint64(y)
elif (x.dtype not in (numpy.int32, numpy.int64, numpy.float32, numpy.float64)):
    x = _toint64(x)
    y = _toint64(y)
if (rank is True):
    return WeightedTauResult(((_weightedrankedtau(x, y, None, weigher, additive) + _weightedrankedtau(y, x, None, weigher, additive)) / 2), numpy.nan)
if (rank is False):
    tempResult = arange(x.size, dtype=numpy.intp)
	
===================================================================	
kstest: 1120	
----------------------------	

'\n    Perform the Kolmogorov-Smirnov test for goodness of fit.\n\n    This performs a test of the distribution G(x) of an observed\n    random variable against a given distribution F(x). Under the null\n    hypothesis the two distributions are identical, G(x)=F(x). The\n    alternative hypothesis can be either \'two-sided\' (default), \'less\'\n    or \'greater\'. The KS test is only valid for continuous distributions.\n\n    Parameters\n    ----------\n    rvs : str, array or callable\n        If a string, it should be the name of a distribution in `scipy.stats`.\n        If an array, it should be a 1-D array of observations of random\n        variables.\n        If a callable, it should be a function to generate random variables;\n        it is required to have a keyword argument `size`.\n    cdf : str or callable\n        If a string, it should be the name of a distribution in `scipy.stats`.\n        If `rvs` is a string then `cdf` can be False or the same as `rvs`.\n        If a callable, that callable is used to calculate the cdf.\n    args : tuple, sequence, optional\n        Distribution parameters, used if `rvs` or `cdf` are strings.\n    N : int, optional\n        Sample size if `rvs` is string or callable.  Default is 20.\n    alternative : {\'two-sided\', \'less\',\'greater\'}, optional\n        Defines the alternative hypothesis (see explanation above).\n        Default is \'two-sided\'.\n    mode : \'approx\' (default) or \'asymp\', optional\n        Defines the distribution used for calculating the p-value.\n\n          - \'approx\' : use approximation to exact distribution of test statistic\n          - \'asymp\' : use asymptotic distribution of test statistic\n\n    Returns\n    -------\n    statistic : float\n        KS test statistic, either D, D+ or D-.\n    pvalue :  float\n        One-tailed or two-tailed p-value.\n\n    Notes\n    -----\n    In the one-sided test, the alternative is that the empirical\n    cumulative distribution function of the random variable is "less"\n    or "greater" than the cumulative distribution function F(x) of the\n    hypothesis, ``G(x)<=F(x)``, resp. ``G(x)>=F(x)``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n\n    >>> x = np.linspace(-15, 15, 9)\n    >>> stats.kstest(x, \'norm\')\n    (0.44435602715924361, 0.038850142705171065)\n\n    >>> np.random.seed(987654321) # set random seed to get the same result\n    >>> stats.kstest(\'norm\', False, N=100)\n    (0.058352892479417884, 0.88531190944151261)\n\n    The above lines are equivalent to:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.norm.rvs(size=100), \'norm\')\n    (0.058352892479417884, 0.88531190944151261)\n\n    *Test against one-sided alternative hypothesis*\n\n    Shift distribution to larger values, so that ``cdf_dgp(x) < norm.cdf(x)``:\n\n    >>> np.random.seed(987654321)\n    >>> x = stats.norm.rvs(loc=0.2, size=100)\n    >>> stats.kstest(x,\'norm\', alternative = \'less\')\n    (0.12464329735846891, 0.040989164077641749)\n\n    Reject equal distribution against alternative hypothesis: less\n\n    >>> stats.kstest(x,\'norm\', alternative = \'greater\')\n    (0.0072115233216311081, 0.98531158590396395)\n\n    Don\'t reject equal distribution against alternative hypothesis: greater\n\n    >>> stats.kstest(x,\'norm\', mode=\'asymp\')\n    (0.12464329735846891, 0.08944488871182088)\n\n    *Testing t distributed random variables against normal distribution*\n\n    With 100 degrees of freedom the t distribution looks close to the normal\n    distribution, and the K-S test does not reject the hypothesis that the\n    sample came from the normal distribution:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.t.rvs(100,size=100),\'norm\')\n    (0.072018929165471257, 0.67630062862479168)\n\n    With 3 degrees of freedom the t distribution looks sufficiently different\n    from the normal distribution, that we can reject the hypothesis that the\n    sample came from the normal distribution at the 10% level:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.t.rvs(3,size=100),\'norm\')\n    (0.131016895759829, 0.058826222555312224)\n\n    '
if isinstance(rvs, string_types):
    if ((not cdf) or (cdf == rvs)):
        cdf = getattr(distributions, rvs).cdf
        rvs = getattr(distributions, rvs).rvs
    else:
        raise AttributeError('if rvs is string, cdf has to be the same distribution')
if isinstance(cdf, string_types):
    cdf = getattr(distributions, cdf).cdf
if callable(rvs):
    kwds = {'size': N}
    vals = numpy.sort(rvs(*args, **kwds))
else:
    vals = numpy.sort(rvs)
    N = len(vals)
cdfvals = cdf(vals, *args)
if (alternative == 'two_sided'):
    alternative = 'two-sided'
if (alternative in ['two-sided', 'greater']):
    tempResult = arange(1.0, (N + 1))
	
===================================================================	
kstest: 1124	
----------------------------	

'\n    Perform the Kolmogorov-Smirnov test for goodness of fit.\n\n    This performs a test of the distribution G(x) of an observed\n    random variable against a given distribution F(x). Under the null\n    hypothesis the two distributions are identical, G(x)=F(x). The\n    alternative hypothesis can be either \'two-sided\' (default), \'less\'\n    or \'greater\'. The KS test is only valid for continuous distributions.\n\n    Parameters\n    ----------\n    rvs : str, array or callable\n        If a string, it should be the name of a distribution in `scipy.stats`.\n        If an array, it should be a 1-D array of observations of random\n        variables.\n        If a callable, it should be a function to generate random variables;\n        it is required to have a keyword argument `size`.\n    cdf : str or callable\n        If a string, it should be the name of a distribution in `scipy.stats`.\n        If `rvs` is a string then `cdf` can be False or the same as `rvs`.\n        If a callable, that callable is used to calculate the cdf.\n    args : tuple, sequence, optional\n        Distribution parameters, used if `rvs` or `cdf` are strings.\n    N : int, optional\n        Sample size if `rvs` is string or callable.  Default is 20.\n    alternative : {\'two-sided\', \'less\',\'greater\'}, optional\n        Defines the alternative hypothesis (see explanation above).\n        Default is \'two-sided\'.\n    mode : \'approx\' (default) or \'asymp\', optional\n        Defines the distribution used for calculating the p-value.\n\n          - \'approx\' : use approximation to exact distribution of test statistic\n          - \'asymp\' : use asymptotic distribution of test statistic\n\n    Returns\n    -------\n    statistic : float\n        KS test statistic, either D, D+ or D-.\n    pvalue :  float\n        One-tailed or two-tailed p-value.\n\n    Notes\n    -----\n    In the one-sided test, the alternative is that the empirical\n    cumulative distribution function of the random variable is "less"\n    or "greater" than the cumulative distribution function F(x) of the\n    hypothesis, ``G(x)<=F(x)``, resp. ``G(x)>=F(x)``.\n\n    Examples\n    --------\n    >>> from scipy import stats\n\n    >>> x = np.linspace(-15, 15, 9)\n    >>> stats.kstest(x, \'norm\')\n    (0.44435602715924361, 0.038850142705171065)\n\n    >>> np.random.seed(987654321) # set random seed to get the same result\n    >>> stats.kstest(\'norm\', False, N=100)\n    (0.058352892479417884, 0.88531190944151261)\n\n    The above lines are equivalent to:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.norm.rvs(size=100), \'norm\')\n    (0.058352892479417884, 0.88531190944151261)\n\n    *Test against one-sided alternative hypothesis*\n\n    Shift distribution to larger values, so that ``cdf_dgp(x) < norm.cdf(x)``:\n\n    >>> np.random.seed(987654321)\n    >>> x = stats.norm.rvs(loc=0.2, size=100)\n    >>> stats.kstest(x,\'norm\', alternative = \'less\')\n    (0.12464329735846891, 0.040989164077641749)\n\n    Reject equal distribution against alternative hypothesis: less\n\n    >>> stats.kstest(x,\'norm\', alternative = \'greater\')\n    (0.0072115233216311081, 0.98531158590396395)\n\n    Don\'t reject equal distribution against alternative hypothesis: greater\n\n    >>> stats.kstest(x,\'norm\', mode=\'asymp\')\n    (0.12464329735846891, 0.08944488871182088)\n\n    *Testing t distributed random variables against normal distribution*\n\n    With 100 degrees of freedom the t distribution looks close to the normal\n    distribution, and the K-S test does not reject the hypothesis that the\n    sample came from the normal distribution:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.t.rvs(100,size=100),\'norm\')\n    (0.072018929165471257, 0.67630062862479168)\n\n    With 3 degrees of freedom the t distribution looks sufficiently different\n    from the normal distribution, that we can reject the hypothesis that the\n    sample came from the normal distribution at the 10% level:\n\n    >>> np.random.seed(987654321)\n    >>> stats.kstest(stats.t.rvs(3,size=100),\'norm\')\n    (0.131016895759829, 0.058826222555312224)\n\n    '
if isinstance(rvs, string_types):
    if ((not cdf) or (cdf == rvs)):
        cdf = getattr(distributions, rvs).cdf
        rvs = getattr(distributions, rvs).rvs
    else:
        raise AttributeError('if rvs is string, cdf has to be the same distribution')
if isinstance(cdf, string_types):
    cdf = getattr(distributions, cdf).cdf
if callable(rvs):
    kwds = {'size': N}
    vals = numpy.sort(rvs(*args, **kwds))
else:
    vals = numpy.sort(rvs)
    N = len(vals)
cdfvals = cdf(vals, *args)
if (alternative == 'two_sided'):
    alternative = 'two-sided'
if (alternative in ['two-sided', 'greater']):
    Dplus = ((np.arange(1.0, (N + 1)) / N) - cdfvals).max()
    if (alternative == 'greater'):
        return KstestResult(Dplus, distributions.ksone.sf(Dplus, N))
if (alternative in ['two-sided', 'less']):
    tempResult = arange(0.0, N)
	
===================================================================	
binned_statistic_dd: 115	
----------------------------	

"\n    Compute a multidimensional binned statistic for a set of data.\n\n    This is a generalization of a histogramdd function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values within each bin.\n\n    Parameters\n    ----------\n    sample : array_like\n        Data to histogram passed as a sequence of D arrays of length N, or\n        as an (N,D) array.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is such a list, the statistic will be\n        computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : sequence or int, optional\n        The bin specification must be in one of the following forms:\n\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (nx, ny, ... = bins).\n          * The number of bins for all dimensions (nx = ny = ... = bins).\n\n    range : sequence, optional\n        A sequence of lower and upper bin edges to be used if the edges are\n        not given explicitely in `bins`. Defaults to the minimum and maximum\n        values along each dimension.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section of\n        `binned_statistic_2d`.\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : ndarray, shape(nx1, nx2, nx3,...)\n        The values of the selected statistic in each two-dimensional bin.\n    bin_edges : list of ndarrays\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    binnumber : (N,) array of ints or (D,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open in each dimension.  In\n    other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n    ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n    last bin, however, is ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n\n    .. versionadded:: 0.11.0\n\n    "
known_stats = ['mean', 'median', 'count', 'sum', 'std', 'min', 'max']
if ((not callable(statistic)) and (statistic not in known_stats)):
    raise ValueError(('invalid statistic %r' % (statistic,)))
try:
    (Dlen, Ndim) = sample.shape
except (AttributeError, ValueError):
    sample = np.atleast_2d(sample).T
    (Dlen, Ndim) = sample.shape
values = numpy.asarray(values)
input_shape = list(values.shape)
values = numpy.atleast_2d(values)
(Vdim, Vlen) = values.shape
if ((statistic != 'count') and (Vlen != Dlen)):
    raise AttributeError('The number of `values` elements must match the length of each `sample` dimension.')
nbin = numpy.empty(Ndim, int)
edges = (Ndim * [None])
dedges = (Ndim * [None])
try:
    M = len(bins)
    if (M != Ndim):
        raise AttributeError('The dimension of bins must be equal to the dimension of the sample x.')
except TypeError:
    bins = (Ndim * [bins])
if (range is None):
    smin = numpy.atleast_1d(numpy.array(sample.min(axis=0), float))
    smax = numpy.atleast_1d(numpy.array(sample.max(axis=0), float))
else:
    smin = numpy.zeros(Ndim)
    smax = numpy.zeros(Ndim)
    for i in xrange(Ndim):
        (smin[i], smax[i]) = range[i]
for i in xrange(len(smin)):
    if (smin[i] == smax[i]):
        smin[i] = (smin[i] - 0.5)
        smax[i] = (smax[i] + 0.5)
for i in xrange(Ndim):
    if numpy.isscalar(bins[i]):
        nbin[i] = (bins[i] + 2)
        edges[i] = numpy.linspace(smin[i], smax[i], (nbin[i] - 1))
    else:
        edges[i] = numpy.asarray(bins[i], float)
        nbin[i] = (len(edges[i]) + 1)
    dedges[i] = numpy.diff(edges[i])
nbin = numpy.asarray(nbin)
sampBin = {}
for i in xrange(Ndim):
    sampBin[i] = numpy.digitize(sample[:, i], edges[i])
for i in xrange(Ndim):
    decimal = (int((- numpy.log10(dedges[i].min()))) + 6)
    on_edge = numpy.where((numpy.around(sample[:, i], decimal) == numpy.around(edges[i][(- 1)], decimal)))[0]
    sampBin[i][on_edge] -= 1
ni = nbin.argsort()
binnumbers = numpy.zeros(Dlen, int)
for i in xrange(0, (Ndim - 1)):
    binnumbers += (sampBin[ni[i]] * nbin[ni[(i + 1):]].prod())
binnumbers += sampBin[ni[(- 1)]]
result = numpy.empty([Vdim, nbin.prod()], float)
if (statistic == 'mean'):
    result.fill(numpy.nan)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        result[(vv, a)] = (flatsum[a] / flatcount[a])
elif (statistic == 'std'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        flatsum2 = numpy.bincount(binnumbers, (values[vv] ** 2))
        result[(vv, a)] = numpy.sqrt(((flatsum2[a] / flatcount[a]) - ((flatsum[a] / flatcount[a]) ** 2)))
elif (statistic == 'count'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    tempResult = arange(len(flatcount))
	
===================================================================	
binned_statistic_dd: 121	
----------------------------	

"\n    Compute a multidimensional binned statistic for a set of data.\n\n    This is a generalization of a histogramdd function.  A histogram divides\n    the space into bins, and returns the count of the number of points in\n    each bin.  This function allows the computation of the sum, mean, median,\n    or other statistic of the values within each bin.\n\n    Parameters\n    ----------\n    sample : array_like\n        Data to histogram passed as a sequence of D arrays of length N, or\n        as an (N,D) array.\n    values : (N,) array_like or list of (N,) array_like\n        The data on which the statistic will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is such a list, the statistic will be\n        computed on each independently.\n    statistic : string or callable, optional\n        The statistic to compute (default is 'mean').\n        The following statistics are available:\n\n          * 'mean' : compute the mean of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'median' : compute the median of values for points within each\n            bin. Empty bins will be represented by NaN.\n          * 'count' : compute the count of points within each bin.  This is\n            identical to an unweighted histogram.  `values` array is not\n            referenced.\n          * 'sum' : compute the sum of values for points within each bin.\n            This is identical to a weighted histogram.\n          * 'min' : compute the minimum of values for points within each bin.\n            Empty bins will be represented by NaN.\n          * 'max' : compute the maximum of values for point within each bin.\n            Empty bins will be represented by NaN.\n          * function : a user-defined function which takes a 1D array of\n            values, and outputs a single numerical statistic. This function\n            will be called on the values in each bin.  Empty bins will be\n            represented by function([]), or NaN if this returns an error.\n\n    bins : sequence or int, optional\n        The bin specification must be in one of the following forms:\n\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (nx, ny, ... = bins).\n          * The number of bins for all dimensions (nx = ny = ... = bins).\n\n    range : sequence, optional\n        A sequence of lower and upper bin edges to be used if the edges are\n        not given explicitely in `bins`. Defaults to the minimum and maximum\n        values along each dimension.\n    expand_binnumbers : bool, optional\n        'False' (default): the returned `binnumber` is a shape (N,) array of\n        linearized bin indices.\n        'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n        ndarray, where each row gives the bin numbers in the corresponding\n        dimension.\n        See the `binnumber` returned value, and the `Examples` section of\n        `binned_statistic_2d`.\n\n        .. versionadded:: 0.17.0\n\n    Returns\n    -------\n    statistic : ndarray, shape(nx1, nx2, nx3,...)\n        The values of the selected statistic in each two-dimensional bin.\n    bin_edges : list of ndarrays\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    binnumber : (N,) array of ints or (D,N) ndarray of ints\n        This assigns to each element of `sample` an integer that represents the\n        bin in which this observation falls.  The representation depends on the\n        `expand_binnumbers` argument.  See `Notes` for details.\n\n\n    See Also\n    --------\n    numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n\n    Notes\n    -----\n    Binedges:\n    All but the last (righthand-most) bin is half-open in each dimension.  In\n    other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n    ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n    last bin, however, is ``[3, 4]``, which *includes* 4.\n\n    `binnumber`:\n    This returned argument assigns to each element of `sample` an integer that\n    represents the bin in which it belongs.  The representation depends on the\n    `expand_binnumbers` argument. If 'False' (default): The returned\n    `binnumber` is a shape (N,) array of linearized indices mapping each\n    element of `sample` to its corresponding bin (using row-major ordering).\n    If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n    each row indicates bin placements for each dimension respectively.  In each\n    dimension, a binnumber of `i` means the corresponding value is between\n    (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n\n    .. versionadded:: 0.11.0\n\n    "
known_stats = ['mean', 'median', 'count', 'sum', 'std', 'min', 'max']
if ((not callable(statistic)) and (statistic not in known_stats)):
    raise ValueError(('invalid statistic %r' % (statistic,)))
try:
    (Dlen, Ndim) = sample.shape
except (AttributeError, ValueError):
    sample = np.atleast_2d(sample).T
    (Dlen, Ndim) = sample.shape
values = numpy.asarray(values)
input_shape = list(values.shape)
values = numpy.atleast_2d(values)
(Vdim, Vlen) = values.shape
if ((statistic != 'count') and (Vlen != Dlen)):
    raise AttributeError('The number of `values` elements must match the length of each `sample` dimension.')
nbin = numpy.empty(Ndim, int)
edges = (Ndim * [None])
dedges = (Ndim * [None])
try:
    M = len(bins)
    if (M != Ndim):
        raise AttributeError('The dimension of bins must be equal to the dimension of the sample x.')
except TypeError:
    bins = (Ndim * [bins])
if (range is None):
    smin = numpy.atleast_1d(numpy.array(sample.min(axis=0), float))
    smax = numpy.atleast_1d(numpy.array(sample.max(axis=0), float))
else:
    smin = numpy.zeros(Ndim)
    smax = numpy.zeros(Ndim)
    for i in xrange(Ndim):
        (smin[i], smax[i]) = range[i]
for i in xrange(len(smin)):
    if (smin[i] == smax[i]):
        smin[i] = (smin[i] - 0.5)
        smax[i] = (smax[i] + 0.5)
for i in xrange(Ndim):
    if numpy.isscalar(bins[i]):
        nbin[i] = (bins[i] + 2)
        edges[i] = numpy.linspace(smin[i], smax[i], (nbin[i] - 1))
    else:
        edges[i] = numpy.asarray(bins[i], float)
        nbin[i] = (len(edges[i]) + 1)
    dedges[i] = numpy.diff(edges[i])
nbin = numpy.asarray(nbin)
sampBin = {}
for i in xrange(Ndim):
    sampBin[i] = numpy.digitize(sample[:, i], edges[i])
for i in xrange(Ndim):
    decimal = (int((- numpy.log10(dedges[i].min()))) + 6)
    on_edge = numpy.where((numpy.around(sample[:, i], decimal) == numpy.around(edges[i][(- 1)], decimal)))[0]
    sampBin[i][on_edge] -= 1
ni = nbin.argsort()
binnumbers = numpy.zeros(Dlen, int)
for i in xrange(0, (Ndim - 1)):
    binnumbers += (sampBin[ni[i]] * nbin[ni[(i + 1):]].prod())
binnumbers += sampBin[ni[(- 1)]]
result = numpy.empty([Vdim, nbin.prod()], float)
if (statistic == 'mean'):
    result.fill(numpy.nan)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        result[(vv, a)] = (flatsum[a] / flatcount[a])
elif (statistic == 'std'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = flatcount.nonzero()
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        flatsum2 = numpy.bincount(binnumbers, (values[vv] ** 2))
        result[(vv, a)] = numpy.sqrt(((flatsum2[a] / flatcount[a]) - ((flatsum[a] / flatcount[a]) ** 2)))
elif (statistic == 'count'):
    result.fill(0)
    flatcount = numpy.bincount(binnumbers, None)
    a = numpy.arange(len(flatcount))
    result[:, a] = flatcount[numpy.newaxis, :]
elif (statistic == 'sum'):
    result.fill(0)
    for vv in xrange(Vdim):
        flatsum = numpy.bincount(binnumbers, values[vv])
        tempResult = arange(len(flatsum))
	
===================================================================	
genpareto_gen.__munp: 946	
----------------------------	

val = 0.0
tempResult = arange(0, (n + 1))
	
===================================================================	
genextreme_gen._munp: 1037	
----------------------------	

tempResult = arange(0, (n + 1))
	
===================================================================	
hypergeom_gen._logsf: 222	
----------------------------	

'\n        More precise calculation than log(sf)\n        '
res = []
for (quant, tot, good, draw) in zip(k, M, n, N):
    tempResult = arange((quant + 1), (draw + 1))
	
===================================================================	
hypergeom_gen._sf: 214	
----------------------------	

"More precise calculation, 1 - cdf doesn't cut it."
res = []
for (quant, tot, good, draw) in zip(k, M, n, N):
    tempResult = arange((quant + 1), (draw + 1))
	
===================================================================	
_iter_chunked: 1512	
----------------------------	

'Iterate from x0 to x1 in chunks of chunksize and steps inc.\n\n    x0 must be finite, x1 need not be. In the latter case, the iterator is infinite.\n    Handles both x0 < x1 and x0 > x1. In the latter case, iterates downwards\n    (make sure to set inc < 0.)\n\n    >>> [x for x in _iter_chunked(2, 5, inc=2)]\n    [array([2, 4])]\n    >>> [x for x in _iter_chunked(2, 11, inc=2)]\n    [array([2, 4, 6, 8]), array([10])]\n    >>> [x for x in _iter_chunked(2, -5, inc=-2)]\n    [array([ 2,  0, -2, -4])]\n    >>> [x for x in _iter_chunked(2, -9, inc=-2)]\n    [array([ 2,  0, -2, -4]), array([-6, -8])]\n\n    '
if (inc == 0):
    raise ValueError('Cannot increment by zero.')
if (chunksize <= 0):
    raise ValueError(('Chunk size must be positive; got %s.' % chunksize))
s = (1 if (inc > 0) else (- 1))
stepsize = abs((chunksize * inc))
x = x0
while (((x - x1) * inc) < 0):
    delta = min(stepsize, abs((x - x1)))
    step = (delta * s)
    tempResult = arange(x, (x + step), inc)
	
===================================================================	
_expect: 1472	
----------------------------	

'Helper for computing the expectation value of `fun`.'
if ((ub - lb) <= chunksize):
    tempResult = arange(lb, (ub + 1), inc)
	
===================================================================	
theilslopes: 58	
----------------------------	

'\n    Computes the Theil-Sen estimator for a set of points (x, y).\n\n    `theilslopes` implements a method for robust linear regression.  It\n    computes the slope as the median of all slopes between paired values.\n\n    Parameters\n    ----------\n    y : array_like\n        Dependent variable.\n    x : array_like or None, optional\n        Independent variable. If None, use ``arange(len(y))`` instead.\n    alpha : float, optional\n        Confidence degree between 0 and 1. Default is 95% confidence.\n        Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\n        interpreted as "find the 90% confidence interval".\n\n    Returns\n    -------\n    medslope : float\n        Theil slope.\n    medintercept : float\n        Intercept of the Theil line, as ``median(y) - medslope*median(x)``.\n    lo_slope : float\n        Lower bound of the confidence interval on `medslope`.\n    up_slope : float\n        Upper bound of the confidence interval on `medslope`.\n\n    Notes\n    -----\n    The implementation of `theilslopes` follows [1]_. The intercept is\n    not defined in [1]_, and here it is defined as ``median(y) -\n    medslope*median(x)``, which is given in [3]_. Other definitions of\n    the intercept exist in the literature. A confidence interval for\n    the intercept is not given as this question is not addressed in\n    [1]_.\n\n    References\n    ----------\n    .. [1] P.K. Sen, "Estimates of the regression coefficient based on Kendall\'s tau",\n           J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\n    .. [2] H. Theil, "A rank-invariant method of linear and polynomial\n           regression analysis I, II and III",  Nederl. Akad. Wetensch., Proc.\n           53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\n    .. [3] W.L. Conover, "Practical nonparametric statistics", 2nd ed.,\n           John Wiley and Sons, New York, pp. 493.\n\n    Examples\n    --------\n    >>> from scipy import stats\n    >>> import matplotlib.pyplot as plt\n\n    >>> x = np.linspace(-5, 5, num=150)\n    >>> y = x + np.random.normal(size=x.size)\n    >>> y[11:15] += 10  # add outliers\n    >>> y[-5:] -= 7\n\n    Compute the slope, intercept and 90% confidence interval.  For comparison,\n    also compute the least-squares fit with `linregress`:\n\n    >>> res = stats.theilslopes(y, x, 0.90)\n    >>> lsq_res = stats.linregress(x, y)\n\n    Plot the results. The Theil-Sen regression line is shown in red, with the\n    dashed red lines illustrating the confidence interval of the slope (note\n    that the dashed red lines are not the confidence interval of the regression\n    as the confidence interval of the intercept is not included). The green\n    line shows the least-squares fit for comparison.\n\n    >>> fig = plt.figure()\n    >>> ax = fig.add_subplot(111)\n    >>> ax.plot(x, y, \'b.\')\n    >>> ax.plot(x, res[1] + res[0] * x, \'r-\')\n    >>> ax.plot(x, res[1] + res[2] * x, \'r--\')\n    >>> ax.plot(x, res[1] + res[3] * x, \'r--\')\n    >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, \'g-\')\n    >>> plt.show()\n\n    '
y = np.array(y).flatten()
if (x is None):
    tempResult = arange(len(y), dtype=float)
	
===================================================================	
TestBinnedStatistic.test_1d_range_keyword: 101	
----------------------------	

numpy.random.seed(9865)
tempResult = arange(30)
	
===================================================================	
test_margins: 20	
----------------------------	

a = numpy.array([1])
m = margins(a)
assert_equal(len(m), 1)
m0 = m[0]
assert_array_equal(m0, numpy.array([1]))
a = numpy.array([[1]])
(m0, m1) = margins(a)
expected0 = numpy.array([[1]])
expected1 = numpy.array([[1]])
assert_array_equal(m0, expected0)
assert_array_equal(m1, expected1)
tempResult = arange(12)
	
===================================================================	
test_margins: 26	
----------------------------	

a = numpy.array([1])
m = margins(a)
assert_equal(len(m), 1)
m0 = m[0]
assert_array_equal(m0, numpy.array([1]))
a = numpy.array([[1]])
(m0, m1) = margins(a)
expected0 = numpy.array([[1]])
expected1 = numpy.array([[1]])
assert_array_equal(m0, expected0)
assert_array_equal(m1, expected1)
a = np.arange(12).reshape(2, 6)
(m0, m1) = margins(a)
expected0 = numpy.array([[15], [51]])
expected1 = numpy.array([[6, 8, 10, 12, 14, 16]])
assert_array_equal(m0, expected0)
assert_array_equal(m1, expected1)
tempResult = arange(24)
	
===================================================================	
TestDLaplace.test_stats: 722	
----------------------------	

a = 1.0
dl = scipy.stats.dlaplace(a)
(m, v, s, k) = dl.stats('mvsk')
N = 37
tempResult = arange((- N), (N + 1))
	
===================================================================	
TestFitMethod.test_fix_fit_norm: 1154	
----------------------------	

tempResult = arange(1, 6)
	
===================================================================	
TestSkellam.test_pmf: 930	
----------------------------	

tempResult = arange((- 10), 15)
	
===================================================================	
TestGenExpon.test_pdf_unity_area: 914	
----------------------------	

from scipy.integrate import simps
tempResult = arange(0, 10, 0.01)
	
===================================================================	
TestNct.test_broadcasting: 1504	
----------------------------	

tempResult = arange(4, 7)
	
===================================================================	
TestSkellam.test_cdf: 936	
----------------------------	

tempResult = arange((- 10), 15)
	
===================================================================	
TestGenExpon.test_cdf_bounds: 918	
----------------------------	

tempResult = arange(0, 10, 0.01)
	
===================================================================	
test_ncx2_tails_ticket_955: 1891	
----------------------------	

tempResult = arange(20, 25, 0.2)
	
===================================================================	
test_ncx2_tails_ticket_955: 1892	
----------------------------	

a = scipy.stats.ncx2.cdf(numpy.arange(20, 25, 0.2), 2, 107.458615)
tempResult = arange(20, 25, 0.2)
	
===================================================================	
TestFitMethod.test_fix_fit_gamma: 1166	
----------------------------	

tempResult = arange(1, 6)
	
===================================================================	
TestTrapz.test_trapz_vect: 1688	
----------------------------	

c = numpy.array([0.1, 0.2, 0.3])
d = numpy.array([0.5, 0.6])[:, None]
x = numpy.array([0.15, 0.25, 0.9])
v = scipy.stats.trapz.pdf(x, c, d)
(cc, dd, xx) = numpy.broadcast_arrays(c, d, x)
res = numpy.empty(xx.size, dtype=xx.dtype)
tempResult = arange(xx.size)
	
===================================================================	
test_ncx2_tails_pdf: 1898	
----------------------------	

with warnings.catch_warnings():
    warnings.simplefilter('ignore', RuntimeWarning)
    tempResult = arange(340, 350)
	
===================================================================	
test_ncx2_tails_pdf: 1899	
----------------------------	

with warnings.catch_warnings():
    warnings.simplefilter('ignore', RuntimeWarning)
    assert_equal(scipy.stats.ncx2.pdf(1, numpy.arange(340, 350), 2), 0)
    tempResult = arange(340, 350)
	
===================================================================	
TestArrayArgument.test_noexception: 1032	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_kde_integer_input: 147	
----------------------------	

'Regression test for #1181.'
tempResult = arange(5)
	
===================================================================	
TestKstatVar.test_nan_input: 637	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestFligner.test_data: 422	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestKstat.test_nan_input: 622	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestCircFuncs.test_circmean_range: 919	
----------------------------	

tempResult = arange(0, 2, 0.1)
	
===================================================================	
TestFligner.test_empty_arg: 451	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestKstat.test_kstat_bad_arg: 627	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestMood.test_mood: 457	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestBayes_mvs.test_result_attributes: 41	
----------------------------	

tempResult = arange(15)
	
===================================================================	
TestShapiro.test_nan_input: 118	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestAnderson.test_gumbel: 160	
----------------------------	

v = numpy.ones(100)
v[0] = 0.0
(a2, crit, sig) = scipy.stats.anderson(v, 'gumbel')
n = len(v)
(xbar, s) = scipy.stats.gumbel_l.fit(v)
logcdf = scipy.stats.gumbel_l.logcdf(v, xbar, s)
logsf = scipy.stats.gumbel_l.logsf(v, xbar, s)
tempResult = arange(1, (n + 1))
	
===================================================================	
TestCompareWithStats.test_describe_result_attributes: 809	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestCompareWithStats.test_sem: 787	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestCompareWithStats.test_trimboth: 862	
----------------------------	

tempResult = arange(20)
	
===================================================================	
test_plotting_positions: 420	
----------------------------	

tempResult = arange(3)
	
===================================================================	
TestPercentile.test_percentile: 330	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestTtest_rel.test_invalid_input_size: 531	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestTtest_rel.test_invalid_input_size: 531	
----------------------------	

tempResult = arange(11)
	
===================================================================	
TestTtest_rel.test_invalid_input_size: 532	
----------------------------	

assert_raises(ValueError, scipy.stats.mstats.ttest_rel, numpy.arange(10), numpy.arange(11))
tempResult = arange(24)
	
===================================================================	
TestMisc.test_idealfourths: 28	
----------------------------	

'Tests ideal-fourths'
tempResult = arange(100)
	
===================================================================	
TestWishart.test_frozen: 601	
----------------------------	

dim = 4
tempResult = arange(dim)
	
===================================================================	
TestWishart.test_frozen: 602	
----------------------------	

dim = 4
scale = numpy.diag((numpy.arange(dim) + 1))
tempResult = arange(((dim * (dim - 1)) // 2))
	
===================================================================	
TestWishart.test_frozen: 606	
----------------------------	

dim = 4
scale = numpy.diag((numpy.arange(dim) + 1))
scale[numpy.tril_indices(dim, k=(- 1))] = numpy.arange(((dim * (dim - 1)) // 2))
scale = numpy.dot(scale.T, scale)
X = []
for i in range(5):
    tempResult = arange(dim)
	
===================================================================	
TestWishart.test_frozen: 607	
----------------------------	

dim = 4
scale = numpy.diag((numpy.arange(dim) + 1))
scale[numpy.tril_indices(dim, k=(- 1))] = numpy.arange(((dim * (dim - 1)) // 2))
scale = numpy.dot(scale.T, scale)
X = []
for i in range(5):
    x = numpy.diag((numpy.arange(dim) + ((i + 1) ** 2)))
    tempResult = arange(((dim * (dim - 1)) // 2))
	
===================================================================	
TestWishart.test_is_scaled_chisquared: 644	
----------------------------	

numpy.random.seed(482974)
sn = 500
df = 10
dim = 4
tempResult = arange(4)
	
===================================================================	
TestWishart.test_is_scaled_chisquared: 645	
----------------------------	

numpy.random.seed(482974)
sn = 500
df = 10
dim = 4
scale = numpy.diag((numpy.arange(4) + 1))
tempResult = arange(6)
	
===================================================================	
TestWishart.test_1D_is_chisquared: 625	
----------------------------	

numpy.random.seed(482974)
sn = 500
dim = 1
scale = numpy.eye(dim)
tempResult = arange(1, 10, 2, dtype=float)
	
===================================================================	
TestMultivariateNormal.test_input_shape: 26	
----------------------------	

tempResult = arange(3)
	
===================================================================	
TestInvwishart.test_1D_is_invgamma: 790	
----------------------------	

numpy.random.seed(482974)
sn = 500
dim = 1
scale = numpy.eye(dim)
tempResult = arange(5, 20, 2, dtype=float)
	
===================================================================	
TestInvwishart.test_frozen: 767	
----------------------------	

dim = 4
tempResult = arange(dim)
	
===================================================================	
TestInvwishart.test_frozen: 768	
----------------------------	

dim = 4
scale = numpy.diag((numpy.arange(dim) + 1))
tempResult = arange(((dim * (dim - 1)) / 2))
	
===================================================================	
TestInvwishart.test_frozen: 772	
----------------------------	

dim = 4
scale = numpy.diag((numpy.arange(dim) + 1))
scale[numpy.tril_indices(dim, k=(- 1))] = numpy.arange(((dim * (dim - 1)) / 2))
scale = numpy.dot(scale.T, scale)
X = []
for i in range(5):
    tempResult = arange(dim)
	
===================================================================	
TestInvwishart.test_frozen: 773	
----------------------------	

dim = 4
scale = numpy.diag((numpy.arange(dim) + 1))
scale[numpy.tril_indices(dim, k=(- 1))] = numpy.arange(((dim * (dim - 1)) / 2))
scale = numpy.dot(scale.T, scale)
X = []
for i in range(5):
    x = numpy.diag((numpy.arange(dim) + ((i + 1) ** 2)))
    tempResult = arange(((dim * (dim - 1)) / 2))
	
===================================================================	
module: 151	
----------------------------	

from __future__ import division, print_function, absolute_import
import numpy as np
from numpy.testing import TestCase, run_module_suite, assert_equal, assert_array_equal, dec
from scipy.stats import rankdata, tiecorrect

class TestTieCorrect(TestCase):

    def test_empty(self):
        'An empty array requires no correction, should return 1.0.'
        ranks = numpy.array([], dtype=numpy.float64)
        c = tiecorrect(ranks)
        assert_equal(c, 1.0)

    def test_one(self):
        'A single element requires no correction, should return 1.0.'
        ranks = numpy.array([1.0], dtype=numpy.float64)
        c = tiecorrect(ranks)
        assert_equal(c, 1.0)

    def test_no_correction(self):
        'Arrays with no ties require no correction.'
        ranks = numpy.arange(2.0)
        c = tiecorrect(ranks)
        assert_equal(c, 1.0)
        ranks = numpy.arange(3.0)
        c = tiecorrect(ranks)
        assert_equal(c, 1.0)

    def test_basic(self):
        'Check a few basic examples of the tie correction factor.'
        ranks = numpy.array([1.0, 2.5, 2.5])
        c = tiecorrect(ranks)
        T = 2.0
        N = ranks.size
        expected = (1.0 - (((T ** 3) - T) / ((N ** 3) - N)))
        assert_equal(c, expected)
        ranks = numpy.array([1.5, 1.5, 3.0])
        c = tiecorrect(ranks)
        T = 2.0
        N = ranks.size
        expected = (1.0 - (((T ** 3) - T) / ((N ** 3) - N)))
        assert_equal(c, expected)
        ranks = numpy.array([1.0, 3.0, 3.0, 3.0])
        c = tiecorrect(ranks)
        T = 3.0
        N = ranks.size
        expected = (1.0 - (((T ** 3) - T) / ((N ** 3) - N)))
        assert_equal(c, expected)
        ranks = numpy.array([1.5, 1.5, 4.0, 4.0, 4.0])
        c = tiecorrect(ranks)
        T1 = 2.0
        T2 = 3.0
        N = ranks.size
        expected = (1.0 - ((((T1 ** 3) - T1) + ((T2 ** 3) - T2)) / ((N ** 3) - N)))
        assert_equal(c, expected)

    def test_overflow(self):
        (ntie, k) = (2000, 5)
        a = numpy.repeat(numpy.arange(k), ntie)
        n = a.size
        out = tiecorrect(rankdata(a))
        assert_equal(out, (1.0 - ((k * ((ntie ** 3) - ntie)) / float(((n ** 3) - n)))))

class TestRankData(TestCase):

    def test_empty(self):
        'stats.rankdata([]) should return an empty array.'
        a = numpy.array([], dtype=int)
        r = rankdata(a)
        assert_array_equal(r, numpy.array([], dtype=numpy.float64))
        r = rankdata([])
        assert_array_equal(r, numpy.array([], dtype=numpy.float64))

    def test_one(self):
        'Check stats.rankdata with an array of length 1.'
        data = [100]
        a = numpy.array(data, dtype=int)
        r = rankdata(a)
        assert_array_equal(r, numpy.array([1.0], dtype=numpy.float64))
        r = rankdata(data)
        assert_array_equal(r, numpy.array([1.0], dtype=numpy.float64))

    def test_basic(self):
        'Basic tests of stats.rankdata.'
        data = [100, 10, 50]
        expected = numpy.array([3.0, 1.0, 2.0], dtype=numpy.float64)
        a = numpy.array(data, dtype=int)
        r = rankdata(a)
        assert_array_equal(r, expected)
        r = rankdata(data)
        assert_array_equal(r, expected)
        data = [40, 10, 30, 10, 50]
        expected = numpy.array([4.0, 1.5, 3.0, 1.5, 5.0], dtype=numpy.float64)
        a = numpy.array(data, dtype=int)
        r = rankdata(a)
        assert_array_equal(r, expected)
        r = rankdata(data)
        assert_array_equal(r, expected)
        data = [20, 20, 20, 10, 10, 10]
        expected = numpy.array([5.0, 5.0, 5.0, 2.0, 2.0, 2.0], dtype=numpy.float64)
        a = numpy.array(data, dtype=int)
        r = rankdata(a)
        assert_array_equal(r, expected)
        r = rankdata(data)
        assert_array_equal(r, expected)
        a2d = a.reshape(2, 3)
        r = rankdata(a2d)
        assert_array_equal(r, expected)

    def test_rankdata_object_string(self):
        min_rank = (lambda a: [(1 + sum(((i < j) for i in a))) for j in a])
        max_rank = (lambda a: [sum(((i <= j) for i in a)) for j in a])
        ordinal_rank = (lambda a: min_rank([(x, i) for (i, x) in enumerate(a)]))

        def average_rank(a):
            return [((i + j) / 2.0) for (i, j) in zip(min_rank(a), max_rank(a))]

        def dense_rank(a):
            b = numpy.unique(a)
            return [(1 + sum(((i < j) for i in b))) for j in a]
        rankf = dict(min=min_rank, max=max_rank, ordinal=ordinal_rank, average=average_rank, dense=dense_rank)

        def check_ranks(a):
            for method in ('min', 'max', 'dense', 'ordinal', 'average'):
                out = rankdata(a, method=method)
                assert_array_equal(out, rankf[method](a))
        val = ['foo', 'bar', 'qux', 'xyz', 'abc', 'efg', 'ace', 'qwe', 'qaz']
        check_ranks(numpy.random.choice(val, 200))
        check_ranks(np.random.choice(val, 200).astype('object'))
        val = numpy.array([0, 1, 2, 2.718, 3, 3.141], dtype='object')
        check_ranks(np.random.choice(val, 200).astype('object'))

    def test_large_int(self):
        data = numpy.array([(2 ** 60), ((2 ** 60) + 1)], dtype=numpy.uint64)
        r = rankdata(data)
        assert_array_equal(r, [1.0, 2.0])
        data = numpy.array([(2 ** 60), ((2 ** 60) + 1)], dtype=numpy.int64)
        r = rankdata(data)
        assert_array_equal(r, [1.0, 2.0])
        data = numpy.array([(2 ** 60), ((- (2 ** 60)) + 1)], dtype=numpy.int64)
        r = rankdata(data)
        assert_array_equal(r, [2.0, 1.0])

    def test_big_tie(self):
        for n in [10000, 100000, 1000000]:
            data = numpy.ones(n, dtype=int)
            r = rankdata(data)
            expected_rank = (0.5 * (n + 1))
            assert_array_equal(r, (expected_rank * data), ('test failed with n=%d' % n))
tempResult = arange(1.0, 31.0)
	
===================================================================	
TestTieCorrect.test_no_correction: 23	
----------------------------	

'Arrays with no ties require no correction.'
tempResult = arange(2.0)
	
===================================================================	
TestTieCorrect.test_no_correction: 26	
----------------------------	

'Arrays with no ties require no correction.'
ranks = numpy.arange(2.0)
c = tiecorrect(ranks)
assert_equal(c, 1.0)
tempResult = arange(3.0)
	
===================================================================	
TestTieCorrect.test_overflow: 60	
----------------------------	

(ntie, k) = (2000, 5)
tempResult = arange(k)
	
===================================================================	
test_weightedtau_vs_quadratic: 607	
----------------------------	


def wkq(x, y, rank, weigher, add):
    tot = conc = disc = u = v = 0
    for i in range(len(x)):
        for j in range(len(x)):
            w = ((weigher(rank[i]) + weigher(rank[j])) if add else (weigher(rank[i]) * weigher(rank[j])))
            tot += w
            if (x[i] == x[j]):
                u += w
            if (y[i] == y[j]):
                v += w
            if (((x[i] < x[j]) and (y[i] < y[j])) or ((x[i] > x[j]) and (y[i] > y[j]))):
                conc += w
            elif (((x[i] < x[j]) and (y[i] > y[j])) or ((x[i] > x[j]) and (y[i] < y[j]))):
                disc += w
    return (((conc - disc) / numpy.sqrt((tot - u))) / numpy.sqrt((tot - v)))
numpy.random.seed(42)
for s in range(3, 10):
    a = []
    for i in range(s):
        a += ([i] * i)
    b = list(a)
    numpy.random.shuffle(a)
    numpy.random.shuffle(b)
    tempResult = arange(len(a), dtype=numpy.intp)
	
===================================================================	
TestMoments.test_skew_propagate_nan: 1553	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestMoments.test_variation_propagate_nan: 1528	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestScoreatpercentile.test_sequence_per: 943	
----------------------------	

x = (arange(8) * 0.5)
expected = numpy.array([0, 3.5, 1.75])
res = scipy.stats.scoreatpercentile(x, [0, 100, 50])
assert_allclose(res, expected)
assert_(isinstance(res, numpy.ndarray))
assert_allclose(scipy.stats.scoreatpercentile(x, numpy.array([0, 100, 50])), expected)
tempResult = arange(12)
	
===================================================================	
TestRegression.test_regress_two_inputs: 705	
----------------------------	

tempResult = arange(2)
	
===================================================================	
TestRegression.test_regress_two_inputs: 706	
----------------------------	

x = numpy.arange(2)
tempResult = arange(3, 5)
	
===================================================================	
TestMoments.test_moment_propagate_nan: 1510	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestMoments.test_skewness: 1542	
----------------------------	

y = scipy.stats.skew(self.scalar_testcase)
assert_approx_equal(y, 0.0)
y = scipy.stats.skew(self.testmathworks)
assert_approx_equal(y, (- 0.29322304336607), 10)
y = scipy.stats.skew(self.testmathworks, bias=0)
assert_approx_equal(y, (- 0.43711110502394), 10)
y = scipy.stats.skew(self.testcase)
assert_approx_equal(y, 0.0, 10)
tempResult = arange(10.0)
	
===================================================================	
TestKruskal.test_nan_policy: 2780	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestTrimmedStats.test_tmax: 70	
----------------------------	

assert_equal(scipy.stats.tmax(4), 4)
tempResult = arange(10)
	
===================================================================	
TestTrimmedStats.test_tmax: 78	
----------------------------	

assert_equal(scipy.stats.tmax(4), 4)
x = numpy.arange(10)
assert_equal(scipy.stats.tmax(x), 9)
assert_equal(scipy.stats.tmax(x, upperlimit=9), 9)
assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), 8)
x = x.reshape((5, 2))
assert_equal(scipy.stats.tmax(x, upperlimit=9, inclusive=False), [8, 7])
assert_equal(scipy.stats.tmax(x, axis=1), [1, 3, 5, 7, 9])
assert_equal(scipy.stats.tmax(x, axis=None), 9)
tempResult = arange(10.0)
	
===================================================================	
TestRankSums.test_ranksums_result_attributes: 2235	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestRankSums.test_ranksums_result_attributes: 2235	
----------------------------	

tempResult = arange(25)
	
===================================================================	
TestIQR.test_nanpolicy: 1394	
----------------------------	

numpy_version = NumpyVersion(numpy.__version__)
tempResult = arange(15.0)
	
===================================================================	
test_percentileofscore: 1645	
----------------------------	

pcos = scipy.stats.percentileofscore
assert_equal(pcos([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 4), 40.0)
for (kind, result) in [('mean', 35.0), ('strict', 30.0), ('weak', 40.0)]:
    tempResult = arange(10)
	
===================================================================	
test_skewtest_too_few_samples: 2263	
----------------------------	

tempResult = arange(7.0)
	
===================================================================	
TestTrimmedStats.test_tmin: 52	
----------------------------	

assert_equal(scipy.stats.tmin(4), 4)
tempResult = arange(10)
	
===================================================================	
TestTrimmedStats.test_tmin: 60	
----------------------------	

assert_equal(scipy.stats.tmin(4), 4)
x = numpy.arange(10)
assert_equal(scipy.stats.tmin(x), 0)
assert_equal(scipy.stats.tmin(x, lowerlimit=0), 0)
assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), 1)
x = x.reshape((5, 2))
assert_equal(scipy.stats.tmin(x, lowerlimit=0, inclusive=False), [2, 1])
assert_equal(scipy.stats.tmin(x, axis=1), [0, 2, 4, 6, 8])
assert_equal(scipy.stats.tmin(x, axis=None), 0)
tempResult = arange(10.0)
	
===================================================================	
TestRegression.test_regress_two_inputs_horizontal_line: 712	
----------------------------	

tempResult = arange(2)
	
===================================================================	
TestMoments.test_kurtosis_propagate_nan: 1578	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestRegression.test_linregress: 679	
----------------------------	

tempResult = arange(11)
	
===================================================================	
TestRegression.test_linregress: 680	
----------------------------	

x = numpy.arange(11)
tempResult = arange(5, 16)
	
===================================================================	
TestTrim.test_trim_mean: 2609	
----------------------------	

a = numpy.array([4, 8, 2, 0, 9, 5, 10, 1, 7, 3, 6])
idx = numpy.array([3, 5, 0, 1, 2, 4])
tempResult = arange(24)
	
===================================================================	
TestTrim.test_trim_mean: 2610	
----------------------------	

a = numpy.array([4, 8, 2, 0, 9, 5, 10, 1, 7, 3, 6])
idx = numpy.array([3, 5, 0, 1, 2, 4])
a2 = np.arange(24).reshape(6, 4)[idx, :]
tempResult = arange(24)
	
===================================================================	
TestTrim.test_trim_mean: 2614	
----------------------------	

a = numpy.array([4, 8, 2, 0, 9, 5, 10, 1, 7, 3, 6])
idx = numpy.array([3, 5, 0, 1, 2, 4])
a2 = np.arange(24).reshape(6, 4)[idx, :]
a3 = np.arange(24).reshape(6, 4, order='F')[idx, :]
assert_equal(scipy.stats.trim_mean(a3, (2 / 6.0)), numpy.array([2.5, 8.5, 14.5, 20.5]))
assert_equal(scipy.stats.trim_mean(a2, (2 / 6.0)), numpy.array([10.0, 11.0, 12.0, 13.0]))
idx4 = numpy.array([1, 0, 3, 2])
tempResult = arange(24)
	
===================================================================	
test_obrientransform: 2381	
----------------------------	

x1 = numpy.array([0, 2, 4])
t1 = scipy.stats.obrientransform(x1)
expected = [7, (- 2), 7]
assert_allclose(t1[0], expected)
x2 = numpy.array([0, 3, 6, 9])
t2 = scipy.stats.obrientransform(x2)
expected = numpy.array([30, 0, 0, 30])
assert_allclose(t2[0], expected)
(a, b) = scipy.stats.obrientransform(x1, x2)
assert_equal(a, t1[0])
assert_equal(b, t2[0])
(a, b, c) = scipy.stats.obrientransform(x1, x2, x1)
assert_equal(a, t1[0])
assert_equal(b, t2[0])
assert_equal(c, t1[0])
tempResult = arange(5)
	
===================================================================	
TestIQR.test_interpolation: 1314	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestIQR.test_interpolation: 1315	
----------------------------	

x = numpy.arange(5)
tempResult = arange(4)
	
===================================================================	
TestDescribe.test_describe_result_attributes: 2158	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestDescribe.test_describe_numbers: 2140	
----------------------------	

x = numpy.vstack((numpy.ones((3, 4)), (2 * numpy.ones((2, 4)))))
(nc, mmc) = (5, ([1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0]))
mc = numpy.array([1.4, 1.4, 1.4, 1.4])
vc = numpy.array([0.3, 0.3, 0.3, 0.3])
skc = ([0.4082482904638636] * 4)
kurtc = ([(- 1.833333333333333)] * 4)
(n, mm, m, v, sk, kurt) = scipy.stats.describe(x)
assert_equal(n, nc)
assert_equal(mm, mmc)
assert_equal(m, mc)
assert_equal(v, vc)
assert_array_almost_equal(sk, skc, decimal=13)
assert_array_almost_equal(kurt, kurtc, decimal=13)
(n, mm, m, v, sk, kurt) = scipy.stats.describe(x.T, axis=1)
assert_equal(n, nc)
assert_equal(mm, mmc)
assert_equal(m, mc)
assert_equal(v, vc)
assert_array_almost_equal(sk, skc, decimal=13)
assert_array_almost_equal(kurt, kurtc, decimal=13)
tempResult = arange(10.0)
	
===================================================================	
TestIQR.test_2D: 1268	
----------------------------	

tempResult = arange(15)
	
===================================================================	
TestIQR.test_scalarlike: 1256	
----------------------------	

tempResult = arange(1)
	
===================================================================	
TestVariability.test_sem: 1126	
----------------------------	

with warnings.catch_warnings():
    warnings.filterwarnings('ignore', category=RuntimeWarning)
    y = scipy.stats.sem(self.scalar_testcase)
assert_(numpy.isnan(y))
y = scipy.stats.sem(self.testcase)
assert_approx_equal(y, 0.6454972244)
n = len(self.testcase)
assert_allclose((scipy.stats.sem(self.testcase, ddof=0) * numpy.sqrt((n / (n - 2)))), scipy.stats.sem(self.testcase, ddof=2))
tempResult = arange(10.0)
	
===================================================================	
TestTrim.test_trimboth: 2596	
----------------------------	

tempResult = arange(11)
	
===================================================================	
TestTrim.test_trimboth: 2597	
----------------------------	

a = numpy.arange(11)
tempResult = arange(3, 8)
	
===================================================================	
TestTrim.test_trimboth: 2599	
----------------------------	

a = numpy.arange(11)
assert_equal(numpy.sort(scipy.stats.trimboth(a, (3 / 11.0))), numpy.arange(3, 8))
assert_equal(numpy.sort(scipy.stats.trimboth(a, 0.2)), numpy.array([2, 3, 4, 5, 6, 7, 8]))
tempResult = arange(24)
	
===================================================================	
TestTrim.test_trimboth: 2599	
----------------------------	

a = numpy.arange(11)
assert_equal(numpy.sort(scipy.stats.trimboth(a, (3 / 11.0))), numpy.arange(3, 8))
assert_equal(numpy.sort(scipy.stats.trimboth(a, 0.2)), numpy.array([2, 3, 4, 5, 6, 7, 8]))
tempResult = arange(4, 20)
	
===================================================================	
TestTrim.test_trimboth: 2600	
----------------------------	

a = numpy.arange(11)
assert_equal(numpy.sort(scipy.stats.trimboth(a, (3 / 11.0))), numpy.arange(3, 8))
assert_equal(numpy.sort(scipy.stats.trimboth(a, 0.2)), numpy.array([2, 3, 4, 5, 6, 7, 8]))
assert_equal(numpy.sort(scipy.stats.trimboth(np.arange(24).reshape(6, 4), 0.2)), np.arange(4, 20).reshape(4, 4))
tempResult = arange(24)
	
===================================================================	
TestTrim.test_trimboth: 2601	
----------------------------	

a = numpy.arange(11)
assert_equal(numpy.sort(scipy.stats.trimboth(a, (3 / 11.0))), numpy.arange(3, 8))
assert_equal(numpy.sort(scipy.stats.trimboth(a, 0.2)), numpy.array([2, 3, 4, 5, 6, 7, 8]))
assert_equal(numpy.sort(scipy.stats.trimboth(np.arange(24).reshape(6, 4), 0.2)), np.arange(4, 20).reshape(4, 4))
assert_equal(numpy.sort(scipy.stats.trimboth(np.arange(24).reshape(4, 6).T, (2 / 6.0))), numpy.array([[2, 8, 14, 20], [3, 9, 15, 21]]))
tempResult = arange(24)
	
===================================================================	
TestIQR.test_basic: 1215	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestCorrSpearmanr.test_nan_policy: 298	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
test_spearmanr: 440	
----------------------------	

x1 = [1, 2, 3, 4, 5]
x2 = [5, 6, 7, 8, 7]
expected = (0.8207826816681233, 0.0885870053135438)
res = scipy.stats.spearmanr(x1, x2)
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
attributes = ('correlation', 'pvalue')
res = scipy.stats.spearmanr(x1, x2)
check_named_results(res, attributes)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', RuntimeWarning)
    assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
numpy.random.seed(7546)
x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
corr = [[1.0, 0.3], [0.3, 1.0]]
x = numpy.dot(numpy.linalg.cholesky(corr), x)
expected = (0.28659685838743354, 6.579862219051161e-11)
res = scipy.stats.spearmanr(x[0], x[1])
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
tempResult = arange(10.0)
	
===================================================================	
test_spearmanr: 446	
----------------------------	

x1 = [1, 2, 3, 4, 5]
x2 = [5, 6, 7, 8, 7]
expected = (0.8207826816681233, 0.0885870053135438)
res = scipy.stats.spearmanr(x1, x2)
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
attributes = ('correlation', 'pvalue')
res = scipy.stats.spearmanr(x1, x2)
check_named_results(res, attributes)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', RuntimeWarning)
    assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
numpy.random.seed(7546)
x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
corr = [[1.0, 0.3], [0.3, 1.0]]
x = numpy.dot(numpy.linalg.cholesky(corr), x)
expected = (0.28659685838743354, 6.579862219051161e-11)
res = scipy.stats.spearmanr(x[0], x[1])
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
x = numpy.arange(10.0)
x[9] = numpy.nan
assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
tempResult = arange(10.0)
	
===================================================================	
test_spearmanr: 447	
----------------------------	

x1 = [1, 2, 3, 4, 5]
x2 = [5, 6, 7, 8, 7]
expected = (0.8207826816681233, 0.0885870053135438)
res = scipy.stats.spearmanr(x1, x2)
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
attributes = ('correlation', 'pvalue')
res = scipy.stats.spearmanr(x1, x2)
check_named_results(res, attributes)
with warnings.catch_warnings():
    warnings.simplefilter('ignore', RuntimeWarning)
    assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
    assert_equal(scipy.stats.spearmanr([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.spearmanr([], []), (numpy.nan, numpy.nan))
numpy.random.seed(7546)
x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
corr = [[1.0, 0.3], [0.3, 1.0]]
x = numpy.dot(numpy.linalg.cholesky(corr), x)
expected = (0.28659685838743354, 6.579862219051161e-11)
res = scipy.stats.spearmanr(x[0], x[1])
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
assert_approx_equal(scipy.stats.spearmanr([1, 1, 2], [1, 1, 2])[0], 1.0)
x = numpy.arange(10.0)
x[9] = numpy.nan
assert_array_equal(scipy.stats.spearmanr(x, x), (numpy.nan, numpy.nan))
assert_allclose(scipy.stats.spearmanr(x, x, nan_policy='omit'), (1.0, 0))
assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='raise')
assert_raises(ValueError, scipy.stats.spearmanr, x, x, nan_policy='foobar')
x = numpy.arange(10.0)
tempResult = arange(20.0)
	
===================================================================	
test_kendalltau: 490	
----------------------------	

x1 = [12, 2, 1, 12, 2]
x2 = [1, 4, 7, 1, 0]
expected = ((- 0.47140452079103173), 0.2827454599327748)
res = scipy.stats.kendalltau(x1, x2)
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
attributes = ('correlation', 'pvalue')
res = scipy.stats.kendalltau(x1, x2)
check_named_results(res, attributes)
assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
numpy.random.seed(7546)
x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
corr = [[1.0, 0.3], [0.3, 1.0]]
x = numpy.dot(numpy.linalg.cholesky(corr), x)
expected = (0.19291382765531062, 1.1337095377742629e-10)
res = scipy.stats.kendalltau(x[0], x[1])
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
tempResult = arange(10.0)
	
===================================================================	
test_kendalltau: 496	
----------------------------	

x1 = [12, 2, 1, 12, 2]
x2 = [1, 4, 7, 1, 0]
expected = ((- 0.47140452079103173), 0.2827454599327748)
res = scipy.stats.kendalltau(x1, x2)
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
attributes = ('correlation', 'pvalue')
res = scipy.stats.kendalltau(x1, x2)
check_named_results(res, attributes)
assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
numpy.random.seed(7546)
x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
corr = [[1.0, 0.3], [0.3, 1.0]]
x = numpy.dot(numpy.linalg.cholesky(corr), x)
expected = (0.19291382765531062, 1.1337095377742629e-10)
res = scipy.stats.kendalltau(x[0], x[1])
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
x = numpy.arange(10.0)
x[9] = numpy.nan
assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
tempResult = arange(10.0)
	
===================================================================	
test_kendalltau: 497	
----------------------------	

x1 = [12, 2, 1, 12, 2]
x2 = [1, 4, 7, 1, 0]
expected = ((- 0.47140452079103173), 0.2827454599327748)
res = scipy.stats.kendalltau(x1, x2)
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
attributes = ('correlation', 'pvalue')
res = scipy.stats.kendalltau(x1, x2)
check_named_results(res, attributes)
assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([2, 0, 2], [2, 2, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([2, 2, 2], [2, 0, 2]), (numpy.nan, numpy.nan))
assert_equal(scipy.stats.kendalltau([], []), (numpy.nan, numpy.nan))
numpy.random.seed(7546)
x = numpy.array([numpy.random.normal(loc=1, scale=1, size=500), numpy.random.normal(loc=1, scale=1, size=500)])
corr = [[1.0, 0.3], [0.3, 1.0]]
x = numpy.dot(numpy.linalg.cholesky(corr), x)
expected = (0.19291382765531062, 1.1337095377742629e-10)
res = scipy.stats.kendalltau(x[0], x[1])
assert_approx_equal(res[0], expected[0])
assert_approx_equal(res[1], expected[1])
assert_approx_equal(scipy.stats.kendalltau([1, 1, 2], [1, 1, 2])[0], 1.0)
x = numpy.arange(10.0)
x[9] = numpy.nan
assert_array_equal(scipy.stats.kendalltau(x, x), (numpy.nan, numpy.nan))
assert_allclose(scipy.stats.kendalltau(x, x, nan_policy='omit'), (1.0, 0.00017455009626808976), rtol=1e-06)
assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='raise')
assert_raises(ValueError, scipy.stats.kendalltau, x, x, nan_policy='foobar')
x = numpy.arange(10.0)
tempResult = arange(20.0)
	
===================================================================	
TestIQR.test_empty: 1234	
----------------------------	

assert_equal(scipy.stats.iqr([]), numpy.nan)
tempResult = arange(0)
	
===================================================================	
TestIQR.test_constant: 1247	
----------------------------	

x = numpy.ones((7, 4))
assert_equal(scipy.stats.iqr(x), 0.0)
assert_array_equal(scipy.stats.iqr(x, axis=0), numpy.zeros(4))
assert_array_equal(scipy.stats.iqr(x, axis=1), numpy.zeros(7))
with _numpy_version_warn_context_mgr('1.9.0a', RuntimeWarning, 4):
    assert_equal(scipy.stats.iqr(x, interpolation='linear'), 0.0)
    assert_equal(scipy.stats.iqr(x, interpolation='midpoint'), 0.0)
    assert_equal(scipy.stats.iqr(x, interpolation='nearest'), 0.0)
    assert_equal(scipy.stats.iqr(x, interpolation='lower'), 0.0)
    assert_equal(scipy.stats.iqr(x, interpolation='higher'), 0.0)
tempResult = arange(6)
	
===================================================================	
TestIQR.test_rng: 1304	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestFisherExact.test_raises: 265	
----------------------------	

tempResult = arange(6)
	
===================================================================	
test_ttest_rel: 1905	
----------------------------	

(tr, pr) = (0.8124859138916569, 0.41846234511362157)
tpr = ([tr, (- tr)], [pr, pr])
rvs1 = numpy.linspace(1, 100, 100)
rvs2 = numpy.linspace(1.01, 99.989, 100)
rvs1_2D = numpy.array([numpy.linspace(1, 100, 100), numpy.linspace(1.01, 99.989, 100)])
rvs2_2D = numpy.array([numpy.linspace(1.01, 99.989, 100), numpy.linspace(1, 100, 100)])
(t, p) = scipy.stats.ttest_rel(rvs1, rvs2, axis=0)
assert_array_almost_equal([t, p], (tr, pr))
(t, p) = scipy.stats.ttest_rel(rvs1_2D.T, rvs2_2D.T, axis=0)
assert_array_almost_equal([t, p], tpr)
(t, p) = scipy.stats.ttest_rel(rvs1_2D, rvs2_2D, axis=1)
assert_array_almost_equal([t, p], tpr)
with warnings.catch_warnings():
    warnings.filterwarnings('ignore', category=RuntimeWarning)
    (t, p) = scipy.stats.ttest_rel(4.0, 3.0)
assert_(numpy.isnan(t))
assert_(numpy.isnan(p))
attributes = ('statistic', 'pvalue')
res = scipy.stats.ttest_rel(rvs1, rvs2, axis=0)
check_named_results(res, attributes)
rvs1_3D = numpy.dstack([rvs1_2D, rvs1_2D, rvs1_2D])
rvs2_3D = numpy.dstack([rvs2_2D, rvs2_2D, rvs2_2D])
(t, p) = scipy.stats.ttest_rel(rvs1_3D, rvs2_3D, axis=1)
assert_array_almost_equal(numpy.abs(t), tr)
assert_array_almost_equal(numpy.abs(p), pr)
assert_equal(t.shape, (2, 3))
(t, p) = scipy.stats.ttest_rel(numpy.rollaxis(rvs1_3D, 2), numpy.rollaxis(rvs2_3D, 2), axis=2)
assert_array_almost_equal(numpy.abs(t), tr)
assert_array_almost_equal(numpy.abs(p), pr)
assert_equal(t.shape, (3, 2))
numpy.random.seed(12345678)
x = scipy.stats.norm.rvs(loc=5, scale=10, size=501)
x[500] = numpy.nan
y = (scipy.stats.norm.rvs(loc=5, scale=10, size=501) + scipy.stats.norm.rvs(scale=0.2, size=501))
y[500] = numpy.nan
with warnings.catch_warnings():
    warnings.filterwarnings('ignore', category=RuntimeWarning)
    assert_array_equal(scipy.stats.ttest_rel(x, x), (numpy.nan, numpy.nan))
    assert_array_almost_equal(scipy.stats.ttest_rel(x, y, nan_policy='omit'), (0.25299925303978066, 0.8003729814201519))
    assert_raises(ValueError, scipy.stats.ttest_rel, x, y, nan_policy='raise')
    assert_raises(ValueError, scipy.stats.ttest_rel, x, y, nan_policy='foobar')
    (t, p) = scipy.stats.ttest_rel([0, 0, 0], [1, 1, 1])
    assert_equal((numpy.abs(t), p), (numpy.inf, 0))
    assert_equal(scipy.stats.ttest_rel([0, 0, 0], [0, 0, 0]), (numpy.nan, numpy.nan))
olderr = numpy.seterr(all='ignore')
try:
    anan = numpy.array([[1, numpy.nan], [(- 1), 1]])
    assert_equal(scipy.stats.ttest_rel(anan, numpy.zeros((2, 2))), ([0, numpy.nan], [1, numpy.nan]))
finally:
    numpy.seterr(**olderr)
tempResult = arange(24)
	
===================================================================	
TestTrim.test_trim1: 2584	
----------------------------	

tempResult = arange(11)
	
===================================================================	
TestTrim.test_trim1: 2585	
----------------------------	

a = numpy.arange(11)
tempResult = arange(10)
	
===================================================================	
TestTrim.test_trim1: 2586	
----------------------------	

a = numpy.arange(11)
assert_equal(numpy.sort(scipy.stats.trim1(a, 0.1)), numpy.arange(10))
tempResult = arange(9)
	
===================================================================	
TestTrim.test_trim1: 2587	
----------------------------	

a = numpy.arange(11)
assert_equal(numpy.sort(scipy.stats.trim1(a, 0.1)), numpy.arange(10))
assert_equal(numpy.sort(scipy.stats.trim1(a, 0.2)), numpy.arange(9))
tempResult = arange(2, 11)
	
===================================================================	
TestTrim.test_trim1: 2588	
----------------------------	

a = numpy.arange(11)
assert_equal(numpy.sort(scipy.stats.trim1(a, 0.1)), numpy.arange(10))
assert_equal(numpy.sort(scipy.stats.trim1(a, 0.2)), numpy.arange(9))
assert_equal(numpy.sort(scipy.stats.trim1(a, 0.2, tail='left')), numpy.arange(2, 11))
tempResult = arange(3, 11)
	
===================================================================	
TestMoments.test_moment: 1502	
----------------------------	

y = scipy.stats.moment(self.scalar_testcase)
assert_approx_equal(y, 0.0)
y = scipy.stats.moment(self.testcase, 0)
assert_approx_equal(y, 1.0)
y = scipy.stats.moment(self.testcase, 1)
assert_approx_equal(y, 0.0, 10)
y = scipy.stats.moment(self.testcase, 2)
assert_approx_equal(y, 1.25)
y = scipy.stats.moment(self.testcase, 3)
assert_approx_equal(y, 0.0)
y = scipy.stats.moment(self.testcase, 4)
assert_approx_equal(y, 2.5625)
y = scipy.stats.moment(self.testcase, [1, 2, 3, 4])
assert_allclose(y, [0, 1.25, 0, 2.5625])
y = scipy.stats.moment(self.testcase, 0.0)
assert_approx_equal(y, 1.0)
assert_raises(ValueError, scipy.stats.moment, self.testcase, 1.2)
y = scipy.stats.moment(self.testcase, [1.0, 2, 3, 4.0])
assert_allclose(y, [0, 1.25, 0, 2.5625])
y = scipy.stats.moment([])
assert_equal(y, numpy.nan)
tempResult = arange(10.0)
	
===================================================================	
test_normalitytests: 2212	
----------------------------	

(yield (assert_raises, ValueError, scipy.stats.skewtest, 4.0))
(yield (assert_raises, ValueError, scipy.stats.kurtosistest, 4.0))
(yield (assert_raises, ValueError, scipy.stats.normaltest, 4.0))
(st_normal, st_skew, st_kurt) = (3.92371918, 1.98078826, (- 0.01403734))
(pv_normal, pv_skew, pv_kurt) = (0.14059673, 0.04761502, 0.98880019)
x = (numpy.array((((- 2), (- 1), 0, 1, 2, 3) * 4)) ** 2)
attributes = ('statistic', 'pvalue')
(yield (assert_array_almost_equal, scipy.stats.normaltest(x), (st_normal, pv_normal)))
check_named_results(scipy.stats.normaltest(x), attributes)
(yield (assert_array_almost_equal, scipy.stats.skewtest(x), (st_skew, pv_skew)))
check_named_results(scipy.stats.skewtest(x), attributes)
(yield (assert_array_almost_equal, scipy.stats.kurtosistest(x), (st_kurt, pv_kurt)))
check_named_results(scipy.stats.kurtosistest(x), attributes)
(yield (assert_array_almost_equal, scipy.stats.normaltest(x, axis=None), (st_normal, pv_normal)))
(yield (assert_array_almost_equal, scipy.stats.skewtest(x, axis=None), (st_skew, pv_skew)))
(yield (assert_array_almost_equal, scipy.stats.kurtosistest(x, axis=None), (st_kurt, pv_kurt)))
tempResult = arange(10.0)
	
===================================================================	
test_normalitytests: 2219	
----------------------------	

(yield (assert_raises, ValueError, scipy.stats.skewtest, 4.0))
(yield (assert_raises, ValueError, scipy.stats.kurtosistest, 4.0))
(yield (assert_raises, ValueError, scipy.stats.normaltest, 4.0))
(st_normal, st_skew, st_kurt) = (3.92371918, 1.98078826, (- 0.01403734))
(pv_normal, pv_skew, pv_kurt) = (0.14059673, 0.04761502, 0.98880019)
x = (numpy.array((((- 2), (- 1), 0, 1, 2, 3) * 4)) ** 2)
attributes = ('statistic', 'pvalue')
(yield (assert_array_almost_equal, scipy.stats.normaltest(x), (st_normal, pv_normal)))
check_named_results(scipy.stats.normaltest(x), attributes)
(yield (assert_array_almost_equal, scipy.stats.skewtest(x), (st_skew, pv_skew)))
check_named_results(scipy.stats.skewtest(x), attributes)
(yield (assert_array_almost_equal, scipy.stats.kurtosistest(x), (st_kurt, pv_kurt)))
check_named_results(scipy.stats.kurtosistest(x), attributes)
(yield (assert_array_almost_equal, scipy.stats.normaltest(x, axis=None), (st_normal, pv_normal)))
(yield (assert_array_almost_equal, scipy.stats.skewtest(x, axis=None), (st_skew, pv_skew)))
(yield (assert_array_almost_equal, scipy.stats.kurtosistest(x, axis=None), (st_kurt, pv_kurt)))
x = numpy.arange(10.0)
x[9] = numpy.nan
assert_array_equal(scipy.stats.skewtest(x), (numpy.nan, numpy.nan))
expected = (1.018464355396213, 0.308457331951535)
assert_array_almost_equal(scipy.stats.skewtest(x, nan_policy='omit'), expected)
assert_raises(ValueError, scipy.stats.skewtest, x, nan_policy='raise')
assert_raises(ValueError, scipy.stats.skewtest, x, nan_policy='foobar')
tempResult = arange(30.0)
	
===================================================================	
TestRegression.test_nan_input: 733	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
test_kurtosistest_too_few_samples: 2267	
----------------------------	

tempResult = arange(4.0)
	
===================================================================	
TestIQR.test_scale: 1431	
----------------------------	

numpy_version = NumpyVersion(numpy.__version__)
tempResult = arange(15.0)
	
===================================================================	
TestMoments.test_kurtosis: 1567	
----------------------------	

y = scipy.stats.kurtosis(self.scalar_testcase)
assert_approx_equal(y, (- 3.0))
y = scipy.stats.kurtosis(self.testmathworks, 0, fisher=0, bias=1)
assert_approx_equal(y, 2.1658856802973, 10)
y = scipy.stats.kurtosis(self.testmathworks, fisher=0, bias=0)
assert_approx_equal(y, 3.663542721189047, 10)
y = scipy.stats.kurtosis(self.testcase, 0, 0)
assert_approx_equal(y, 1.64)
tempResult = arange(10.0)
	
===================================================================	
TestIQR.test_axis: 1287	
----------------------------	

o = numpy.random.normal(size=(71, 23))
x = numpy.dstack(([o] * 10))
q = scipy.stats.iqr(o)
assert_equal(scipy.stats.iqr(x, axis=(0, 1)), q)
x = numpy.rollaxis(x, (- 1), 0)
assert_equal(scipy.stats.iqr(x, axis=(2, 1)), q)
x = x.swapaxes(0, 1)
assert_equal(scipy.stats.iqr(x, axis=(0, 2)), q)
x = x.swapaxes(0, 1)
assert_equal(scipy.stats.iqr(x, axis=(0, 1, 2)), scipy.stats.iqr(x, axis=None))
assert_equal(scipy.stats.iqr(x, axis=(0,)), scipy.stats.iqr(x, axis=0))
tempResult = arange((((3 * 5) * 7) * 11))
	
===================================================================	
TestMoments.test_variation: 1520	
----------------------------	

y = scipy.stats.variation(self.scalar_testcase)
assert_approx_equal(y, 0.0)
y = scipy.stats.variation(self.testcase)
assert_approx_equal(y, 0.44721359549996, 10)
tempResult = arange(10.0)
	
===================================================================	
verify_gauss_quad: 241	
----------------------------	

(x, w, mu) = root_func(N, True)
tempResult = arange(N)
	
***************************************************	
sklearn_sklearn-0.18.0: 324	
===================================================================	
_CalibratedClassifier._preproc: 105	
----------------------------	

n_classes = len(self.classes_)
if hasattr(self.base_estimator, 'decision_function'):
    df = self.base_estimator.decision_function(X)
    if (df.ndim == 1):
        df = df[:, numpy.newaxis]
elif hasattr(self.base_estimator, 'predict_proba'):
    df = self.base_estimator.predict_proba(X)
    if (n_classes == 2):
        df = df[:, 1:]
else:
    raise RuntimeError('classifier has no decision_function or predict_proba method.')
tempResult = arange(df.shape[1])
	
===================================================================	
_shuffle: 575	
----------------------------	

'Return a shuffled copy of y eventually shuffle among same labels.'
if (labels is None):
    ind = random_state.permutation(len(y))
else:
    tempResult = arange(len(labels))
	
===================================================================	
_PartitionIterator.__iter__: 37	
----------------------------	

tempResult = arange(self.n)
	
===================================================================	
cross_val_predict: 439	
----------------------------	

"Generate cross-validated estimates for each input data point\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit' and 'predict'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be, for example a list, or an array at least 2d.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross-validation,\n        - integer, to specify the number of folds.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train/test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_jobs : integer, optional\n        The number of CPUs to use to do the computation. -1 means\n        'all CPUs'.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    Returns\n    -------\n    preds : ndarray\n        This is the result of calling 'predict'\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.cross_validation import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y)\n    "
(X, y) = indexable(X, y)
cv = check_cv(cv, X, y, classifier=is_classifier(estimator))
parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
preds_blocks = parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params) for (train, test) in cv))
preds = [p for (p, _) in preds_blocks]
locs = numpy.concatenate([loc for (_, loc) in preds_blocks])
if (not _check_is_partition(locs, _num_samples(X))):
    raise ValueError('cross_val_predict only works for partitions')
inv_locs = numpy.empty(len(locs), dtype=int)
tempResult = arange(len(locs))
	
===================================================================	
KFold.__init__: 110	
----------------------------	

super(KFold, self).__init__(n, n_folds, shuffle, random_state)
tempResult = arange(n)
	
===================================================================	
_fit_ovo_binary: 189	
----------------------------	

'Fit a single binary estimator (one-vs-one).'
cond = numpy.logical_or((y == i), (y == j))
y = y[cond]
y_binary = numpy.empty(y.shape, numpy.int)
y_binary[(y == i)] = 0
y_binary[(y == j)] = 1
tempResult = arange(X.shape[0])
	
===================================================================	
affinity_propagation: 27	
----------------------------	

'Perform Affinity Propagation Clustering of data\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n\n    S : array-like, shape (n_samples, n_samples)\n        Matrix of similarities between points\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations\n\n    damping : float, optional, default: 0.5\n        Damping factor between 0.5 and 1.\n\n    copy : boolean, optional, default: True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency\n\n    verbose : boolean, optional, default: False\n        The verbosity level\n\n    return_n_iter : bool, default False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n\n    cluster_centers_indices : array, shape (n_clusters,)\n        index of clusters centers\n\n    labels : array, shape (n_samples,)\n        cluster labels for each point\n\n    n_iter : int\n        number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    See examples/cluster/plot_affinity_propagation.py for an example.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages\n    Between Data Points", Science Feb. 2007\n    '
S = as_float_array(S, copy=copy)
n_samples = S.shape[0]
if (S.shape[0] != S.shape[1]):
    raise ValueError(('S must be a square array (shape=%s)' % repr(S.shape)))
if (preference is None):
    preference = numpy.median(S)
if ((damping < 0.5) or (damping >= 1)):
    raise ValueError('damping must be >= 0.5 and < 1')
random_state = numpy.random.RandomState(0)
S.flat[::(n_samples + 1)] = preference
A = numpy.zeros((n_samples, n_samples))
R = numpy.zeros((n_samples, n_samples))
tmp = numpy.zeros((n_samples, n_samples))
S += (((np.finfo(np.double).eps * S) + (np.finfo(np.double).tiny * 100)) * random_state.randn(n_samples, n_samples))
e = numpy.zeros((n_samples, convergence_iter))
tempResult = arange(n_samples)
	
===================================================================	
affinity_propagation: 65	
----------------------------	

'Perform Affinity Propagation Clustering of data\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n\n    S : array-like, shape (n_samples, n_samples)\n        Matrix of similarities between points\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations\n\n    damping : float, optional, default: 0.5\n        Damping factor between 0.5 and 1.\n\n    copy : boolean, optional, default: True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency\n\n    verbose : boolean, optional, default: False\n        The verbosity level\n\n    return_n_iter : bool, default False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n\n    cluster_centers_indices : array, shape (n_clusters,)\n        index of clusters centers\n\n    labels : array, shape (n_samples,)\n        cluster labels for each point\n\n    n_iter : int\n        number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    See examples/cluster/plot_affinity_propagation.py for an example.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages\n    Between Data Points", Science Feb. 2007\n    '
S = as_float_array(S, copy=copy)
n_samples = S.shape[0]
if (S.shape[0] != S.shape[1]):
    raise ValueError(('S must be a square array (shape=%s)' % repr(S.shape)))
if (preference is None):
    preference = numpy.median(S)
if ((damping < 0.5) or (damping >= 1)):
    raise ValueError('damping must be >= 0.5 and < 1')
random_state = numpy.random.RandomState(0)
S.flat[::(n_samples + 1)] = preference
A = numpy.zeros((n_samples, n_samples))
R = numpy.zeros((n_samples, n_samples))
tmp = numpy.zeros((n_samples, n_samples))
S += (((np.finfo(np.double).eps * S) + (np.finfo(np.double).tiny * 100)) * random_state.randn(n_samples, n_samples))
e = numpy.zeros((n_samples, convergence_iter))
ind = numpy.arange(n_samples)
for it in range(max_iter):
    numpy.add(A, S, tmp)
    I = numpy.argmax(tmp, axis=1)
    Y = tmp[(ind, I)]
    tmp[(ind, I)] = (- numpy.inf)
    Y2 = numpy.max(tmp, axis=1)
    numpy.subtract(S, Y[:, None], tmp)
    tmp[(ind, I)] = (S[(ind, I)] - Y2)
    tmp *= (1 - damping)
    R *= damping
    R += tmp
    numpy.maximum(R, 0, tmp)
    tmp.flat[::(n_samples + 1)] = R.flat[::(n_samples + 1)]
    tmp -= numpy.sum(tmp, axis=0)
    dA = np.diag(tmp).copy()
    tmp.clip(0, numpy.inf, tmp)
    tmp.flat[::(n_samples + 1)] = dA
    tmp *= (1 - damping)
    A *= damping
    A -= tmp
    E = ((numpy.diag(A) + numpy.diag(R)) > 0)
    e[:, (it % convergence_iter)] = E
    K = numpy.sum(E, axis=0)
    if (it >= convergence_iter):
        se = numpy.sum(e, axis=1)
        unconverged = (numpy.sum(((se == convergence_iter) + (se == 0))) != n_samples)
        if (((not unconverged) and (K > 0)) or (it == max_iter)):
            if verbose:
                print(('Converged after %d iterations.' % it))
            break
else:
    if verbose:
        print('Did not converge')
I = numpy.where((numpy.diag((A + R)) > 0))[0]
K = I.size
if (K > 0):
    c = numpy.argmax(S[:, I], axis=1)
    tempResult = arange(K)
	
===================================================================	
affinity_propagation: 71	
----------------------------	

'Perform Affinity Propagation Clustering of data\n\n    Read more in the :ref:`User Guide <affinity_propagation>`.\n\n    Parameters\n    ----------\n\n    S : array-like, shape (n_samples, n_samples)\n        Matrix of similarities between points\n\n    preference : array-like, shape (n_samples,) or float, optional\n        Preferences for each point - points with larger values of\n        preferences are more likely to be chosen as exemplars. The number of\n        exemplars, i.e. of clusters, is influenced by the input preferences\n        value. If the preferences are not passed as arguments, they will be\n        set to the median of the input similarities (resulting in a moderate\n        number of clusters). For a smaller amount of clusters, this can be set\n        to the minimum value of the similarities.\n\n    convergence_iter : int, optional, default: 15\n        Number of iterations with no change in the number\n        of estimated clusters that stops the convergence.\n\n    max_iter : int, optional, default: 200\n        Maximum number of iterations\n\n    damping : float, optional, default: 0.5\n        Damping factor between 0.5 and 1.\n\n    copy : boolean, optional, default: True\n        If copy is False, the affinity matrix is modified inplace by the\n        algorithm, for memory efficiency\n\n    verbose : boolean, optional, default: False\n        The verbosity level\n\n    return_n_iter : bool, default False\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n\n    cluster_centers_indices : array, shape (n_clusters,)\n        index of clusters centers\n\n    labels : array, shape (n_samples,)\n        cluster labels for each point\n\n    n_iter : int\n        number of iterations run. Returned only if `return_n_iter` is\n        set to True.\n\n    Notes\n    -----\n    See examples/cluster/plot_affinity_propagation.py for an example.\n\n    References\n    ----------\n    Brendan J. Frey and Delbert Dueck, "Clustering by Passing Messages\n    Between Data Points", Science Feb. 2007\n    '
S = as_float_array(S, copy=copy)
n_samples = S.shape[0]
if (S.shape[0] != S.shape[1]):
    raise ValueError(('S must be a square array (shape=%s)' % repr(S.shape)))
if (preference is None):
    preference = numpy.median(S)
if ((damping < 0.5) or (damping >= 1)):
    raise ValueError('damping must be >= 0.5 and < 1')
random_state = numpy.random.RandomState(0)
S.flat[::(n_samples + 1)] = preference
A = numpy.zeros((n_samples, n_samples))
R = numpy.zeros((n_samples, n_samples))
tmp = numpy.zeros((n_samples, n_samples))
S += (((np.finfo(np.double).eps * S) + (np.finfo(np.double).tiny * 100)) * random_state.randn(n_samples, n_samples))
e = numpy.zeros((n_samples, convergence_iter))
ind = numpy.arange(n_samples)
for it in range(max_iter):
    numpy.add(A, S, tmp)
    I = numpy.argmax(tmp, axis=1)
    Y = tmp[(ind, I)]
    tmp[(ind, I)] = (- numpy.inf)
    Y2 = numpy.max(tmp, axis=1)
    numpy.subtract(S, Y[:, None], tmp)
    tmp[(ind, I)] = (S[(ind, I)] - Y2)
    tmp *= (1 - damping)
    R *= damping
    R += tmp
    numpy.maximum(R, 0, tmp)
    tmp.flat[::(n_samples + 1)] = R.flat[::(n_samples + 1)]
    tmp -= numpy.sum(tmp, axis=0)
    dA = np.diag(tmp).copy()
    tmp.clip(0, numpy.inf, tmp)
    tmp.flat[::(n_samples + 1)] = dA
    tmp *= (1 - damping)
    A *= damping
    A -= tmp
    E = ((numpy.diag(A) + numpy.diag(R)) > 0)
    e[:, (it % convergence_iter)] = E
    K = numpy.sum(E, axis=0)
    if (it >= convergence_iter):
        se = numpy.sum(e, axis=1)
        unconverged = (numpy.sum(((se == convergence_iter) + (se == 0))) != n_samples)
        if (((not unconverged) and (K > 0)) or (it == max_iter)):
            if verbose:
                print(('Converged after %d iterations.' % it))
            break
else:
    if verbose:
        print('Did not converge')
I = numpy.where((numpy.diag((A + R)) > 0))[0]
K = I.size
if (K > 0):
    c = numpy.argmax(S[:, I], axis=1)
    c[I] = numpy.arange(K)
    for k in range(K):
        ii = numpy.where((c == k))[0]
        j = numpy.argmax(numpy.sum(S[(ii[:, numpy.newaxis], ii)], axis=0))
        I[k] = ii[j]
    c = numpy.argmax(S[:, I], axis=1)
    tempResult = arange(K)
	
===================================================================	
Birch._global_clustering: 272	
----------------------------	

'\n        Global clustering for the subclusters obtained after fitting\n        '
clusterer = self.n_clusters
centroids = self.subcluster_centers_
compute_labels = ((X is not None) and self.compute_labels)
not_enough_centroids = False
if isinstance(clusterer, int):
    clusterer = AgglomerativeClustering(n_clusters=self.n_clusters)
    if (len(centroids) < self.n_clusters):
        not_enough_centroids = True
elif ((clusterer is not None) and (not hasattr(clusterer, 'fit_predict'))):
    raise ValueError('n_clusters should be an instance of ClusterMixin or an int')
self._subcluster_norms = row_norms(self.subcluster_centers_, squared=True)
if ((clusterer is None) or not_enough_centroids):
    tempResult = arange(len(centroids))
	
===================================================================	
dbscan: 25	
----------------------------	

'Perform DBSCAN clustering from vector array or distance matrix.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    X : array or sparse (CSR) matrix of shape (n_samples, n_features), or             array of shape (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric=\'precomputed\'``.\n\n    eps : float, optional\n        The maximum distance between two samples for them to be considered\n        as in the same neighborhood.\n\n    min_samples : int, optional\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : string, or callable\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is "precomputed", X is assumed to be a distance matrix and\n        must be square. X may be a sparse matrix, in which case only "nonzero"\n        elements may be considered neighbors for DBSCAN.\n\n    algorithm : {\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'}, optional\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, optional\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    sample_weight : array, shape (n_samples,), optional\n        Weight of each sample, such that a sample with a weight of at least\n        ``min_samples`` is by itself a core sample; a sample with negative\n        weight may inhibit its eps-neighbor from being core.\n        Note that weights are absolute, and default to 1.\n\n    n_jobs : int, optional (default = 1)\n        The number of parallel jobs to run for neighbors search.\n        If ``-1``, then the number of jobs is set to the number of CPU cores.\n\n    Returns\n    -------\n    core_samples : array [n_core_samples]\n        Indices of core samples.\n\n    labels : array [n_samples]\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    Notes\n    -----\n    See examples/cluster/plot_dbscan.py for an example.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n).\n\n    Sparse neighborhoods can be precomputed using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>`\n    with ``mode=\'distance\'``.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n    '
if (not (eps > 0.0)):
    raise ValueError('eps must be positive.')
X = check_array(X, accept_sparse='csr')
if (sample_weight is not None):
    sample_weight = numpy.asarray(sample_weight)
    check_consistent_length(X, sample_weight)
if ((metric == 'precomputed') and scipy.sparse.issparse(X)):
    neighborhoods = numpy.empty(X.shape[0], dtype=object)
    X.sum_duplicates()
    X_mask = (X.data <= eps)
    masked_indices = astype(X.indices, numpy.intp, copy=False)[X_mask]
    masked_indptr = numpy.cumsum(X_mask)[(X.indptr[1:] - 1)]
    tempResult = arange(X.shape[0])
	
===================================================================	
dbscan: 26	
----------------------------	

'Perform DBSCAN clustering from vector array or distance matrix.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    X : array or sparse (CSR) matrix of shape (n_samples, n_features), or             array of shape (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric=\'precomputed\'``.\n\n    eps : float, optional\n        The maximum distance between two samples for them to be considered\n        as in the same neighborhood.\n\n    min_samples : int, optional\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : string, or callable\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by metrics.pairwise.pairwise_distances for its\n        metric parameter.\n        If metric is "precomputed", X is assumed to be a distance matrix and\n        must be square. X may be a sparse matrix, in which case only "nonzero"\n        elements may be considered neighbors for DBSCAN.\n\n    algorithm : {\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'}, optional\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, optional (default = 30)\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, optional\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    sample_weight : array, shape (n_samples,), optional\n        Weight of each sample, such that a sample with a weight of at least\n        ``min_samples`` is by itself a core sample; a sample with negative\n        weight may inhibit its eps-neighbor from being core.\n        Note that weights are absolute, and default to 1.\n\n    n_jobs : int, optional (default = 1)\n        The number of parallel jobs to run for neighbors search.\n        If ``-1``, then the number of jobs is set to the number of CPU cores.\n\n    Returns\n    -------\n    core_samples : array [n_core_samples]\n        Indices of core samples.\n\n    labels : array [n_samples]\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    Notes\n    -----\n    See examples/cluster/plot_dbscan.py for an example.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n).\n\n    Sparse neighborhoods can be precomputed using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>`\n    with ``mode=\'distance\'``.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise".\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n    '
if (not (eps > 0.0)):
    raise ValueError('eps must be positive.')
X = check_array(X, accept_sparse='csr')
if (sample_weight is not None):
    sample_weight = numpy.asarray(sample_weight)
    check_consistent_length(X, sample_weight)
if ((metric == 'precomputed') and scipy.sparse.issparse(X)):
    neighborhoods = numpy.empty(X.shape[0], dtype=object)
    X.sum_duplicates()
    X_mask = (X.data <= eps)
    masked_indices = astype(X.indices, numpy.intp, copy=False)[X_mask]
    masked_indptr = numpy.cumsum(X_mask)[(X.indptr[1:] - 1)]
    masked_indices = numpy.insert(masked_indices, masked_indptr, numpy.arange(X.shape[0]))
    tempResult = arange(1, X.shape[0])
	
===================================================================	
linkage_tree: 188	
----------------------------	

'Linkage agglomerative clustering based on a Feature matrix.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array, shape (n_samples, n_features)\n        feature matrix representing n_samples samples to be clustered\n\n    connectivity : sparse matrix (optional).\n        connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is None, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int (optional)\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. In this case, the\n        complete tree is not computed, thus the \'children\' output is of\n        limited use, and the \'parents\' output should rather be used.\n        This option is valid only when specifying a connectivity matrix.\n\n    linkage : {"average", "complete"}, optional, default: "complete"\n        Which linkage criteria to use. The linkage criterion determines which\n        distance to use between sets of observation.\n            - average uses the average of the distances of each observation of\n              the two sets\n            - complete or maximum linkage uses the maximum distances between\n              all observations of the two sets.\n\n    affinity : string or callable, optional, default: "euclidean".\n        which metric to use. Can be "euclidean", "manhattan", or any\n        distance know to paired distance (see metric.pairwise)\n\n    return_distance : bool, default False\n        whether or not to return the distances between the clusters.\n\n    Returns\n    -------\n    children : 2D array, shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    n_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree.\n\n    parents : 1D array, shape (n_nodes, ) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere \'None\' is returned.\n\n    distances : ndarray, shape (n_nodes-1,)\n        Returned when return_distance is set to True.\n\n        distances[i] refers to the distance between children[i][0] and\n        children[i][1] when they are merged.\n\n    See also\n    --------\n    ward_tree : hierarchical clustering with ward linkage\n    '
X = numpy.asarray(X)
if (X.ndim == 1):
    X = numpy.reshape(X, ((- 1), 1))
(n_samples, n_features) = X.shape
linkage_choices = {'complete': _hierarchical.max_merge, 'average': _hierarchical.average_merge}
try:
    join_func = linkage_choices[linkage]
except KeyError:
    raise ValueError(('Unknown linkage option, linkage should be one of %s, but %s was given' % (linkage_choices.keys(), linkage)))
if (connectivity is None):
    from scipy.cluster import hierarchy
    if (n_clusters is not None):
        warnings.warn('Partial build of the tree is implemented only for structured clustering (i.e. with explicit connectivity). The algorithm will build the full tree and only retain the lower branches required for the specified number of clusters', stacklevel=2)
    if (affinity == 'precomputed'):
        (i, j) = numpy.triu_indices(X.shape[0], k=1)
        X = X[(i, j)]
    elif (affinity == 'l2'):
        affinity = 'euclidean'
    elif (affinity in ('l1', 'manhattan')):
        affinity = 'cityblock'
    elif callable(affinity):
        X = affinity(X)
        (i, j) = numpy.triu_indices(X.shape[0], k=1)
        X = X[(i, j)]
    out = scipy.cluster.hierarchy.linkage(X, method=linkage, metric=affinity)
    children_ = out[:, :2].astype(numpy.int)
    if return_distance:
        distances = out[:, 2]
        return (children_, 1, n_samples, None, distances)
    return (children_, 1, n_samples, None)
(connectivity, n_components) = _fix_connectivity(X, connectivity)
connectivity = connectivity.tocoo()
diag_mask = (connectivity.row != connectivity.col)
connectivity.row = connectivity.row[diag_mask]
connectivity.col = connectivity.col[diag_mask]
connectivity.data = connectivity.data[diag_mask]
del diag_mask
if (affinity == 'precomputed'):
    distances = X[(connectivity.row, connectivity.col)]
else:
    distances = paired_distances(X[connectivity.row], X[connectivity.col], metric=affinity)
connectivity.data = distances
if (n_clusters is None):
    n_nodes = ((2 * n_samples) - 1)
else:
    assert (n_clusters <= n_samples)
    n_nodes = ((2 * n_samples) - n_clusters)
if return_distance:
    distances = numpy.empty((n_nodes - n_samples))
A = numpy.empty(n_nodes, dtype=object)
inertia = list()
connectivity = connectivity.tolil()
for (ind, (data, row)) in enumerate(zip(connectivity.data, connectivity.rows)):
    A[ind] = IntFloatDict(numpy.asarray(row, dtype=numpy.intp), numpy.asarray(data, dtype=numpy.float64))
    inertia.extend((_hierarchical.WeightedEdge(d, ind, r) for (r, d) in zip(row, data) if (r < ind)))
del connectivity
heapify(inertia)
tempResult = arange(n_nodes, dtype=numpy.intp)
	
===================================================================	
ward_tree: 88	
----------------------------	

"Ward clustering based on a Feature matrix.\n\n    Recursively merges the pair of clusters that minimally increases\n    within-cluster variance.\n\n    The inertia matrix uses a Heapq-based representation.\n\n    This is the structured version, that takes into account some topological\n    structure between samples.\n\n    Read more in the :ref:`User Guide <hierarchical_clustering>`.\n\n    Parameters\n    ----------\n    X : array, shape (n_samples, n_features)\n        feature matrix  representing n_samples samples to be clustered\n\n    connectivity : sparse matrix (optional).\n        connectivity matrix. Defines for each sample the neighboring samples\n        following a given structure of the data. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        Default is None, i.e, the Ward algorithm is unstructured.\n\n    n_clusters : int (optional)\n        Stop early the construction of the tree at n_clusters. This is\n        useful to decrease computation time if the number of clusters is\n        not small compared to the number of samples. In this case, the\n        complete tree is not computed, thus the 'children' output is of\n        limited use, and the 'parents' output should rather be used.\n        This option is valid only when specifying a connectivity matrix.\n\n    return_distance: bool (optional)\n        If True, return the distance between the clusters.\n\n    Returns\n    -------\n    children : 2D array, shape (n_nodes-1, 2)\n        The children of each non-leaf node. Values less than `n_samples`\n        correspond to leaves of the tree which are the original samples.\n        A node `i` greater than or equal to `n_samples` is a non-leaf\n        node and has children `children_[i - n_samples]`. Alternatively\n        at the i-th iteration, children[i][0] and children[i][1]\n        are merged to form node `n_samples + i`\n\n    n_components : int\n        The number of connected components in the graph.\n\n    n_leaves : int\n        The number of leaves in the tree\n\n    parents : 1D array, shape (n_nodes, ) or None\n        The parent of each node. Only returned when a connectivity matrix\n        is specified, elsewhere 'None' is returned.\n\n    distances : 1D array, shape (n_nodes-1, )\n        Only returned if return_distance is set to True (for compatibility).\n        The distances between the centers of the nodes. `distances[i]`\n        corresponds to a weighted euclidean distance between\n        the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to\n        leaves of the tree, then `distances[i]` is their unweighted euclidean\n        distance. Distances are updated in the following way\n        (from scipy.hierarchy.linkage):\n\n        The new entry :math:`d(u,v)` is computed as follows,\n\n        .. math::\n\n           d(u,v) = \\sqrt{\\frac{|v|+|s|}\n                               {T}d(v,s)^2\n                        + \\frac{|v|+|t|}\n                               {T}d(v,t)^2\n                        - \\frac{|v|}\n                               {T}d(s,t)^2}\n\n        where :math:`u` is the newly joined cluster consisting of\n        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n        :math:`|*|` is the cardinality of its argument. This is also\n        known as the incremental algorithm.\n    "
X = numpy.asarray(X)
if (X.ndim == 1):
    X = numpy.reshape(X, ((- 1), 1))
(n_samples, n_features) = X.shape
if (connectivity is None):
    from scipy.cluster import hierarchy
    if (n_clusters is not None):
        warnings.warn('Partial build of the tree is implemented only for structured clustering (i.e. with explicit connectivity). The algorithm will build the full tree and only retain the lower branches required for the specified number of clusters', stacklevel=2)
    out = scipy.cluster.hierarchy.ward(X)
    children_ = out[:, :2].astype(numpy.intp)
    if return_distance:
        distances = out[:, 2]
        return (children_, 1, n_samples, None, distances)
    else:
        return (children_, 1, n_samples, None)
(connectivity, n_components) = _fix_connectivity(X, connectivity)
if (n_clusters is None):
    n_nodes = ((2 * n_samples) - 1)
else:
    if (n_clusters > n_samples):
        raise ValueError(('Cannot provide more clusters than samples. %i n_clusters was asked, and there are %i samples.' % (n_clusters, n_samples)))
    n_nodes = ((2 * n_samples) - n_clusters)
coord_row = []
coord_col = []
A = []
for (ind, row) in enumerate(connectivity.rows):
    A.append(row)
    row = [i for i in row if (i < ind)]
    coord_row.extend((len(row) * [ind]))
    coord_col.extend(row)
coord_row = numpy.array(coord_row, dtype=numpy.intp, order='C')
coord_col = numpy.array(coord_col, dtype=numpy.intp, order='C')
moments_1 = numpy.zeros(n_nodes, order='C')
moments_1[:n_samples] = 1
moments_2 = numpy.zeros((n_nodes, n_features), order='C')
moments_2[:n_samples] = X
inertia = numpy.empty(len(coord_row), dtype=numpy.float64, order='C')
_hierarchical.compute_ward_dist(moments_1, moments_2, coord_row, coord_col, inertia)
inertia = list(externals.six.moves.zip(inertia, coord_row, coord_col))
heapify(inertia)
tempResult = arange(n_nodes, dtype=numpy.intp)
	
===================================================================	
discretize: 43	
----------------------------	

'Search for a partition matrix (clustering) which is closest to the\n    eigenvector embedding.\n\n    Parameters\n    ----------\n    vectors : array-like, shape: (n_samples, n_clusters)\n        The embedding space of the samples.\n\n    copy : boolean, optional, default: True\n        Whether to copy vectors, or perform in-place normalization.\n\n    max_svd_restarts : int, optional, default: 30\n        Maximum number of attempts to restart SVD if convergence fails\n\n    n_iter_max : int, optional, default: 30\n        Maximum number of iterations to attempt in rotation and partition\n        matrix search if machine precision convergence is not reached\n\n    random_state: int seed, RandomState instance, or None (default)\n        A pseudo random number generator used for the initialization of the\n        of the rotation matrix\n\n    Returns\n    -------\n    labels : array of integers, shape: n_samples\n        The labels of the clusters.\n\n    References\n    ----------\n\n    - Multiclass spectral clustering, 2003\n      Stella X. Yu, Jianbo Shi\n      http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf\n\n    Notes\n    -----\n\n    The eigenvector embedding is used to iteratively search for the\n    closest discrete partition.  First, the eigenvector embedding is\n    normalized to the space of partition matrices. An optimal discrete\n    partition matrix closest to this normalized embedding multiplied by\n    an initial rotation is calculated.  Fixing this discrete partition\n    matrix, an optimal rotation matrix is calculated.  These two\n    calculations are performed until convergence.  The discrete partition\n    matrix is returned as the clustering solution.  Used in spectral\n    clustering, this method tends to be faster and more robust to random\n    initialization than k-means.\n\n    '
from scipy.sparse import csc_matrix
from scipy.linalg import LinAlgError
random_state = check_random_state(random_state)
vectors = as_float_array(vectors, copy=copy)
eps = np.finfo(float).eps
(n_samples, n_components) = vectors.shape
norm_ones = numpy.sqrt(n_samples)
for i in range(vectors.shape[1]):
    vectors[:, i] = ((vectors[:, i] / norm(vectors[:, i])) * norm_ones)
    if (vectors[(0, i)] != 0):
        vectors[:, i] = (((- 1) * vectors[:, i]) * numpy.sign(vectors[(0, i)]))
vectors = (vectors / numpy.sqrt((vectors ** 2).sum(axis=1))[:, numpy.newaxis])
svd_restarts = 0
has_converged = False
while ((svd_restarts < max_svd_restarts) and (not has_converged)):
    rotation = numpy.zeros((n_components, n_components))
    rotation[:, 0] = vectors[random_state.randint(n_samples), :].T
    c = numpy.zeros(n_samples)
    for j in range(1, n_components):
        c += numpy.abs(numpy.dot(vectors, rotation[:, (j - 1)]))
        rotation[:, j] = vectors[c.argmin(), :].T
    last_objective_value = 0.0
    n_iter = 0
    while (not has_converged):
        n_iter += 1
        t_discrete = numpy.dot(vectors, rotation)
        labels = t_discrete.argmax(axis=1)
        tempResult = arange(0, n_samples)
	
===================================================================	
test_get_submatrix: 31	
----------------------------	

tempResult = arange(20)
	
===================================================================	
test_errors: 153	
----------------------------	

tempResult = arange(25)
	
===================================================================	
test_errors: 171	
----------------------------	

data = np.arange(25).reshape((5, 5))
model = SpectralBiclustering(n_clusters=(3, 3, 3))
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering(n_clusters='abc')
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering(n_clusters=(3, 'abc'))
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering(method='unknown')
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering(svd_method='unknown')
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering(n_components=0)
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering(n_best=0)
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering(n_components=3, n_best=4)
assert_raises(ValueError, model.fit, data)
model = SpectralBiclustering()
tempResult = arange(27)
	
===================================================================	
test_birch_predict: 43	
----------------------------	

rng = numpy.random.RandomState(0)
X = generate_clustered_data(n_clusters=3, n_features=3, n_samples_per_cluster=10)
tempResult = arange(30)
	
===================================================================	
test_dbscan_core_samples_toy: 175	
----------------------------	

X = [[0], [2], [3], [4], [6], [8], [10]]
n_samples = len(X)
for algorithm in ['brute', 'kd_tree', 'ball_tree']:
    (core_samples, labels) = dbscan(X, algorithm=algorithm, eps=1, min_samples=1)
    tempResult = arange(n_samples)
	
===================================================================	
test_ward_linkage_tree_return_distance: 187	
----------------------------	

(n, p) = (10, 5)
rng = numpy.random.RandomState(0)
connectivity = numpy.ones((n, n))
for i in range(5):
    X = (0.1 * rng.normal(size=(n, p)))
    tempResult = arange(n)
	
===================================================================	
assess_same_labelling: 142	
----------------------------	

'Util for comparison with scipy'
co_clust = []
for cut in [cut1, cut2]:
    n = len(cut)
    k = (cut.max() + 1)
    ecut = numpy.zeros((n, k))
    tempResult = arange(n)
	
===================================================================	
test_ward_tree_children_order: 175	
----------------------------	

(n, p) = (10, 5)
rng = numpy.random.RandomState(0)
connectivity = numpy.ones((n, n))
for i in range(5):
    X = (0.1 * rng.normal(size=(n, p)))
    tempResult = arange(n)
	
===================================================================	
test_int_float_dict: 242	
----------------------------	

rng = numpy.random.RandomState(0)
keys = numpy.unique(rng.randint(100, size=10).astype(numpy.intp))
values = rng.rand(len(keys))
d = IntFloatDict(keys, values)
for (key, value) in zip(keys, values):
    assert (d[key] == value)
tempResult = arange(50)
	
===================================================================	
test_scikit_vs_scipy: 153	
----------------------------	

(n, p, k) = (10, 5, 3)
rng = numpy.random.RandomState(0)
connectivity = numpy.ones((n, n))
for linkage in sklearn.cluster.hierarchical._TREE_BUILDERS.keys():
    for i in range(5):
        X = (0.1 * rng.normal(size=(n, p)))
        tempResult = arange(n)
	
===================================================================	
test_predict_minibatch_kmeanspp_init_sparse_input: 374	
----------------------------	

mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, init='k-means++', n_init=10).fit(X_csr)
assert_array_equal(mb_k_means.predict(X_csr), mb_k_means.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_)
tempResult = arange(n_clusters)
	
===================================================================	
test_predict_minibatch_dense_input: 366	
----------------------------	

mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, random_state=40).fit(X)
pred = mb_k_means.predict(mb_k_means.cluster_centers_)
tempResult = arange(n_clusters)
	
===================================================================	
test_predict: 345	
----------------------------	

km = KMeans(n_clusters=n_clusters, random_state=42)
km.fit(X)
pred = km.predict(km.cluster_centers_)
tempResult = arange(n_clusters)
	
===================================================================	
test_predict_minibatch_random_init_sparse_input: 381	
----------------------------	

mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=10).fit(X_csr)
assert_array_equal(mb_k_means.predict(X_csr), mb_k_means.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_)
tempResult = arange(n_clusters)
	
===================================================================	
test_discretize: 111	
----------------------------	

random_state = numpy.random.RandomState(seed)
for n_samples in [50, 100, 150, 500]:
    for n_class in range(2, 10):
        y_true = random_state.randint(0, (n_class + 1), n_samples)
        y_true = numpy.array(y_true, numpy.float)
        tempResult = arange(n_samples)
	
===================================================================	
graph_lasso: 64	
----------------------------	

"l1-penalized covariance estimator\n\n    Read more in the :ref:`User Guide <sparse_inverse_covariance>`.\n\n    Parameters\n    ----------\n    emp_cov : 2D ndarray, shape (n_features, n_features)\n        Empirical covariance from which to compute the covariance estimate.\n\n    alpha : positive float\n        The regularization parameter: the higher alpha, the more\n        regularization, the sparser the inverse covariance.\n\n    cov_init : 2D array (n_features, n_features), optional\n        The initial guess for the covariance.\n\n    mode : {'cd', 'lars'}\n        The Lasso solver to use: coordinate descent or LARS. Use LARS for\n        very sparse underlying graphs, where p > n. Elsewhere prefer cd\n        which is more numerically stable.\n\n    tol : positive float, optional\n        The tolerance to declare convergence: if the dual gap goes below\n        this value, iterations are stopped.\n\n    enet_tol : positive float, optional\n        The tolerance for the elastic net solver used to calculate the descent\n        direction. This parameter controls the accuracy of the search direction\n        for a given column update, not of the overall parameter estimate. Only\n        used for mode='cd'.\n\n    max_iter : integer, optional\n        The maximum number of iterations.\n\n    verbose : boolean, optional\n        If verbose is True, the objective function and dual gap are\n        printed at each iteration.\n\n    return_costs : boolean, optional\n        If return_costs is True, the objective function and dual gap\n        at each iteration are returned.\n\n    eps : float, optional\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems.\n\n    return_n_iter : bool, optional\n        Whether or not to return the number of iterations.\n\n    Returns\n    -------\n    covariance : 2D ndarray, shape (n_features, n_features)\n        The estimated covariance matrix.\n\n    precision : 2D ndarray, shape (n_features, n_features)\n        The estimated (sparse) precision matrix.\n\n    costs : list of (objective, dual_gap) pairs\n        The list of values of the objective function and the dual gap at\n        each iteration. Returned only if return_costs is True.\n\n    n_iter : int\n        Number of iterations. Returned only if `return_n_iter` is set to True.\n\n    See Also\n    --------\n    GraphLasso, GraphLassoCV\n\n    Notes\n    -----\n    The algorithm employed to solve this problem is the GLasso algorithm,\n    from the Friedman 2008 Biostatistics paper. It is the same algorithm\n    as in the R `glasso` package.\n\n    One possible difference with the `glasso` R package is that the\n    diagonal coefficients are not penalized.\n\n    "
(_, n_features) = emp_cov.shape
if (alpha == 0):
    if return_costs:
        precision_ = scipy.linalg.inv(emp_cov)
        cost = ((- 2.0) * log_likelihood(emp_cov, precision_))
        cost += (n_features * numpy.log((2 * numpy.pi)))
        d_gap = (numpy.sum((emp_cov * precision_)) - n_features)
        if return_n_iter:
            return (emp_cov, precision_, (cost, d_gap), 0)
        else:
            return (emp_cov, precision_, (cost, d_gap))
    elif return_n_iter:
        return (emp_cov, scipy.linalg.inv(emp_cov), 0)
    else:
        return (emp_cov, scipy.linalg.inv(emp_cov))
if (cov_init is None):
    covariance_ = emp_cov.copy()
else:
    covariance_ = cov_init.copy()
covariance_ *= 0.95
diagonal = emp_cov.flat[::(n_features + 1)]
covariance_.flat[::(n_features + 1)] = diagonal
precision_ = pinvh(covariance_)
tempResult = arange(n_features)
	
===================================================================	
fast_mcd: 145	
----------------------------	

'Estimates the Minimum Covariance Determinant matrix.\n\n    Read more in the :ref:`User Guide <robust_covariance>`.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n      The data matrix, with p features and n samples.\n\n    support_fraction : float, 0 < support_fraction < 1\n          The proportion of points to be included in the support of the raw\n          MCD estimate. Default is None, which implies that the minimum\n          value of support_fraction will be used within the algorithm:\n          `[n_sample + n_features + 1] / 2`.\n\n    random_state : integer or numpy.RandomState, optional\n        The generator used to randomly subsample. If an integer is\n        given, it fixes the seed. Defaults to the global numpy random\n        number generator.\n\n    cov_computation_method : callable, default empirical_covariance\n        The function which will be used to compute the covariance.\n        Must return shape (n_features, n_features)\n\n    Notes\n    -----\n    The FastMCD algorithm has been introduced by Rousseuw and Van Driessen\n    in "A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n    1999, American Statistical Association and the American Society\n    for Quality, TECHNOMETRICS".\n    The principle is to compute robust estimates and random subsets before\n    pooling them into a larger subsets, and finally into the full data set.\n    Depending on the size of the initial sample, we have one, two or three\n    such computation levels.\n\n    Note that only raw estimates are returned. If one is interested in\n    the correction and reweighting steps described in [Rouseeuw1999]_,\n    see the MinCovDet object.\n\n    References\n    ----------\n\n    .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance\n        Determinant Estimator, 1999, American Statistical Association\n        and the American Society for Quality, TECHNOMETRICS\n\n    .. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n        Asymptotics For The Minimum Covariance Determinant Estimator,\n        The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400\n\n    Returns\n    -------\n    location : array-like, shape (n_features,)\n        Robust location of the data.\n\n    covariance : array-like, shape (n_features, n_features)\n        Robust covariance of the features.\n\n    support : array-like, type boolean, shape (n_samples,)\n        A mask of the observations that have been used to compute\n        the robust location and covariance estimates of the data set.\n\n    '
random_state = check_random_state(random_state)
X = check_array(X, ensure_min_samples=2, estimator='fast_mcd')
(n_samples, n_features) = X.shape
if (support_fraction is None):
    n_support = int(numpy.ceil((0.5 * ((n_samples + n_features) + 1))))
else:
    n_support = int((support_fraction * n_samples))
if (n_features == 1):
    if (n_support < n_samples):
        X_sorted = numpy.sort(numpy.ravel(X))
        diff = (X_sorted[n_support:] - X_sorted[:(n_samples - n_support)])
        halves_start = numpy.where((diff == numpy.min(diff)))[0]
        location = (0.5 * (X_sorted[(n_support + halves_start)] + X_sorted[halves_start]).mean())
        support = numpy.zeros(n_samples, dtype=bool)
        X_centered = (X - location)
        support[numpy.argsort(numpy.abs(X_centered), 0)[:n_support]] = True
        covariance = numpy.asarray([[numpy.var(X[support])]])
        location = numpy.array([location])
        precision = pinvh(covariance)
        dist = (np.dot(X_centered, precision) * X_centered).sum(axis=1)
    else:
        support = numpy.ones(n_samples, dtype=bool)
        covariance = numpy.asarray([[numpy.var(X)]])
        location = numpy.asarray([numpy.mean(X)])
        X_centered = (X - location)
        precision = pinvh(covariance)
        dist = (np.dot(X_centered, precision) * X_centered).sum(axis=1)
if ((n_samples > 500) and (n_features > 1)):
    n_subsets = (n_samples // 300)
    n_samples_subsets = (n_samples // n_subsets)
    samples_shuffle = random_state.permutation(n_samples)
    h_subset = int(numpy.ceil((n_samples_subsets * (n_support / float(n_samples)))))
    n_trials_tot = 500
    n_best_sub = 10
    n_trials = max(10, (n_trials_tot // n_subsets))
    n_best_tot = (n_subsets * n_best_sub)
    all_best_locations = numpy.zeros((n_best_tot, n_features))
    try:
        all_best_covariances = numpy.zeros((n_best_tot, n_features, n_features))
    except MemoryError:
        all_best_covariances = numpy.zeros((n_best_tot, n_features, n_features))
        n_best_tot = 10
        n_best_sub = 2
    for i in range(n_subsets):
        low_bound = (i * n_samples_subsets)
        high_bound = (low_bound + n_samples_subsets)
        current_subset = X[samples_shuffle[low_bound:high_bound]]
        (best_locations_sub, best_covariances_sub, _, _) = select_candidates(current_subset, h_subset, n_trials, select=n_best_sub, n_iter=2, cov_computation_method=cov_computation_method, random_state=random_state)
        tempResult = arange((i * n_best_sub), ((i + 1) * n_best_sub))
	
===================================================================	
test_ledoit_wolf: 107	
----------------------------	

X_centered = (X - X.mean(axis=0))
lw = LedoitWolf(assume_centered=True)
lw.fit(X_centered)
shrinkage_ = lw.shrinkage_
score_ = lw.score(X_centered)
assert_almost_equal(ledoit_wolf_shrinkage(X_centered, assume_centered=True), shrinkage_)
assert_almost_equal(ledoit_wolf_shrinkage(X_centered, assume_centered=True, block_size=6), shrinkage_)
(lw_cov_from_mle, lw_shinkrage_from_mle) = ledoit_wolf(X_centered, assume_centered=True)
assert_array_almost_equal(lw_cov_from_mle, lw.covariance_, 4)
assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)
scov = ShrunkCovariance(shrinkage=lw.shrinkage_, assume_centered=True)
scov.fit(X_centered)
assert_array_almost_equal(scov.covariance_, lw.covariance_, 4)
X_1d = X[:, 0].reshape(((- 1), 1))
lw = LedoitWolf(assume_centered=True)
lw.fit(X_1d)
(lw_cov_from_mle, lw_shinkrage_from_mle) = ledoit_wolf(X_1d, assume_centered=True)
assert_array_almost_equal(lw_cov_from_mle, lw.covariance_, 4)
assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)
assert_array_almost_equal(((X_1d ** 2).sum() / n_samples), lw.covariance_, 4)
lw = LedoitWolf(store_precision=False, assume_centered=True)
lw.fit(X_centered)
assert_almost_equal(lw.score(X_centered), score_, 4)
assert (lw.precision_ is None)
lw = LedoitWolf()
lw.fit(X)
assert_almost_equal(lw.shrinkage_, shrinkage_, 4)
assert_almost_equal(lw.shrinkage_, ledoit_wolf_shrinkage(X))
assert_almost_equal(lw.shrinkage_, ledoit_wolf(X)[1])
assert_almost_equal(lw.score(X), score_, 4)
(lw_cov_from_mle, lw_shinkrage_from_mle) = ledoit_wolf(X)
assert_array_almost_equal(lw_cov_from_mle, lw.covariance_, 4)
assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)
scov = ShrunkCovariance(shrinkage=lw.shrinkage_)
scov.fit(X)
assert_array_almost_equal(scov.covariance_, lw.covariance_, 4)
X_1d = X[:, 0].reshape(((- 1), 1))
lw = LedoitWolf()
lw.fit(X_1d)
(lw_cov_from_mle, lw_shinkrage_from_mle) = ledoit_wolf(X_1d)
assert_array_almost_equal(lw_cov_from_mle, lw.covariance_, 4)
assert_almost_equal(lw_shinkrage_from_mle, lw.shrinkage_)
assert_array_almost_equal(empirical_covariance(X_1d), lw.covariance_, 4)
tempResult = arange(5)
	
===================================================================	
test_covariance: 34	
----------------------------	

cov = EmpiricalCovariance()
cov.fit(X)
emp_cov = empirical_covariance(X)
assert_array_almost_equal(emp_cov, cov.covariance_, 4)
assert_almost_equal(cov.error_norm(emp_cov), 0)
assert_almost_equal(cov.error_norm(emp_cov, norm='spectral'), 0)
assert_almost_equal(cov.error_norm(emp_cov, norm='frobenius'), 0)
assert_almost_equal(cov.error_norm(emp_cov, scaling=False), 0)
assert_almost_equal(cov.error_norm(emp_cov, squared=False), 0)
assert_raises(NotImplementedError, cov.error_norm, emp_cov, norm='foo')
mahal_dist = cov.mahalanobis(X)
assert_greater(numpy.amin(mahal_dist), 0)
X_1d = X[:, 0].reshape(((- 1), 1))
cov = EmpiricalCovariance()
cov.fit(X_1d)
assert_array_almost_equal(empirical_covariance(X_1d), cov.covariance_, 4)
assert_almost_equal(cov.error_norm(empirical_covariance(X_1d)), 0)
assert_almost_equal(cov.error_norm(empirical_covariance(X_1d), norm='spectral'), 0)
tempResult = arange(5)
	
===================================================================	
test_oas: 185	
----------------------------	

X_centered = (X - X.mean(axis=0))
oa = OAS(assume_centered=True)
oa.fit(X_centered)
shrinkage_ = oa.shrinkage_
score_ = oa.score(X_centered)
(oa_cov_from_mle, oa_shinkrage_from_mle) = oas(X_centered, assume_centered=True)
assert_array_almost_equal(oa_cov_from_mle, oa.covariance_, 4)
assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)
scov = ShrunkCovariance(shrinkage=oa.shrinkage_, assume_centered=True)
scov.fit(X_centered)
assert_array_almost_equal(scov.covariance_, oa.covariance_, 4)
X_1d = X[:, 0:1]
oa = OAS(assume_centered=True)
oa.fit(X_1d)
(oa_cov_from_mle, oa_shinkrage_from_mle) = oas(X_1d, assume_centered=True)
assert_array_almost_equal(oa_cov_from_mle, oa.covariance_, 4)
assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)
assert_array_almost_equal(((X_1d ** 2).sum() / n_samples), oa.covariance_, 4)
oa = OAS(store_precision=False, assume_centered=True)
oa.fit(X_centered)
assert_almost_equal(oa.score(X_centered), score_, 4)
assert (oa.precision_ is None)
oa = OAS()
oa.fit(X)
assert_almost_equal(oa.shrinkage_, shrinkage_, 4)
assert_almost_equal(oa.score(X), score_, 4)
(oa_cov_from_mle, oa_shinkrage_from_mle) = oas(X)
assert_array_almost_equal(oa_cov_from_mle, oa.covariance_, 4)
assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)
scov = ShrunkCovariance(shrinkage=oa.shrinkage_)
scov.fit(X)
assert_array_almost_equal(scov.covariance_, oa.covariance_, 4)
X_1d = X[:, 0].reshape(((- 1), 1))
oa = OAS()
oa.fit(X_1d)
(oa_cov_from_mle, oa_shinkrage_from_mle) = oas(X_1d)
assert_array_almost_equal(oa_cov_from_mle, oa.covariance_, 4)
assert_almost_equal(oa_shinkrage_from_mle, oa.shrinkage_)
assert_array_almost_equal(empirical_covariance(X_1d), oa.covariance_, 4)
tempResult = arange(5)
	
===================================================================	
test_graph_lasso_iris_singular: 56	
----------------------------	

tempResult = arange(10, 13)
	
===================================================================	
test_fast_mcd_on_invalid_input: 24	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_mcd_class_on_invalid_input: 28	
----------------------------	

tempResult = arange(100)
	
===================================================================	
load_files: 72	
----------------------------	

"Load text files with categories as subfolder names.\n\n    Individual samples are assumed to be files stored a two levels folder\n    structure such as the following:\n\n        container_folder/\n            category_1_folder/\n                file_1.txt\n                file_2.txt\n                ...\n                file_42.txt\n            category_2_folder/\n                file_43.txt\n                file_44.txt\n                ...\n\n    The folder names are used as supervised signal label names. The\n    individual file names are not important.\n\n    This function does not try to extract features into a numpy array or\n    scipy sparse matrix. In addition, if load_content is false it\n    does not try to load the files in memory.\n\n    To use text files in a scikit-learn classification or clustering\n    algorithm, you will need to use the `sklearn.feature_extraction.text`\n    module to build a feature extraction transformer that suits your\n    problem.\n\n    If you set load_content=True, you should also specify the encoding of\n    the text using the 'encoding' parameter. For many modern text files,\n    'utf-8' will be the correct encoding. If you leave encoding equal to None,\n    then the content will be made of bytes instead of Unicode, and you will\n    not be able to use most functions in `sklearn.feature_extraction.text`.\n\n    Similar feature extractors should be built for other kind of unstructured\n    data input such as images, audio, video, ...\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    Parameters\n    ----------\n    container_path : string or unicode\n        Path to the main folder holding one subfolder per category\n\n    description: string or unicode, optional (default=None)\n        A paragraph describing the characteristic of the dataset: its source,\n        reference, etc.\n\n    categories : A collection of strings or None, optional (default=None)\n        If None (default), load all the categories.\n        If not None, list of category names to load (other categories ignored).\n\n    load_content : boolean, optional (default=True)\n        Whether to load or not the content of the different files. If\n        true a 'data' attribute containing the text information is present\n        in the data structure returned. If not, a filenames attribute\n        gives the path to the files.\n\n    encoding : string or None (default is None)\n        If None, do not try to decode the content of the files (e.g. for\n        images or other non-text content).\n        If not None, encoding to use to decode text files to Unicode if\n        load_content is True.\n\n    decode_error: {'strict', 'ignore', 'replace'}, optional\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. Passed as keyword\n        argument 'errors' to bytes.decode.\n\n    shuffle : bool, optional (default=True)\n        Whether or not to shuffle the data: might be important for models that\n        make the assumption that the samples are independent and identically\n        distributed (i.i.d.), such as stochastic gradient descent.\n\n    random_state : int, RandomState instance or None, optional (default=0)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    data : Bunch\n        Dictionary-like object, the interesting attributes are: either\n        data, the raw text data to learn, or 'filenames', the files\n        holding it, 'target', the classification labels (integer index),\n        'target_names', the meaning of the labels, and 'DESCR', the full\n        description of the dataset.\n    "
target = []
target_names = []
filenames = []
folders = [f for f in sorted(listdir(container_path)) if isdir(join(container_path, f))]
if (categories is not None):
    folders = [f for f in folders if (f in categories)]
for (label, folder) in enumerate(folders):
    target_names.append(folder)
    folder_path = join(container_path, folder)
    documents = [join(folder_path, d) for d in sorted(listdir(folder_path))]
    target.extend((len(documents) * [label]))
    filenames.extend(documents)
filenames = numpy.array(filenames)
target = numpy.array(target)
if shuffle:
    random_state = check_random_state(random_state)
    tempResult = arange(filenames.shape[0])
	
===================================================================	
load_digits: 143	
----------------------------	

"Load and return the digits dataset (classification).\n\n    Each datapoint is a 8x8 image of a digit.\n\n    =================   ==============\n    Classes                         10\n    Samples per class             ~180\n    Samples total                 1797\n    Dimensionality                  64\n    Features             integers 0-16\n    =================   ==============\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    Parameters\n    ----------\n    n_class : integer, between 0 and 10, optional (default=10)\n        The number of classes to return.\n\n    return_X_y : boolean, default=False.\n        If True, returns ``(data, target)`` instead of a Bunch object.\n        See below for more information about the `data` and `target` object.\n\n        .. versionadded:: 0.18\n\n    Returns\n    -------\n    data : Bunch\n        Dictionary-like object, the interesting attributes are:\n        'data', the data to learn, 'images', the images corresponding\n        to each sample, 'target', the classification labels for each\n        sample, 'target_names', the meaning of the labels, and 'DESCR',\n        the full description of the dataset.\n\n    (data, target) : tuple if ``return_X_y`` is True\n\n        .. versionadded:: 0.18\n\n    Examples\n    --------\n    To load the data and visualize the images::\n\n        >>> from sklearn.datasets import load_digits\n        >>> digits = load_digits()\n        >>> print(digits.data.shape)\n        (1797, 64)\n        >>> import matplotlib.pyplot as plt #doctest: +SKIP\n        >>> plt.gray() #doctest: +SKIP\n        >>> plt.matshow(digits.images[0]) #doctest: +SKIP\n        >>> plt.show() #doctest: +SKIP\n    "
module_path = dirname(__file__)
data = numpy.loadtxt(join(module_path, 'data', 'digits.csv.gz'), delimiter=',')
with open(join(module_path, 'descr', 'digits.rst')) as f:
    descr = f.read()
target = data[:, (- 1)].astype(numpy.int)
flat_data = data[:, :(- 1)]
images = flat_data.view()
images.shape = ((- 1), 8, 8)
if (n_class < 10):
    idx = (target < n_class)
    (flat_data, target) = (flat_data[idx], target[idx])
    images = images[idx]
if return_X_y:
    return (flat_data, target)
tempResult = arange(10)
	
===================================================================	
fetch_covtype: 43	
----------------------------	

"Load the covertype dataset, downloading it if necessary.\n\n    Read more in the :ref:`User Guide <datasets>`.\n\n    Parameters\n    ----------\n    data_home : string, optional\n        Specify another download and cache folder for the datasets. By default\n        all scikit learn data is stored in '~/scikit_learn_data' subfolders.\n\n    download_if_missing : boolean, default=True\n        If False, raise a IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        Random state for shuffling the dataset.\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    shuffle : bool, default=False\n        Whether to shuffle dataset.\n\n    Returns\n    -------\n    dataset : dict-like object with the following attributes:\n\n    dataset.data : numpy array of shape (581012, 54)\n        Each row corresponds to the 54 features in the dataset.\n\n    dataset.target : numpy array of shape (581012,)\n        Each value corresponds to one of the 7 forest covertypes with values\n        ranging between 1 to 7.\n\n    dataset.DESCR : string\n        Description of the forest covertype dataset.\n\n    "
data_home = get_data_home(data_home=data_home)
covtype_dir = join(data_home, 'covertype')
samples_path = _pkl_filepath(covtype_dir, 'samples')
targets_path = _pkl_filepath(covtype_dir, 'targets')
available = exists(samples_path)
if (download_if_missing and (not available)):
    makedirs(covtype_dir, exist_ok=True)
    logger.warning(('Downloading %s' % URL))
    f = BytesIO(urlopen(URL).read())
    Xy = numpy.genfromtxt(GzipFile(fileobj=f), delimiter=',')
    X = Xy[:, :(- 1)]
    y = Xy[:, (- 1)].astype(numpy.int32)
    externals.joblib.dump(X, samples_path, compress=9)
    externals.joblib.dump(y, targets_path, compress=9)
try:
    (X, y)
except NameError:
    X = externals.joblib.load(samples_path)
    y = externals.joblib.load(targets_path)
if shuffle:
    tempResult = arange(X.shape[0])
	
===================================================================	
_fetch_lfw_people: 124	
----------------------------	

'Perform the actual data loading for the lfw people dataset\n\n    This operation is meant to be cached by a joblib wrapper.\n    '
(person_names, file_paths) = ([], [])
for person_name in sorted(listdir(data_folder_path)):
    folder_path = join(data_folder_path, person_name)
    if (not isdir(folder_path)):
        continue
    paths = [join(folder_path, f) for f in listdir(folder_path)]
    n_pictures = len(paths)
    if (n_pictures >= min_faces_per_person):
        person_name = person_name.replace('_', ' ')
        person_names.extend(([person_name] * n_pictures))
        file_paths.extend(paths)
n_faces = len(file_paths)
if (n_faces == 0):
    raise ValueError(('min_faces_per_person=%d is too restrictive' % min_faces_per_person))
target_names = numpy.unique(person_names)
target = numpy.searchsorted(target_names, person_names)
faces = _load_imgs(file_paths, slice_, color, resize)
tempResult = arange(n_faces)
	
===================================================================	
_inverse_permutation: 113	
----------------------------	

'inverse permutation p'
n = p.size
s = numpy.zeros(n, dtype=numpy.int32)
tempResult = arange(n, dtype=numpy.int32)
	
===================================================================	
make_low_rank_matrix: 275	
----------------------------	

"Generate a mostly low rank matrix with bell-shaped singular values\n\n    Most of the variance can be explained by a bell-shaped curve of width\n    effective_rank: the low rank part of the singular values profile is::\n\n        (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\n    The remaining singular values' tail is fat, decreasing as::\n\n        tail_strength * exp(-0.1 * i / effective_rank).\n\n    The low rank part of the profile can be considered the structured\n    signal part of the data while the tail can be considered the noisy\n    part of the data that cannot be summarized by a low number of linear\n    components (singular vectors).\n\n    This kind of singular profiles is often seen in practice, for instance:\n     - gray level pictures of faces\n     - TF-IDF vectors of text documents crawled from the web\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    effective_rank : int, optional (default=10)\n        The approximate number of singular vectors required to explain most of\n        the data by linear combinations.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The matrix.\n    "
generator = check_random_state(random_state)
n = min(n_samples, n_features)
(u, _) = scipy.linalg.qr(generator.randn(n_samples, n), mode='economic')
(v, _) = scipy.linalg.qr(generator.randn(n_features, n), mode='economic')
tempResult = arange(n, dtype=numpy.float64)
	
===================================================================	
make_blobs: 230	
----------------------------	

'Generate isotropic Gaussian blobs for clustering.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The total number of points equally divided among clusters.\n\n    n_features : int, optional (default=2)\n        The number of features for each sample.\n\n    centers : int or array of shape [n_centers, n_features], optional\n        (default=3)\n        The number of centers to generate, or the fixed center locations.\n\n    cluster_std: float or sequence of floats, optional (default=1.0)\n        The standard deviation of the clusters.\n\n    center_box: pair of floats (min, max), optional (default=(-10.0, 10.0))\n        The bounding box for each cluster center when centers are\n        generated at random.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The generated samples.\n\n    y : array of shape [n_samples]\n        The integer labels for cluster membership of each sample.\n\n    Examples\n    --------\n    >>> from sklearn.datasets.samples_generator import make_blobs\n    >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n    ...                   random_state=0)\n    >>> print(X.shape)\n    (10, 2)\n    >>> y\n    array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n\n    See also\n    --------\n    make_classification: a more intricate variant\n    '
generator = check_random_state(random_state)
if isinstance(centers, numbers.Integral):
    centers = generator.uniform(center_box[0], center_box[1], size=(centers, n_features))
else:
    centers = check_array(centers)
    n_features = centers.shape[1]
if isinstance(cluster_std, numbers.Real):
    cluster_std = (numpy.ones(len(centers)) * cluster_std)
X = []
y = []
n_centers = centers.shape[0]
n_samples_per_center = ([int((n_samples // n_centers))] * n_centers)
for i in range((n_samples % n_centers)):
    n_samples_per_center[i] += 1
for (i, (n, std)) in enumerate(zip(n_samples_per_center, cluster_std)):
    X.append((centers[i] + generator.normal(scale=std, size=(n, n_features))))
    y += ([i] * n)
X = numpy.concatenate(X)
y = numpy.array(y)
if shuffle:
    tempResult = arange(n_samples)
	
===================================================================	
make_regression: 163	
----------------------------	

'Generate a random regression problem.\n\n    The input set can either be well conditioned (by default) or have a low\n    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n    more details.\n\n    The output is generated by applying a (potentially biased) random linear\n    regression model with `n_informative` nonzero regressors to the previously\n    generated input and some gaussian centered noise with some adjustable\n    scale.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=100)\n        The number of features.\n\n    n_informative : int, optional (default=10)\n        The number of informative features, i.e., the number of features used\n        to build the linear model used to generate the output.\n\n    n_targets : int, optional (default=1)\n        The number of regression targets, i.e., the dimension of the y output\n        vector associated with a sample. By default, the output is a scalar.\n\n    bias : float, optional (default=0.0)\n        The bias term in the underlying linear model.\n\n    effective_rank : int or None, optional (default=None)\n        if not None:\n            The approximate number of singular vectors required to explain most\n            of the input data by linear combinations. Using this kind of\n            singular spectrum in the input allows the generator to reproduce\n            the correlations often observed in practice.\n        if None:\n            The input set is well conditioned, centered and gaussian with\n            unit variance.\n\n    tail_strength : float between 0.0 and 1.0, optional (default=0.5)\n        The relative importance of the fat noisy tail of the singular values\n        profile if `effective_rank` is not None.\n\n    noise : float, optional (default=0.0)\n        The standard deviation of the gaussian noise applied to the output.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    coef : boolean, optional (default=False)\n        If True, the coefficients of the underlying linear model are returned.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The input samples.\n\n    y : array of shape [n_samples] or [n_samples, n_targets]\n        The output values.\n\n    coef : array of shape [n_features] or [n_features, n_targets], optional\n        The coefficient of the underlying linear model. It is returned only if\n        coef is True.\n    '
n_informative = min(n_features, n_informative)
generator = check_random_state(random_state)
if (effective_rank is None):
    X = generator.randn(n_samples, n_features)
else:
    X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=effective_rank, tail_strength=tail_strength, random_state=generator)
ground_truth = numpy.zeros((n_features, n_targets))
ground_truth[:n_informative, :] = (100 * generator.rand(n_informative, n_targets))
y = (numpy.dot(X, ground_truth) + bias)
if (noise > 0.0):
    y += generator.normal(scale=noise, size=y.shape)
if shuffle:
    (X, y) = util_shuffle(X, y, random_state=generator)
    tempResult = arange(n_features)
	
===================================================================	
make_classification: 83	
----------------------------	

'Generate a random n-class classification problem.\n\n    This initially creates clusters of points normally distributed (std=1)\n    about vertices of a `2 * class_sep`-sided hypercube, and assigns an equal\n    number of clusters to each class. It introduces interdependence between\n    these features and adds various types of further noise to the data.\n\n    Prior to shuffling, `X` stacks a number of these primary "informative"\n    features, "redundant" linear combinations of these, "repeated" duplicates\n    of sampled features, and arbitrary noise for and remaining features.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int, optional (default=100)\n        The number of samples.\n\n    n_features : int, optional (default=20)\n        The total number of features. These comprise `n_informative`\n        informative features, `n_redundant` redundant features, `n_repeated`\n        duplicated features and `n_features-n_informative-n_redundant-\n        n_repeated` useless features drawn at random.\n\n    n_informative : int, optional (default=2)\n        The number of informative features. Each class is composed of a number\n        of gaussian clusters each located around the vertices of a hypercube\n        in a subspace of dimension `n_informative`. For each cluster,\n        informative features are drawn independently from  N(0, 1) and then\n        randomly linearly combined within each cluster in order to add\n        covariance. The clusters are then placed on the vertices of the\n        hypercube.\n\n    n_redundant : int, optional (default=2)\n        The number of redundant features. These features are generated as\n        random linear combinations of the informative features.\n\n    n_repeated : int, optional (default=0)\n        The number of duplicated features, drawn randomly from the informative\n        and the redundant features.\n\n    n_classes : int, optional (default=2)\n        The number of classes (or labels) of the classification problem.\n\n    n_clusters_per_class : int, optional (default=2)\n        The number of clusters per class.\n\n    weights : list of floats or None (default=None)\n        The proportions of samples assigned to each class. If None, then\n        classes are balanced. Note that if `len(weights) == n_classes - 1`,\n        then the last class weight is automatically inferred.\n        More than `n_samples` samples may be returned if the sum of `weights`\n        exceeds 1.\n\n    flip_y : float, optional (default=0.01)\n        The fraction of samples whose class are randomly exchanged.\n\n    class_sep : float, optional (default=1.0)\n        The factor multiplying the hypercube dimension.\n\n    hypercube : boolean, optional (default=True)\n        If True, the clusters are put on the vertices of a hypercube. If\n        False, the clusters are put on the vertices of a random polytope.\n\n    shift : float, array of shape [n_features] or None, optional (default=0.0)\n        Shift features by the specified value. If None, then features\n        are shifted by a random value drawn in [-class_sep, class_sep].\n\n    scale : float, array of shape [n_features] or None, optional (default=1.0)\n        Multiply features by the specified value. If None, then features\n        are scaled by a random value drawn in [1, 100]. Note that scaling\n        happens after shifting.\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples and the features.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The generated samples.\n\n    y : array of shape [n_samples]\n        The integer labels for class membership of each sample.\n\n    Notes\n    -----\n    The algorithm is adapted from Guyon [1] and was designed to generate\n    the "Madelon" dataset.\n\n    References\n    ----------\n    .. [1] I. Guyon, "Design of experiments for the NIPS 2003 variable\n           selection benchmark", 2003.\n\n    See also\n    --------\n    make_blobs: simplified variant\n    make_multilabel_classification: unrelated generator for multilabel tasks\n    '
generator = check_random_state(random_state)
if (((n_informative + n_redundant) + n_repeated) > n_features):
    raise ValueError('Number of informative, redundant and repeated features must sum to less than the number of total features')
if ((2 ** n_informative) < (n_classes * n_clusters_per_class)):
    raise ValueError('n_classes * n_clusters_per_class must be smaller or equal 2 ** n_informative')
if (weights and (len(weights) not in [n_classes, (n_classes - 1)])):
    raise ValueError('Weights specified but incompatible with number of classes.')
n_useless = (((n_features - n_informative) - n_redundant) - n_repeated)
n_clusters = (n_classes * n_clusters_per_class)
if (weights and (len(weights) == (n_classes - 1))):
    weights.append((1.0 - sum(weights)))
if (weights is None):
    weights = ([(1.0 / n_classes)] * n_classes)
    weights[(- 1)] = (1.0 - sum(weights[:(- 1)]))
n_samples_per_cluster = []
for k in range(n_clusters):
    n_samples_per_cluster.append(int(((n_samples * weights[(k % n_classes)]) / n_clusters_per_class)))
for i in range((n_samples - sum(n_samples_per_cluster))):
    n_samples_per_cluster[(i % n_clusters)] += 1
X = numpy.zeros((n_samples, n_features))
y = numpy.zeros(n_samples, dtype=numpy.int)
centroids = _generate_hypercube(n_clusters, n_informative, generator).astype(float)
centroids *= (2 * class_sep)
centroids -= class_sep
if (not hypercube):
    centroids *= generator.rand(n_clusters, 1)
    centroids *= generator.rand(1, n_informative)
X[:, :n_informative] = generator.randn(n_samples, n_informative)
stop = 0
for (k, centroid) in enumerate(centroids):
    (start, stop) = (stop, (stop + n_samples_per_cluster[k]))
    y[start:stop] = (k % n_classes)
    X_k = X[start:stop, :n_informative]
    A = ((2 * generator.rand(n_informative, n_informative)) - 1)
    X_k[...] = numpy.dot(X_k, A)
    X_k += centroid
if (n_redundant > 0):
    B = ((2 * generator.rand(n_informative, n_redundant)) - 1)
    X[:, n_informative:(n_informative + n_redundant)] = numpy.dot(X[:, :n_informative], B)
if (n_repeated > 0):
    n = (n_informative + n_redundant)
    indices = (((n - 1) * generator.rand(n_repeated)) + 0.5).astype(numpy.intp)
    X[:, n:(n + n_repeated)] = X[:, indices]
if (n_useless > 0):
    X[:, (- n_useless):] = generator.randn(n_samples, n_useless)
if (flip_y >= 0.0):
    flip_mask = (generator.rand(n_samples) < flip_y)
    y[flip_mask] = generator.randint(n_classes, size=flip_mask.sum())
if (shift is None):
    shift = (((2 * generator.rand(n_features)) - 1) * class_sep)
X += shift
if (scale is None):
    scale = (1 + (100 * generator.rand(n_features)))
X *= scale
if shuffle:
    (X, y) = util_shuffle(X, y, random_state=generator)
    tempResult = arange(n_features)
	
===================================================================	
make_sparse_coded_signal: 288	
----------------------------	

'Generate a signal as a sparse combination of dictionary elements.\n\n    Returns a matrix Y = DX, such as D is (n_features, n_components),\n    X is (n_components, n_samples) and each column of X has exactly\n    n_nonzero_coefs non-zero elements.\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    n_samples : int\n        number of samples to generate\n\n    n_components:  int,\n        number of components in the dictionary\n\n    n_features : int\n        number of features of the dataset to generate\n\n    n_nonzero_coefs : int\n        number of active (non-zero) coefficients in each sample\n\n    random_state: int or RandomState instance, optional (default=None)\n        seed used by the pseudo random number generator\n\n    Returns\n    -------\n    data: array of shape [n_features, n_samples]\n        The encoded signal (Y).\n\n    dictionary: array of shape [n_features, n_components]\n        The dictionary with normalized components (D).\n\n    code: array of shape [n_components, n_samples]\n        The sparse code such that each column of this matrix has exactly\n        n_nonzero_coefs non-zero items (X).\n\n    '
generator = check_random_state(random_state)
D = generator.randn(n_features, n_components)
D /= numpy.sqrt(numpy.sum((D ** 2), axis=0))
X = numpy.zeros((n_components, n_samples))
for i in range(n_samples):
    tempResult = arange(n_components)
	
===================================================================	
make_gaussian_quantiles: 368	
----------------------------	

'Generate isotropic Gaussian and label samples by quantile\n\n    This classification dataset is constructed by taking a multi-dimensional\n    standard normal distribution and defining classes separated by nested\n    concentric multi-dimensional spheres such that roughly equal numbers of\n    samples are in each class (quantiles of the :math:`\\chi^2` distribution).\n\n    Read more in the :ref:`User Guide <sample_generators>`.\n\n    Parameters\n    ----------\n    mean : array of shape [n_features], optional (default=None)\n        The mean of the multi-dimensional normal distribution.\n        If None then use the origin (0, 0, ...).\n\n    cov : float, optional (default=1.)\n        The covariance matrix will be this value times the unit matrix. This\n        dataset only produces symmetric normal distributions.\n\n    n_samples : int, optional (default=100)\n        The total number of points equally divided among classes.\n\n    n_features : int, optional (default=2)\n        The number of features for each sample.\n\n    n_classes : int, optional (default=3)\n        The number of classes\n\n    shuffle : boolean, optional (default=True)\n        Shuffle the samples.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`.\n\n    Returns\n    -------\n    X : array of shape [n_samples, n_features]\n        The generated samples.\n\n    y : array of shape [n_samples]\n        The integer labels for quantile membership of each sample.\n\n    Notes\n    -----\n    The dataset is from Zhu et al [1].\n\n    References\n    ----------\n    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.\n\n    '
if (n_samples < n_classes):
    raise ValueError('n_samples must be at least n_classes')
generator = check_random_state(random_state)
if (mean is None):
    mean = numpy.zeros(n_features)
else:
    mean = numpy.array(mean)
X = generator.multivariate_normal(mean, (cov * numpy.identity(n_features)), (n_samples,))
idx = numpy.argsort(numpy.sum(((X - mean[numpy.newaxis, :]) ** 2), axis=1))
X = X[idx, :]
step = (n_samples // n_classes)
tempResult = arange(n_classes)
	
===================================================================	
construct_grids: 48	
----------------------------	

'Construct the map grid from the batch object\n\n    Parameters\n    ----------\n    batch : Batch object\n        The object returned by :func:`fetch_species_distributions`\n\n    Returns\n    -------\n    (xgrid, ygrid) : 1-D arrays\n        The grid corresponding to the values in batch.coverages\n    '
xmin = (batch.x_left_lower_corner + batch.grid_size)
xmax = (xmin + (batch.Nx * batch.grid_size))
ymin = (batch.y_left_lower_corner + batch.grid_size)
ymax = (ymin + (batch.Ny * batch.grid_size))
tempResult = arange(xmin, xmax, batch.grid_size)
	
===================================================================	
construct_grids: 49	
----------------------------	

'Construct the map grid from the batch object\n\n    Parameters\n    ----------\n    batch : Batch object\n        The object returned by :func:`fetch_species_distributions`\n\n    Returns\n    -------\n    (xgrid, ygrid) : 1-D arrays\n        The grid corresponding to the values in batch.coverages\n    '
xmin = (batch.x_left_lower_corner + batch.grid_size)
xmax = (xmin + (batch.Nx * batch.grid_size))
ymin = (batch.y_left_lower_corner + batch.grid_size)
ymax = (ymin + (batch.Ny * batch.grid_size))
xgrid = numpy.arange(xmin, xmax, batch.grid_size)
tempResult = arange(ymin, ymax, batch.grid_size)
	
===================================================================	
fetch_20newsgroups: 138	
----------------------------	

"Load the filenames and data from the 20 newsgroups dataset.\n\n    Read more in the :ref:`User Guide <20newsgroups>`.\n\n    Parameters\n    ----------\n    subset: 'train' or 'test', 'all', optional\n        Select the dataset to load: 'train' for the training set, 'test'\n        for the test set, 'all' for both, with shuffled ordering.\n\n    data_home: optional, default: None\n        Specify a download and cache folder for the datasets. If None,\n        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n\n    categories: None or collection of string or unicode\n        If None (default), load all the categories.\n        If not None, list of category names to load (other categories\n        ignored).\n\n    shuffle: bool, optional\n        Whether or not to shuffle the data: might be important for models that\n        make the assumption that the samples are independent and identically\n        distributed (i.i.d.), such as stochastic gradient descent.\n\n    random_state: numpy random number generator or seed integer\n        Used to shuffle the dataset.\n\n    download_if_missing: optional, True by default\n        If False, raise an IOError if the data is not locally available\n        instead of trying to download the data from the source site.\n\n    remove: tuple\n        May contain any subset of ('headers', 'footers', 'quotes'). Each of\n        these are kinds of text that will be detected and removed from the\n        newsgroup posts, preventing classifiers from overfitting on\n        metadata.\n\n        'headers' removes newsgroup headers, 'footers' removes blocks at the\n        ends of posts that look like signatures, and 'quotes' removes lines\n        that appear to be quoting another post.\n\n        'headers' follows an exact standard; the other filters are not always\n        correct.\n    "
data_home = get_data_home(data_home=data_home)
cache_path = _pkl_filepath(data_home, CACHE_NAME)
twenty_home = os.path.join(data_home, '20news_home')
cache = None
if os.path.exists(cache_path):
    try:
        with open(cache_path, 'rb') as f:
            compressed_content = f.read()
        uncompressed_content = codecs.decode(compressed_content, 'zlib_codec')
        cache = pickle.loads(uncompressed_content)
    except Exception as e:
        print((80 * '_'))
        print('Cache loading failed')
        print((80 * '_'))
        print(e)
if (cache is None):
    if download_if_missing:
        logger.info('Downloading 20news dataset. This may take a few minutes.')
        cache = download_20newsgroups(target_dir=twenty_home, cache_path=cache_path)
    else:
        raise IOError('20Newsgroups dataset not found')
if (subset in ('train', 'test')):
    data = cache[subset]
elif (subset == 'all'):
    data_lst = list()
    target = list()
    filenames = list()
    for subset in ('train', 'test'):
        data = cache[subset]
        data_lst.extend(data.data)
        target.extend(data.target)
        filenames.extend(data.filenames)
    data.data = data_lst
    data.target = numpy.array(target)
    data.filenames = numpy.array(filenames)
else:
    raise ValueError(("subset can only be 'train', 'test' or 'all', got '%s'" % subset))
data.description = 'the 20 newsgroups by date dataset'
if ('headers' in remove):
    data.data = [strip_newsgroup_header(text) for text in data.data]
if ('footers' in remove):
    data.data = [strip_newsgroup_footer(text) for text in data.data]
if ('quotes' in remove):
    data.data = [strip_newsgroup_quoting(text) for text in data.data]
if (categories is not None):
    labels = [(data.target_names.index(cat), cat) for cat in categories]
    labels.sort()
    (labels, categories) = zip(*labels)
    mask = numpy.in1d(data.target, labels)
    data.filenames = data.filenames[mask]
    data.target = data.target[mask]
    data.target = numpy.searchsorted(labels, data.target)
    data.target_names = list(categories)
    data_lst = numpy.array(data.data, dtype=object)
    data_lst = data_lst[mask]
    data.data = data_lst.tolist()
if shuffle:
    random_state = check_random_state(random_state)
    tempResult = arange(data.target.shape[0])
	
===================================================================	
setup_module: 63	
----------------------------	

'Test fixture run once and common to all tests of this module'
if (imsave is None):
    raise SkipTest('PIL not installed.')
if (not os.path.exists(LFW_HOME)):
    os.makedirs(LFW_HOME)
random_state = random.Random(42)
np_rng = numpy.random.RandomState(42)
counts = {}
for name in FAKE_NAMES:
    folder_name = os.path.join(LFW_HOME, 'lfw_funneled', name)
    if (not os.path.exists(folder_name)):
        os.makedirs(folder_name)
    n_faces = np_rng.randint(1, 5)
    counts[name] = n_faces
    for i in range(n_faces):
        file_path = os.path.join(folder_name, (name + ('_%04d.jpg' % i)))
        uniface = np_rng.randint(0, 255, size=(250, 250, 3))
        try:
            imsave(file_path, uniface)
        except ImportError:
            raise SkipTest('PIL not installed')
with open(os.path.join(LFW_HOME, 'lfw_funneled', '.test.swp'), 'wb') as f:
    f.write(sklearn.externals.six.b('Text file to be ignored by the dataset loader.'))
with open(os.path.join(LFW_HOME, 'pairsDevTrain.txt'), 'wb') as f:
    f.write(sklearn.externals.six.b('10\n'))
    more_than_two = [name for (name, count) in sklearn.externals.six.iteritems(counts) if (count >= 2)]
    for i in range(5):
        name = random_state.choice(more_than_two)
        (first, second) = random_state.sample(range(counts[name]), 2)
        f.write(sklearn.externals.six.b(('%s\t%d\t%d\n' % (name, first, second))))
    for i in range(5):
        (first_name, second_name) = random_state.sample(FAKE_NAMES, 2)
        tempResult = arange(counts[first_name])
	
===================================================================	
setup_module: 64	
----------------------------	

'Test fixture run once and common to all tests of this module'
if (imsave is None):
    raise SkipTest('PIL not installed.')
if (not os.path.exists(LFW_HOME)):
    os.makedirs(LFW_HOME)
random_state = random.Random(42)
np_rng = numpy.random.RandomState(42)
counts = {}
for name in FAKE_NAMES:
    folder_name = os.path.join(LFW_HOME, 'lfw_funneled', name)
    if (not os.path.exists(folder_name)):
        os.makedirs(folder_name)
    n_faces = np_rng.randint(1, 5)
    counts[name] = n_faces
    for i in range(n_faces):
        file_path = os.path.join(folder_name, (name + ('_%04d.jpg' % i)))
        uniface = np_rng.randint(0, 255, size=(250, 250, 3))
        try:
            imsave(file_path, uniface)
        except ImportError:
            raise SkipTest('PIL not installed')
with open(os.path.join(LFW_HOME, 'lfw_funneled', '.test.swp'), 'wb') as f:
    f.write(sklearn.externals.six.b('Text file to be ignored by the dataset loader.'))
with open(os.path.join(LFW_HOME, 'pairsDevTrain.txt'), 'wb') as f:
    f.write(sklearn.externals.six.b('10\n'))
    more_than_two = [name for (name, count) in sklearn.externals.six.iteritems(counts) if (count >= 2)]
    for i in range(5):
        name = random_state.choice(more_than_two)
        (first, second) = random_state.sample(range(counts[name]), 2)
        f.write(sklearn.externals.six.b(('%s\t%d\t%d\n' % (name, first, second))))
    for i in range(5):
        (first_name, second_name) = random_state.sample(FAKE_NAMES, 2)
        first_index = random_state.choice(numpy.arange(counts[first_name]))
        tempResult = arange(counts[second_name])
	
===================================================================	
test_dump_query_id: 249	
----------------------------	

(X, y) = load_svmlight_file(datafile)
X = X.toarray()
tempResult = arange(X.shape[0])
	
===================================================================	
test_dump: 148	
----------------------------	

(X_sparse, y_dense) = load_svmlight_file(datafile)
X_dense = X_sparse.toarray()
y_sparse = scipy.sparse.csr_matrix(y_dense)
tempResult = arange(X_sparse.shape[0])
	
===================================================================	
test_dump: 149	
----------------------------	

(X_sparse, y_dense) = load_svmlight_file(datafile)
X_dense = X_sparse.toarray()
y_sparse = scipy.sparse.csr_matrix(y_dense)
X_sliced = X_sparse[numpy.arange(X_sparse.shape[0])]
tempResult = arange(y_sparse.shape[0])
	
===================================================================	
_update_coordinate_descent: 219	
----------------------------	

'Helper function for _fit_coordinate_descent\n\n    Update W to minimize the objective function, iterating once over all\n    coordinates. By symmetry, to update H, one can call\n    _update_coordinate_descent(X.T, Ht, W, ...)\n\n    '
n_components = Ht.shape[1]
HHt = fast_dot(Ht.T, Ht)
XHt = safe_sparse_dot(X, Ht)
if (l2_reg != 0.0):
    HHt.flat[::(n_components + 1)] += l2_reg
if (l1_reg != 0.0):
    XHt -= l1_reg
if shuffle:
    permutation = random_state.permutation(n_components)
else:
    tempResult = arange(n_components)
	
===================================================================	
test_incremental_pca_batch_signs: 84	
----------------------------	

rng = numpy.random.RandomState(1999)
n_samples = 100
n_features = 3
X = rng.randn(n_samples, n_features)
all_components = []
tempResult = arange(10, 20)
	
===================================================================	
test_incremental_pca_partial_fit: 113	
----------------------------	

rng = numpy.random.RandomState(1999)
(n, p) = (50, 3)
X = rng.randn(n, p)
X[:, 1] *= 1e-05
X += [5, 4, 3]
batch_size = 10
ipca = IncrementalPCA(n_components=2, batch_size=batch_size).fit(X)
pipca = IncrementalPCA(n_components=2, batch_size=batch_size)
tempResult = arange(0, (n + 1), batch_size)
	
===================================================================	
test_incremental_pca_batch_values: 97	
----------------------------	

rng = numpy.random.RandomState(1999)
n_samples = 100
n_features = 3
X = rng.randn(n_samples, n_features)
all_components = []
tempResult = arange(20, 40, 3)
	
===================================================================	
test_gridsearch_pipeline: 110	
----------------------------	

(X, y) = make_circles(n_samples=400, factor=0.3, noise=0.05, random_state=0)
kpca = KernelPCA(kernel='rbf', n_components=2)
pipeline = Pipeline([('kernel_pca', kpca), ('Perceptron', Perceptron())])
tempResult = arange((- 2), 2)
	
===================================================================	
test_gridsearch_pipeline_precomputed: 119	
----------------------------	

(X, y) = make_circles(n_samples=400, factor=0.3, noise=0.05, random_state=0)
kpca = KernelPCA(kernel='precomputed', n_components=2)
pipeline = Pipeline([('kernel_pca', kpca), ('Perceptron', Perceptron())])
tempResult = arange(1, 5)
	
===================================================================	
test_nls_nn_output: 72	
----------------------------	

tempResult = arange(1, 5)
	
===================================================================	
test_sparse_input: 130	
----------------------------	

from scipy.sparse import csc_matrix
A = numpy.abs(random_state.randn(10, 10))
tempResult = arange(5)
	
===================================================================	
test_nls_close: 77	
----------------------------	

tempResult = arange(1, 5)
	
===================================================================	
test_safe_compute_error: 184	
----------------------------	

A = numpy.abs(random_state.randn(10, 10))
tempResult = arange(5)
	
===================================================================	
test_non_negative_factorization_consistency: 156	
----------------------------	

A = numpy.abs(random_state.randn(10, 10))
tempResult = arange(5)
	
===================================================================	
test_nmf_fit_nn_output: 57	
----------------------------	

tempResult = arange(1, 6)
	
===================================================================	
test_nmf_fit_nn_output: 57	
----------------------------	

tempResult = arange(1, 6)
	
===================================================================	
test_pca: 24	
----------------------------	

X = iris.data
tempResult = arange(X.shape[1])
	
===================================================================	
test_pca_randomized_solver: 65	
----------------------------	

X = iris.data
tempResult = arange(1, X.shape[1])
	
===================================================================	
test_pca_arpack_solver: 43	
----------------------------	

X = iris.data
d = X.shape[1]
tempResult = arange(1, d)
	
===================================================================	
_parallel_predict_log_proba: 99	
----------------------------	

'Private function used to compute log probabilities within a job.'
n_samples = X.shape[0]
log_proba = numpy.empty((n_samples, n_classes))
log_proba.fill((- numpy.inf))
tempResult = arange(n_classes, dtype=numpy.int)
	
===================================================================	
_generate_unsampled_indices: 37	
----------------------------	

'Private function used to forest._set_oob_score function.'
sample_indices = _generate_sample_indices(random_state, n_samples)
sample_counts = bincount(sample_indices, minlength=n_samples)
unsampled_mask = (sample_counts == 0)
tempResult = arange(n_samples)
	
===================================================================	
plot_partial_dependence: 153	
----------------------------	

"Partial dependence plots for ``features``.\n\n    The ``len(features)`` plots are arranged in a grid with ``n_cols``\n    columns. Two-way partial dependence plots are plotted as contour\n    plots.\n\n    Read more in the :ref:`User Guide <partial_dependence>`.\n\n    Parameters\n    ----------\n    gbrt : BaseGradientBoosting\n        A fitted gradient boosting model.\n    X : array-like, shape=(n_samples, n_features)\n        The data on which ``gbrt`` was trained.\n    features : seq of tuples or ints\n        If seq[i] is an int or a tuple with one int value, a one-way\n        PDP is created; if seq[i] is a tuple of two ints, a two-way\n        PDP is created.\n    feature_names : seq of str\n        Name of each feature; feature_names[i] holds\n        the name of the feature with index i.\n    label : object\n        The class label for which the PDPs should be computed.\n        Only if gbrt is a multi-class model. Must be in ``gbrt.classes_``.\n    n_cols : int\n        The number of columns in the grid plot (default: 3).\n    percentiles : (low, high), default=(0.05, 0.95)\n        The lower and upper percentile used to create the extreme values\n        for the PDP axes.\n    grid_resolution : int, default=100\n        The number of equally spaced points on the axes.\n    n_jobs : int\n        The number of CPUs to use to compute the PDs. -1 means 'all CPUs'.\n        Defaults to 1.\n    verbose : int\n        Verbose output during PD computations. Defaults to 0.\n    ax : Matplotlib axis object, default None\n        An axis object onto which the plots will be drawn.\n    line_kw : dict\n        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.\n        For one-way partial dependence plots.\n    contour_kw : dict\n        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.\n        For two-way partial dependence plots.\n    fig_kw : dict\n        Dict with keywords passed to the figure() call.\n        Note that all keywords not recognized above will be automatically\n        included here.\n\n    Returns\n    -------\n    fig : figure\n        The Matplotlib Figure object.\n    axs : seq of Axis objects\n        A seq of Axis objects, one for each subplot.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.ensemble import GradientBoostingRegressor\n    >>> X, y = make_friedman1()\n    >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n    >>> fig, axs = plot_partial_dependence(clf, X, [0, (0, 1)]) #doctest: +SKIP\n    ...\n    "
import matplotlib.pyplot as plt
from matplotlib import transforms
from matplotlib.ticker import MaxNLocator
from matplotlib.ticker import ScalarFormatter
if (not isinstance(gbrt, BaseGradientBoosting)):
    raise ValueError('gbrt has to be an instance of BaseGradientBoosting')
if (gbrt.estimators_.shape[0] == 0):
    raise ValueError(('Call %s.fit before partial_dependence' % gbrt.__class__.__name__))
if (hasattr(gbrt, 'classes_') and (numpy.size(gbrt.classes_) > 2)):
    if (label is None):
        raise ValueError('label is not given for multi-class PDP')
    label_idx = numpy.searchsorted(gbrt.classes_, label)
    if (gbrt.classes_[label_idx] != label):
        raise ValueError(('label %s not in ``gbrt.classes_``' % str(label)))
else:
    label_idx = 0
X = check_array(X, dtype=DTYPE, order='C')
if (gbrt.n_features != X.shape[1]):
    raise ValueError('X.shape[1] does not match gbrt.n_features')
if (line_kw is None):
    line_kw = {'color': 'green'}
if (contour_kw is None):
    contour_kw = {}
if (feature_names is None):
    feature_names = [str(i) for i in range(gbrt.n_features)]
elif isinstance(feature_names, numpy.ndarray):
    feature_names = feature_names.tolist()

def convert_feature(fx):
    if isinstance(fx, externals.six.string_types):
        try:
            fx = feature_names.index(fx)
        except ValueError:
            raise ValueError(('Feature %s not in feature_names' % fx))
    return fx
tmp_features = []
for fxs in features:
    if isinstance(fxs, ((numbers.Integral,) + externals.six.string_types)):
        fxs = (fxs,)
    try:
        fxs = numpy.array([convert_feature(fx) for fx in fxs], dtype=numpy.int32)
    except TypeError:
        raise ValueError('features must be either int, str, or tuple of int/str')
    if (not (1 <= numpy.size(fxs) <= 2)):
        raise ValueError('target features must be either one or two')
    tmp_features.append(fxs)
features = tmp_features
names = []
try:
    for fxs in features:
        l = []
        for i in fxs:
            l.append(feature_names[i])
        names.append(l)
except IndexError:
    raise ValueError(('features[i] must be in [0, n_features) but was %d' % i))
pd_result = Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(partial_dependence)(gbrt, fxs, X=X, grid_resolution=grid_resolution, percentiles=percentiles) for fxs in features))
pdp_lim = {}
for (pdp, axes) in pd_result:
    (min_pd, max_pd) = (pdp[label_idx].min(), pdp[label_idx].max())
    n_fx = len(axes)
    (old_min_pd, old_max_pd) = pdp_lim.get(n_fx, (min_pd, max_pd))
    min_pd = min(min_pd, old_min_pd)
    max_pd = max(max_pd, old_max_pd)
    pdp_lim[n_fx] = (min_pd, max_pd)
if (2 in pdp_lim):
    Z_level = numpy.linspace(*pdp_lim[2], num=8)
if (ax is None):
    fig = matplotlib.pyplot.figure(**fig_kw)
else:
    fig = ax.get_figure()
    fig.clear()
n_cols = min(n_cols, len(features))
n_rows = int(numpy.ceil((len(features) / float(n_cols))))
axs = []
for (i, fx, name, (pdp, axes)) in zip(count(), features, names, pd_result):
    ax = fig.add_subplot(n_rows, n_cols, (i + 1))
    if (len(axes) == 1):
        ax.plot(axes[0], pdp[label_idx].ravel(), **line_kw)
    else:
        assert (len(axes) == 2)
        (XX, YY) = numpy.meshgrid(axes[0], axes[1])
        Z = pdp[label_idx].reshape(list(map(np.size, axes))).T
        CS = ax.contour(XX, YY, Z, levels=Z_level, linewidths=0.5, colors='k')
        ax.contourf(XX, YY, Z, levels=Z_level, vmax=Z_level[(- 1)], vmin=Z_level[0], alpha=0.75, **contour_kw)
        ax.clabel(CS, fmt='%2.2f', colors='k', fontsize=10, inline=True)
    tempResult = arange(0.1, 1.0, 0.1)
	
===================================================================	
plot_partial_dependence: 164	
----------------------------	

"Partial dependence plots for ``features``.\n\n    The ``len(features)`` plots are arranged in a grid with ``n_cols``\n    columns. Two-way partial dependence plots are plotted as contour\n    plots.\n\n    Read more in the :ref:`User Guide <partial_dependence>`.\n\n    Parameters\n    ----------\n    gbrt : BaseGradientBoosting\n        A fitted gradient boosting model.\n    X : array-like, shape=(n_samples, n_features)\n        The data on which ``gbrt`` was trained.\n    features : seq of tuples or ints\n        If seq[i] is an int or a tuple with one int value, a one-way\n        PDP is created; if seq[i] is a tuple of two ints, a two-way\n        PDP is created.\n    feature_names : seq of str\n        Name of each feature; feature_names[i] holds\n        the name of the feature with index i.\n    label : object\n        The class label for which the PDPs should be computed.\n        Only if gbrt is a multi-class model. Must be in ``gbrt.classes_``.\n    n_cols : int\n        The number of columns in the grid plot (default: 3).\n    percentiles : (low, high), default=(0.05, 0.95)\n        The lower and upper percentile used to create the extreme values\n        for the PDP axes.\n    grid_resolution : int, default=100\n        The number of equally spaced points on the axes.\n    n_jobs : int\n        The number of CPUs to use to compute the PDs. -1 means 'all CPUs'.\n        Defaults to 1.\n    verbose : int\n        Verbose output during PD computations. Defaults to 0.\n    ax : Matplotlib axis object, default None\n        An axis object onto which the plots will be drawn.\n    line_kw : dict\n        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.\n        For one-way partial dependence plots.\n    contour_kw : dict\n        Dict with keywords passed to the ``matplotlib.pyplot.plot`` call.\n        For two-way partial dependence plots.\n    fig_kw : dict\n        Dict with keywords passed to the figure() call.\n        Note that all keywords not recognized above will be automatically\n        included here.\n\n    Returns\n    -------\n    fig : figure\n        The Matplotlib Figure object.\n    axs : seq of Axis objects\n        A seq of Axis objects, one for each subplot.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_friedman1\n    >>> from sklearn.ensemble import GradientBoostingRegressor\n    >>> X, y = make_friedman1()\n    >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)\n    >>> fig, axs = plot_partial_dependence(clf, X, [0, (0, 1)]) #doctest: +SKIP\n    ...\n    "
import matplotlib.pyplot as plt
from matplotlib import transforms
from matplotlib.ticker import MaxNLocator
from matplotlib.ticker import ScalarFormatter
if (not isinstance(gbrt, BaseGradientBoosting)):
    raise ValueError('gbrt has to be an instance of BaseGradientBoosting')
if (gbrt.estimators_.shape[0] == 0):
    raise ValueError(('Call %s.fit before partial_dependence' % gbrt.__class__.__name__))
if (hasattr(gbrt, 'classes_') and (numpy.size(gbrt.classes_) > 2)):
    if (label is None):
        raise ValueError('label is not given for multi-class PDP')
    label_idx = numpy.searchsorted(gbrt.classes_, label)
    if (gbrt.classes_[label_idx] != label):
        raise ValueError(('label %s not in ``gbrt.classes_``' % str(label)))
else:
    label_idx = 0
X = check_array(X, dtype=DTYPE, order='C')
if (gbrt.n_features != X.shape[1]):
    raise ValueError('X.shape[1] does not match gbrt.n_features')
if (line_kw is None):
    line_kw = {'color': 'green'}
if (contour_kw is None):
    contour_kw = {}
if (feature_names is None):
    feature_names = [str(i) for i in range(gbrt.n_features)]
elif isinstance(feature_names, numpy.ndarray):
    feature_names = feature_names.tolist()

def convert_feature(fx):
    if isinstance(fx, externals.six.string_types):
        try:
            fx = feature_names.index(fx)
        except ValueError:
            raise ValueError(('Feature %s not in feature_names' % fx))
    return fx
tmp_features = []
for fxs in features:
    if isinstance(fxs, ((numbers.Integral,) + externals.six.string_types)):
        fxs = (fxs,)
    try:
        fxs = numpy.array([convert_feature(fx) for fx in fxs], dtype=numpy.int32)
    except TypeError:
        raise ValueError('features must be either int, str, or tuple of int/str')
    if (not (1 <= numpy.size(fxs) <= 2)):
        raise ValueError('target features must be either one or two')
    tmp_features.append(fxs)
features = tmp_features
names = []
try:
    for fxs in features:
        l = []
        for i in fxs:
            l.append(feature_names[i])
        names.append(l)
except IndexError:
    raise ValueError(('features[i] must be in [0, n_features) but was %d' % i))
pd_result = Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(partial_dependence)(gbrt, fxs, X=X, grid_resolution=grid_resolution, percentiles=percentiles) for fxs in features))
pdp_lim = {}
for (pdp, axes) in pd_result:
    (min_pd, max_pd) = (pdp[label_idx].min(), pdp[label_idx].max())
    n_fx = len(axes)
    (old_min_pd, old_max_pd) = pdp_lim.get(n_fx, (min_pd, max_pd))
    min_pd = min(min_pd, old_min_pd)
    max_pd = max(max_pd, old_max_pd)
    pdp_lim[n_fx] = (min_pd, max_pd)
if (2 in pdp_lim):
    Z_level = numpy.linspace(*pdp_lim[2], num=8)
if (ax is None):
    fig = matplotlib.pyplot.figure(**fig_kw)
else:
    fig = ax.get_figure()
    fig.clear()
n_cols = min(n_cols, len(features))
n_rows = int(numpy.ceil((len(features) / float(n_cols))))
axs = []
for (i, fx, name, (pdp, axes)) in zip(count(), features, names, pd_result):
    ax = fig.add_subplot(n_rows, n_cols, (i + 1))
    if (len(axes) == 1):
        ax.plot(axes[0], pdp[label_idx].ravel(), **line_kw)
    else:
        assert (len(axes) == 2)
        (XX, YY) = numpy.meshgrid(axes[0], axes[1])
        Z = pdp[label_idx].reshape(list(map(np.size, axes))).T
        CS = ax.contour(XX, YY, Z, levels=Z_level, linewidths=0.5, colors='k')
        ax.contourf(XX, YY, Z, levels=Z_level, vmax=Z_level[(- 1)], vmin=Z_level[0], alpha=0.75, **contour_kw)
        ax.clabel(CS, fmt='%2.2f', colors='k', fontsize=10, inline=True)
    deciles = mquantiles(X[:, fx[0]], prob=numpy.arange(0.1, 1.0, 0.1))
    trans = matplotlib.transforms.blended_transform_factory(ax.transData, ax.transAxes)
    ylim = ax.get_ylim()
    ax.vlines(deciles, [0], 0.05, transform=trans, color='k')
    ax.set_xlabel(name[0])
    ax.set_ylim(ylim)
    ax.xaxis.set_major_locator(MaxNLocator(nbins=6, prune='lower'))
    tick_formatter = ScalarFormatter()
    tick_formatter.set_powerlimits(((- 3), 4))
    ax.xaxis.set_major_formatter(tick_formatter)
    if (len(axes) > 1):
        tempResult = arange(0.1, 1.0, 0.1)
	
===================================================================	
AdaBoostRegressor._get_median_predict: 340	
----------------------------	

predictions = np.array([est.predict(X) for est in self.estimators_[:limit]]).T
sorted_idx = numpy.argsort(predictions, axis=1)
weight_cdf = self.estimator_weights_[sorted_idx].cumsum(axis=1)
median_or_above = (weight_cdf >= (0.5 * weight_cdf[:, (- 1)][:, numpy.newaxis]))
median_idx = median_or_above.argmax(axis=1)
tempResult = arange(X.shape[0])
	
===================================================================	
AdaBoostRegressor._get_median_predict: 341	
----------------------------	

predictions = np.array([est.predict(X) for est in self.estimators_[:limit]]).T
sorted_idx = numpy.argsort(predictions, axis=1)
weight_cdf = self.estimator_weights_[sorted_idx].cumsum(axis=1)
median_or_above = (weight_cdf >= (0.5 * weight_cdf[:, (- 1)][:, numpy.newaxis]))
median_idx = median_or_above.argmax(axis=1)
median_estimators = sorted_idx[(numpy.arange(X.shape[0]), median_idx)]
tempResult = arange(X.shape[0])
	
===================================================================	
_to_graph: 73	
----------------------------	

'Auxiliary function for img_to_graph and grid_to_graph\n    '
edges = _make_edges_3d(n_x, n_y, n_z)
if (dtype is None):
    if (img is None):
        dtype = numpy.int
    else:
        dtype = img.dtype
if (img is not None):
    img = numpy.atleast_3d(img)
    weights = _compute_gradient_3d(edges, img)
    if (mask is not None):
        (edges, weights) = _mask_edges_weights(mask, edges, weights)
        diag = img.squeeze()[mask]
    else:
        diag = img.ravel()
    n_voxels = diag.size
else:
    if (mask is not None):
        mask = astype(mask, dtype=numpy.bool, copy=False)
        mask = numpy.asarray(mask, dtype=numpy.bool)
        edges = _mask_edges_weights(mask, edges)
        n_voxels = numpy.sum(mask)
    else:
        n_voxels = ((n_x * n_y) * n_z)
    weights = numpy.ones(edges.shape[1], dtype=dtype)
    diag = numpy.ones(n_voxels, dtype=dtype)
tempResult = arange(n_voxels)
	
===================================================================	
_make_edges_3d: 15	
----------------------------	

'Returns a list of edges for a 3D image.\n\n    Parameters\n    ===========\n    n_x: integer\n        The size of the grid in the x direction.\n    n_y: integer\n        The size of the grid in the y direction.\n    n_z: integer, optional\n        The size of the grid in the z direction, defaults to 1\n    '
tempResult = arange(((n_x * n_y) * n_z))
	
===================================================================	
_mask_edges_weights: 29	
----------------------------	

'Apply a mask to edges (weighted or not)'
tempResult = arange(mask.size)
	
===================================================================	
_mask_edges_weights: 39	
----------------------------	

'Apply a mask to edges (weighted or not)'
inds = numpy.arange(mask.size)
inds = inds[mask.ravel()]
ind_mask = numpy.logical_and(numpy.in1d(edges[0], inds), numpy.in1d(edges[1], inds))
edges = edges[:, ind_mask]
if (weights is not None):
    weights = weights[ind_mask]
if len(edges.ravel()):
    maxval = edges.max()
else:
    maxval = 0
tempResult = arange((maxval + 1))
	
===================================================================	
test_extract_patches_strided: 214	
----------------------------	

image_shapes_1D = [(10,), (10,), (11,), (10,)]
patch_sizes_1D = [(1,), (2,), (3,), (8,)]
patch_steps_1D = [(1,), (1,), (4,), (2,)]
expected_views_1D = [(10,), (9,), (3,), (2,)]
last_patch_1D = [(10,), (8,), (8,), (2,)]
image_shapes_2D = [(10, 20), (10, 20), (10, 20), (11, 20)]
patch_sizes_2D = [(2, 2), (10, 10), (10, 11), (6, 6)]
patch_steps_2D = [(5, 5), (3, 10), (3, 4), (4, 2)]
expected_views_2D = [(2, 4), (1, 2), (1, 3), (2, 8)]
last_patch_2D = [(5, 15), (0, 10), (0, 8), (4, 14)]
image_shapes_3D = [(5, 4, 3), (3, 3, 3), (7, 8, 9), (7, 8, 9)]
patch_sizes_3D = [(2, 2, 3), (2, 2, 2), (1, 7, 3), (1, 3, 3)]
patch_steps_3D = [(1, 2, 10), (1, 1, 1), (2, 1, 3), (3, 3, 4)]
expected_views_3D = [(4, 2, 1), (2, 2, 2), (4, 2, 3), (3, 2, 2)]
last_patch_3D = [(3, 2, 0), (1, 1, 1), (6, 1, 6), (6, 3, 4)]
image_shapes = ((image_shapes_1D + image_shapes_2D) + image_shapes_3D)
patch_sizes = ((patch_sizes_1D + patch_sizes_2D) + patch_sizes_3D)
patch_steps = ((patch_steps_1D + patch_steps_2D) + patch_steps_3D)
expected_views = ((expected_views_1D + expected_views_2D) + expected_views_3D)
last_patches = ((last_patch_1D + last_patch_2D) + last_patch_3D)
for (image_shape, patch_size, patch_step, expected_view, last_patch) in zip(image_shapes, patch_sizes, patch_steps, expected_views, last_patches):
    tempResult = arange(numpy.prod(image_shape))
	
===================================================================	
RFE._fit: 57	
----------------------------	

(X, y) = check_X_y(X, y, 'csc')
n_features = X.shape[1]
if (self.n_features_to_select is None):
    n_features_to_select = (n_features // 2)
else:
    n_features_to_select = self.n_features_to_select
if (0.0 < self.step < 1.0):
    step = int(max(1, (self.step * n_features)))
else:
    step = int(self.step)
if (step <= 0):
    raise ValueError('Step must be >0')
support_ = numpy.ones(n_features, dtype=numpy.bool)
ranking_ = numpy.ones(n_features, dtype=numpy.int)
if step_score:
    self.scores_ = []
while (numpy.sum(support_) > n_features_to_select):
    tempResult = arange(n_features)
	
===================================================================	
RFE._fit: 78	
----------------------------	

(X, y) = check_X_y(X, y, 'csc')
n_features = X.shape[1]
if (self.n_features_to_select is None):
    n_features_to_select = (n_features // 2)
else:
    n_features_to_select = self.n_features_to_select
if (0.0 < self.step < 1.0):
    step = int(max(1, (self.step * n_features)))
else:
    step = int(self.step)
if (step <= 0):
    raise ValueError('Step must be >0')
support_ = numpy.ones(n_features, dtype=numpy.bool)
ranking_ = numpy.ones(n_features, dtype=numpy.int)
if step_score:
    self.scores_ = []
while (numpy.sum(support_) > n_features_to_select):
    features = numpy.arange(n_features)[support_]
    estimator = clone(self.estimator)
    if (self.verbose > 0):
        print(('Fitting estimator with %d features.' % numpy.sum(support_)))
    estimator.fit(X[:, features], y)
    if hasattr(estimator, 'coef_'):
        coefs = estimator.coef_
    elif hasattr(estimator, 'feature_importances_'):
        coefs = estimator.feature_importances_
    else:
        raise RuntimeError('The classifier does not expose "coef_" or "feature_importances_" attributes')
    if (coefs.ndim > 1):
        ranks = numpy.argsort(safe_sqr(coefs).sum(axis=0))
    else:
        ranks = numpy.argsort(safe_sqr(coefs))
    ranks = numpy.ravel(ranks)
    threshold = min(step, (numpy.sum(support_) - n_features_to_select))
    if step_score:
        self.scores_.append(step_score(estimator, features))
    support_[features[ranks][:threshold]] = False
    ranking_[numpy.logical_not(support_)] += 1
tempResult = arange(n_features)
	
===================================================================	
SelectFdr._get_support_mask: 194	
----------------------------	

check_is_fitted(self, 'scores_')
n_features = len(self.pvalues_)
sv = numpy.sort(self.pvalues_)
tempResult = arange(n_features)
	
===================================================================	
module: 27	
----------------------------	

import numpy as np
from scipy import sparse as sp
from nose.tools import assert_raises, assert_equal
from numpy.testing import assert_array_equal
from sklearn.base import BaseEstimator
from sklearn.feature_selection.base import SelectorMixin
from sklearn.utils import check_array

class StepSelector(SelectorMixin, BaseEstimator):
    'Retain every `step` features (beginning with 0)'

    def __init__(self, step=2):
        self.step = step

    def fit(self, X, y=None):
        X = check_array(X, 'csc')
        self.n_input_feats = X.shape[1]
        return self

    def _get_support_mask(self):
        mask = numpy.zeros(self.n_input_feats, dtype=bool)
        mask[::self.step] = True
        return mask
support = ([True, False] * 5)
support_inds = [0, 2, 4, 6, 8]
tempResult = arange(20)
	
===================================================================	
module: 28	
----------------------------	

import numpy as np
from scipy import sparse as sp
from nose.tools import assert_raises, assert_equal
from numpy.testing import assert_array_equal
from sklearn.base import BaseEstimator
from sklearn.feature_selection.base import SelectorMixin
from sklearn.utils import check_array

class StepSelector(SelectorMixin, BaseEstimator):
    'Retain every `step` features (beginning with 0)'

    def __init__(self, step=2):
        self.step = step

    def fit(self, X, y=None):
        X = check_array(X, 'csc')
        self.n_input_feats = X.shape[1]
        return self

    def _get_support_mask(self):
        mask = numpy.zeros(self.n_input_feats, dtype=bool)
        mask[::self.step] = True
        return mask
support = ([True, False] * 5)
support_inds = [0, 2, 4, 6, 8]
X = np.arange(20).reshape(2, 10)
tempResult = arange(0, 20, 2)
	
===================================================================	
test_tied_scores: 311	
----------------------------	

X_train = numpy.array([[0, 0, 0], [1, 1, 1]])
y_train = [0, 1]
for n_features in [1, 2, 3]:
    sel = SelectKBest(chi2, k=n_features).fit(X_train, y_train)
    X_test = sel.transform([[0, 1, 2]])
    tempResult = arange(3)
	
===================================================================	
test_f_regression_center: 79	
----------------------------	

tempResult = arange((- 5), 6)
	
===================================================================	
test_f_regression_input_dtype: 72	
----------------------------	

rng = numpy.random.RandomState(0)
X = rng.rand(10, 20)
tempResult = arange(10)
	
===================================================================	
test_f_oneway_ints: 38	
----------------------------	

rng = numpy.random.RandomState(0)
X = rng.randint(10, size=(10, 10))
tempResult = arange(10)
	
===================================================================	
l1_cross_distances: 27	
----------------------------	

'\n    Computes the nonzero componentwise L1 cross-distances between the vectors\n    in X.\n\n    Parameters\n    ----------\n\n    X: array_like\n        An array with shape (n_samples, n_features)\n\n    Returns\n    -------\n\n    D: array with shape (n_samples * (n_samples - 1) / 2, n_features)\n        The array of componentwise L1 cross-distances.\n\n    ij: arrays with shape (n_samples * (n_samples - 1) / 2, 2)\n        The indices i and j of the vectors in X associated to the cross-\n        distances in D: D[k] = np.abs(X[ij[k, 0]] - Y[ij[k, 1]]).\n    '
X = check_array(X)
(n_samples, n_features) = X.shape
n_nonzero_cross_dist = ((n_samples * (n_samples - 1)) // 2)
ij = numpy.zeros((n_nonzero_cross_dist, 2), dtype=numpy.int)
D = numpy.zeros((n_nonzero_cross_dist, n_features))
ll_1 = 0
for k in range((n_samples - 1)):
    ll_0 = ll_1
    ll_1 = (((ll_0 + n_samples) - k) - 1)
    ij[ll_0:ll_1, 0] = k
    tempResult = arange((k + 1), n_samples)
	
===================================================================	
lars_path: 36	
----------------------------	

'Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n\n    The optimization objective for the case method=\'lasso\' is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    in the case of method=\'lars\', the objective function is only known in\n    the form of an implicit equation (see discussion in [1])\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    -----------\n    X : array, shape: (n_samples, n_features)\n        Input data.\n\n    y : array, shape: (n_samples)\n        Input targets.\n\n    positive : boolean (default=False)\n        Restrict coefficients to be >= 0.\n        When using this option together with method \'lasso\' the model\n        coefficients will not converge to the ordinary-least-squares solution\n        for small values of alpha (neither will they when using method \'lar\'\n        ..). Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent lasso_path function.\n\n    max_iter : integer, optional (default=500)\n        Maximum number of iterations to perform, set to infinity for no limit.\n\n    Gram : None, \'auto\', array, shape: (n_features, n_features), optional\n        Precomputed Gram matrix (X\' * X), if ``\'auto\'``, the Gram\n        matrix is precomputed from the given X, if there are more samples\n        than features.\n\n    alpha_min : float, optional (default=0)\n        Minimum correlation along the path. It corresponds to the\n        regularization parameter alpha parameter in the Lasso.\n\n    method : {\'lar\', \'lasso\'}, optional (default=\'lar\')\n        Specifies the returned model. Select ``\'lar\'`` for Least Angle\n        Regression, ``\'lasso\'`` for the Lasso.\n\n    eps : float, optional (default=``np.finfo(np.float).eps``)\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems.\n\n    copy_X : bool, optional (default=True)\n        If ``False``, ``X`` is overwritten.\n\n    copy_Gram : bool, optional (default=True)\n        If ``False``, ``Gram`` is overwritten.\n\n    verbose : int (default=0)\n        Controls output verbosity.\n\n    return_path : bool, optional (default=True)\n        If ``return_path==True`` returns the entire path, else returns only the\n        last point of the path.\n\n    return_n_iter : bool, optional (default=False)\n        Whether to return the number of iterations.\n\n    Returns\n    --------\n    alphas : array, shape: [n_alphas + 1]\n        Maximum of covariances (in absolute value) at each iteration.\n        ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n        number of nodes in the path with ``alpha >= alpha_min``, whichever\n        is smaller.\n\n    active : array, shape [n_alphas]\n        Indices of active variables at the end of the path.\n\n    coefs : array, shape (n_features, n_alphas + 1)\n        Coefficients along the path\n\n    n_iter : int\n        Number of iterations run. Returned only if return_n_iter is set\n        to True.\n\n    See also\n    --------\n    lasso_path\n    LassoLars\n    Lars\n    LassoLarsCV\n    LarsCV\n    sklearn.decomposition.sparse_encode\n\n    References\n    ----------\n    .. [1] "Least Angle Regression", Effron et al.\n           http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n\n    .. [2] `Wikipedia entry on the Least-angle regression\n           <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n\n    .. [3] `Wikipedia entry on the Lasso\n           <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n\n    '
n_features = X.shape[1]
n_samples = y.size
max_features = min(max_iter, n_features)
if return_path:
    coefs = numpy.zeros(((max_features + 1), n_features))
    alphas = numpy.zeros((max_features + 1))
else:
    (coef, prev_coef) = (numpy.zeros(n_features), numpy.zeros(n_features))
    (alpha, prev_alpha) = (numpy.array([0.0]), numpy.array([0.0]))
(n_iter, n_active) = (0, 0)
tempResult = arange(n_features)
	
===================================================================	
_gram_omp: 79	
----------------------------	

'Orthogonal Matching Pursuit step on a precomputed Gram matrix.\n\n    This function uses the Cholesky decomposition method.\n\n    Parameters\n    ----------\n    Gram : array, shape (n_features, n_features)\n        Gram matrix of the input data matrix\n\n    Xy : array, shape (n_features,)\n        Input targets\n\n    n_nonzero_coefs : int\n        Targeted number of non-zero elements\n\n    tol_0 : float\n        Squared norm of y, required if tol is not None.\n\n    tol : float\n        Targeted squared error, if not None overrides n_nonzero_coefs.\n\n    copy_Gram : bool, optional\n        Whether the gram matrix must be copied by the algorithm. A false\n        value is only helpful if it is already Fortran-ordered, otherwise a\n        copy is made anyway.\n\n    copy_Xy : bool, optional\n        Whether the covariance vector Xy must be copied by the algorithm.\n        If False, it may be overwritten.\n\n    return_path : bool, optional. Default: False\n        Whether to return every value of the nonzero coefficients along the\n        forward path. Useful for cross-validation.\n\n    Returns\n    -------\n    gamma : array, shape (n_nonzero_coefs,)\n        Non-zero elements of the solution\n\n    idx : array, shape (n_nonzero_coefs,)\n        Indices of the positions of the elements in gamma within the solution\n        vector\n\n    coefs : array, shape (n_features, n_nonzero_coefs)\n        The first k values of column k correspond to the coefficient value\n        for the active features at that step. The lower left triangle contains\n        garbage. Only returned if ``return_path=True``.\n\n    n_active : int\n        Number of active features at convergence.\n    '
Gram = (Gram.copy('F') if copy_Gram else numpy.asfortranarray(Gram))
if copy_Xy:
    Xy = Xy.copy()
min_float = np.finfo(Gram.dtype).eps
(nrm2, swap) = scipy.linalg.get_blas_funcs(('nrm2', 'swap'), (Gram,))
(potrs,) = get_lapack_funcs(('potrs',), (Gram,))
tempResult = arange(len(Gram))
	
===================================================================	
_cholesky_omp: 32	
----------------------------	

'Orthogonal Matching Pursuit step using the Cholesky decomposition.\n\n    Parameters\n    ----------\n    X : array, shape (n_samples, n_features)\n        Input dictionary. Columns are assumed to have unit norm.\n\n    y : array, shape (n_samples,)\n        Input targets\n\n    n_nonzero_coefs : int\n        Targeted number of non-zero elements\n\n    tol : float\n        Targeted squared error, if not None overrides n_nonzero_coefs.\n\n    copy_X : bool, optional\n        Whether the design matrix X must be copied by the algorithm. A false\n        value is only helpful if X is already Fortran-ordered, otherwise a\n        copy is made anyway.\n\n    return_path : bool, optional. Default: False\n        Whether to return every value of the nonzero coefficients along the\n        forward path. Useful for cross-validation.\n\n    Returns\n    -------\n    gamma : array, shape (n_nonzero_coefs,)\n        Non-zero elements of the solution\n\n    idx : array, shape (n_nonzero_coefs,)\n        Indices of the positions of the elements in gamma within the solution\n        vector\n\n    coef : array, shape (n_features, n_nonzero_coefs)\n        The first k values of column k correspond to the coefficient value\n        for the active features at that step. The lower left triangle contains\n        garbage. Only returned if ``return_path=True``.\n\n    n_active : int\n        Number of active features at convergence.\n    '
if copy_X:
    X = X.copy('F')
else:
    X = numpy.asfortranarray(X)
min_float = np.finfo(X.dtype).eps
(nrm2, swap) = scipy.linalg.get_blas_funcs(('nrm2', 'swap'), (X,))
(potrs,) = get_lapack_funcs(('potrs',), (X,))
alpha = numpy.dot(X.T, y)
residual = y
gamma = numpy.empty(0)
n_active = 0
tempResult = arange(X.shape[1])
	
===================================================================	
RANSACRegressor.fit: 100	
----------------------------	

'Fit estimator using RANSAC algorithm.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape [n_samples, n_features]\n            Training data.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_targets]\n            Target values.\n\n        sample_weight: array-like, shape = [n_samples]\n            Individual weights for each sample\n            raises error if sample_weight is passed and base_estimator\n            fit method does not support it.\n\n        Raises\n        ------\n        ValueError\n            If no valid consensus set could be found. This occurs if\n            `is_data_valid` and `is_model_valid` return False for all\n            `max_trials` randomly chosen sub-samples.\n\n        '
X = check_array(X, accept_sparse='csr')
y = check_array(y, ensure_2d=False)
check_consistent_length(X, y)
if (self.base_estimator is not None):
    base_estimator = clone(self.base_estimator)
else:
    base_estimator = LinearRegression()
if (self.min_samples is None):
    min_samples = (X.shape[1] + 1)
elif (0 < self.min_samples < 1):
    min_samples = numpy.ceil((self.min_samples * X.shape[0]))
elif (self.min_samples >= 1):
    if ((self.min_samples % 1) != 0):
        raise ValueError('Absolute number of samples must be an integer value.')
    min_samples = self.min_samples
else:
    raise ValueError('Value for `min_samples` must be scalar and positive.')
if (min_samples > X.shape[0]):
    raise ValueError('`min_samples` may not be larger than number of samples ``X.shape[0]``.')
if ((self.stop_probability < 0) or (self.stop_probability > 1)):
    raise ValueError('`stop_probability` must be in range [0, 1].')
if (self.residual_threshold is None):
    residual_threshold = numpy.median(numpy.abs((y - numpy.median(y))))
else:
    residual_threshold = self.residual_threshold
if (self.residual_metric is not None):
    warnings.warn("'residual_metric' was deprecated in version 0.18 and will be removed in version 0.20. Use 'loss' instead.", DeprecationWarning)
if (self.loss == 'absolute_loss'):
    if (y.ndim == 1):
        loss_function = (lambda y_true, y_pred: numpy.abs((y_true - y_pred)))
    else:
        loss_function = (lambda y_true, y_pred: numpy.sum(numpy.abs((y_true - y_pred)), axis=1))
elif (self.loss == 'squared_loss'):
    if (y.ndim == 1):
        loss_function = (lambda y_true, y_pred: ((y_true - y_pred) ** 2))
    else:
        loss_function = (lambda y_true, y_pred: numpy.sum(((y_true - y_pred) ** 2), axis=1))
elif callable(self.loss):
    loss_function = self.loss
else:
    raise ValueError(("loss should be 'absolute_loss', 'squared_loss' or a callable.Got %s. " % self.loss))
random_state = check_random_state(self.random_state)
try:
    base_estimator.set_params(random_state=random_state)
except ValueError:
    pass
estimator_fit_has_sample_weight = has_fit_parameter(base_estimator, 'sample_weight')
estimator_name = type(base_estimator).__name__
if ((sample_weight is not None) and (not estimator_fit_has_sample_weight)):
    raise ValueError(('%s does not support sample_weight. Samples weights are only used for the calibration itself.' % estimator_name))
if (sample_weight is not None):
    sample_weight = numpy.asarray(sample_weight)
n_inliers_best = 0
score_best = numpy.inf
inlier_mask_best = None
X_inlier_best = None
y_inlier_best = None
n_samples = X.shape[0]
tempResult = arange(n_samples)
	
===================================================================	
test_no_warning_for_zero_mse: 278	
----------------------------	

tempResult = arange(10, dtype=float)
	
===================================================================	
test_lasso_lars_vs_lasso_cd_ill_conditioned: 190	
----------------------------	

rng = numpy.random.RandomState(42)
(n, m) = (70, 100)
k = 5
X = rng.randn(n, m)
w = numpy.zeros((m, 1))
tempResult = arange(0, m)
	
===================================================================	
test_lars_add_features: 220	
----------------------------	

n = 5
tempResult = arange(1, (n + 1))
	
===================================================================	
test_lars_add_features: 220	
----------------------------	

n = 5
tempResult = arange(n)
	
===================================================================	
test_lars_add_features: 221	
----------------------------	

n = 5
H = (1.0 / (numpy.arange(1, (n + 1)) + numpy.arange(n)[:, numpy.newaxis]))
tempResult = arange(n)
	
===================================================================	
module: 16	
----------------------------	

import numpy as np
import scipy.sparse as sp
from sklearn.utils.testing import assert_less
from sklearn.utils.testing import assert_greater
from sklearn.utils.testing import assert_array_almost_equal, assert_array_equal
from sklearn.utils.testing import assert_almost_equal
from sklearn.utils.testing import assert_raises
from sklearn.base import ClassifierMixin
from sklearn.utils import check_random_state
from sklearn.datasets import load_iris
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import PassiveAggressiveRegressor
iris = load_iris()
random_state = check_random_state(12)
tempResult = arange(iris.data.shape[0])
	
===================================================================	
module: 12	
----------------------------	

import numpy as np
import scipy.sparse as sp
from sklearn.utils.testing import assert_array_almost_equal
from sklearn.utils.testing import assert_true
from sklearn.utils.testing import assert_raises
from sklearn.utils import check_random_state
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron
iris = load_iris()
random_state = check_random_state(12)
tempResult = arange(iris.data.shape[0])
	
===================================================================	
module: 15	
----------------------------	

from scipy import sparse
import numpy as np
from scipy import sparse
from numpy.testing import assert_equal, assert_raises
from numpy.testing import assert_array_almost_equal
from numpy.testing import assert_array_equal
from sklearn.utils import check_random_state
from sklearn.utils.testing import assert_raises_regexp
from sklearn.utils.testing import assert_less
from sklearn.utils.testing import assert_warns
from sklearn.utils.testing import assert_almost_equal
from sklearn.linear_model import LinearRegression, RANSACRegressor, Lasso
from sklearn.linear_model.ransac import _dynamic_max_trials
tempResult = arange((- 200), 200)
	
===================================================================	
test_ransac_score: 77	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_ransac_predict: 88	
----------------------------	

tempResult = arange(100)
	
===================================================================	
module: 36	
----------------------------	

import numpy as np
import scipy.sparse as sp
from scipy import linalg
from itertools import product
from sklearn.utils.testing import assert_true
from sklearn.utils.testing import assert_almost_equal
from sklearn.utils.testing import assert_array_almost_equal
from sklearn.utils.testing import assert_equal
from sklearn.utils.testing import assert_array_equal
from sklearn.utils.testing import assert_greater
from sklearn.utils.testing import assert_raises
from sklearn.utils.testing import assert_raise_message
from sklearn.utils.testing import ignore_warnings
from sklearn.utils.testing import assert_warns
from sklearn import datasets
from sklearn.metrics import mean_squared_error
from sklearn.metrics import make_scorer
from sklearn.metrics import get_scorer
from sklearn.linear_model.base import LinearRegression
from sklearn.linear_model.ridge import ridge_regression
from sklearn.linear_model.ridge import Ridge
from sklearn.linear_model.ridge import _RidgeGCV
from sklearn.linear_model.ridge import RidgeCV
from sklearn.linear_model.ridge import RidgeClassifier
from sklearn.linear_model.ridge import RidgeClassifierCV
from sklearn.linear_model.ridge import _solve_cholesky
from sklearn.linear_model.ridge import _solve_cholesky_kernel
from sklearn.datasets import make_regression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.utils import check_random_state
from sklearn.datasets import make_multilabel_classification
diabetes = sklearn.datasets.load_diabetes()
(X_diabetes, y_diabetes) = (diabetes.data, diabetes.target)
tempResult = arange(X_diabetes.shape[0])
	
===================================================================	
test_ridge_individual_penalties: 195	
----------------------------	

rng = numpy.random.RandomState(42)
(n_samples, n_features, n_targets) = (20, 10, 5)
X = rng.randn(n_samples, n_features)
y = rng.randn(n_samples, n_targets)
tempResult = arange(n_targets)
	
===================================================================	
_test_ridge_loo: 214	
----------------------------	

n_samples = X_diabetes.shape[0]
ret = []
ridge_gcv = _RidgeGCV(fit_intercept=False)
ridge = Ridge(alpha=1.0, fit_intercept=False)
decomp = ridge_gcv._pre_compute(X_diabetes, y_diabetes)
(errors, c) = ridge_gcv._errors(1.0, y_diabetes, *decomp)
(values, c) = ridge_gcv._values(1.0, y_diabetes, *decomp)
errors2 = []
values2 = []
for i in range(n_samples):
    tempResult = arange(n_samples)
	
===================================================================	
DenseSGDClassifierTestCase.test_balanced_weight: 432	
----------------------------	

(X, y) = (iris.data, iris.target)
X = scale(X)
tempResult = arange(X.shape[0])
	
===================================================================	
DenseSGDClassifierTestCase.test_sgd_l1: 368	
----------------------------	

n = len(X4)
rng = numpy.random.RandomState(13)
tempResult = arange(n)
	
===================================================================	
DenseSGDClassifierTestCase.test_wrong_sample_weights: 471	
----------------------------	

clf = self.factory(alpha=0.1, n_iter=1000, fit_intercept=False)
tempResult = arange(7)
	
===================================================================	
barycenter_kneighbors_graph: 40	
----------------------------	

"Computes the barycenter weighted graph of k-Neighbors for points in X\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}\n        Sample data, shape = (n_samples, n_features), in the form of a\n        numpy array, sparse array, precomputed tree, or NearestNeighbors\n        object.\n\n    n_neighbors : int\n        Number of neighbors for each sample.\n\n    reg : float, optional\n        Amount of regularization when solving the least-squares\n        problem. Only relevant if mode='barycenter'. If None, use the\n        default.\n\n    n_jobs : int, optional (default = 1)\n        The number of parallel jobs to run for neighbors search.\n        If ``-1``, then the number of jobs is set to the number of CPU cores.\n\n    Returns\n    -------\n    A : sparse matrix in CSR format, shape = [n_samples, n_samples]\n        A[i, j] is assigned the weight of edge that connects i to j.\n\n    See also\n    --------\n    sklearn.neighbors.kneighbors_graph\n    sklearn.neighbors.radius_neighbors_graph\n    "
knn = NearestNeighbors((n_neighbors + 1), n_jobs=n_jobs).fit(X)
X = knn._fit_X
n_samples = X.shape[0]
ind = knn.kneighbors(X, return_distance=False)[:, 1:]
data = barycenter_weights(X, X[ind], reg=reg)
tempResult = arange(0, ((n_samples * n_neighbors) + 1), n_neighbors)
	
===================================================================	
_smacof_single: 45	
----------------------------	

'\n    Computes multidimensional scaling using SMACOF algorithm\n\n    Parameters\n    ----------\n    similarities: symmetric ndarray, shape [n * n]\n        similarities between the points\n\n    metric: boolean, optional, default: True\n        compute metric or nonmetric SMACOF algorithm\n\n    n_components: int, optional, default: 2\n        number of dimension in which to immerse the similarities\n        overwritten if initial array is provided.\n\n    init: {None or ndarray}, optional\n        if None, randomly chooses the initial configuration\n        if ndarray, initialize the SMACOF algorithm with this array\n\n    max_iter: int, optional, default: 300\n        Maximum number of iterations of the SMACOF algorithm for a single run\n\n    verbose: int, optional, default: 0\n        level of verbosity\n\n    eps: float, optional, default: 1e-6\n        relative tolerance w.r.t stress to declare converge\n\n    random_state: integer or numpy.RandomState, optional\n        The generator used to initialize the centers. If an integer is\n        given, it fixes the seed. Defaults to the global numpy random\n        number generator.\n\n    Returns\n    -------\n    X: ndarray (n_samples, n_components), float\n               coordinates of the n_samples points in a n_components-space\n\n    stress_: float\n        The final value of the stress (sum of squared distance of the\n        disparities and the distances for all constrained points)\n\n    n_iter : int\n        Number of iterations run.\n\n    '
similarities = check_symmetric(similarities, raise_exception=True)
n_samples = similarities.shape[0]
random_state = check_random_state(random_state)
sim_flat = ((1 - np.tri(n_samples)) * similarities).ravel()
sim_flat_w = sim_flat[(sim_flat != 0)]
if (init is None):
    X = random_state.rand((n_samples * n_components))
    X = X.reshape((n_samples, n_components))
else:
    n_components = init.shape[1]
    if (n_samples != init.shape[0]):
        raise ValueError(('init matrix should be of shape (%d, %d)' % (n_samples, n_components)))
    X = init
old_stress = None
ir = IsotonicRegression()
for it in range(max_iter):
    dis = euclidean_distances(X)
    if metric:
        disparities = similarities
    else:
        dis_flat = dis.ravel()
        dis_flat_w = dis_flat[(sim_flat != 0)]
        disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)
        disparities = dis_flat.copy()
        disparities[(sim_flat != 0)] = disparities_flat
        disparities = disparities.reshape((n_samples, n_samples))
        disparities *= numpy.sqrt((((n_samples * (n_samples - 1)) / 2) / (disparities ** 2).sum()))
    stress = (((dis.ravel() - disparities.ravel()) ** 2).sum() / 2)
    dis[(dis == 0)] = 1e-05
    ratio = (disparities / dis)
    B = (- ratio)
    tempResult = arange(len(B))
	
===================================================================	
_smacof_single: 45	
----------------------------	

'\n    Computes multidimensional scaling using SMACOF algorithm\n\n    Parameters\n    ----------\n    similarities: symmetric ndarray, shape [n * n]\n        similarities between the points\n\n    metric: boolean, optional, default: True\n        compute metric or nonmetric SMACOF algorithm\n\n    n_components: int, optional, default: 2\n        number of dimension in which to immerse the similarities\n        overwritten if initial array is provided.\n\n    init: {None or ndarray}, optional\n        if None, randomly chooses the initial configuration\n        if ndarray, initialize the SMACOF algorithm with this array\n\n    max_iter: int, optional, default: 300\n        Maximum number of iterations of the SMACOF algorithm for a single run\n\n    verbose: int, optional, default: 0\n        level of verbosity\n\n    eps: float, optional, default: 1e-6\n        relative tolerance w.r.t stress to declare converge\n\n    random_state: integer or numpy.RandomState, optional\n        The generator used to initialize the centers. If an integer is\n        given, it fixes the seed. Defaults to the global numpy random\n        number generator.\n\n    Returns\n    -------\n    X: ndarray (n_samples, n_components), float\n               coordinates of the n_samples points in a n_components-space\n\n    stress_: float\n        The final value of the stress (sum of squared distance of the\n        disparities and the distances for all constrained points)\n\n    n_iter : int\n        Number of iterations run.\n\n    '
similarities = check_symmetric(similarities, raise_exception=True)
n_samples = similarities.shape[0]
random_state = check_random_state(random_state)
sim_flat = ((1 - np.tri(n_samples)) * similarities).ravel()
sim_flat_w = sim_flat[(sim_flat != 0)]
if (init is None):
    X = random_state.rand((n_samples * n_components))
    X = X.reshape((n_samples, n_components))
else:
    n_components = init.shape[1]
    if (n_samples != init.shape[0]):
        raise ValueError(('init matrix should be of shape (%d, %d)' % (n_samples, n_components)))
    X = init
old_stress = None
ir = IsotonicRegression()
for it in range(max_iter):
    dis = euclidean_distances(X)
    if metric:
        disparities = similarities
    else:
        dis_flat = dis.ravel()
        dis_flat_w = dis_flat[(sim_flat != 0)]
        disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)
        disparities = dis_flat.copy()
        disparities[(sim_flat != 0)] = disparities_flat
        disparities = disparities.reshape((n_samples, n_samples))
        disparities *= numpy.sqrt((((n_samples * (n_samples - 1)) / 2) / (disparities ** 2).sum()))
    stress = (((dis.ravel() - disparities.ravel()) ** 2).sum() / 2)
    dis[(dis == 0)] = 1e-05
    ratio = (disparities / dis)
    B = (- ratio)
    tempResult = arange(len(B))
	
===================================================================	
test_lle_manifold: 46	
----------------------------	

rng = numpy.random.RandomState(0)
tempResult = arange(18)
	
===================================================================	
test_trustworthiness: 163	
----------------------------	

random_state = check_random_state(0)
X = random_state.randn(100, 2)
assert_equal(trustworthiness(X, (5.0 + (X / 10.0))), 1.0)
tempResult = arange(100)
	
===================================================================	
test_trustworthiness: 167	
----------------------------	

random_state = check_random_state(0)
X = random_state.randn(100, 2)
assert_equal(trustworthiness(X, (5.0 + (X / 10.0))), 1.0)
X = np.arange(100).reshape((- 1), 1)
X_embedded = X.copy()
random_state.shuffle(X_embedded)
assert_less(trustworthiness(X, X_embedded), 0.6)
tempResult = arange(5)
	
===================================================================	
hinge_loss: 398	
----------------------------	

"Average hinge loss (non-regularized)\n\n    In binary class case, assuming labels in y_true are encoded with +1 and -1,\n    when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n    always negative (since the signs disagree), implying ``1 - margin`` is\n    always greater than 1.  The cumulated hinge loss is therefore an upper\n    bound of the number of mistakes made by the classifier.\n\n    In multiclass case, the function expects that either all the labels are\n    included in y_true or an optional labels argument is provided which\n    contains all the labels. The multilabel margin is calculated according\n    to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n    is an upper bound of the number of mistakes made by the classifier.\n\n    Read more in the :ref:`User Guide <hinge_loss>`.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        True target, consisting of integers of two values. The positive label\n        must be greater than the negative label.\n\n    pred_decision : array, shape = [n_samples] or [n_samples, n_classes]\n        Predicted decisions, as output by decision_function (floats).\n\n    labels : array, optional, default None\n        Contains all the labels for the problem. Used in multiclass hinge loss.\n\n    sample_weight : array-like of shape = [n_samples], optional\n        Sample weights.\n\n    Returns\n    -------\n    loss : float\n\n    References\n    ----------\n    .. [1] `Wikipedia entry on the Hinge loss\n           <https://en.wikipedia.org/wiki/Hinge_loss>`_\n\n    .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n           Implementation of Multiclass Kernel-based Vector\n           Machines. Journal of Machine Learning Research 2,\n           (2001), 265-292\n\n    .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n           by Robert C. Moore, John DeNero.\n           <http://www.ttic.edu/sigml/symposium2011/papers/\n           Moore+DeNero_Regularization.pdf>`_\n\n    Examples\n    --------\n    >>> from sklearn import svm\n    >>> from sklearn.metrics import hinge_loss\n    >>> X = [[0], [1]]\n    >>> y = [-1, 1]\n    >>> est = svm.LinearSVC(random_state=0)\n    >>> est.fit(X, y)\n    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n         intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n         multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n         verbose=0)\n    >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n    >>> pred_decision  # doctest: +ELLIPSIS\n    array([-2.18...,  2.36...,  0.09...])\n    >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS\n    0.30...\n\n    In the multiclass case:\n\n    >>> X = np.array([[0], [1], [2], [3]])\n    >>> Y = np.array([0, 1, 2, 3])\n    >>> labels = np.array([0, 1, 2, 3])\n    >>> est = svm.LinearSVC()\n    >>> est.fit(X, Y)\n    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n         intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n         verbose=0)\n    >>> pred_decision = est.decision_function([[-1], [2], [3]])\n    >>> y_true = [0, 2, 3]\n    >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS\n    0.56...\n    "
check_consistent_length(y_true, pred_decision, sample_weight)
pred_decision = check_array(pred_decision, ensure_2d=False)
y_true = column_or_1d(y_true)
y_true_unique = numpy.unique(y_true)
if (y_true_unique.size > 2):
    if ((labels is None) and (pred_decision.ndim > 1) and (numpy.size(y_true_unique) != pred_decision.shape[1])):
        raise ValueError('Please include all labels in y_true or pass labels as third argument')
    if (labels is None):
        labels = y_true_unique
    le = LabelEncoder()
    le.fit(labels)
    y_true = le.transform(y_true)
    mask = numpy.ones_like(pred_decision, dtype=bool)
    tempResult = arange(y_true.shape[0])
	
===================================================================	
cohen_kappa_score: 100	
----------------------------	

'Cohen\'s kappa: a statistic that measures inter-annotator agreement.\n\n    This function computes Cohen\'s kappa [1]_, a score that expresses the level\n    of agreement between two annotators on a classification problem. It is\n    defined as\n\n    .. math::\n        \\kappa = (p_o - p_e) / (1 - p_e)\n\n    where :math:`p_o` is the empirical probability of agreement on the label\n    assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n    the expected agreement when both annotators assign labels randomly.\n    :math:`p_e` is estimated using a per-annotator empirical prior over the\n    class labels [2]_.\n\n    Read more in the :ref:`User Guide <cohen_kappa>`.\n\n    Parameters\n    ----------\n    y1 : array, shape = [n_samples]\n        Labels assigned by the first annotator.\n\n    y2 : array, shape = [n_samples]\n        Labels assigned by the second annotator. The kappa statistic is\n        symmetric, so swapping ``y1`` and ``y2`` doesn\'t change the value.\n\n    labels : array, shape = [n_classes], optional\n        List of labels to index the matrix. This may be used to select a\n        subset of labels. If None, all labels that appear at least once in\n        ``y1`` or ``y2`` are used.\n\n    weights : str, optional\n        List of weighting type to calculate the score. None means no weighted;\n        "linear" means linear weighted; "quadratic" means quadratic weighted.\n\n    Returns\n    -------\n    kappa : float\n        The kappa statistic, which is a number between -1 and 1. The maximum\n        value means complete agreement; zero or lower means chance agreement.\n\n    References\n    ----------\n    .. [1] J. Cohen (1960). "A coefficient of agreement for nominal scales".\n           Educational and Psychological Measurement 20(1):37-46.\n           doi:10.1177/001316446002000104.\n    .. [2] `R. Artstein and M. Poesio (2008). "Inter-coder agreement for\n           computational linguistics". Computational Linguistics 34(4):555-596.\n           <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_\n    .. [3] `Wikipedia entry for the Cohen\'s kappa.\n            <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n    '
confusion = confusion_matrix(y1, y2, labels=labels)
n_classes = confusion.shape[0]
sum0 = numpy.sum(confusion, axis=0)
sum1 = numpy.sum(confusion, axis=1)
expected = (numpy.outer(sum0, sum1) / numpy.sum(sum0))
if (weights is None):
    w_mat = numpy.ones([n_classes, n_classes], dtype=numpy.int)
    w_mat.flat[::(n_classes + 1)] = 0
elif ((weights == 'linear') or (weights == 'quadratic')):
    w_mat = numpy.zeros([n_classes, n_classes], dtype=numpy.int)
    tempResult = arange(n_classes)
	
===================================================================	
pairwise_distances_argmin_min: 120	
----------------------------	

"Compute minimum distances between one point and a set of points.\n\n    This function computes for each row in X, the index of the row of Y which\n    is closest (according to the specified distance). The minimal distances are\n    also returned.\n\n    This is mostly equivalent to calling:\n\n        (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n         pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n\n    but uses much less memory, and is faster for large arrays.\n\n    Parameters\n    ----------\n    X, Y : {array-like, sparse matrix}\n        Arrays containing points. Respective shapes (n_samples1, n_features)\n        and (n_samples2, n_features)\n\n    batch_size : integer\n        To reduce memory consumption over the naive solution, data are\n        processed in batches, comprising batch_size rows of X and\n        batch_size rows of Y. The default value is quite conservative, but\n        can be changed for fine-tuning. The larger the number, the larger the\n        memory usage.\n\n    metric : string or callable, default 'euclidean'\n        metric to use for distance computation. Any metric from scikit-learn\n        or scipy.spatial.distance can be used.\n\n        If metric is a callable function, it is called on each\n        pair of instances (rows) and the resulting value recorded. The callable\n        should take two arrays as input and return one value indicating the\n        distance between them. This works for Scipy's metrics, but is less\n        efficient than passing the metric name as a string.\n\n        Distance matrices are not supported.\n\n        Valid values for metric are:\n\n        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n          'manhattan']\n\n        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n          'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n          'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n          'sqeuclidean', 'yule']\n\n        See the documentation for scipy.spatial.distance for details on these\n        metrics.\n\n    metric_kwargs : dict, optional\n        Keyword arguments to pass to specified metric function.\n\n    axis : int, optional, default 1\n        Axis along which the argmin and distances are to be computed.\n\n    Returns\n    -------\n    argmin : numpy.ndarray\n        Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n\n    distances : numpy.ndarray\n        distances[i] is the distance between the i-th row in X and the\n        argmin[i]-th row in Y.\n\n    See also\n    --------\n    sklearn.metrics.pairwise_distances\n    sklearn.metrics.pairwise_distances_argmin\n    "
dist_func = None
if (metric in PAIRWISE_DISTANCE_FUNCTIONS):
    dist_func = PAIRWISE_DISTANCE_FUNCTIONS[metric]
elif ((not callable(metric)) and (not isinstance(metric, str))):
    raise ValueError("'metric' must be a string or a callable")
(X, Y) = check_pairwise_arrays(X, Y)
if (metric_kwargs is None):
    metric_kwargs = {}
if (axis == 0):
    (X, Y) = (Y, X)
indices = numpy.empty(X.shape[0], dtype=numpy.intp)
values = numpy.empty(X.shape[0])
values.fill(numpy.infty)
for chunk_x in gen_batches(X.shape[0], batch_size):
    X_chunk = X[chunk_x, :]
    for chunk_y in gen_batches(Y.shape[0], batch_size):
        Y_chunk = Y[chunk_y, :]
        if (dist_func is not None):
            if (metric == 'euclidean'):
                d_chunk = safe_sparse_dot(X_chunk, Y_chunk.T, dense_output=True)
                d_chunk *= (- 2)
                d_chunk += row_norms(X_chunk, squared=True)[:, numpy.newaxis]
                d_chunk += row_norms(Y_chunk, squared=True)[numpy.newaxis, :]
                numpy.maximum(d_chunk, 0, d_chunk)
            else:
                d_chunk = dist_func(X_chunk, Y_chunk, **metric_kwargs)
        else:
            d_chunk = pairwise_distances(X_chunk, Y_chunk, metric=metric, **metric_kwargs)
        min_indices = d_chunk.argmin(axis=1)
        tempResult = arange((chunk_x.stop - chunk_x.start))
	
===================================================================	
test_exactly_zero_info_score: 140	
----------------------------	

for i in np.logspace(1, 4, 4).astype(numpy.int):
    tempResult = arange(i, dtype=numpy.int)
	
===================================================================	
test_contingency_matrix: 125	
----------------------------	

labels_a = numpy.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])
labels_b = numpy.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])
C = contingency_matrix(labels_a, labels_b)
tempResult = arange(1, 5)
	
===================================================================	
test_contingency_matrix: 125	
----------------------------	

labels_a = numpy.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])
labels_b = numpy.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])
C = contingency_matrix(labels_a, labels_b)
tempResult = arange(1, 5)
	
===================================================================	
test_correct_labelsize: 56	
----------------------------	

dataset = sklearn.datasets.load_iris()
X = dataset.data
tempResult = arange(X.shape[0])
	
===================================================================	
test_calinski_harabaz_score: 77	
----------------------------	

rng = numpy.random.RandomState(seed=0)
assert_raise_message(ValueError, 'Number of labels is', calinski_harabaz_score, rng.rand(10, 2), numpy.zeros(10))
tempResult = arange(10)
	
===================================================================	
test_classification_report_multiclass_with_digits: 350	
----------------------------	

iris = sklearn.datasets.load_iris()
(y_true, y_pred, _) = make_prediction(dataset=iris, binary=False)
expected_report = '             precision    recall  f1-score   support\n\n     setosa    0.82609   0.79167   0.80851        24\n versicolor    0.33333   0.09677   0.15000        31\n  virginica    0.41860   0.90000   0.57143        20\n\navg / total    0.51375   0.53333   0.47310        75\n'
tempResult = arange(len(iris.target_names))
	
===================================================================	
make_prediction: 56	
----------------------------	

'Make some classification predictions on a toy dataset using a SVC\n\n    If binary is True restrict to a binary classification problem instead of a\n    multiclass classification problem\n    '
if (dataset is None):
    dataset = sklearn.datasets.load_iris()
X = dataset.data
y = dataset.target
if binary:
    (X, y) = (X[(y < 2)], y[(y < 2)])
(n_samples, n_features) = X.shape
tempResult = arange(n_samples)
	
===================================================================	
test_precision_recall_f_ignored_labels: 130	
----------------------------	

y_true = [1, 1, 2, 3]
y_pred = [1, 3, 3, 3]
tempResult = arange(5)
	
===================================================================	
test_precision_recall_f_ignored_labels: 131	
----------------------------	

y_true = [1, 1, 2, 3]
y_pred = [1, 3, 3, 3]
y_true_bin = label_binarize(y_true, classes=numpy.arange(5))
tempResult = arange(5)
	
===================================================================	
test_classification_report_multiclass: 340	
----------------------------	

iris = sklearn.datasets.load_iris()
(y_true, y_pred, _) = make_prediction(dataset=iris, binary=False)
expected_report = '             precision    recall  f1-score   support\n\n     setosa       0.83      0.79      0.81        24\n versicolor       0.33      0.10      0.15        31\n  virginica       0.42      0.90      0.57        20\n\navg / total       0.51      0.53      0.47        75\n'
tempResult = arange(len(iris.target_names))
	
===================================================================	
test_precision_recall_f_extra_labels: 110	
----------------------------	

y_true = [1, 3, 3, 2]
y_pred = [1, 1, 3, 2]
tempResult = arange(5)
	
===================================================================	
test_precision_recall_f_extra_labels: 111	
----------------------------	

y_true = [1, 3, 3, 2]
y_pred = [1, 1, 3, 2]
y_true_bin = label_binarize(y_true, classes=numpy.arange(5))
tempResult = arange(5)
	
===================================================================	
test_precision_recall_f_extra_labels: 123	
----------------------------	

y_true = [1, 3, 3, 2]
y_pred = [1, 1, 3, 2]
y_true_bin = label_binarize(y_true, classes=numpy.arange(5))
y_pred_bin = label_binarize(y_pred, classes=numpy.arange(5))
data = [(y_true, y_pred), (y_true_bin, y_pred_bin)]
for (i, (y_true, y_pred)) in enumerate(data):
    actual = recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average=None)
    assert_array_almost_equal([0.0, 1.0, 1.0, 0.5, 0.0], actual)
    actual = recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average='macro')
    assert_array_almost_equal(numpy.mean([0.0, 1.0, 1.0, 0.5, 0.0]), actual)
    for average in ['micro', 'weighted', 'samples']:
        if ((average == 'samples') and (i == 0)):
            continue
        assert_almost_equal(recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average=average), recall_score(y_true, y_pred, labels=None, average=average))
for average in [None, 'macro', 'micro', 'samples']:
    tempResult = arange(6)
	
===================================================================	
test_precision_recall_f_extra_labels: 124	
----------------------------	

y_true = [1, 3, 3, 2]
y_pred = [1, 1, 3, 2]
y_true_bin = label_binarize(y_true, classes=numpy.arange(5))
y_pred_bin = label_binarize(y_pred, classes=numpy.arange(5))
data = [(y_true, y_pred), (y_true_bin, y_pred_bin)]
for (i, (y_true, y_pred)) in enumerate(data):
    actual = recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average=None)
    assert_array_almost_equal([0.0, 1.0, 1.0, 0.5, 0.0], actual)
    actual = recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average='macro')
    assert_array_almost_equal(numpy.mean([0.0, 1.0, 1.0, 0.5, 0.0]), actual)
    for average in ['micro', 'weighted', 'samples']:
        if ((average == 'samples') and (i == 0)):
            continue
        assert_almost_equal(recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average=average), recall_score(y_true, y_pred, labels=None, average=average))
for average in [None, 'macro', 'micro', 'samples']:
    assert_raises(ValueError, recall_score, y_true_bin, y_pred_bin, labels=numpy.arange(6), average=average)
    tempResult = arange((- 1), 4)
	
===================================================================	
test_check_preserve_type: 466	
----------------------------	

tempResult = arange(40)
	
===================================================================	
test_check_preserve_type: 467	
----------------------------	

XA = np.resize(np.arange(40), (5, 8)).astype(numpy.float32)
tempResult = arange(40)
	
===================================================================	
test_check_different_dimensions: 417	
----------------------------	

tempResult = arange(45)
	
===================================================================	
test_check_different_dimensions: 418	
----------------------------	

XA = numpy.resize(numpy.arange(45), (5, 9))
tempResult = arange(32)
	
===================================================================	
test_check_different_dimensions: 420	
----------------------------	

XA = numpy.resize(numpy.arange(45), (5, 9))
XB = numpy.resize(numpy.arange(32), (4, 8))
assert_raises(ValueError, check_pairwise_arrays, XA, XB)
tempResult = arange((4 * 9))
	
===================================================================	
test_check_XB_returned: 406	
----------------------------	

tempResult = arange(40)
	
===================================================================	
test_check_XB_returned: 407	
----------------------------	

XA = numpy.resize(numpy.arange(40), (5, 8))
tempResult = arange(32)
	
===================================================================	
test_check_XB_returned: 411	
----------------------------	

XA = numpy.resize(numpy.arange(40), (5, 8))
XB = numpy.resize(numpy.arange(32), (4, 8))
(XA_checked, XB_checked) = check_pairwise_arrays(XA, XB)
assert_array_equal(XA, XA_checked)
assert_array_equal(XB, XB_checked)
tempResult = arange(40)
	
===================================================================	
test_pairwise_parallel: 146	
----------------------------	

tempResult = arange(1, 5)
	
===================================================================	
test_check_invalid_dimensions: 424	
----------------------------	

tempResult = arange(45)
	
===================================================================	
test_check_invalid_dimensions: 425	
----------------------------	

XA = np.arange(45).reshape(9, 5)
tempResult = arange(32)
	
===================================================================	
test_check_invalid_dimensions: 427	
----------------------------	

XA = np.arange(45).reshape(9, 5)
XB = np.arange(32).reshape(4, 8)
assert_raises(ValueError, check_pairwise_arrays, XA, XB)
tempResult = arange(45)
	
===================================================================	
test_check_invalid_dimensions: 428	
----------------------------	

XA = np.arange(45).reshape(9, 5)
XB = np.arange(32).reshape(4, 8)
assert_raises(ValueError, check_pairwise_arrays, XA, XB)
XA = np.arange(45).reshape(9, 5)
tempResult = arange(32)
	
===================================================================	
test_check_dense_matrices: 400	
----------------------------	

tempResult = arange(40)
	
===================================================================	
make_prediction: 39	
----------------------------	

'Make some classification predictions on a toy dataset using a SVC\n\n    If binary is True restrict to a binary classification problem instead of a\n    multiclass classification problem\n    '
if (dataset is None):
    dataset = sklearn.datasets.load_iris()
X = dataset.data
y = dataset.target
if binary:
    (X, y) = (X[(y < 2)], y[(y < 2)])
(n_samples, n_features) = X.shape
tempResult = arange(n_samples)
	
===================================================================	
check_lrap_without_tie_and_increasing_score: 501	
----------------------------	

for n_labels in range(2, 10):
    tempResult = arange(n_labels)
	
===================================================================	
test_regression_metrics: 18	
----------------------------	

tempResult = arange(n_samples)
	
===================================================================	
BaseMixture._initialize_parameters: 71	
----------------------------	

'Initialize the model parameters.\n\n        Parameters\n        ----------\n        X : array-like, shape  (n_samples, n_features)\n\n        random_state: RandomState\n            A random number generator instance.\n        '
(n_samples, _) = X.shape
if (self.init_params == 'kmeans'):
    resp = numpy.zeros((n_samples, self.n_components))
    label = cluster.KMeans(n_clusters=self.n_components, n_init=1, random_state=random_state).fit(X).labels_
    tempResult = arange(n_samples)
	
===================================================================	
BayesianGaussianMixture._estimate_log_prob: 181	
----------------------------	

(_, n_features) = X.shape
log_gauss = (_estimate_log_gaussian_prob(X, self.means_, self.precisions_cholesky_, self.covariance_type) - ((0.5 * n_features) * numpy.log(self.degrees_of_freedom_)))
tempResult = arange(0, n_features)
	
===================================================================	
_log_wishart_norm: 22	
----------------------------	

'Compute the log of the Wishart distribution normalization term.\n\n    Parameters\n    ----------\n    degrees_of_freedom : array-like, shape (n_components,)\n        The number of degrees of freedom on the covariance Wishart\n        distributions.\n\n    log_det_precision_chol : array-like, shape (n_components,)\n         The determinant of the precision matrix for each component.\n\n    n_features : int\n        The number of features.\n\n    Return\n    ------\n    log_wishart_norm : array-like, shape (n_components,)\n        The log normalization of the Wishart distribution.\n    '
tempResult = arange(n_features)
	
===================================================================	
wishart_logz: 49	
----------------------------	

'The logarithm of the normalization constant for the wishart distribution'
z = 0.0
z += (((0.5 * v) * n_features) * numpy.log(2))
z += ((0.25 * (n_features * (n_features - 1))) * numpy.log(numpy.pi))
z += ((0.5 * v) * numpy.log(dets))
tempResult = arange(n_features)
	
===================================================================	
wishart_log_det: 38	
----------------------------	

'Expected value of the log of the determinant of a Wishart\n\n    The expected value of the logarithm of the determinant of a\n    wishart-distributed random variable with the specified parameters.'
tempResult = arange((- 1), (n_features - 1))
	
===================================================================	
test_log_wishart_norm: 29	
----------------------------	

rng = numpy.random.RandomState(0)
(n_components, n_features) = (5, 2)
degrees_of_freedom = (numpy.abs(rng.rand(n_components)) + 1.0)
log_det_precisions_chol = (n_features * numpy.log(range(2, (2 + n_components))))
expected_norm = numpy.empty(5)
for (k, (degrees_of_freedom_k, log_det_k)) in enumerate(zip(degrees_of_freedom, log_det_precisions_chol)):
    tempResult = arange(0, n_features)
	
===================================================================	
GMMTester.test_eval: 124	
----------------------------	

if (not self.do_test_eval):
    return
g = self.model(n_components=self.n_components, covariance_type=self.covariance_type, random_state=rng)
g.means_ = (20 * self.means)
g.covars_ = self.covars[self.covariance_type]
g.weights_ = self.weights
tempResult = arange(self.n_components)
	
===================================================================	
PredefinedSplit.split: 424	
----------------------------	

'Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        '
tempResult = arange(len(self.test_fold))
	
===================================================================	
TimeSeriesSplit.split: 214	
----------------------------	

'Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        '
(X, y, groups) = indexable(X, y, groups)
n_samples = _num_samples(X)
n_splits = self.n_splits
n_folds = (n_splits + 1)
if (n_folds > n_samples):
    raise ValueError('Cannot have number of folds ={0} greater than the number of samples: {1}.'.format(n_folds, n_samples))
tempResult = arange(n_samples)
	
===================================================================	
BaseCrossValidator.split: 34	
----------------------------	

'Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        y : array-like, of length n_samples\n            The target variable for supervised learning problems.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n        '
(X, y, groups) = indexable(X, y, groups)
tempResult = arange(_num_samples(X))
	
===================================================================	
KFold._iter_test_indices: 123	
----------------------------	

n_samples = _num_samples(X)
tempResult = arange(n_samples)
	
===================================================================	
_shuffle: 175	
----------------------------	

'Return a shuffled copy of y eventually shuffle among same groups.'
if (groups is None):
    indices = random_state.permutation(len(y))
else:
    tempResult = arange(len(groups))
	
===================================================================	
cross_val_predict: 110	
----------------------------	

"Generate cross-validated estimates for each input data point\n\n    Read more in the :ref:`User Guide <cross_validation>`.\n\n    Parameters\n    ----------\n    estimator : estimator object implementing 'fit' and 'predict'\n        The object to use to fit the data.\n\n    X : array-like\n        The data to fit. Can be, for example a list, or an array at least 2d.\n\n    y : array-like, optional, default: None\n        The target variable to try to predict in the case of\n        supervised learning.\n\n    groups : array-like, with shape (n_samples,), optional\n        Group labels for the samples used while splitting the dataset into\n        train/test set.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross validation,\n          - integer, to specify the number of folds in a `(Stratified)KFold`,\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train, test splits.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    n_jobs : integer, optional\n        The number of CPUs to use to do the computation. -1 means\n        'all CPUs'.\n\n    verbose : integer, optional\n        The verbosity level.\n\n    fit_params : dict, optional\n        Parameters to pass to the fit method of the estimator.\n\n    pre_dispatch : int, or string, optional\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A string, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    method : string, optional, default: 'predict'\n        Invokes the passed method name of the passed estimator.\n\n    Returns\n    -------\n    predictions : ndarray\n        This is the result of calling ``method``\n\n    Examples\n    --------\n    >>> from sklearn import datasets, linear_model\n    >>> from sklearn.model_selection import cross_val_predict\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data[:150]\n    >>> y = diabetes.target[:150]\n    >>> lasso = linear_model.Lasso()\n    >>> y_pred = cross_val_predict(lasso, X, y)\n    "
(X, y, groups) = indexable(X, y, groups)
cv = check_cv(cv, y, classifier=is_classifier(estimator))
if (not callable(getattr(estimator, method))):
    raise AttributeError('{} not implemented in estimator'.format(method))
parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
prediction_blocks = parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params, method) for (train, test) in cv.split(X, y, groups)))
predictions = [pred_block_i for (pred_block_i, _) in prediction_blocks]
test_indices = numpy.concatenate([indices_i for (_, indices_i) in prediction_blocks])
if (not _check_is_permutation(test_indices, _num_samples(X))):
    raise ValueError('cross_val_predict only works for partitions')
inv_test_indices = numpy.empty(len(test_indices), dtype=int)
tempResult = arange(len(test_indices))
	
===================================================================	
test_refit: 331	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_gridsearch_nd: 337	
----------------------------	

tempResult = arange((((10 * 5) * 3) * 2))
	
===================================================================	
test_gridsearch_nd: 338	
----------------------------	

X_4d = np.arange((((10 * 5) * 3) * 2)).reshape(10, 5, 3, 2)
tempResult = arange(((10 * 7) * 11))
	
===================================================================	
test_predict_proba_disabled: 642	
----------------------------	

tempResult = arange(20)
	
===================================================================	
test_X_as_list: 347	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_grid_search_allows_nans: 649	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
test_pandas_input: 372	
----------------------------	

types = [(MockDataFrame, MockDataFrame)]
try:
    from pandas import Series, DataFrame
    types.append((DataFrame, Series))
except ImportError:
    pass
tempResult = arange(100)
	
===================================================================	
test_stochastic_gradient_loss_param: 712	
----------------------------	

param_grid = {'loss': ['log']}
tempResult = arange(24)
	
===================================================================	
test_y_as_list: 356	
----------------------------	

tempResult = arange(100)
	
===================================================================	
module: 53	
----------------------------	

'Test the split module'
from __future__ import division
import warnings
import numpy as np
from scipy.sparse import coo_matrix, csc_matrix, csr_matrix
from scipy import stats
from scipy.misc import comb
from itertools import combinations
from sklearn.utils.fixes import combinations_with_replacement
from sklearn.utils.testing import assert_true
from sklearn.utils.testing import assert_false
from sklearn.utils.testing import assert_equal
from sklearn.utils.testing import assert_almost_equal
from sklearn.utils.testing import assert_raises
from sklearn.utils.testing import assert_raises_regexp
from sklearn.utils.testing import assert_greater
from sklearn.utils.testing import assert_greater_equal
from sklearn.utils.testing import assert_not_equal
from sklearn.utils.testing import assert_array_almost_equal
from sklearn.utils.testing import assert_array_equal
from sklearn.utils.testing import assert_warns_message
from sklearn.utils.testing import assert_raise_message
from sklearn.utils.testing import ignore_warnings
from sklearn.utils.validation import _num_samples
from sklearn.utils.mocking import MockDataFrame
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GroupKFold
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.model_selection import LeavePOut
from sklearn.model_selection import LeavePGroupsOut
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GroupShuffleSplit
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import PredefinedSplit
from sklearn.model_selection import check_cv
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.model_selection._split import _validate_shuffle_split
from sklearn.model_selection._split import _CVIterableWrapper
from sklearn.model_selection._split import _build_repr
from sklearn.datasets import load_digits
from sklearn.datasets import make_classification
from sklearn.externals import six
from sklearn.externals.six.moves import zip
from sklearn.svm import SVC
X = numpy.ones(10)
tempResult = arange(10)
	
===================================================================	
test_train_test_split_allow_nans: 551	
----------------------------	

tempResult = arange(200, dtype=numpy.float64)
	
===================================================================	
train_test_split_sparse: 521	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_stratified_shuffle_split_init: 335	
----------------------------	

tempResult = arange(7)
	
===================================================================	
test_stratified_shuffle_split_init: 340	
----------------------------	

X = numpy.arange(7)
y = numpy.asarray([0, 1, 1, 1, 2, 2, 2])
assert_raises(ValueError, next, StratifiedShuffleSplit(3, 0.2).split(X, y))
assert_raises(ValueError, next, StratifiedShuffleSplit(3, 2).split(X, y))
assert_raises(ValueError, next, StratifiedShuffleSplit(3, 3, 2).split(X, y))
tempResult = arange(9)
	
===================================================================	
test_train_test_split: 480	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_train_test_split: 482	
----------------------------	

X = np.arange(100).reshape((10, 10))
X_s = coo_matrix(X)
tempResult = arange(10)
	
===================================================================	
test_train_test_split: 492	
----------------------------	

X = np.arange(100).reshape((10, 10))
X_s = coo_matrix(X)
y = numpy.arange(10)
split = train_test_split(X, y, test_size=None, train_size=0.5)
(X_train, X_test, y_train, y_test) = split
assert_equal(len(y_test), len(y_train))
assert_array_equal(X_train[:, 0], (y_train * 10))
assert_array_equal(X_test[:, 0], (y_test * 10))
split = train_test_split(X, X_s, y.tolist())
(X_train, X_test, X_s_train, X_s_test, y_train, y_test) = split
assert_true(isinstance(y_train, list))
assert_true(isinstance(y_test, list))
tempResult = arange((((10 * 5) * 3) * 2))
	
===================================================================	
test_train_test_split: 493	
----------------------------	

X = np.arange(100).reshape((10, 10))
X_s = coo_matrix(X)
y = numpy.arange(10)
split = train_test_split(X, y, test_size=None, train_size=0.5)
(X_train, X_test, y_train, y_test) = split
assert_equal(len(y_test), len(y_train))
assert_array_equal(X_train[:, 0], (y_train * 10))
assert_array_equal(X_test[:, 0], (y_test * 10))
split = train_test_split(X, X_s, y.tolist())
(X_train, X_test, X_s_train, X_s_test, y_train, y_test) = split
assert_true(isinstance(y_train, list))
assert_true(isinstance(y_test, list))
X_4d = np.arange((((10 * 5) * 3) * 2)).reshape(10, 5, 3, 2)
tempResult = arange(((10 * 7) * 11))
	
===================================================================	
test_permutation_test_score_allow_nans: 293	
----------------------------	

tempResult = arange(200, dtype=numpy.float64)
	
===================================================================	
test_permutation_score: 287	
----------------------------	

iris = load_iris()
X = iris.data
X_sparse = coo_matrix(X)
y = iris.target
svm = SVC(kernel='linear')
cv = StratifiedKFold(2)
(score, scores, pvalue) = permutation_test_score(svm, X, y, n_permutations=30, cv=cv, scoring='accuracy')
assert_greater(score, 0.9)
assert_almost_equal(pvalue, 0.0, 1)
(score_group, _, pvalue_group) = permutation_test_score(svm, X, y, n_permutations=30, cv=cv, scoring='accuracy', groups=numpy.ones(y.size), random_state=0)
assert_true((score_group == score))
assert_true((pvalue_group == pvalue))
svm_sparse = SVC(kernel='linear')
cv_sparse = StratifiedKFold(2)
(score_group, _, pvalue_group) = permutation_test_score(svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse, scoring='accuracy', groups=numpy.ones(y.size), random_state=0)
assert_true((score_group == score))
assert_true((pvalue_group == pvalue))

def custom_score(y_true, y_pred):
    return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) / y_true.shape[0])
scorer = make_scorer(custom_score)
(score, _, pvalue) = permutation_test_score(svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)
assert_almost_equal(score, 0.93, 2)
assert_almost_equal(pvalue, 0.01, 3)
tempResult = arange(len(y))
	
===================================================================	
test_cross_val_score_allow_nans: 300	
----------------------------	

tempResult = arange(200, dtype=numpy.float64)
	
===================================================================	
test_check_is_permutation: 499	
----------------------------	

rng = numpy.random.RandomState(0)
tempResult = arange(100)
	
===================================================================	
LSHForest._get_candidates: 123	
----------------------------	

'Performs the Synchronous ascending phase.\n\n        Returns an array of candidates, their distance ranks and\n        distances.\n        '
index_size = self._fit_X.shape[0]
n_candidates = 0
candidate_set = set()
min_candidates = (self.n_candidates * self.n_estimators)
while ((max_depth > self.min_hash_match) and ((n_candidates < min_candidates) or (len(candidate_set) < n_neighbors))):
    left_mask = self._left_mask[max_depth]
    right_mask = self._right_mask[max_depth]
    for i in range(self.n_estimators):
        (start, stop) = _find_matching_indices(self.trees_[i], bin_queries[i], left_mask, right_mask)
        n_candidates += (stop - start)
        candidate_set.update(self.original_indices_[i][start:stop].tolist())
    max_depth -= 1
candidates = numpy.fromiter(candidate_set, count=len(candidate_set), dtype=numpy.intp)
if (candidates.shape[0] < n_neighbors):
    warnings.warn(('Number of candidates is not sufficient to retrieve %i neighbors with min_hash_match = %i. Candidates are filled up uniformly from unselected indices.' % (n_neighbors, self.min_hash_match)))
    tempResult = arange(0, index_size)
	
===================================================================	
LSHForest.partial_fit: 229	
----------------------------	

'\n        Inserts new data into the already fitted LSH Forest.\n        Cost is proportional to new total size, so additions\n        should be batched.\n\n        Parameters\n        ----------\n        X : array_like or sparse (CSR) matrix, shape (n_samples, n_features)\n            New data point to be inserted into the LSH Forest.\n        '
X = check_array(X, accept_sparse='csr')
if (not hasattr(self, 'hash_functions_')):
    return self.fit(X)
if (X.shape[1] != self._fit_X.shape[1]):
    raise ValueError('Number of features in X and fitted array does not match.')
n_samples = X.shape[0]
n_indexed = self._fit_X.shape[0]
for i in range(self.n_estimators):
    bin_X = self.hash_functions_[i].transform(X)[:, 0]
    positions = self.trees_[i].searchsorted(bin_X)
    self.trees_[i] = numpy.insert(self.trees_[i], positions, bin_X)
    tempResult = arange(n_indexed, (n_indexed + n_samples))
	
===================================================================	
_find_longest_prefix_match: 31	
----------------------------	

'Find the longest prefix match in tree for each query in bin_X\n\n    Most significant bits are considered as the prefix.\n    '
hi = numpy.empty_like(bin_X, dtype=numpy.intp)
hi.fill(hash_size)
lo = numpy.zeros_like(bin_X, dtype=numpy.intp)
res = numpy.empty_like(bin_X, dtype=numpy.intp)
(left_idx, right_idx) = _find_matching_indices(tree, bin_X, left_masks[hi], right_masks[hi])
found = (right_idx > left_idx)
res[found] = lo[found] = hash_size
tempResult = arange(bin_X.shape[0])
	
===================================================================	
KNeighborsMixin.kneighbors: 191	
----------------------------	

"Finds the K-neighbors of a point.\n\n        Returns indices of and distances to the neighbors of each point.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int\n            Number of neighbors to get (default is the value\n            passed to the constructor).\n\n        return_distance : boolean, optional. Defaults to True.\n            If False, distances will not be returned\n\n        Returns\n        -------\n        dist : array\n            Array representing the lengths to points, only present if\n            return_distance=True\n\n        ind : array\n            Indices of the nearest points in the population matrix.\n\n        Examples\n        --------\n        In the following example, we construct a NeighborsClassifier\n        class from an array representing our data set and ask who's\n        the closest point to [1,1,1]\n\n        >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=1)\n        >>> neigh.fit(samples) # doctest: +ELLIPSIS\n        NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n        >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n        (array([[ 0.5]]), array([[2]]...))\n\n        As you can see, it returns [[0.5]], and [[2]], which means that the\n        element is at distance 0.5 and is the third element of samples\n        (indexes start at 0). You can also query for multiple points:\n\n        >>> X = [[0., 1., 0.], [1., 0., 1.]]\n        >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n        array([[1],\n               [2]]...)\n\n        "
if (self._fit_method is None):
    raise NotFittedError('Must fit neighbors before querying.')
if (n_neighbors is None):
    n_neighbors = self.n_neighbors
if (X is not None):
    query_is_train = False
    X = check_array(X, accept_sparse='csr')
else:
    query_is_train = True
    X = self._fit_X
    n_neighbors += 1
train_size = self._fit_X.shape[0]
if (n_neighbors > train_size):
    raise ValueError(('Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d' % (train_size, n_neighbors)))
(n_samples, _) = X.shape
tempResult = arange(n_samples)
	
===================================================================	
KNeighborsMixin.kneighbors_graph: 246	
----------------------------	

"Computes the (weighted) graph of k-Neighbors for points in X\n\n        Parameters\n        ----------\n        X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n            The query point or points.\n            If not provided, neighbors of each indexed point are returned.\n            In this case, the query point is not considered its own neighbor.\n\n        n_neighbors : int\n            Number of neighbors for each sample.\n            (default is value passed to the constructor).\n\n        mode : {'connectivity', 'distance'}, optional\n            Type of returned matrix: 'connectivity' will return the\n            connectivity matrix with ones and zeros, in 'distance' the\n            edges are Euclidean distance between points.\n\n        Returns\n        -------\n        A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]\n            n_samples_fit is the number of samples in the fitted data\n            A[i, j] is assigned the weight of edge that connects i to j.\n\n        Examples\n        --------\n        >>> X = [[0], [3], [1]]\n        >>> from sklearn.neighbors import NearestNeighbors\n        >>> neigh = NearestNeighbors(n_neighbors=2)\n        >>> neigh.fit(X) # doctest: +ELLIPSIS\n        NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n        >>> A = neigh.kneighbors_graph(X)\n        >>> A.toarray()\n        array([[ 1.,  0.,  1.],\n               [ 0.,  1.,  1.],\n               [ 1.,  0.,  1.]])\n\n        See also\n        --------\n        NearestNeighbors.radius_neighbors_graph\n        "
if (n_neighbors is None):
    n_neighbors = self.n_neighbors
if (X is not None):
    X = check_array(X, accept_sparse='csr')
    n_samples1 = X.shape[0]
else:
    n_samples1 = self._fit_X.shape[0]
n_samples2 = self._fit_X.shape[0]
n_nonzero = (n_samples1 * n_neighbors)
tempResult = arange(0, (n_nonzero + 1), n_neighbors)
	
===================================================================	
KNeighborsClassifier.predict_proba: 54	
----------------------------	

"Return probability estimates for the test data X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n            Test samples.\n\n        Returns\n        -------\n        p : array of shape = [n_samples, n_classes], or a list of n_outputs\n            of such arrays if n_outputs > 1.\n            The class probabilities of the input samples. Classes are ordered\n            by lexicographic order.\n        "
X = check_array(X, accept_sparse='csr')
(neigh_dist, neigh_ind) = self.kneighbors(X)
classes_ = self.classes_
_y = self._y
if (not self.outputs_2d_):
    _y = self._y.reshape(((- 1), 1))
    classes_ = [self.classes_]
n_samples = X.shape[0]
weights = _get_weights(neigh_dist, self.weights)
if (weights is None):
    weights = numpy.ones_like(neigh_ind)
tempResult = arange(X.shape[0])
	
===================================================================	
test_simultaneous_sort: 207	
----------------------------	

dist = np.random.random((n_rows, n_pts)).astype(DTYPE)
tempResult = arange(n_pts)
	
===================================================================	
test_simultaneous_sort: 212	
----------------------------	

dist = np.random.random((n_rows, n_pts)).astype(DTYPE)
ind = (np.arange(n_pts) + np.zeros((n_rows, 1))).astype(ITYPE)
dist2 = dist.copy()
ind2 = ind.copy()
simultaneous_sort(dist, ind)
i = numpy.argsort(dist2, axis=1)
tempResult = arange(n_rows)
	
===================================================================	
brute_force_neighbors: 22	
----------------------------	

D = DistanceMetric.get_metric(metric, **kwargs).pairwise(Y, X)
ind = numpy.argsort(D, axis=1)[:, :k]
tempResult = arange(Y.shape[0])
	
===================================================================	
test_neighbors_heap: 188	
----------------------------	

heap = NeighborsHeap(n_pts, n_nbrs)
for row in range(n_pts):
    d_in = np.random.random((2 * n_nbrs)).astype(DTYPE)
    tempResult = arange((2 * n_nbrs), dtype=ITYPE)
	
===================================================================	
test_simultaneous_sort: 166	
----------------------------	

dist = np.random.random((n_rows, n_pts)).astype(DTYPE)
tempResult = arange(n_pts)
	
===================================================================	
test_simultaneous_sort: 171	
----------------------------	

dist = np.random.random((n_rows, n_pts)).astype(DTYPE)
ind = (np.arange(n_pts) + np.zeros((n_rows, 1))).astype(ITYPE)
dist2 = dist.copy()
ind2 = ind.copy()
simultaneous_sort(dist, ind)
i = numpy.argsort(dist2, axis=1)
tempResult = arange(n_rows)
	
===================================================================	
brute_force_neighbors: 15	
----------------------------	

D = DistanceMetric.get_metric(metric, **kwargs).pairwise(Y, X)
ind = numpy.argsort(D, axis=1)[:, :k]
tempResult = arange(Y.shape[0])
	
===================================================================	
test_neighbors_heap: 147	
----------------------------	

heap = NeighborsHeap(n_pts, n_nbrs)
for row in range(n_pts):
    d_in = np.random.random((2 * n_nbrs)).astype(DTYPE)
    tempResult = arange((2 * n_nbrs), dtype=ITYPE)
	
===================================================================	
test_precomputed: 96	
----------------------------	

'Tests unsupervised NearestNeighbors with a distance matrix.'
rng = numpy.random.RandomState(random_state)
X = rng.random_sample((10, 4))
Y = rng.random_sample((3, 4))
DXX = sklearn.metrics.pairwise_distances(X, metric='euclidean')
DYX = sklearn.metrics.pairwise_distances(Y, X, metric='euclidean')
for method in ['kneighbors']:
    nbrs_X = sklearn.neighbors.NearestNeighbors(n_neighbors=3)
    nbrs_X.fit(X)
    (dist_X, ind_X) = getattr(nbrs_X, method)(Y)
    nbrs_D = sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric='precomputed')
    nbrs_D.fit(DXX)
    (dist_D, ind_D) = getattr(nbrs_D, method)(DYX)
    assert_array_almost_equal(dist_X, dist_D)
    assert_array_almost_equal(ind_X, ind_D)
    nbrs_D = sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='auto', metric='precomputed')
    nbrs_D.fit(DXX)
    (dist_D, ind_D) = getattr(nbrs_D, method)(DYX)
    assert_array_almost_equal(dist_X, dist_D)
    assert_array_almost_equal(ind_X, ind_D)
    (dist_X, ind_X) = getattr(nbrs_X, method)(None)
    (dist_D, ind_D) = getattr(nbrs_D, method)(None)
    assert_array_almost_equal(dist_X, dist_D)
    assert_array_almost_equal(ind_X, ind_D)
    assert_raises(ValueError, getattr(nbrs_D, method), X)
tempResult = arange(X.shape[0])
	
===================================================================	
test_neighbors_digits: 470	
----------------------------	

X = digits.data.astype('uint8')
Y = digits.target
(n_samples, n_features) = X.shape
train_test_boundary = int((n_samples * 0.8))
tempResult = arange(0, train_test_boundary)
	
===================================================================	
test_neighbors_digits: 471	
----------------------------	

X = digits.data.astype('uint8')
Y = digits.target
(n_samples, n_features) = X.shape
train_test_boundary = int((n_samples * 0.8))
train = numpy.arange(0, train_test_boundary)
tempResult = arange(train_test_boundary, n_samples)
	
===================================================================	
BernoulliRBM.score_samples: 100	
----------------------------	

'Compute the pseudo-likelihood of X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} shape (n_samples, n_features)\n            Values of the visible layer. Must be all-boolean (not checked).\n\n        Returns\n        -------\n        pseudo_likelihood : array-like, shape (n_samples,)\n            Value of the pseudo-likelihood (proxy for likelihood).\n\n        Notes\n        -----\n        This method is not deterministic: it computes a quantity called the\n        free energy on X, then on a randomly corrupted version of X, and\n        returns the log of the logistic function of the difference.\n        '
check_is_fitted(self, 'components_')
v = check_array(X, accept_sparse='csr')
rng = check_random_state(self.random_state)
tempResult = arange(v.shape[0])
	
===================================================================	
test_alpha: 35	
----------------------------	

X = X_digits_binary[:100]
y = y_digits_binary[:100]
alpha_vectors = []
tempResult = arange(2)
	
===================================================================	
test_gradient: 82	
----------------------------	

for n_labels in [2, 3]:
    n_samples = 5
    n_features = 10
    X = numpy.random.random((n_samples, n_features))
    tempResult = arange(n_samples)
	
===================================================================	
test_score_samples: 107	
----------------------------	

rng = numpy.random.RandomState(42)
X = numpy.vstack([numpy.zeros(1000), numpy.ones(1000)])
rbm1 = BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng)
rbm1.fit(X)
assert_true((rbm1.score_samples(X) < (- 300)).all())
rbm1.random_state = 42
d_score = rbm1.score_samples(X)
rbm1.random_state = 42
s_score = rbm1.score_samples(lil_matrix(X))
assert_almost_equal(d_score, s_score)
with numpy.errstate(under='ignore'):
    tempResult = arange(1000)
	
===================================================================	
test_sgd_optimizer_nesterovs_momentum: 48	
----------------------------	

params = [numpy.zeros(shape) for shape in shapes]
lr = 0.1
tempResult = arange(0.5, 0.9, 0.1)
	
===================================================================	
test_adam_optimizer: 64	
----------------------------	

params = [numpy.zeros(shape) for shape in shapes]
lr = 0.001
epsilon = 1e-08
tempResult = arange(0.9, 1.0, 0.05)
	
===================================================================	
test_adam_optimizer: 65	
----------------------------	

params = [numpy.zeros(shape) for shape in shapes]
lr = 0.001
epsilon = 1e-08
for beta_1 in numpy.arange(0.9, 1.0, 0.05):
    tempResult = arange(0.995, 1.0, 0.001)
	
===================================================================	
test_sgd_optimizer_momentum: 26	
----------------------------	

params = [numpy.zeros(shape) for shape in shapes]
lr = 0.1
tempResult = arange(0.5, 0.9, 0.1)
	
===================================================================	
OneHotEncoder._transform: 716	
----------------------------	

'Assumes X contains only categorical features.'
X = check_array(X, dtype=numpy.int)
if numpy.any((X < 0)):
    raise ValueError('X needs to contain only non-negative integers.')
(n_samples, n_features) = X.shape
indices = self.feature_indices_
if (n_features != (indices.shape[0] - 1)):
    raise ValueError(('X has different shape than during fitting. Expected %d, got %d.' % ((indices.shape[0] - 1), n_features)))
mask = (X < self.n_values_).ravel()
if numpy.any((~ mask)):
    if (self.handle_unknown not in ['error', 'ignore']):
        raise ValueError(('handle_unknown should be either error or unknown got %s' % self.handle_unknown))
    if (self.handle_unknown == 'error'):
        raise ValueError(('unknown categorical feature present %s during transform.' % X.ravel()[(~ mask)]))
column_indices = (X + indices[:(- 1)]).ravel()[mask]
tempResult = arange(n_samples, dtype=numpy.int32)
	
===================================================================	
add_dummy_feature: 606	
----------------------------	

'Augment dataset with an additional dummy feature.\n\n    This is useful for fitting an intercept term with implementations which\n    cannot otherwise fit it directly.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        Data.\n\n    value : float\n        Value to use for the dummy feature.\n\n    Returns\n    -------\n\n    X : {array, sparse matrix}, shape [n_samples, n_features + 1]\n        Same data with dummy feature added as first column.\n\n    Examples\n    --------\n\n    >>> from sklearn.preprocessing import add_dummy_feature\n    >>> add_dummy_feature([[0, 1], [1, 0]])\n    array([[ 1.,  0.,  1.],\n           [ 1.,  1.,  0.]])\n    '
X = check_array(X, accept_sparse=['csc', 'csr', 'coo'], dtype=FLOAT_DTYPES)
(n_samples, n_features) = X.shape
shape = (n_samples, (n_features + 1))
if scipy.sparse.issparse(X):
    if scipy.sparse.isspmatrix_coo(X):
        col = (X.col + 1)
        col = numpy.concatenate((numpy.zeros(n_samples), col))
        tempResult = arange(n_samples)
	
===================================================================	
add_dummy_feature: 612	
----------------------------	

'Augment dataset with an additional dummy feature.\n\n    This is useful for fitting an intercept term with implementations which\n    cannot otherwise fit it directly.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        Data.\n\n    value : float\n        Value to use for the dummy feature.\n\n    Returns\n    -------\n\n    X : {array, sparse matrix}, shape [n_samples, n_features + 1]\n        Same data with dummy feature added as first column.\n\n    Examples\n    --------\n\n    >>> from sklearn.preprocessing import add_dummy_feature\n    >>> add_dummy_feature([[0, 1], [1, 0]])\n    array([[ 1.,  0.,  1.],\n           [ 1.,  1.,  0.]])\n    '
X = check_array(X, accept_sparse=['csc', 'csr', 'coo'], dtype=FLOAT_DTYPES)
(n_samples, n_features) = X.shape
shape = (n_samples, (n_features + 1))
if scipy.sparse.issparse(X):
    if scipy.sparse.isspmatrix_coo(X):
        col = (X.col + 1)
        col = numpy.concatenate((numpy.zeros(n_samples), col))
        row = numpy.concatenate((numpy.arange(n_samples), X.row))
        data = numpy.concatenate(((numpy.ones(n_samples) * value), X.data))
        return scipy.sparse.coo_matrix((data, (row, col)), shape)
    elif scipy.sparse.isspmatrix_csc(X):
        indptr = (X.indptr + n_samples)
        indptr = numpy.concatenate((numpy.array([0]), indptr))
        tempResult = arange(n_samples)
	
===================================================================	
OneHotEncoder._fit_transform: 686	
----------------------------	

'Assumes X contains only categorical features.'
X = check_array(X, dtype=numpy.int)
if numpy.any((X < 0)):
    raise ValueError('X needs to contain only non-negative integers.')
(n_samples, n_features) = X.shape
if (isinstance(self.n_values, externals.six.string_types) and (self.n_values == 'auto')):
    n_values = (numpy.max(X, axis=0) + 1)
elif isinstance(self.n_values, numbers.Integral):
    if (np.max(X, axis=0) >= self.n_values).any():
        raise ValueError(('Feature out of bounds for n_values=%d' % self.n_values))
    n_values = numpy.empty(n_features, dtype=numpy.int)
    n_values.fill(self.n_values)
else:
    try:
        n_values = numpy.asarray(self.n_values, dtype=int)
    except (ValueError, TypeError):
        raise TypeError(("Wrong type for parameter `n_values`. Expected 'auto', int or array of ints, got %r" % type(X)))
    if ((n_values.ndim < 1) or (n_values.shape[0] != X.shape[1])):
        raise ValueError('Shape mismatch: if n_values is an array, it has to be of shape (n_features,).')
self.n_values_ = n_values
n_values = numpy.hstack([[0], n_values])
indices = numpy.cumsum(n_values)
self.feature_indices_ = indices
column_indices = (X + indices[:(- 1)]).ravel()
tempResult = arange(n_samples, dtype=numpy.int32)
	
===================================================================	
_transform_selected: 629	
----------------------------	

'Apply a transform function to portion of selected features\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        Dense array or sparse matrix.\n\n    transform : callable\n        A callable transform(X) -> X_transformed\n\n    copy : boolean, optional\n        Copy X even if it could be avoided.\n\n    selected: "all" or array of indices or mask\n        Specify which features to apply the transform to.\n\n    Returns\n    -------\n    X : array or sparse matrix, shape=(n_samples, n_features_new)\n    '
X = check_array(X, accept_sparse='csc', copy=copy, dtype=FLOAT_DTYPES)
if (isinstance(selected, externals.six.string_types) and (selected == 'all')):
    return transform(X)
if (len(selected) == 0):
    return X
n_features = X.shape[1]
tempResult = arange(n_features)
	
===================================================================	
Imputer.transform: 160	
----------------------------	

'Impute all missing values in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            The input data to complete.\n        '
if (self.axis == 0):
    check_is_fitted(self, 'statistics_')
    X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)
    statistics = self.statistics_
    if (X.shape[1] != statistics.shape[0]):
        raise ValueError(('X has %d features per sample, expected %d' % (X.shape[1], self.statistics_.shape[0])))
else:
    X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)
    if scipy.sparse.issparse(X):
        statistics = self._sparse_fit(X, self.strategy, self.missing_values, self.axis)
    else:
        statistics = self._dense_fit(X, self.strategy, self.missing_values, self.axis)
invalid_mask = numpy.isnan(statistics)
valid_mask = numpy.logical_not(invalid_mask)
valid_statistics = statistics[valid_mask]
valid_statistics_indexes = numpy.where(valid_mask)[0]
tempResult = arange(X.shape[(not self.axis)])
	
===================================================================	
Imputer.transform: 169	
----------------------------	

'Impute all missing values in X.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            The input data to complete.\n        '
if (self.axis == 0):
    check_is_fitted(self, 'statistics_')
    X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)
    statistics = self.statistics_
    if (X.shape[1] != statistics.shape[0]):
        raise ValueError(('X has %d features per sample, expected %d' % (X.shape[1], self.statistics_.shape[0])))
else:
    X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)
    if scipy.sparse.issparse(X):
        statistics = self._sparse_fit(X, self.strategy, self.missing_values, self.axis)
    else:
        statistics = self._dense_fit(X, self.strategy, self.missing_values, self.axis)
invalid_mask = numpy.isnan(statistics)
valid_mask = numpy.logical_not(invalid_mask)
valid_statistics = statistics[valid_mask]
valid_statistics_indexes = numpy.where(valid_mask)[0]
missing = numpy.arange(X.shape[(not self.axis)])[invalid_mask]
if ((self.axis == 0) and invalid_mask.any()):
    if self.verbose:
        warnings.warn(('Deleting features without observed values: %s' % missing))
    X = X[:, valid_statistics_indexes]
elif ((self.axis == 1) and invalid_mask.any()):
    raise ValueError(('Some rows only contain missing values: %s' % missing))
if (scipy.sparse.issparse(X) and (self.missing_values != 0)):
    mask = _get_mask(X.data, self.missing_values)
    tempResult = arange((len(X.indptr) - 1), dtype=numpy.int)
	
===================================================================	
LabelEncoder.inverse_transform: 59	
----------------------------	

'Transform labels back to original encoding.\n\n        Parameters\n        ----------\n        y : numpy array of shape [n_samples]\n            Target values.\n\n        Returns\n        -------\n        y : numpy array of shape [n_samples]\n        '
check_is_fitted(self, 'classes_')
tempResult = arange(len(self.classes_))
	
===================================================================	
_inverse_binarize_multiclass: 187	
----------------------------	

'Inverse label binarization transformation for multiclass.\n\n    Multiclass uses the maximal score instead of a threshold.\n    '
classes = numpy.asarray(classes)
if scipy.sparse.issparse(y):
    y = y.tocsr()
    (n_samples, n_outputs) = y.shape
    tempResult = arange(n_outputs)
	
===================================================================	
_inverse_binarize_multiclass: 198	
----------------------------	

'Inverse label binarization transformation for multiclass.\n\n    Multiclass uses the maximal score instead of a threshold.\n    '
classes = numpy.asarray(classes)
if scipy.sparse.issparse(y):
    y = y.tocsr()
    (n_samples, n_outputs) = y.shape
    outputs = numpy.arange(n_outputs)
    row_max = sparse_min_max(y, 1)[1]
    row_nnz = numpy.diff(y.indptr)
    y_data_repeated_max = numpy.repeat(row_max, row_nnz)
    y_i_all_argmax = numpy.flatnonzero((y_data_repeated_max == y.data))
    if (row_max[(- 1)] == 0):
        y_i_all_argmax = numpy.append(y_i_all_argmax, [len(y.data)])
    index_first_argmax = numpy.searchsorted(y_i_all_argmax, y.indptr[:(- 1)])
    y_ind_ext = numpy.append(y.indices, [0])
    y_i_argmax = y_ind_ext[y_i_all_argmax[index_first_argmax]]
    y_i_argmax[numpy.where((row_nnz == 0))[0]] = 0
    tempResult = arange(n_samples)
	
===================================================================	
test_polynomial_features: 77	
----------------------------	

tempResult = arange(6)
	
===================================================================	
test_polynomial_features: 80	
----------------------------	

X1 = numpy.arange(6)[:, numpy.newaxis]
P1 = numpy.hstack([numpy.ones_like(X1), X1, (X1 ** 2), (X1 ** 3)])
deg1 = 3
tempResult = arange(6)
	
===================================================================	
test_polynomial_feature_names: 96	
----------------------------	

tempResult = arange(30)
	
===================================================================	
test_np_log: 31	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_delegate_to_func: 19	
----------------------------	

args_store = []
kwargs_store = {}
tempResult = arange(10)
	
===================================================================	
test_imputation_mean_median: 80	
----------------------------	

rng = numpy.random.RandomState(0)
dim = 10
dec = 10
shape = ((dim * dim), (dim + dec))
zeros = numpy.zeros(shape[0])
tempResult = arange(1, (shape[0] + 1))
	
===================================================================	
test_inverse_binarize_multiclass: 313	
----------------------------	

tempResult = arange(3)
	
===================================================================	
BaseLibSVM._sparse_fit: 146	
----------------------------	

X.data = numpy.asarray(X.data, dtype=numpy.float64, order='C')
X.sort_indices()
kernel_type = self._sparse_kernels.index(kernel)
libsvm_sparse.set_verbosity_wrap(self.verbose)
(self.support_, self.support_vectors_, dual_coef_data, self.intercept_, self.n_support_, self.probA_, self.probB_, self.fit_status_) = libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices, X.indptr, y, solver_type, kernel_type, self.degree, self._gamma, self.coef0, self.tol, self.C, self.class_weight_, sample_weight, self.nu, self.cache_size, self.epsilon, int(self.shrinking), int(self.probability), self.max_iter, random_seed)
self._warn_from_fit_status()
if hasattr(self, 'classes_'):
    n_class = (len(self.classes_) - 1)
else:
    n_class = 1
n_SV = self.support_vectors_.shape[0]
tempResult = arange(n_SV)
	
===================================================================	
BaseLibSVM._sparse_fit: 147	
----------------------------	

X.data = numpy.asarray(X.data, dtype=numpy.float64, order='C')
X.sort_indices()
kernel_type = self._sparse_kernels.index(kernel)
libsvm_sparse.set_verbosity_wrap(self.verbose)
(self.support_, self.support_vectors_, dual_coef_data, self.intercept_, self.n_support_, self.probA_, self.probB_, self.fit_status_) = libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices, X.indptr, y, solver_type, kernel_type, self.degree, self._gamma, self.coef0, self.tol, self.C, self.class_weight_, sample_weight, self.nu, self.cache_size, self.epsilon, int(self.shrinking), int(self.probability), self.max_iter, random_seed)
self._warn_from_fit_status()
if hasattr(self, 'classes_'):
    n_class = (len(self.classes_) - 1)
else:
    n_class = 1
n_SV = self.support_vectors_.shape[0]
dual_coef_indices = numpy.tile(numpy.arange(n_SV), n_class)
tempResult = arange(0, (dual_coef_indices.size + 1), (dual_coef_indices.size / n_class))
	
===================================================================	
test_unsorted_indices: 76	
----------------------------	

digits = load_digits()
(X, y) = (digits.data[:50], digits.target[:50])
X_test = scipy.sparse.csr_matrix(digits.data[50:100])
X_sparse = scipy.sparse.csr_matrix(X)
coef_dense = svm.SVC(kernel='linear', probability=True, random_state=0).fit(X, y).coef_
sparse_svc = svm.SVC(kernel='linear', probability=True, random_state=0).fit(X_sparse, y)
coef_sorted = sparse_svc.coef_
assert_array_almost_equal(coef_dense, coef_sorted.toarray())
tempResult = arange(X.shape[0])
	
===================================================================	
test_unsorted_indices: 77	
----------------------------	

digits = load_digits()
(X, y) = (digits.data[:50], digits.target[:50])
X_test = scipy.sparse.csr_matrix(digits.data[50:100])
X_sparse = scipy.sparse.csr_matrix(X)
coef_dense = svm.SVC(kernel='linear', probability=True, random_state=0).fit(X, y).coef_
sparse_svc = svm.SVC(kernel='linear', probability=True, random_state=0).fit(X_sparse, y)
coef_sorted = sparse_svc.coef_
assert_array_almost_equal(coef_dense, coef_sorted.toarray())
X_sparse_unsorted = X_sparse[numpy.arange(X.shape[0])]
tempResult = arange(X_test.shape[0])
	
===================================================================	
test_auto_weight: 276	
----------------------------	

from sklearn.linear_model import LogisticRegression
from sklearn.utils import compute_class_weight
(X, y) = (iris.data[:, :2], (iris.target + 1))
tempResult = arange(y.size)
	
===================================================================	
test_immutable_coef_property: 458	
----------------------------	

svms = [svm.SVC(kernel='linear').fit(iris.data, iris.target), svm.NuSVC(kernel='linear').fit(iris.data, iris.target), svm.SVR(kernel='linear').fit(iris.data, iris.target), svm.NuSVR(kernel='linear').fit(iris.data, iris.target), svm.OneClassSVM(kernel='linear').fit(iris.data)]
for clf in svms:
    tempResult = arange(3)
	
===================================================================	
test_clone_pandas_dataframe: 206	
----------------------------	


class DummyEstimator(BaseEstimator, TransformerMixin):
    'This is a dummy class for generating numerical features\n\n        This feature extractor extracts numerical features from pandas data\n        frame.\n\n        Parameters\n        ----------\n\n        df: pandas data frame\n            The pandas data frame parameter.\n\n        Notes\n        -----\n        '

    def __init__(self, df=None, scalar_param=1):
        self.df = df
        self.scalar_param = scalar_param

    def fit(self, X, y=None):
        pass

    def transform(self, X, y=None):
        pass
tempResult = arange(10)
	
===================================================================	
module: 90	
----------------------------	

'Test the cross_validation module'
from __future__ import division
import warnings
import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse import csr_matrix
from scipy import stats
from sklearn.exceptions import ConvergenceWarning
from sklearn.utils.testing import assert_true
from sklearn.utils.testing import assert_false
from sklearn.utils.testing import assert_equal
from sklearn.utils.testing import assert_almost_equal
from sklearn.utils.testing import assert_raises
from sklearn.utils.testing import assert_greater
from sklearn.utils.testing import assert_greater_equal
from sklearn.utils.testing import assert_less
from sklearn.utils.testing import assert_not_equal
from sklearn.utils.testing import assert_array_almost_equal
from sklearn.utils.testing import assert_array_equal
from sklearn.utils.testing import assert_warns_message
from sklearn.utils.testing import assert_raise_message
from sklearn.utils.testing import ignore_warnings
from sklearn.utils.mocking import CheckingClassifier, MockDataFrame
with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    from sklearn import cross_validation as cval
from sklearn.datasets import make_regression
from sklearn.datasets import load_boston
from sklearn.datasets import load_digits
from sklearn.datasets import load_iris
from sklearn.datasets import make_multilabel_classification
from sklearn.metrics import explained_variance_score
from sklearn.metrics import make_scorer
from sklearn.metrics import precision_score
from sklearn.externals import six
from sklearn.externals.six.moves import zip
from sklearn.linear_model import Ridge
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.preprocessing import Imputer
from sklearn.pipeline import Pipeline

class MockClassifier(object):
    'Dummy classifier to test the cross-validation'

    def __init__(self, a=0, allow_nd=False):
        self.a = a
        self.allow_nd = allow_nd

    def fit(self, X, Y=None, sample_weight=None, class_prior=None, sparse_sample_weight=None, sparse_param=None, dummy_int=None, dummy_str=None, dummy_obj=None, callback=None):
        'The dummy arguments are to test that this fit function can\n        accept non-array arguments through cross-validation, such as:\n            - int\n            - str (this is actually array-like)\n            - object\n            - function\n        '
        self.dummy_int = dummy_int
        self.dummy_str = dummy_str
        self.dummy_obj = dummy_obj
        if (callback is not None):
            callback(self)
        if self.allow_nd:
            X = X.reshape(len(X), (- 1))
        if ((X.ndim >= 3) and (not self.allow_nd)):
            raise ValueError('X cannot be d')
        if (sample_weight is not None):
            assert_true((sample_weight.shape[0] == X.shape[0]), 'MockClassifier extra fit_param sample_weight.shape[0] is {0}, should be {1}'.format(sample_weight.shape[0], X.shape[0]))
        if (class_prior is not None):
            assert_true((class_prior.shape[0] == len(numpy.unique(y))), 'MockClassifier extra fit_param class_prior.shape[0] is {0}, should be {1}'.format(class_prior.shape[0], len(numpy.unique(y))))
        if (sparse_sample_weight is not None):
            fmt = 'MockClassifier extra fit_param sparse_sample_weight.shape[0] is {0}, should be {1}'
            assert_true((sparse_sample_weight.shape[0] == X.shape[0]), fmt.format(sparse_sample_weight.shape[0], X.shape[0]))
        if (sparse_param is not None):
            fmt = 'MockClassifier extra fit_param sparse_param.shape is ({0}, {1}), should be ({2}, {3})'
            assert_true((sparse_param.shape == P_sparse.shape), fmt.format(sparse_param.shape[0], sparse_param.shape[1], P_sparse.shape[0], P_sparse.shape[1]))
        return self

    def predict(self, T):
        if self.allow_nd:
            T = T.reshape(len(T), (- 1))
        return T[:, 0]

    def score(self, X=None, Y=None):
        return (1.0 / (1 + numpy.abs(self.a)))

    def get_params(self, deep=False):
        return {'a': self.a, 'allow_nd': self.allow_nd}
X = numpy.ones((10, 2))
X_sparse = coo_matrix(X)
W_sparse = coo_matrix((numpy.array([1]), (numpy.array([1]), numpy.array([0]))), shape=(10, 1))
P_sparse = coo_matrix(numpy.eye(5))
tempResult = arange(10)
	
===================================================================	
test_train_test_split_allow_nans: 699	
----------------------------	

tempResult = arange(200, dtype=numpy.float64)
	
===================================================================	
test_permutation_test_score_allow_nans: 705	
----------------------------	

tempResult = arange(200, dtype=numpy.float64)
	
===================================================================	
test_train_test_split: 519	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_train_test_split: 521	
----------------------------	

X = np.arange(100).reshape((10, 10))
X_s = coo_matrix(X)
tempResult = arange(10)
	
===================================================================	
test_train_test_split: 536	
----------------------------	

X = np.arange(100).reshape((10, 10))
X_s = coo_matrix(X)
y = numpy.arange(10)
split = sklearn.cross_validation.train_test_split(X, y, test_size=None, train_size=0.5)
(X_train, X_test, y_train, y_test) = split
assert_equal(len(y_test), len(y_train))
assert_array_equal(X_train[:, 0], (y_train * 10))
assert_array_equal(X_test[:, 0], (y_test * 10))
with warnings.catch_warnings(record=True):
    split = sklearn.cross_validation.train_test_split(X, X_s, y.tolist())
(X_train, X_test, X_s_train, X_s_test, y_train, y_test) = split
assert_array_equal(X_train, X_s_train.toarray())
assert_array_equal(X_test, X_s_test.toarray())
split = sklearn.cross_validation.train_test_split(X, X_s, y.tolist())
(X_train, X_test, X_s_train, X_s_test, y_train, y_test) = split
assert_true(isinstance(y_train, list))
assert_true(isinstance(y_test, list))
tempResult = arange((((10 * 5) * 3) * 2))
	
===================================================================	
test_train_test_split: 537	
----------------------------	

X = np.arange(100).reshape((10, 10))
X_s = coo_matrix(X)
y = numpy.arange(10)
split = sklearn.cross_validation.train_test_split(X, y, test_size=None, train_size=0.5)
(X_train, X_test, y_train, y_test) = split
assert_equal(len(y_test), len(y_train))
assert_array_equal(X_train[:, 0], (y_train * 10))
assert_array_equal(X_test[:, 0], (y_test * 10))
with warnings.catch_warnings(record=True):
    split = sklearn.cross_validation.train_test_split(X, X_s, y.tolist())
(X_train, X_test, X_s_train, X_s_test, y_train, y_test) = split
assert_array_equal(X_train, X_s_train.toarray())
assert_array_equal(X_test, X_s_test.toarray())
split = sklearn.cross_validation.train_test_split(X, X_s, y.tolist())
(X_train, X_test, X_s_train, X_s_test, y_train, y_test) = split
assert_true(isinstance(y_train, list))
assert_true(isinstance(y_test, list))
X_4d = np.arange((((10 * 5) * 3) * 2)).reshape(10, 5, 3, 2)
tempResult = arange(((10 * 7) * 11))
	
===================================================================	
test_permutation_score: 618	
----------------------------	

iris = load_iris()
X = iris.data
X_sparse = coo_matrix(X)
y = iris.target
svm = SVC(kernel='linear')
cv = sklearn.cross_validation.StratifiedKFold(y, 2)
(score, scores, pvalue) = sklearn.cross_validation.permutation_test_score(svm, X, y, n_permutations=30, cv=cv, scoring='accuracy')
assert_greater(score, 0.9)
assert_almost_equal(pvalue, 0.0, 1)
(score_label, _, pvalue_label) = sklearn.cross_validation.permutation_test_score(svm, X, y, n_permutations=30, cv=cv, scoring='accuracy', labels=numpy.ones(y.size), random_state=0)
assert_true((score_label == score))
assert_true((pvalue_label == pvalue))
svm_sparse = SVC(kernel='linear')
cv_sparse = sklearn.cross_validation.StratifiedKFold(y, 2)
(score_label, _, pvalue_label) = sklearn.cross_validation.permutation_test_score(svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse, scoring='accuracy', labels=numpy.ones(y.size), random_state=0)
assert_true((score_label == score))
assert_true((pvalue_label == pvalue))

def custom_score(y_true, y_pred):
    return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) / y_true.shape[0])
scorer = make_scorer(custom_score)
(score, _, pvalue) = sklearn.cross_validation.permutation_test_score(svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)
assert_almost_equal(score, 0.93, 2)
assert_almost_equal(pvalue, 0.01, 3)
tempResult = arange(len(y))
	
===================================================================	
test_cross_val_score_allow_nans: 692	
----------------------------	

tempResult = arange(200, dtype=numpy.float64)
	
===================================================================	
test_shuffle_kfold: 199	
----------------------------	

kf = sklearn.cross_validation.KFold(300, 3, shuffle=True, random_state=0)
tempResult = arange(300)
	
===================================================================	
test_shuffle_kfold: 202	
----------------------------	

kf = sklearn.cross_validation.KFold(300, 3, shuffle=True, random_state=0)
ind = numpy.arange(300)
all_folds = None
for (train, test) in kf:
    tempResult = arange(100)
	
===================================================================	
test_shuffle_kfold: 203	
----------------------------	

kf = sklearn.cross_validation.KFold(300, 3, shuffle=True, random_state=0)
ind = numpy.arange(300)
all_folds = None
for (train, test) in kf:
    assert_true(numpy.any((numpy.arange(100) != ind[test])))
    tempResult = arange(100, 200)
	
===================================================================	
test_shuffle_kfold: 204	
----------------------------	

kf = sklearn.cross_validation.KFold(300, 3, shuffle=True, random_state=0)
ind = numpy.arange(300)
all_folds = None
for (train, test) in kf:
    assert_true(numpy.any((numpy.arange(100) != ind[test])))
    assert_true(numpy.any((numpy.arange(100, 200) != ind[test])))
    tempResult = arange(200, 300)
	
===================================================================	
test_check_is_partition: 818	
----------------------------	

tempResult = arange(100)
	
===================================================================	
module: 35	
----------------------------	

import sys
import numpy as np
from nose import SkipTest
from sklearn.utils.testing import assert_array_equal
from sklearn.utils.testing import assert_array_almost_equal
from sklearn.utils.testing import assert_equal
from sklearn.utils.testing import assert_almost_equal
from sklearn.utils.testing import assert_true
from sklearn.utils.testing import assert_raises
from sklearn.utils.testing import assert_raise_message
from sklearn.utils.testing import assert_warns
from sklearn.utils.testing import assert_greater
from sklearn.utils.testing import ignore_warnings
from sklearn.datasets import make_blobs
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.discriminant_analysis import _cov
version = sys.version_info
if (version[0] == 3):
    if (version[1] == 3):
        reload = None
    else:
        from importlib import reload
X = numpy.array([[(- 2), (- 1)], [(- 1), (- 1)], [(- 1), (- 2)], [1, 1], [1, 2], [2, 1]], dtype='f')
y = numpy.array([1, 1, 1, 2, 2, 2])
y3 = numpy.array([1, 1, 2, 2, 3, 3])
X1 = numpy.array([[(- 2)], [(- 1)], [(- 1)], [1], [1], [2]], dtype='f')
X6 = numpy.array([[0, 0], [(- 2), (- 2)], [(- 2), (- 1)], [(- 1), (- 1)], [(- 1), (- 2)], [1, 3], [1, 2], [2, 1], [2, 2]])
y6 = numpy.array([1, 1, 1, 1, 1, 2, 2, 2, 2])
y7 = numpy.array([1, 2, 3, 2, 3, 1, 2, 3, 1])
X7 = numpy.array([[(- 3)], [(- 2)], [(- 1)], [(- 1)], [0], [1], [1], [2], [3]])
X2 = numpy.array([[(- 3), 0], [(- 2), 0], [(- 1), 0], [(- 1), 0], [0, 0], [1, 0], [1, 0], [2, 0], [3, 0]])
y4 = numpy.array([1, 1, 1, 1, 1, 1, 1, 1, 2])
tempResult = arange(8)
	
===================================================================	
test_lda_orthogonality: 119	
----------------------------	

means = numpy.array([[0, 0, (- 1)], [0, 2, 0], [0, (- 2), 0], [0, 0, 5]])
scatter = numpy.array([[0.1, 0, 0], [(- 0.1), 0, 0], [0, 0.1, 0], [0, (- 0.1), 0], [0, 0, 0.1], [0, 0, (- 0.1)]])
X = (means[:, np.newaxis, :] + scatter[np.newaxis, :, :]).reshape(((- 1), 3))
tempResult = arange(means.shape[0])
	
===================================================================	
test_covariance: 209	
----------------------------	

(x, y) = make_blobs(n_samples=100, n_features=5, centers=1, random_state=42)
tempResult = arange((x.shape[1] ** 2))
	
===================================================================	
test_refit: 310	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_gridsearch_nd: 316	
----------------------------	

tempResult = arange((((10 * 5) * 3) * 2))
	
===================================================================	
test_gridsearch_nd: 317	
----------------------------	

X_4d = np.arange((((10 * 5) * 3) * 2)).reshape(10, 5, 3, 2)
tempResult = arange(((10 * 7) * 11))
	
===================================================================	
test_X_as_list: 326	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_predict_proba_disabled: 465	
----------------------------	

tempResult = arange(20)
	
===================================================================	
test_grid_search_allows_nans: 472	
----------------------------	

tempResult = arange(20, dtype=numpy.float64)
	
===================================================================	
test_pandas_input: 350	
----------------------------	

types = [(MockDataFrame, MockDataFrame)]
try:
    from pandas import Series, DataFrame
    types.append((DataFrame, Series))
except ImportError:
    pass
tempResult = arange(100)
	
===================================================================	
test_y_as_list: 335	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_isotonic_regression_reversed: 99	
----------------------------	

y = numpy.array([10, 9, 10, 7, 6, 6.1, 5])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_oob_bad: 188	
----------------------------	

y = numpy.array([3, 7, 5, 9, 8, 7, 10])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_sample_weight_parameter_default_value: 136	
----------------------------	

ir = IsotonicRegression()
rng = numpy.random.RandomState(42)
n = 100
tempResult = arange(n)
	
===================================================================	
test_isotonic_sample_weight_parameter_default_value: 137	
----------------------------	

ir = IsotonicRegression()
rng = numpy.random.RandomState(42)
n = 100
x = numpy.arange(n)
tempResult = arange(n)
	
===================================================================	
test_isotonic_min_max_boundaries: 146	
----------------------------	

ir = IsotonicRegression(y_min=2, y_max=4)
n = 6
tempResult = arange(n)
	
===================================================================	
test_isotonic_min_max_boundaries: 147	
----------------------------	

ir = IsotonicRegression(y_min=2, y_max=4)
n = 6
x = numpy.arange(n)
tempResult = arange(n)
	
===================================================================	
test_isotonic_regression: 57	
----------------------------	

y = numpy.array([3, 7, 5, 9, 8, 7, 10])
y_ = numpy.array([3, 6, 6, 8, 8, 8, 10])
assert_array_equal(y_, isotonic_regression(y))
y = numpy.array([10, 0, 2])
y_ = numpy.array([4, 4, 4])
assert_array_equal(y_, isotonic_regression(y))
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_oob_nan: 180	
----------------------------	

y = numpy.array([3, 7, 5, 9, 8, 7, 10])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_pickle: 202	
----------------------------	

y = numpy.array([3, 7, 5, 9, 8, 7, 10])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_auto_increasing: 115	
----------------------------	

y = numpy.array([5, 6.1, 6, 7, 10, 9, 10])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_oob_raise: 163	
----------------------------	

y = numpy.array([3, 7, 5, 9, 8, 7, 10])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_oob_clip: 170	
----------------------------	

y = numpy.array([3, 7, 5, 9, 8, 7, 10])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_auto_decreasing: 104	
----------------------------	

y = numpy.array([10, 9, 10, 7, 6, 6.1, 5])
tempResult = arange(len(y))
	
===================================================================	
test_isotonic_regression_oob_bad_after: 194	
----------------------------	

y = numpy.array([3, 7, 5, 9, 8, 7, 10])
tempResult = arange(len(y))
	
===================================================================	
test_input_check_partial_fit: 195	
----------------------------	

for cls in [BernoulliNB, MultinomialNB]:
    assert_raises(ValueError, cls().partial_fit, X2, y2[:(- 1)], classes=numpy.unique(y2))
    assert_raises(ValueError, cls().partial_fit, X2, y2)
    clf = cls()
    clf.partial_fit(X2, y2, classes=numpy.unique(y2))
    tempResult = arange(42)
	
===================================================================	
_color_brew: 15	
----------------------------	

'Generate n colors with equally spaced hues.\n\n    Parameters\n    ----------\n    n : int\n        The number of colors required.\n\n    Returns\n    -------\n    color_list : list, length n\n        List of n tuples of form (R, G, B) being the components of each color.\n    '
color_list = []
(s, v) = (0.75, 0.9)
c = (s * v)
m = (v - c)
tempResult = arange(25, 385, (360.0 / n))
	
===================================================================	
test_arrayrepr: 162	
----------------------------	

tempResult = arange(10000)
	
===================================================================	
test_arrayrepr: 163	
----------------------------	

X = numpy.arange(10000)[:, numpy.newaxis]
tempResult = arange(10000)
	
===================================================================	
test_sample_weight_invalid: 519	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_sample_weight: 485	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_sample_weight: 493	
----------------------------	

X = numpy.arange(100)[:, numpy.newaxis]
y = numpy.ones(100)
y[:50] = 0.0
sample_weight = numpy.ones(100)
sample_weight[(y == 0)] = 0.0
clf = DecisionTreeClassifier(random_state=0)
clf.fit(X, y, sample_weight=sample_weight)
assert_array_equal(clf.predict(X), numpy.ones(100))
tempResult = arange(200)
	
===================================================================	
check_explicit_sparse_zeros: 733	
----------------------------	

TreeEstimator = ALL_TREES[tree]
n_samples = n_features
tempResult = arange(n_samples)
	
===================================================================	
check_supervised_y_2d: 750	
----------------------------	

if ('MultiTask' in name):
    return
rnd = numpy.random.RandomState(0)
X = rnd.uniform(size=(10, 3))
tempResult = arange(10)
	
===================================================================	
check_fit_score_takes_y: 449	
----------------------------	

rnd = numpy.random.RandomState(0)
X = rnd.uniform(size=(10, 3))
tempResult = arange(10)
	
===================================================================	
rankdata: 280	
----------------------------	

if (method not in ('average', 'min', 'max', 'dense', 'ordinal')):
    raise ValueError('unknown method "{0}"'.format(method))
arr = numpy.ravel(numpy.asarray(a))
algo = ('mergesort' if (method == 'ordinal') else 'quicksort')
sorter = numpy.argsort(arr, kind=algo)
inv = numpy.empty(sorter.size, dtype=numpy.intp)
tempResult = arange(sorter.size, dtype=numpy.intp)
	
===================================================================	
_unique_indicator: 23	
----------------------------	

tempResult = arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])
	
===================================================================	
resample: 71	
----------------------------	

"Resample arrays or sparse matrices in a consistent way\n\n    The default strategy implements one step of the bootstrapping\n    procedure.\n\n    Parameters\n    ----------\n    *arrays : sequence of indexable data-structures\n        Indexable data-structures can be arrays, lists, dataframes or scipy\n        sparse matrices with consistent first dimension.\n\n    replace : boolean, True by default\n        Implements resampling with replacement. If False, this will implement\n        (sliced) random permutations.\n\n    n_samples : int, None by default\n        Number of samples to generate. If left to None this is\n        automatically set to the first dimension of the arrays.\n        If replace is False it should not be larger than the length of\n        arrays.\n\n    random_state : int or RandomState instance\n        Control the shuffling for reproducible behavior.\n\n    Returns\n    -------\n    resampled_arrays : sequence of indexable data-structures\n        Sequence of resampled views of the collections. The original arrays are\n        not impacted.\n\n    Examples\n    --------\n    It is possible to mix sparse and dense arrays in the same run::\n\n      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n      >>> y = np.array([0, 1, 2])\n\n      >>> from scipy.sparse import coo_matrix\n      >>> X_sparse = coo_matrix(X)\n\n      >>> from sklearn.utils import resample\n      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n      >>> X\n      array([[ 1.,  0.],\n             [ 2.,  1.],\n             [ 1.,  0.]])\n\n      >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n          with 4 stored elements in Compressed Sparse Row format>\n\n      >>> X_sparse.toarray()\n      array([[ 1.,  0.],\n             [ 2.,  1.],\n             [ 1.,  0.]])\n\n      >>> y\n      array([0, 1, 0])\n\n      >>> resample(y, n_samples=2, random_state=0)\n      array([0, 1])\n\n\n    See also\n    --------\n    :func:`sklearn.utils.shuffle`\n    "
random_state = check_random_state(options.pop('random_state', None))
replace = options.pop('replace', True)
max_n_samples = options.pop('n_samples', None)
if options:
    raise ValueError(('Unexpected kw arguments: %r' % options.keys()))
if (len(arrays) == 0):
    return None
first = arrays[0]
n_samples = (first.shape[0] if hasattr(first, 'shape') else len(first))
if (max_n_samples is None):
    max_n_samples = n_samples
elif ((max_n_samples > n_samples) and (not replace)):
    raise ValueError(('Cannot sample %d out of arrays with dim %dwhen replace is False' % (max_n_samples, n_samples)))
check_consistent_length(*arrays)
if replace:
    indices = random_state.randint(0, n_samples, size=(max_n_samples,))
else:
    tempResult = arange(n_samples)
	
===================================================================	
safe_mask: 26	
----------------------------	

'Return a mask which is safe to use on X.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Data on which to apply mask.\n\n    mask: array\n        Mask to be used on X.\n\n    Returns\n    -------\n        mask\n    '
mask = numpy.asarray(mask)
if numpy.issubdtype(mask.dtype, numpy.int):
    return mask
if hasattr(X, 'toarray'):
    tempResult = arange(mask.shape[0])
	
===================================================================	
test_compute_class_weight_not_present: 27	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_compute_class_weight_not_present: 31	
----------------------------	

classes = numpy.arange(4)
y = numpy.asarray([0, 0, 0, 1, 1, 2])
assert_raises(ValueError, compute_class_weight, 'auto', classes, y)
assert_raises(ValueError, compute_class_weight, 'balanced', classes, y)
tempResult = arange(2)
	
===================================================================	
test_compute_class_weight_dict: 37	
----------------------------	

tempResult = arange(3)
	
===================================================================	
test_cartesian: 236	
----------------------------	

axes = (numpy.array([1, 2, 3]), numpy.array([4, 5]), numpy.array([6, 7]))
true_out = numpy.array([[1, 4, 6], [1, 4, 7], [1, 5, 6], [1, 5, 7], [2, 4, 6], [2, 4, 7], [2, 5, 6], [2, 5, 7], [3, 4, 6], [3, 4, 7], [3, 5, 6], [3, 5, 7]])
out = cartesian(axes)
assert_array_equal(true_out, out)
tempResult = arange(3)
	
===================================================================	
test_incremental_variance_ddof: 361	
----------------------------	

rng = numpy.random.RandomState(1999)
X = rng.randn(50, 10)
(n_samples, n_features) = X.shape
for batch_size in [11, 20, 37]:
    tempResult = arange(0, X.shape[0], batch_size)
	
===================================================================	
Get no callers of function numpy.arange at line 221 col 10.	
===================================================================	
test_int_float_dict_argmin: 24	
----------------------------	

tempResult = arange(100, dtype=numpy.intp)
	
===================================================================	
test_int_float_dict_argmin: 25	
----------------------------	

keys = numpy.arange(100, dtype=numpy.intp)
tempResult = arange(100, dtype=numpy.float64)
	
===================================================================	
test_expit: 13	
----------------------------	

assert_almost_equal(expit(1000.0), (1.0 / (1.0 + numpy.exp((- 1000.0)))), decimal=16)
assert_almost_equal(expit((- 1000.0)), (numpy.exp((- 1000.0)) / (1.0 + numpy.exp((- 1000.0)))), decimal=16)
tempResult = arange(10)
	
===================================================================	
test_graph_laplacian: 7	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_graph_laplacian: 7	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_graph_laplacian: 7	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_graph_laplacian: 7	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_unique_labels: 45	
----------------------------	

assert_raises(ValueError, unique_labels)
tempResult = arange(10)
	
===================================================================	
test_unique_labels: 46	
----------------------------	

assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(xrange(10)), numpy.arange(10))
tempResult = arange(10)
	
===================================================================	
test_unique_labels: 46	
----------------------------	

assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(xrange(10)), numpy.arange(10))
tempResult = arange(10)
	
===================================================================	
test_unique_labels: 48	
----------------------------	

assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(xrange(10)), numpy.arange(10))
assert_array_equal(unique_labels(numpy.arange(10)), numpy.arange(10))
assert_array_equal(unique_labels([4, 0, 2]), numpy.array([0, 2, 4]))
tempResult = arange(3)
	
===================================================================	
test_unique_labels: 49	
----------------------------	

assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(xrange(10)), numpy.arange(10))
assert_array_equal(unique_labels(numpy.arange(10)), numpy.arange(10))
assert_array_equal(unique_labels([4, 0, 2]), numpy.array([0, 2, 4]))
assert_array_equal(unique_labels(numpy.array([[0, 0, 1], [1, 0, 1], [0, 0, 0]])), numpy.arange(3))
tempResult = arange(3)
	
===================================================================	
test_unique_labels: 50	
----------------------------	

assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(xrange(10)), numpy.arange(10))
assert_array_equal(unique_labels(numpy.arange(10)), numpy.arange(10))
assert_array_equal(unique_labels([4, 0, 2]), numpy.array([0, 2, 4]))
assert_array_equal(unique_labels(numpy.array([[0, 0, 1], [1, 0, 1], [0, 0, 0]])), numpy.arange(3))
assert_array_equal(unique_labels(numpy.array([[0, 0, 1], [0, 0, 0]])), numpy.arange(3))
tempResult = arange(5)
	
===================================================================	
test_unique_labels: 51	
----------------------------	

assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(xrange(10)), numpy.arange(10))
assert_array_equal(unique_labels(numpy.arange(10)), numpy.arange(10))
assert_array_equal(unique_labels([4, 0, 2]), numpy.array([0, 2, 4]))
assert_array_equal(unique_labels(numpy.array([[0, 0, 1], [1, 0, 1], [0, 0, 0]])), numpy.arange(3))
assert_array_equal(unique_labels(numpy.array([[0, 0, 1], [0, 0, 0]])), numpy.arange(3))
assert_array_equal(unique_labels([4, 0, 2], xrange(5)), numpy.arange(5))
tempResult = arange(3)
	
===================================================================	
test_unique_labels: 54	
----------------------------	

assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(xrange(10)), numpy.arange(10))
assert_array_equal(unique_labels(numpy.arange(10)), numpy.arange(10))
assert_array_equal(unique_labels([4, 0, 2]), numpy.array([0, 2, 4]))
assert_array_equal(unique_labels(numpy.array([[0, 0, 1], [1, 0, 1], [0, 0, 0]])), numpy.arange(3))
assert_array_equal(unique_labels(numpy.array([[0, 0, 1], [0, 0, 0]])), numpy.arange(3))
assert_array_equal(unique_labels([4, 0, 2], xrange(5)), numpy.arange(5))
assert_array_equal(unique_labels((0, 1, 2), (0,), (2, 1)), numpy.arange(3))
assert_raises(ValueError, unique_labels, [4, 0, 2], numpy.ones((5, 5)))
assert_raises(ValueError, unique_labels, numpy.ones((5, 4)), numpy.ones((5, 5)))
tempResult = arange(5)
	
===================================================================	
module: 12	
----------------------------	

import numpy as np
import scipy.sparse as sp
from sklearn.utils.seq_dataset import ArrayDataset, CSRDataset
from sklearn.datasets import load_iris
from numpy.testing import assert_array_equal
from nose.tools import assert_equal
iris = load_iris()
X = iris.data.astype(numpy.float64)
y = iris.target.astype(numpy.float64)
X_csr = scipy.sparse.csr_matrix(X)
tempResult = arange(y.size, dtype=numpy.float64)
	
===================================================================	
test_column_or_1d: 106	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_column_or_1d: 106	
----------------------------	

tempResult = arange(30)
	
===================================================================	
test_shuffle_dont_convert_to_array: 160	
----------------------------	

a = ['a', 'b', 'c']
b = numpy.array(['a', 'b', 'c'], dtype=object)
c = [1, 2, 3]
d = MockDataFrame(numpy.array([['a', 0], ['b', 1], ['c', 2]], dtype=object))
tempResult = arange(6)
	
===================================================================	
test_np_matrix: 50	
----------------------------	

tempResult = arange(12)
	
===================================================================	
test_check_array: 90	
----------------------------	

X = [[1, 2], [3, 4]]
X_csr = scipy.sparse.csr_matrix(X)
assert_raises(TypeError, check_array, X_csr)
assert_warns(DeprecationWarning, check_array, [0, 1, 2])
X_array = check_array([0, 1, 2])
assert_equal(X_array.ndim, 2)
X_array = check_array([0, 1, 2], ensure_2d=False)
assert_equal(X_array.ndim, 1)
tempResult = arange(8)
	
===================================================================	
test_check_array: 93	
----------------------------	

X = [[1, 2], [3, 4]]
X_csr = scipy.sparse.csr_matrix(X)
assert_raises(TypeError, check_array, X_csr)
assert_warns(DeprecationWarning, check_array, [0, 1, 2])
X_array = check_array([0, 1, 2])
assert_equal(X_array.ndim, 2)
X_array = check_array([0, 1, 2], ensure_2d=False)
assert_equal(X_array.ndim, 1)
X_ndim = np.arange(8).reshape(2, 2, 2)
assert_raises(ValueError, check_array, X_ndim)
check_array(X_ndim, allow_nd=True)
tempResult = arange(4)
	
===================================================================	
test_check_array: 97	
----------------------------	

X = [[1, 2], [3, 4]]
X_csr = scipy.sparse.csr_matrix(X)
assert_raises(TypeError, check_array, X_csr)
assert_warns(DeprecationWarning, check_array, [0, 1, 2])
X_array = check_array([0, 1, 2])
assert_equal(X_array.ndim, 2)
X_array = check_array([0, 1, 2], ensure_2d=False)
assert_equal(X_array.ndim, 1)
X_ndim = np.arange(8).reshape(2, 2, 2)
assert_raises(ValueError, check_array, X_ndim)
check_array(X_ndim, allow_nd=True)
X_inf = np.arange(4).reshape(2, 2).astype(numpy.float)
X_inf[(0, 0)] = numpy.inf
assert_raises(ValueError, check_array, X_inf)
check_array(X_inf, force_all_finite=False)
tempResult = arange(4)
	
===================================================================	
test_check_array: 101	
----------------------------	

X = [[1, 2], [3, 4]]
X_csr = scipy.sparse.csr_matrix(X)
assert_raises(TypeError, check_array, X_csr)
assert_warns(DeprecationWarning, check_array, [0, 1, 2])
X_array = check_array([0, 1, 2])
assert_equal(X_array.ndim, 2)
X_array = check_array([0, 1, 2], ensure_2d=False)
assert_equal(X_array.ndim, 1)
X_ndim = np.arange(8).reshape(2, 2, 2)
assert_raises(ValueError, check_array, X_ndim)
check_array(X_ndim, allow_nd=True)
X_inf = np.arange(4).reshape(2, 2).astype(numpy.float)
X_inf[(0, 0)] = numpy.inf
assert_raises(ValueError, check_array, X_inf)
check_array(X_inf, force_all_finite=False)
X_nan = np.arange(4).reshape(2, 2).astype(numpy.float)
X_nan[(0, 0)] = numpy.nan
assert_raises(ValueError, check_array, X_nan)
check_array(X_inf, force_all_finite=False)
tempResult = arange(4)
	
===================================================================	
test_as_float_array: 32	
----------------------------	

X = numpy.ones((3, 10), dtype=numpy.int32)
tempResult = arange(10, dtype=numpy.int32)
	
===================================================================	
test_as_float_array: 43	
----------------------------	

X = numpy.ones((3, 10), dtype=numpy.int32)
X = (X + numpy.arange(10, dtype=numpy.int32))
X2 = as_float_array(X, copy=False)
numpy.testing.assert_equal(X2.dtype, numpy.float32)
X = X.astype(numpy.int64)
X2 = as_float_array(X, copy=True)
assert_true((as_float_array(X, False) is not X))
numpy.testing.assert_equal(X2.dtype, numpy.float64)
X = numpy.ones((3, 2), dtype=numpy.float32)
assert_true((as_float_array(X, copy=False) is X))
X = numpy.asfortranarray(X)
assert_true(numpy.isfortran(as_float_array(X, copy=True)))
tempResult = arange(5)
	
===================================================================	
test_as_float_array: 43	
----------------------------	

X = numpy.ones((3, 10), dtype=numpy.int32)
X = (X + numpy.arange(10, dtype=numpy.int32))
X2 = as_float_array(X, copy=False)
numpy.testing.assert_equal(X2.dtype, numpy.float32)
X = X.astype(numpy.int64)
X2 = as_float_array(X, copy=True)
assert_true((as_float_array(X, False) is not X))
numpy.testing.assert_equal(X2.dtype, numpy.float64)
X = numpy.ones((3, 2), dtype=numpy.float32)
assert_true((as_float_array(X, copy=False) is X))
X = numpy.asfortranarray(X)
assert_true(numpy.isfortran(as_float_array(X, copy=True)))
tempResult = arange(5)
	
***************************************************	
matplotlib_matplotlib-2.0.0: 639	
===================================================================	
module: 12	
----------------------------	

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid.axislines import SubplotZero
import numpy as np
fig = matplotlib.pyplot.figure(1, (4, 3))
ax = SubplotZero(fig, 1, 1, 1)
fig.add_subplot(ax)
ax.axis['xzero'].set_visible(True)
ax.axis['xzero'].label.set_text('Axis Zero')
for n in ['bottom', 'top', 'right']:
    ax.axis[n].set_visible(False)
tempResult = arange(0, (2 * numpy.pi), 0.01)
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import numpy as np
ax = matplotlib.pyplot.subplot(111)
tempResult = arange(100)
	
===================================================================	
squiggle_xy: 17	
----------------------------	

	
===================================================================	
module: 19	
----------------------------	

'\n=========================\nSimple animation examples\n=========================\n\nThis example contains two animations. The first is a random walk plot. The\nsecond is an image animation.\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def update_line(num, data, line):
    line.set_data(data[..., :num])
    return (line,)
fig1 = matplotlib.pyplot.figure()
data = numpy.random.rand(2, 25)
(l,) = matplotlib.pyplot.plot([], [], 'r-')
matplotlib.pyplot.xlim(0, 1)
matplotlib.pyplot.ylim(0, 1)
matplotlib.pyplot.xlabel('x')
matplotlib.pyplot.title('test')
line_ani = matplotlib.animation.FuncAnimation(fig1, update_line, 25, fargs=(data, l), interval=50, blit=True)
fig2 = matplotlib.pyplot.figure()
tempResult = arange((- 9), 10)
	
===================================================================	
module: 20	
----------------------------	

'\n=========================\nSimple animation examples\n=========================\n\nThis example contains two animations. The first is a random walk plot. The\nsecond is an image animation.\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def update_line(num, data, line):
    line.set_data(data[..., :num])
    return (line,)
fig1 = matplotlib.pyplot.figure()
data = numpy.random.rand(2, 25)
(l,) = matplotlib.pyplot.plot([], [], 'r-')
matplotlib.pyplot.xlim(0, 1)
matplotlib.pyplot.ylim(0, 1)
matplotlib.pyplot.xlabel('x')
matplotlib.pyplot.title('test')
line_ani = matplotlib.animation.FuncAnimation(fig1, update_line, 25, fargs=(data, l), interval=50, blit=True)
fig2 = matplotlib.pyplot.figure()
x = numpy.arange((- 9), 10)
tempResult = arange((- 9), 10)
	
===================================================================	
module: 23	
----------------------------	

'\n=========================\nSimple animation examples\n=========================\n\nThis example contains two animations. The first is a random walk plot. The\nsecond is an image animation.\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def update_line(num, data, line):
    line.set_data(data[..., :num])
    return (line,)
fig1 = matplotlib.pyplot.figure()
data = numpy.random.rand(2, 25)
(l,) = matplotlib.pyplot.plot([], [], 'r-')
matplotlib.pyplot.xlim(0, 1)
matplotlib.pyplot.ylim(0, 1)
matplotlib.pyplot.xlabel('x')
matplotlib.pyplot.title('test')
line_ani = matplotlib.animation.FuncAnimation(fig1, update_line, 25, fargs=(data, l), interval=50, blit=True)
fig2 = matplotlib.pyplot.figure()
x = numpy.arange((- 9), 10)
y = np.arange((- 9), 10).reshape((- 1), 1)
base = numpy.hypot(x, y)
ims = []
tempResult = arange(15)
	
===================================================================	
module: 24	
----------------------------	

'\n===================\nSaving an animation\n===================\n\nThis example showcases the same animations as `basic_example.py`, but instead\nof displaying the animation to the user, it writes to files using a\nMovieWriter instance.\n'
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def update_line(num, data, line):
    line.set_data(data[..., :num])
    return (line,)
Writer = matplotlib.animation.writers['ffmpeg']
writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)
fig1 = matplotlib.pyplot.figure()
data = numpy.random.rand(2, 25)
(l,) = matplotlib.pyplot.plot([], [], 'r-')
matplotlib.pyplot.xlim(0, 1)
matplotlib.pyplot.ylim(0, 1)
matplotlib.pyplot.xlabel('x')
matplotlib.pyplot.title('test')
line_ani = matplotlib.animation.FuncAnimation(fig1, update_line, 25, fargs=(data, l), interval=50, blit=True)
line_ani.save('lines.mp4', writer=writer)
fig2 = matplotlib.pyplot.figure()
tempResult = arange((- 9), 10)
	
===================================================================	
module: 25	
----------------------------	

'\n===================\nSaving an animation\n===================\n\nThis example showcases the same animations as `basic_example.py`, but instead\nof displaying the animation to the user, it writes to files using a\nMovieWriter instance.\n'
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def update_line(num, data, line):
    line.set_data(data[..., :num])
    return (line,)
Writer = matplotlib.animation.writers['ffmpeg']
writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)
fig1 = matplotlib.pyplot.figure()
data = numpy.random.rand(2, 25)
(l,) = matplotlib.pyplot.plot([], [], 'r-')
matplotlib.pyplot.xlim(0, 1)
matplotlib.pyplot.ylim(0, 1)
matplotlib.pyplot.xlabel('x')
matplotlib.pyplot.title('test')
line_ani = matplotlib.animation.FuncAnimation(fig1, update_line, 25, fargs=(data, l), interval=50, blit=True)
line_ani.save('lines.mp4', writer=writer)
fig2 = matplotlib.pyplot.figure()
x = numpy.arange((- 9), 10)
tempResult = arange((- 9), 10)
	
===================================================================	
module: 28	
----------------------------	

'\n===================\nSaving an animation\n===================\n\nThis example showcases the same animations as `basic_example.py`, but instead\nof displaying the animation to the user, it writes to files using a\nMovieWriter instance.\n'
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.animation as animation

def update_line(num, data, line):
    line.set_data(data[..., :num])
    return (line,)
Writer = matplotlib.animation.writers['ffmpeg']
writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)
fig1 = matplotlib.pyplot.figure()
data = numpy.random.rand(2, 25)
(l,) = matplotlib.pyplot.plot([], [], 'r-')
matplotlib.pyplot.xlim(0, 1)
matplotlib.pyplot.ylim(0, 1)
matplotlib.pyplot.xlabel('x')
matplotlib.pyplot.title('test')
line_ani = matplotlib.animation.FuncAnimation(fig1, update_line, 25, fargs=(data, l), interval=50, blit=True)
line_ani.save('lines.mp4', writer=writer)
fig2 = matplotlib.pyplot.figure()
x = numpy.arange((- 9), 10)
y = np.arange((- 9), 10).reshape((- 1), 1)
base = numpy.hypot(x, y)
ims = []
tempResult = arange(15)
	
===================================================================	
module: 36	
----------------------------	

'\n================\nThe Bayes update\n================\n\nThis animation displays the posterior estimate updates as it is refitted when\nnew data arrives.\nThe vertical line represents the theoretical value to which the plotted\ndistribution should converge.\n'
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as ss
from matplotlib.animation import FuncAnimation

class UpdateDist(object):

    def __init__(self, ax, prob=0.5):
        self.success = 0
        self.prob = prob
        (self.line,) = ax.plot([], [], 'k-')
        self.x = numpy.linspace(0, 1, 200)
        self.ax = ax
        self.ax.set_xlim(0, 1)
        self.ax.set_ylim(0, 15)
        self.ax.grid(True)
        self.ax.axvline(prob, linestyle='--', color='black')

    def init(self):
        self.success = 0
        self.line.set_data([], [])
        return (self.line,)

    def __call__(self, i):
        if (i == 0):
            return self.init()
        if (numpy.random.rand(1) < self.prob):
            self.success += 1
        y = scipy.stats.beta.pdf(self.x, (self.success + 1), ((i - self.success) + 1))
        self.line.set_data(self.x, y)
        return (self.line,)
(fig, ax) = matplotlib.pyplot.subplots()
ud = UpdateDist(ax, prob=0.7)
tempResult = arange(100)
	
===================================================================	
module: 25	
----------------------------	

'\n===========================\nThe double pendulum problem\n===========================\n\nThis animation illustrates the double pendulum problem.\n'
from numpy import sin, cos
import numpy as np
import matplotlib.pyplot as plt
import scipy.integrate as integrate
import matplotlib.animation as animation
G = 9.8
L1 = 1.0
L2 = 1.0
M1 = 1.0
M2 = 1.0

def derivs(state, t):
    dydx = numpy.zeros_like(state)
    dydx[0] = state[1]
    del_ = (state[2] - state[0])
    den1 = (((M1 + M2) * L1) - (((M2 * L1) * cos(del_)) * cos(del_)))
    dydx[1] = (((((((((M2 * L1) * state[1]) * state[1]) * sin(del_)) * cos(del_)) + (((M2 * G) * sin(state[2])) * cos(del_))) + ((((M2 * L2) * state[3]) * state[3]) * sin(del_))) - (((M1 + M2) * G) * sin(state[0]))) / den1)
    dydx[2] = state[3]
    den2 = ((L2 / L1) * den1)
    dydx[3] = ((((((((((- M2) * L2) * state[3]) * state[3]) * sin(del_)) * cos(del_)) + ((((M1 + M2) * G) * sin(state[0])) * cos(del_))) - (((((M1 + M2) * L1) * state[1]) * state[1]) * sin(del_))) - (((M1 + M2) * G) * sin(state[2]))) / den2)
    return dydx
dt = 0.05
tempResult = arange(0.0, 20, dt)
	
===================================================================	
module: 54	
----------------------------	

'\n===========================\nThe double pendulum problem\n===========================\n\nThis animation illustrates the double pendulum problem.\n'
from numpy import sin, cos
import numpy as np
import matplotlib.pyplot as plt
import scipy.integrate as integrate
import matplotlib.animation as animation
G = 9.8
L1 = 1.0
L2 = 1.0
M1 = 1.0
M2 = 1.0

def derivs(state, t):
    dydx = numpy.zeros_like(state)
    dydx[0] = state[1]
    del_ = (state[2] - state[0])
    den1 = (((M1 + M2) * L1) - (((M2 * L1) * cos(del_)) * cos(del_)))
    dydx[1] = (((((((((M2 * L1) * state[1]) * state[1]) * sin(del_)) * cos(del_)) + (((M2 * G) * sin(state[2])) * cos(del_))) + ((((M2 * L2) * state[3]) * state[3]) * sin(del_))) - (((M1 + M2) * G) * sin(state[0]))) / den1)
    dydx[2] = state[3]
    den2 = ((L2 / L1) * den1)
    dydx[3] = ((((((((((- M2) * L2) * state[3]) * state[3]) * sin(del_)) * cos(del_)) + ((((M1 + M2) * G) * sin(state[0])) * cos(del_))) - (((((M1 + M2) * L1) * state[1]) * state[1]) * sin(del_))) - (((M1 + M2) * G) * sin(state[2]))) / den2)
    return dydx
dt = 0.05
t = numpy.arange(0.0, 20, dt)
th1 = 120.0
w1 = 0.0
th2 = (- 10.0)
w2 = 0.0
state = numpy.radians([th1, w1, th2, w2])
y = scipy.integrate.odeint(derivs, state, t)
x1 = (L1 * sin(y[:, 0]))
y1 = ((- L1) * cos(y[:, 0]))
x2 = ((L2 * sin(y[:, 2])) + x1)
y2 = (((- L2) * cos(y[:, 2])) + y1)
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, autoscale_on=False, xlim=((- 2), 2), ylim=((- 2), 2))
ax.grid()
(line,) = ax.plot([], [], 'o-', lw=2)
time_template = 'time = %.1fs'
time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)

def init():
    line.set_data([], [])
    time_text.set_text('')
    return (line, time_text)

def animate(i):
    thisx = [0, x1[i], x2[i]]
    thisy = [0, y1[i], y2[i]]
    line.set_data(thisx, thisy)
    time_text.set_text((time_template % (i * dt)))
    return (line, time_text)
tempResult = arange(1, len(y))
	
===================================================================	
module: 7	
----------------------------	

'\nA simple example of an animated plot\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(0, (2 * numpy.pi), 0.01)
	
===================================================================	
module: 17	
----------------------------	

'\nA simple example of an animated plot\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
(fig, ax) = matplotlib.pyplot.subplots()
x = numpy.arange(0, (2 * numpy.pi), 0.01)
(line,) = ax.plot(x, numpy.sin(x))

def animate(i):
    line.set_ydata(numpy.sin((x + (i / 10.0))))
    return (line,)

def init():
    line.set_ydata(numpy.ma.array(x, mask=True))
    return (line,)
tempResult = arange(1, 200)
	
===================================================================	
module: 8	
----------------------------	

'\n========\nBarchart\n========\n\nA bar plot with errorbars and height labels on individual bars\n'
import numpy as np
import matplotlib.pyplot as plt
N = 5
men_means = (20, 35, 30, 35, 27)
men_std = (2, 3, 4, 1, 2)
tempResult = arange(N)
	
===================================================================	
module: 8	
----------------------------	

'\n=========================================================\nLine, Poly and RegularPoly Collection with autoscaling\n=========================================================\n\nFor the first two subplots, we will use spirals.  Their\nsize will be set in plot units, not data units.  Their positions\nwill be set in data units by using the "offsets" and "transOffset"\nkwargs of the LineCollection and PolyCollection.\n\nThe third subplot will make regular polygons, with the same\ntype of scaling and positioning as in the first two.\n\nThe last subplot illustrates the use of "offsets=(xo,yo)",\nthat is, a single tuple instead of a list of tuples, to generate\nsuccessively offset curves, with the offset given in data\nunits.  This behavior is available only for the LineCollection.\n\n'
import matplotlib.pyplot as plt
from matplotlib import collections, colors, transforms
import numpy as np
nverts = 50
npts = 100
tempResult = arange(nverts)
	
===================================================================	
module: 81	
----------------------------	

'\n============\nCustom scale\n============\n\nThis example showcases how to create a custom scale, by implementing the\nscaling use for latitude data in a Mercator Projection.\n'
from __future__ import unicode_literals
import numpy as np
from numpy import ma
from matplotlib import scale as mscale
from matplotlib import transforms as mtransforms
from matplotlib.ticker import Formatter, FixedLocator
from matplotlib import rcParams
rcParams['axes.axisbelow'] = False

class MercatorLatitudeScale(matplotlib.scale.ScaleBase):
    '\n    Scales data in range -pi/2 to pi/2 (-90 to 90 degrees) using\n    the system used to scale latitudes in a Mercator projection.\n\n    The scale function:\n      ln(tan(y) + sec(y))\n\n    The inverse scale function:\n      atan(sinh(y))\n\n    Since the Mercator scale tends to infinity at +/- 90 degrees,\n    there is user-defined threshold, above and below which nothing\n    will be plotted.  This defaults to +/- 85 degrees.\n\n    source:\n    http://en.wikipedia.org/wiki/Mercator_projection\n    '
    name = 'mercator'

    def __init__(self, axis, **kwargs):
        "\n        Any keyword arguments passed to ``set_xscale`` and\n        ``set_yscale`` will be passed along to the scale's\n        constructor.\n\n        thresh: The degree above which to crop the data.\n        "
        matplotlib.scale.ScaleBase.__init__(self)
        thresh = kwargs.pop('thresh', numpy.radians(85))
        if (thresh >= (numpy.pi / 2.0)):
            raise ValueError('thresh must be less than pi/2')
        self.thresh = thresh

    def get_transform(self):
        '\n        Override this method to return a new instance that does the\n        actual transformation of the data.\n\n        The MercatorLatitudeTransform class is defined below as a\n        nested class of this one.\n        '
        return self.MercatorLatitudeTransform(self.thresh)

    def set_default_locators_and_formatters(self, axis):
        '\n        Override to set up the locators and formatters to use with the\n        scale.  This is only required if the scale requires custom\n        locators and formatters.  Writing custom locators and\n        formatters is rather outside the scope of this example, but\n        there are many helpful examples in ``ticker.py``.\n\n        In our case, the Mercator example uses a fixed locator from\n        -90 to 90 degrees and a custom formatter class to put convert\n        the radians to degrees and put a degree symbol after the\n        value::\n        '

        class DegreeFormatter(Formatter):

            def __call__(self, x, pos=None):
                return ('%d°' % numpy.degrees(x))
        axis.set_major_locator(FixedLocator(numpy.radians(numpy.arange((- 90), 90, 10))))
        axis.set_major_formatter(DegreeFormatter())
        axis.set_minor_formatter(DegreeFormatter())

    def limit_range_for_scale(self, vmin, vmax, minpos):
        '\n        Override to limit the bounds of the axis to the domain of the\n        transform.  In the case of Mercator, the bounds should be\n        limited to the threshold that was passed in.  Unlike the\n        autoscaling provided by the tick locators, this range limiting\n        will always be adhered to, whether the axis range is set\n        manually, determined automatically or changed through panning\n        and zooming.\n        '
        return (max(vmin, (- self.thresh)), min(vmax, self.thresh))

    class MercatorLatitudeTransform(matplotlib.transforms.Transform):
        input_dims = 1
        output_dims = 1
        is_separable = True

        def __init__(self, thresh):
            matplotlib.transforms.Transform.__init__(self)
            self.thresh = thresh

        def transform_non_affine(self, a):
            '\n            This transform takes an Nx1 ``numpy`` array and returns a\n            transformed copy.  Since the range of the Mercator scale\n            is limited by the user-specified threshold, the input\n            array must be masked to contain only valid values.\n            ``matplotlib`` will handle masked arrays and remove the\n            out-of-range data from the plot.  Importantly, the\n            ``transform`` method *must* return an array that is the\n            same shape as the input array, since these values need to\n            remain synchronized with values in the other dimension.\n            '
            masked = numpy.ma.masked_where(((a < (- self.thresh)) | (a > self.thresh)), a)
            if masked.mask.any():
                return numpy.ma.log(numpy.abs((numpy.ma.tan(masked) + (1.0 / numpy.ma.cos(masked)))))
            else:
                return numpy.log(numpy.abs((numpy.tan(a) + (1.0 / numpy.cos(a)))))

        def inverted(self):
            '\n            Override this method so matplotlib knows how to get the\n            inverse transform for this transform.\n            '
            return MercatorLatitudeScale.InvertedMercatorLatitudeTransform(self.thresh)

    class InvertedMercatorLatitudeTransform(matplotlib.transforms.Transform):
        input_dims = 1
        output_dims = 1
        is_separable = True

        def __init__(self, thresh):
            matplotlib.transforms.Transform.__init__(self)
            self.thresh = thresh

        def transform_non_affine(self, a):
            return numpy.arctan(numpy.sinh(a))

        def inverted(self):
            return MercatorLatitudeScale.MercatorLatitudeTransform(self.thresh)
matplotlib.scale.register_scale(MercatorLatitudeScale)
if (__name__ == '__main__'):
    import matplotlib.pyplot as plt
    tempResult = arange((- 180.0), 180.0, 0.1)
	
===================================================================	
MercatorLatitudeScale.set_default_locators_and_formatters: 35	
----------------------------	

'\n        Override to set up the locators and formatters to use with the\n        scale.  This is only required if the scale requires custom\n        locators and formatters.  Writing custom locators and\n        formatters is rather outside the scope of this example, but\n        there are many helpful examples in ``ticker.py``.\n\n        In our case, the Mercator example uses a fixed locator from\n        -90 to 90 degrees and a custom formatter class to put convert\n        the radians to degrees and put a degree symbol after the\n        value::\n        '

class DegreeFormatter(Formatter):

    def __call__(self, x, pos=None):
        return ('%d°' % numpy.degrees(x))
tempResult = arange((- 90), 90, 10)
	
===================================================================	
module: 20	
----------------------------	

"\n=====================================\nCustom tick formatter for time series\n=====================================\n\nWhen plotting time series, e.g., financial time series, one often wants\nto leave out days on which there is no data, i.e. weekends.  The example\nbelow shows how to use an 'index formatter' to achieve the desired plot\n"
from __future__ import print_function
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import matplotlib.cbook as cbook
import matplotlib.ticker as ticker
datafile = matplotlib.cbook.get_sample_data('aapl.csv', asfileobj=False)
print(('loading %s' % datafile))
r = matplotlib.mlab.csv2rec(datafile)
r.sort()
r = r[(- 30):]
(fig, axes) = matplotlib.pyplot.subplots(ncols=2, figsize=(8, 4))
ax = axes[0]
ax.plot(r.date, r.adj_close, 'o-')
ax.set_title('Default')
fig.autofmt_xdate()
N = len(r)
tempResult = arange(N)
	
===================================================================	
get_image: 10	
----------------------------	

delta = 0.25
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
make_circle: 15	
----------------------------	

tempResult = arange(0, (numpy.pi * 2.0), 0.01)
	
===================================================================	
module: 5	
----------------------------	

'\n===============================\nLegend using pre-defined labels\n===============================\n\nNotice how the legend labels are defined with the plots!\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0, 3, 0.02)
	
===================================================================	
add_polar_bar: 38	
----------------------------	

ax = fig.add_axes([0.025, 0.075, 0.2, 0.85], projection='polar')
ax.axesPatch.set_alpha(axalpha)
ax.set_axisbelow(True)
N = 7
arc = (2.0 * numpy.pi)
tempResult = arange(0.0, arc, (arc / N))
	
===================================================================	
add_polar_bar: 52	
----------------------------	

ax = fig.add_axes([0.025, 0.075, 0.2, 0.85], projection='polar')
ax.axesPatch.set_alpha(axalpha)
ax.set_axisbelow(True)
N = 7
arc = (2.0 * numpy.pi)
theta = numpy.arange(0.0, arc, (arc / N))
radii = (10 * numpy.array([0.2, 0.6, 0.8, 0.7, 0.4, 0.5, 0.8]))
width = ((numpy.pi / 4) * numpy.array([0.4, 0.4, 0.6, 0.8, 0.2, 0.5, 0.3]))
bars = ax.bar(theta, radii, width=width, bottom=0.0)
for (r, bar) in zip(radii, bars):
    bar.set_facecolor(matplotlib.cm.jet((r / 10.0)))
    bar.set_alpha(0.6)
for label in (ax.get_xticklabels() + ax.get_yticklabels()):
    label.set_visible(False)
for line in (ax.get_ygridlines() + ax.get_xgridlines()):
    line.set_lw(0.8)
    line.set_alpha(0.9)
    line.set_ls('-')
    line.set_color('0.5')
tempResult = arange(1, 9, 2)
	
===================================================================	
module: 22	
----------------------------	

"\n===================================\nScatter plot with pie chart markers\n===================================\n\nThis example makes custom 'pie charts' as the markers for a scatter plot.\n\nThanks to Manuel Metz for the example\n"
import math
import numpy as np
import matplotlib.pyplot as plt
r1 = 0.2
r2 = (r1 + 0.4)
sizes = [60, 80, 120]
x = ([0] + np.cos(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
y = ([0] + np.sin(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
xy1 = list(zip(x, y))
s1 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
xy2 = list(zip(x, y))
s2 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
xy3 = list(zip(x, y))
s3 = max(max(x), max(y))
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(3)
	
===================================================================	
module: 22	
----------------------------	

"\n===================================\nScatter plot with pie chart markers\n===================================\n\nThis example makes custom 'pie charts' as the markers for a scatter plot.\n\nThanks to Manuel Metz for the example\n"
import math
import numpy as np
import matplotlib.pyplot as plt
r1 = 0.2
r2 = (r1 + 0.4)
sizes = [60, 80, 120]
x = ([0] + np.cos(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
y = ([0] + np.sin(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
xy1 = list(zip(x, y))
s1 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
xy2 = list(zip(x, y))
s2 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
xy3 = list(zip(x, y))
s3 = max(max(x), max(y))
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(3)
	
===================================================================	
module: 23	
----------------------------	

"\n===================================\nScatter plot with pie chart markers\n===================================\n\nThis example makes custom 'pie charts' as the markers for a scatter plot.\n\nThanks to Manuel Metz for the example\n"
import math
import numpy as np
import matplotlib.pyplot as plt
r1 = 0.2
r2 = (r1 + 0.4)
sizes = [60, 80, 120]
x = ([0] + np.cos(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
y = ([0] + np.sin(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
xy1 = list(zip(x, y))
s1 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
xy2 = list(zip(x, y))
s2 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
xy3 = list(zip(x, y))
s3 = max(max(x), max(y))
(fig, ax) = matplotlib.pyplot.subplots()
ax.scatter(numpy.arange(3), numpy.arange(3), marker=(xy1, 0), s=[((s1 * s1) * _) for _ in sizes], facecolor='blue')
tempResult = arange(3)
	
===================================================================	
module: 23	
----------------------------	

"\n===================================\nScatter plot with pie chart markers\n===================================\n\nThis example makes custom 'pie charts' as the markers for a scatter plot.\n\nThanks to Manuel Metz for the example\n"
import math
import numpy as np
import matplotlib.pyplot as plt
r1 = 0.2
r2 = (r1 + 0.4)
sizes = [60, 80, 120]
x = ([0] + np.cos(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
y = ([0] + np.sin(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
xy1 = list(zip(x, y))
s1 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
xy2 = list(zip(x, y))
s2 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
xy3 = list(zip(x, y))
s3 = max(max(x), max(y))
(fig, ax) = matplotlib.pyplot.subplots()
ax.scatter(numpy.arange(3), numpy.arange(3), marker=(xy1, 0), s=[((s1 * s1) * _) for _ in sizes], facecolor='blue')
tempResult = arange(3)
	
===================================================================	
module: 24	
----------------------------	

"\n===================================\nScatter plot with pie chart markers\n===================================\n\nThis example makes custom 'pie charts' as the markers for a scatter plot.\n\nThanks to Manuel Metz for the example\n"
import math
import numpy as np
import matplotlib.pyplot as plt
r1 = 0.2
r2 = (r1 + 0.4)
sizes = [60, 80, 120]
x = ([0] + np.cos(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
y = ([0] + np.sin(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
xy1 = list(zip(x, y))
s1 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
xy2 = list(zip(x, y))
s2 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
xy3 = list(zip(x, y))
s3 = max(max(x), max(y))
(fig, ax) = matplotlib.pyplot.subplots()
ax.scatter(numpy.arange(3), numpy.arange(3), marker=(xy1, 0), s=[((s1 * s1) * _) for _ in sizes], facecolor='blue')
ax.scatter(numpy.arange(3), numpy.arange(3), marker=(xy2, 0), s=[((s2 * s2) * _) for _ in sizes], facecolor='green')
tempResult = arange(3)
	
===================================================================	
module: 24	
----------------------------	

"\n===================================\nScatter plot with pie chart markers\n===================================\n\nThis example makes custom 'pie charts' as the markers for a scatter plot.\n\nThanks to Manuel Metz for the example\n"
import math
import numpy as np
import matplotlib.pyplot as plt
r1 = 0.2
r2 = (r1 + 0.4)
sizes = [60, 80, 120]
x = ([0] + np.cos(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
y = ([0] + np.sin(np.linspace(0, ((2 * math.pi) * r1), 10)).tolist())
xy1 = list(zip(x, y))
s1 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r1), ((2 * math.pi) * r2), 10)).tolist())
xy2 = list(zip(x, y))
s2 = max(max(x), max(y))
x = ([0] + np.cos(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
y = ([0] + np.sin(np.linspace(((2 * math.pi) * r2), (2 * math.pi), 10)).tolist())
xy3 = list(zip(x, y))
s3 = max(max(x), max(y))
(fig, ax) = matplotlib.pyplot.subplots()
ax.scatter(numpy.arange(3), numpy.arange(3), marker=(xy1, 0), s=[((s1 * s1) * _) for _ in sizes], facecolor='blue')
ax.scatter(numpy.arange(3), numpy.arange(3), marker=(xy2, 0), s=[((s2 * s2) * _) for _ in sizes], facecolor='green')
tempResult = arange(3)
	
===================================================================	
module: 6	
----------------------------	

'\n================\nUsing span_where\n================\n\nIllustrate some helper functions for shading regions where a logical\nmask is True\n\nSee :meth:`matplotlib.collections.BrokenBarHCollection.span_where`\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.collections as collections
tempResult = arange(0.0, 2, 0.01)
	
===================================================================	
module: 6	
----------------------------	

'\n===========================\nPlots with different scales\n===========================\n\nDemonstrate how to do two plots on the same axes with different left and\nright scales.\n\nThe trick is to use *two different axes* that share the same *x* axis.\nYou can use separate `matplotlib.ticker` formatters and locators as\ndesired since the two axes are independent.\n\nSuch axes are generated by calling the `Axes.twinx` method.  Likewise,\n`Axes.twiny` is available to generate axes that share a *y* axis but\nhave different top and bottom scales.\n\nThe twinx and twiny methods are also exposed as pyplot functions.\n\n'
import numpy as np
import matplotlib.pyplot as plt
(fig, ax1) = matplotlib.pyplot.subplots()
tempResult = arange(0.01, 10.0, 0.01)
	
===================================================================	
module: 16	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1.axes_divider import HBoxDivider
import mpl_toolkits.axes_grid1.axes_size as Size

def make_heights_equal(fig, rect, ax1, ax2, pad):
    (h1, v1) = (mpl_toolkits.axes_grid1.axes_size.AxesX(ax1), mpl_toolkits.axes_grid1.axes_size.AxesY(ax1))
    (h2, v2) = (mpl_toolkits.axes_grid1.axes_size.AxesX(ax2), mpl_toolkits.axes_grid1.axes_size.AxesY(ax2))
    pad_v = mpl_toolkits.axes_grid1.axes_size.Scaled(1)
    pad_h = mpl_toolkits.axes_grid1.axes_size.Fixed(pad)
    my_divider = HBoxDivider(fig, rect, horizontal=[h1, pad_h, h2], vertical=[v1, pad_v, v2])
    ax1.set_axes_locator(my_divider.new_locator(0))
    ax2.set_axes_locator(my_divider.new_locator(2))
if (__name__ == '__main__'):
    tempResult = arange(20)
	
===================================================================	
module: 17	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1.axes_divider import HBoxDivider
import mpl_toolkits.axes_grid1.axes_size as Size

def make_heights_equal(fig, rect, ax1, ax2, pad):
    (h1, v1) = (mpl_toolkits.axes_grid1.axes_size.AxesX(ax1), mpl_toolkits.axes_grid1.axes_size.AxesY(ax1))
    (h2, v2) = (mpl_toolkits.axes_grid1.axes_size.AxesX(ax2), mpl_toolkits.axes_grid1.axes_size.AxesY(ax2))
    pad_v = mpl_toolkits.axes_grid1.axes_size.Scaled(1)
    pad_h = mpl_toolkits.axes_grid1.axes_size.Fixed(pad)
    my_divider = HBoxDivider(fig, rect, horizontal=[h1, pad_h, h2], vertical=[v1, pad_v, v2])
    ax1.set_axes_locator(my_divider.new_locator(0))
    ax2.set_axes_locator(my_divider.new_locator(2))
if (__name__ == '__main__'):
    arr1 = np.arange(20).reshape((4, 5))
    tempResult = arange(20)
	
===================================================================	
curvelinear_test1: 25	
----------------------------	

'\n    grid for custom transform.\n    '

def tr(x, y):
    sgn = numpy.sign(x)
    (x, y) = (numpy.abs(numpy.asarray(x)), numpy.asarray(y))
    return ((sgn * (x ** 0.5)), y)

def inv_tr(x, y):
    sgn = numpy.sign(x)
    (x, y) = (numpy.asarray(x), numpy.asarray(y))
    return ((sgn * (x ** 2)), y)
extreme_finder = mpl_toolkits.axes_grid.angle_helper.ExtremeFinderCycle(20, 20, lon_cycle=None, lat_cycle=None, lon_minmax=None, lat_minmax=None)
grid_helper = GridHelperCurveLinear((tr, inv_tr), extreme_finder=extreme_finder)
ax1 = Subplot(fig, 111, grid_helper=grid_helper)
fig.add_subplot(ax1)
tempResult = arange(25)
	
===================================================================	
module: 17	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
x = numpy.random.randn(1000)
y = numpy.random.randn(1000)
(fig, axScatter) = matplotlib.pyplot.subplots(figsize=(5.5, 5.5))
axScatter.scatter(x, y)
axScatter.set_aspect(1.0)
divider = make_axes_locatable(axScatter)
axHistx = divider.append_axes('top', 1.2, pad=0.1, sharex=axScatter)
axHisty = divider.append_axes('right', 1.2, pad=0.1, sharey=axScatter)
matplotlib.pyplot.setp((axHistx.get_xticklabels() + axHisty.get_yticklabels()), visible=False)
binwidth = 0.25
xymax = numpy.max([numpy.max(numpy.fabs(x)), numpy.max(numpy.fabs(y))])
lim = ((int((xymax / binwidth)) + 1) * binwidth)
tempResult = arange((- lim), (lim + binwidth), binwidth)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid
import numpy as np
tempResult = arange(100)
	
===================================================================	
module: 7	
----------------------------	

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import host_subplot
import mpl_toolkits.axisartist as AA
import numpy as np
ax = host_subplot(111, axes_class=mpl_toolkits.axisartist.Axes)
tempResult = arange(0, (2 * numpy.pi), 0.01)
	
===================================================================	
module: 21	
----------------------------	

'\nDisplay the colors from the default prop_cycle.\n'
import numpy as np
import matplotlib.pyplot as plt
prop_cycle = matplotlib.pyplot.rcParams['axes.prop_cycle']
colors = prop_cycle.by_key()['color']
lwbase = matplotlib.pyplot.rcParams['lines.linewidth']
thin = float(('%.1f' % (lwbase / 2)))
thick = (lwbase * 3)
(fig, axs) = matplotlib.pyplot.subplots(nrows=2, ncols=2, sharex=True, sharey=True)
for icol in range(2):
    if (icol == 0):
        (lwx, lwy) = (thin, lwbase)
    else:
        (lwx, lwy) = (lwbase, thick)
    for irow in range(2):
        for (i, color) in enumerate(colors):
            axs[(irow, icol)].axhline(i, color=color, lw=lwx)
            axs[(irow, icol)].axvline(i, color=color, lw=lwy)
    axs[(1, icol)].set_facecolor('k')
    tempResult = arange(0, 10, 2)
	
===================================================================	
module: 24	
----------------------------	

'\nDisplay the colors from the default prop_cycle.\n'
import numpy as np
import matplotlib.pyplot as plt
prop_cycle = matplotlib.pyplot.rcParams['axes.prop_cycle']
colors = prop_cycle.by_key()['color']
lwbase = matplotlib.pyplot.rcParams['lines.linewidth']
thin = float(('%.1f' % (lwbase / 2)))
thick = (lwbase * 3)
(fig, axs) = matplotlib.pyplot.subplots(nrows=2, ncols=2, sharex=True, sharey=True)
for icol in range(2):
    if (icol == 0):
        (lwx, lwy) = (thin, lwbase)
    else:
        (lwx, lwy) = (lwbase, thick)
    for irow in range(2):
        for (i, color) in enumerate(colors):
            axs[(irow, icol)].axhline(i, color=color, lw=lwx)
            axs[(irow, icol)].axvline(i, color=color, lw=lwy)
    axs[(1, icol)].set_facecolor('k')
    axs[(1, icol)].xaxis.set_ticks(numpy.arange(0, 10, 2))
    axs[(0, icol)].set_title(('line widths (pts): %.1f, %.1f' % (lwx, lwy)), fontsize='medium')
for irow in range(2):
    tempResult = arange(0, 10, 2)
	
===================================================================	
module: 7	
----------------------------	

from __future__ import print_function
'\nDemonstrate/test the idle and timeout API\n\nWARNING: idle_event is deprecated. Use the animations module instead.\n\nThis is only tested on gtk so far and is a prototype implementation\n'
import numpy as np
import matplotlib.pyplot as plt
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

'\nEnable picking on the legend to toggle the original line on and off\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.0, 0.2, 0.1)
	
===================================================================	
Game.__init__: 99	
----------------------------	

self.ax = ax
padAx = padBx = 0.5
padAy = padBy = 0.3
padBx += 6.3
(pA,) = self.ax.barh(padAy, 0.2, height=0.3, color='k', alpha=0.5, edgecolor='b', lw=2, label='Player B', animated=True)
(pB,) = self.ax.barh(padBy, 0.2, height=0.3, left=padBx, color='k', alpha=0.5, edgecolor='r', lw=2, label='Player A', animated=True)
tempResult = arange(0, (2.22 * numpy.pi), 0.01)
	
===================================================================	
module: 124	
----------------------------	

'\nThis is an example to show how to build cross-GUI applications using\nmatplotlib event handling to interact with objects on the canvas\n\n'
import numpy as np
from matplotlib.lines import Line2D
from matplotlib.artist import Artist
from matplotlib.mlab import dist_point_to_segment

class PolygonInteractor(object):
    "\n    An polygon editor.\n\n    Key-bindings\n\n      't' toggle vertex markers on and off.  When vertex markers are on,\n          you can move them, delete them\n\n      'd' delete the vertex under point\n\n      'i' insert a vertex at point.  You must be within epsilon of the\n          line connecting two existing vertices\n\n    "
    showverts = True
    epsilon = 5

    def __init__(self, ax, poly):
        if (poly.figure is None):
            raise RuntimeError('You must first add the polygon to a figure or canvas before defining the interactor')
        self.ax = ax
        canvas = poly.figure.canvas
        self.poly = poly
        (x, y) = zip(*self.poly.xy)
        self.line = Line2D(x, y, marker='o', markerfacecolor='r', animated=True)
        self.ax.add_line(self.line)
        cid = self.poly.add_callback(self.poly_changed)
        self._ind = None
        canvas.mpl_connect('draw_event', self.draw_callback)
        canvas.mpl_connect('button_press_event', self.button_press_callback)
        canvas.mpl_connect('key_press_event', self.key_press_callback)
        canvas.mpl_connect('button_release_event', self.button_release_callback)
        canvas.mpl_connect('motion_notify_event', self.motion_notify_callback)
        self.canvas = canvas

    def draw_callback(self, event):
        self.background = self.canvas.copy_from_bbox(self.ax.bbox)
        self.ax.draw_artist(self.poly)
        self.ax.draw_artist(self.line)
        self.canvas.blit(self.ax.bbox)

    def poly_changed(self, poly):
        'this method is called whenever the polygon object is called'
        vis = self.line.get_visible()
        matplotlib.artist.Artist.update_from(self.line, poly)
        self.line.set_visible(vis)

    def get_ind_under_point(self, event):
        'get the index of the vertex under point if within epsilon tolerance'
        xy = numpy.asarray(self.poly.xy)
        xyt = self.poly.get_transform().transform(xy)
        (xt, yt) = (xyt[:, 0], xyt[:, 1])
        d = numpy.sqrt((((xt - event.x) ** 2) + ((yt - event.y) ** 2)))
        indseq = numpy.nonzero(numpy.equal(d, numpy.amin(d)))[0]
        ind = indseq[0]
        if (d[ind] >= self.epsilon):
            ind = None
        return ind

    def button_press_callback(self, event):
        'whenever a mouse button is pressed'
        if (not self.showverts):
            return
        if (event.inaxes is None):
            return
        if (event.button != 1):
            return
        self._ind = self.get_ind_under_point(event)

    def button_release_callback(self, event):
        'whenever a mouse button is released'
        if (not self.showverts):
            return
        if (event.button != 1):
            return
        self._ind = None

    def key_press_callback(self, event):
        'whenever a key is pressed'
        if (not event.inaxes):
            return
        if (event.key == 't'):
            self.showverts = (not self.showverts)
            self.line.set_visible(self.showverts)
            if (not self.showverts):
                self._ind = None
        elif (event.key == 'd'):
            ind = self.get_ind_under_point(event)
            if (ind is not None):
                self.poly.xy = [tup for (i, tup) in enumerate(self.poly.xy) if (i != ind)]
                self.line.set_data(zip(*self.poly.xy))
        elif (event.key == 'i'):
            xys = self.poly.get_transform().transform(self.poly.xy)
            p = (event.x, event.y)
            for i in range((len(xys) - 1)):
                s0 = xys[i]
                s1 = xys[(i + 1)]
                d = dist_point_to_segment(p, s0, s1)
                if (d <= self.epsilon):
                    self.poly.xy = numpy.array(((list(self.poly.xy[:i]) + [(event.xdata, event.ydata)]) + list(self.poly.xy[i:])))
                    self.line.set_data(zip(*self.poly.xy))
                    break
        self.canvas.draw()

    def motion_notify_callback(self, event):
        'on mouse movement'
        if (not self.showverts):
            return
        if (self._ind is None):
            return
        if (event.inaxes is None):
            return
        if (event.button != 1):
            return
        (x, y) = (event.xdata, event.ydata)
        self.poly.xy[self._ind] = (x, y)
        if (self._ind == 0):
            self.poly.xy[(- 1)] = (x, y)
        elif (self._ind == (len(self.poly.xy) - 1)):
            self.poly.xy[0] = (x, y)
        self.line.set_data(zip(*self.poly.xy))
        self.canvas.restore_region(self.background)
        self.ax.draw_artist(self.poly)
        self.ax.draw_artist(self.line)
        self.canvas.blit(self.ax.bbox)
if (__name__ == '__main__'):
    import matplotlib.pyplot as plt
    from matplotlib.patches import Polygon
    tempResult = arange(0, (2 * numpy.pi), 0.1)
	
===================================================================	
module: 8	
----------------------------	

'\n=========================\nFrontpage contour example\n=========================\n\nThis example reproduces the frontpage contour example.\n'
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import mlab, cm
extent = ((- 3), 3, (- 3), 3)
delta = 0.5
tempResult = arange((- 3.0), 4.001, delta)
	
===================================================================	
module: 9	
----------------------------	

'\n=========================\nFrontpage contour example\n=========================\n\nThis example reproduces the frontpage contour example.\n'
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import mlab, cm
extent = ((- 3), 3, (- 3), 3)
delta = 0.5
x = numpy.arange((- 3.0), 4.001, delta)
tempResult = arange((- 4.0), 3.001, delta)
	
===================================================================	
module: 10	
----------------------------	

'\nSimple demo of a horizontal bar chart.\n'
import matplotlib.pyplot as plt
matplotlib.pyplot.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
matplotlib.pyplot.rcdefaults()
(fig, ax) = matplotlib.pyplot.subplots()
people = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')
tempResult = arange(len(people))
	
===================================================================	
module: 14	
----------------------------	

'\nDifferent linestyles copying those of Tikz/PGF\n'
import numpy as np
import matplotlib.pyplot as plt
from collections import OrderedDict
from matplotlib.transforms import blended_transform_factory
linestyles = OrderedDict([('solid', (0, ())), ('loosely dotted', (0, (1, 10))), ('dotted', (0, (1, 5))), ('densely dotted', (0, (1, 1))), ('loosely dashed', (0, (5, 10))), ('dashed', (0, (5, 5))), ('densely dashed', (0, (5, 1))), ('loosely dashdotted', (0, (3, 10, 1, 10))), ('dashdotted', (0, (3, 5, 1, 5))), ('densely dashdotted', (0, (3, 1, 1, 1))), ('loosely dashdotdotted', (0, (3, 10, 1, 10, 1, 10))), ('dashdotdotted', (0, (3, 5, 1, 5, 1, 5))), ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)))])
matplotlib.pyplot.figure(figsize=(10, 6))
ax = matplotlib.pyplot.subplot(1, 1, 1)
(X, Y) = (numpy.linspace(0, 100, 10), numpy.zeros(10))
for (i, (name, linestyle)) in enumerate(linestyles.items()):
    ax.plot(X, (Y + i), linestyle=linestyle, linewidth=1.5, color='black')
ax.set_ylim((- 0.5), (len(linestyles) - 0.5))
tempResult = arange(len(linestyles))
	
===================================================================	
module: 4	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(100)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
d = np.arange(100).reshape(10, 10)
tempResult = arange(11)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
d = np.arange(100).reshape(10, 10)
tempResult = arange(11)
	
===================================================================	
module: 15	
----------------------------	

from __future__ import print_function
import numpy as np
import matplotlib.mlab as mlab
import matplotlib.cbook as cbook
datafile = matplotlib.cbook.get_sample_data('aapl.csv', asfileobj=False)
print('loading', datafile)
r = matplotlib.mlab.csv2rec(datafile)
r.sort()
r1 = r[(- 10):]
r2 = numpy.empty(12, dtype=[('date', '|O4'), ('high', numpy.float), ('marker', numpy.float)])
r2 = r2.view(numpy.recarray)
r2.date = r.date[(- 17):(- 5)]
r2.high = r.high[(- 17):(- 5)]
tempResult = arange(12)
	
===================================================================	
module: 8	
----------------------------	

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
for (c, z) in zip(['r', 'g', 'b', 'y'], [30, 20, 10, 0]):
    tempResult = arange(20)
	
===================================================================	
module: 11	
----------------------------	

'\nDemonstrate the mixing of 2d and 3d subplots\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
tempResult = arange(0.0, 5.0, 0.1)
	
===================================================================	
module: 12	
----------------------------	

'\nDemonstrate the mixing of 2d and 3d subplots\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
tempResult = arange(0.0, 5.0, 0.02)
	
===================================================================	
module: 13	
----------------------------	

'\nDemonstrate the mixing of 2d and 3d subplots\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
t2 = numpy.arange(0.0, 5.0, 0.02)
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 21	
----------------------------	

'\nDemonstrate the mixing of 2d and 3d subplots\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
t2 = numpy.arange(0.0, 5.0, 0.02)
t3 = numpy.arange(0.0, 2.0, 0.01)
fig = matplotlib.pyplot.figure(figsize=matplotlib.pyplot.figaspect(2.0))
fig.suptitle('A tale of 2 subplots')
ax = fig.add_subplot(2, 1, 1)
l = ax.plot(t1, f(t1), 'bo', t2, f(t2), 'k--', markerfacecolor='green')
ax.grid(True)
ax.set_ylabel('Damped oscillation')
ax = fig.add_subplot(2, 1, 2, projection='3d')
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 23	
----------------------------	

'\nDemonstrate the mixing of 2d and 3d subplots\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
t2 = numpy.arange(0.0, 5.0, 0.02)
t3 = numpy.arange(0.0, 2.0, 0.01)
fig = matplotlib.pyplot.figure(figsize=matplotlib.pyplot.figaspect(2.0))
fig.suptitle('A tale of 2 subplots')
ax = fig.add_subplot(2, 1, 1)
l = ax.plot(t1, f(t1), 'bo', t2, f(t2), 'k--', markerfacecolor='green')
ax.grid(True)
ax.set_ylabel('Damped oscillation')
ax = fig.add_subplot(2, 1, 2, projection='3d')
X = numpy.arange((- 5), 5, 0.25)
xlen = len(X)
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 12	
----------------------------	

from mpl_toolkits.mplot3d import Axes3D
from matplotlib.collections import PolyCollection
import matplotlib.pyplot as plt
from matplotlib import colors as mcolors
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')

def cc(arg):
    return matplotlib.colors.to_rgba(arg, alpha=0.6)
tempResult = arange(0, 10, 0.4)
	
===================================================================	
module: 8	
----------------------------	

'\n==============\n3D quiver plot\n==============\n\nDemonstrates plotting directional arrows at points on a 3d meshgrid.\n'
from mpl_toolkits.mplot3d import axes3d
import matplotlib.pyplot as plt
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 0.8), 1, 0.2)
	
===================================================================	
module: 8	
----------------------------	

'\n==============\n3D quiver plot\n==============\n\nDemonstrates plotting directional arrows at points on a 3d meshgrid.\n'
from mpl_toolkits.mplot3d import axes3d
import matplotlib.pyplot as plt
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 0.8), 1, 0.2)
	
===================================================================	
module: 8	
----------------------------	

'\n==============\n3D quiver plot\n==============\n\nDemonstrates plotting directional arrows at points on a 3d meshgrid.\n'
from mpl_toolkits.mplot3d import axes3d
import matplotlib.pyplot as plt
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 0.8), 1, 0.8)
	
===================================================================	
module: 9	
----------------------------	

'\n====================\n3D plots as subplots\n====================\n\nDemonstrate including 3D plots as subplots.\n'
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.axes3d import Axes3D, get_test_data
from matplotlib import cm
import numpy as np
fig = matplotlib.pyplot.figure(figsize=matplotlib.pyplot.figaspect(0.5))
ax = fig.add_subplot(1, 2, 1, projection='3d')
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 10	
----------------------------	

'\n====================\n3D plots as subplots\n====================\n\nDemonstrate including 3D plots as subplots.\n'
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d.axes3d import Axes3D, get_test_data
from matplotlib import cm
import numpy as np
fig = matplotlib.pyplot.figure(figsize=matplotlib.pyplot.figaspect(0.5))
ax = fig.add_subplot(1, 2, 1, projection='3d')
X = numpy.arange((- 5), 5, 0.25)
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 10	
----------------------------	

'\n======================\n3D surface (color map)\n======================\n\nDemonstrates plotting a 3D surface colored with the coolwarm color map.\nThe surface is made opaque by using antialiased=False.\n\nAlso demonstrates using the LinearLocator and custom formatting for the\nz axis tick labels.\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 11	
----------------------------	

'\n======================\n3D surface (color map)\n======================\n\nDemonstrates plotting a 3D surface colored with the coolwarm color map.\nThe surface is made opaque by using antialiased=False.\n\nAlso demonstrates using the LinearLocator and custom formatting for the\nz axis tick labels.\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
X = numpy.arange((- 5), 5, 0.25)
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 10	
----------------------------	

'\n=========================\n3D surface (checkerboard)\n=========================\n\nDemonstrates plotting a 3D surface colored in a checkerboard pattern.\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 12	
----------------------------	

'\n=========================\n3D surface (checkerboard)\n=========================\n\nDemonstrates plotting a 3D surface colored in a checkerboard pattern.\n'
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
X = numpy.arange((- 5), 5, 0.25)
xlen = len(X)
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 5	
----------------------------	

'\nPyplot animation example.\n\nThe method shown here is only for very simple, low-performance\nuse.  For more demanding applications, look at the animation\nmodule and the examples that use it.\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(6)
	
===================================================================	
module: 6	
----------------------------	

'\nPyplot animation example.\n\nThe method shown here is only for very simple, low-performance\nuse.  For more demanding applications, look at the animation\nmodule and the examples that use it.\n'
import matplotlib.pyplot as plt
import numpy as np
x = numpy.arange(6)
tempResult = arange(5)
	
===================================================================	
module: 8	
----------------------------	

"\nSome examples of how to annotate points in figures.  You specify an\nannotation point xy=(x,y) and a text point xytext=(x,y) for the\nannotated points and text location, respectively.  Optionally, you can\nspecify the coordinate system of xy and xytext with one of the\nfollowing strings for xycoords and textcoords (default is 'data')\n\n\n  'figure points'   : points from the lower left corner of the figure\n  'figure pixels'   : pixels from the lower left corner of the figure\n  'figure fraction' : 0,0 is lower left of figure and 1,1 is upper, right\n  'axes points'     : points from lower left corner of axes\n  'axes pixels'     : pixels from lower left corner of axes\n  'axes fraction'   : 0,0 is lower left of axes and 1,1 is upper right\n  'offset points'   : Specify an offset (in points) from the xy value\n  'offset pixels'   : Specify an offset (in pixels) from the xy value\n  'data'            : use the axes data coordinate system\n\nOptionally, you can specify arrow properties which draws and arrow\nfrom the text to the annotated point by giving a dictionary of arrow\nproperties\n\nValid keys are\n\n          width : the width of the arrow in points\n          frac  : the fraction of the arrow length occupied by the head\n          headwidth : the width of the base of the arrow head in points\n          shrink : move the tip and base some percent away from the\n                   annotated point and text\n          any key for matplotlib.patches.polygon  (e.g., facecolor)\n\nFor physical coordinate systems (points or pixels) the origin is the\n(bottom, left) of the figure or axes.\n"
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, autoscale_on=False, xlim=((- 1), 5), ylim=((- 3), 5))
tempResult = arange(0.0, 5.0, 0.01)
	
===================================================================	
module: 19	
----------------------------	

"\nSome examples of how to annotate points in figures.  You specify an\nannotation point xy=(x,y) and a text point xytext=(x,y) for the\nannotated points and text location, respectively.  Optionally, you can\nspecify the coordinate system of xy and xytext with one of the\nfollowing strings for xycoords and textcoords (default is 'data')\n\n\n  'figure points'   : points from the lower left corner of the figure\n  'figure pixels'   : pixels from the lower left corner of the figure\n  'figure fraction' : 0,0 is lower left of figure and 1,1 is upper, right\n  'axes points'     : points from lower left corner of axes\n  'axes pixels'     : pixels from lower left corner of axes\n  'axes fraction'   : 0,0 is lower left of axes and 1,1 is upper right\n  'offset points'   : Specify an offset (in points) from the xy value\n  'offset pixels'   : Specify an offset (in pixels) from the xy value\n  'data'            : use the axes data coordinate system\n\nOptionally, you can specify arrow properties which draws and arrow\nfrom the text to the annotated point by giving a dictionary of arrow\nproperties\n\nValid keys are\n\n          width : the width of the arrow in points\n          frac  : the fraction of the arrow length occupied by the head\n          headwidth : the width of the base of the arrow head in points\n          shrink : move the tip and base some percent away from the\n                   annotated point and text\n          any key for matplotlib.patches.polygon  (e.g., facecolor)\n\nFor physical coordinate systems (points or pixels) the origin is the\n(bottom, left) of the figure or axes.\n"
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, autoscale_on=False, xlim=((- 1), 5), ylim=((- 3), 5))
t = numpy.arange(0.0, 5.0, 0.01)
s = numpy.cos(((2 * numpy.pi) * t))
(line,) = ax.plot(t, s)
ax.annotate('figure pixels', xy=(10, 10), xycoords='figure pixels')
ax.annotate('figure points', xy=(80, 80), xycoords='figure points')
ax.annotate('point offset from data', xy=(2, 1), xycoords='data', xytext=((- 15), 25), textcoords='offset points', arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='right', verticalalignment='bottom')
ax.annotate('axes fraction', xy=(3, 1), xycoords='data', xytext=(0.8, 0.95), textcoords='axes fraction', arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='right', verticalalignment='top')
ax.annotate('figure fraction', xy=(0.025, 0.975), xycoords='figure fraction', horizontalalignment='left', verticalalignment='top', fontsize=20)
ax.annotate('pixel offset from axes fraction', xy=(1, 0), xycoords='axes fraction', xytext=((- 20), 20), textcoords='offset pixels', horizontalalignment='right', verticalalignment='bottom')
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='polar')
tempResult = arange(0, 1, 0.001)
	
===================================================================	
module: 7	
----------------------------	

import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
import numpy as np
fig = matplotlib.pyplot.figure(1, figsize=(8, 5))
ax = fig.add_subplot(111, autoscale_on=False, xlim=((- 1), 5), ylim=((- 4), 3))
tempResult = arange(0.0, 5.0, 0.01)
	
===================================================================	
module: 10	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np

def f(t):
    'a damped exponential'
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return (s1 * e1)
tempResult = arange(0.0, 5.0, 0.2)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.001
tempResult = arange(0.0, 10.0, dt)
	
===================================================================	
module: 5	
----------------------------	

'\nYou can control the axis tick and grid properties\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 4	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
tempResult = arange((- 1), 2, 0.01)
	
===================================================================	
module: 11	
----------------------------	

'\nBar chart demo with pairs of bars grouped for easy comparison.\n'
import numpy as np
import matplotlib.pyplot as plt
n_groups = 5
means_men = (20, 35, 30, 35, 27)
std_men = (2, 3, 4, 1, 2)
means_women = (25, 32, 34, 20, 25)
std_women = (3, 5, 2, 3, 3)
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(n_groups)
	
===================================================================	
plot_student_results: 39	
----------------------------	

(fig, ax1) = matplotlib.pyplot.subplots(figsize=(9, 7))
fig.subplots_adjust(left=0.115, right=0.88)
fig.canvas.set_window_title('Eldorado K-8 Fitness Chart')
tempResult = arange(len(testNames))
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
N = 5
menMeans = (20, 35, 30, 35, 27)
womenMeans = (25, 32, 34, 20, 25)
menStd = (2, 3, 4, 1, 2)
womenStd = (3, 5, 2, 3, 3)
tempResult = arange(N)
	
===================================================================	
module: 16	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
N = 5
menMeans = (20, 35, 30, 35, 27)
womenMeans = (25, 32, 34, 20, 25)
menStd = (2, 3, 4, 1, 2)
womenStd = (3, 5, 2, 3, 3)
ind = numpy.arange(N)
width = 0.35
p1 = matplotlib.pyplot.bar(ind, menMeans, width, color='#d62728', yerr=menStd)
p2 = matplotlib.pyplot.bar(ind, womenMeans, width, bottom=menMeans, yerr=womenStd)
matplotlib.pyplot.ylabel('Scores')
matplotlib.pyplot.title('Scores by group and gender')
matplotlib.pyplot.xticks(ind, ('G1', 'G2', 'G3', 'G4', 'G5'))
tempResult = arange(0, 81, 10)
	
===================================================================	
module: 63	
----------------------------	

'\nThanks Josh Hemann for the example\n'
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon
numDists = 5
randomDists = ['Normal(1,1)', ' Lognormal(1,1)', 'Exp(1)', 'Gumbel(6,4)', 'Triangular(2,9,11)']
N = 500
numpy.random.seed(0)
norm = numpy.random.normal(1, 1, N)
logn = numpy.random.lognormal(1, 1, N)
expo = numpy.random.exponential(1, N)
gumb = numpy.random.gumbel(6, 4, N)
tria = numpy.random.triangular(2, 9, 11, N)
bootstrapIndices = numpy.random.random_integers(0, (N - 1), N)
normBoot = norm[bootstrapIndices]
expoBoot = expo[bootstrapIndices]
gumbBoot = gumb[bootstrapIndices]
lognBoot = logn[bootstrapIndices]
triaBoot = tria[bootstrapIndices]
data = [norm, normBoot, logn, lognBoot, expo, expoBoot, gumb, gumbBoot, tria, triaBoot]
(fig, ax1) = matplotlib.pyplot.subplots(figsize=(10, 6))
fig.canvas.set_window_title('A Boxplot Example')
matplotlib.pyplot.subplots_adjust(left=0.075, right=0.95, top=0.9, bottom=0.25)
bp = matplotlib.pyplot.boxplot(data, notch=0, sym='+', vert=1, whis=1.5)
matplotlib.pyplot.setp(bp['boxes'], color='black')
matplotlib.pyplot.setp(bp['whiskers'], color='black')
matplotlib.pyplot.setp(bp['fliers'], color='red', marker='+')
ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey', alpha=0.5)
ax1.set_axisbelow(True)
ax1.set_title('Comparison of IID Bootstrap Resampling Across Five Distributions')
ax1.set_xlabel('Distribution')
ax1.set_ylabel('Value')
boxColors = ['darkkhaki', 'royalblue']
numBoxes = (numDists * 2)
medians = list(range(numBoxes))
for i in range(numBoxes):
    box = bp['boxes'][i]
    boxX = []
    boxY = []
    for j in range(5):
        boxX.append(box.get_xdata()[j])
        boxY.append(box.get_ydata()[j])
    boxCoords = list(zip(boxX, boxY))
    k = (i % 2)
    boxPolygon = Polygon(boxCoords, facecolor=boxColors[k])
    ax1.add_patch(boxPolygon)
    med = bp['medians'][i]
    medianX = []
    medianY = []
    for j in range(2):
        medianX.append(med.get_xdata()[j])
        medianY.append(med.get_ydata()[j])
        matplotlib.pyplot.plot(medianX, medianY, 'k')
        medians[i] = medianY[0]
    matplotlib.pyplot.plot([numpy.average(med.get_xdata())], [numpy.average(data[i])], color='w', marker='*', markeredgecolor='k')
ax1.set_xlim(0.5, (numBoxes + 0.5))
top = 40
bottom = (- 5)
ax1.set_ylim(bottom, top)
xtickNames = matplotlib.pyplot.setp(ax1, xticklabels=numpy.repeat(randomDists, 2))
matplotlib.pyplot.setp(xtickNames, rotation=45, fontsize=8)
tempResult = arange(numBoxes)
	
===================================================================	
module: 7	
----------------------------	

'\nCompute the coherence of two signals\n'
import numpy as np
import matplotlib.pyplot as plt
matplotlib.pyplot.subplots_adjust(wspace=0.5)
dt = 0.01
tempResult = arange(0, 30, dt)
	
===================================================================	
module: 4	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 6	
----------------------------	

'\nmatplotlib gives you 5 ways to specify colors,\n\n    1) as a single letter string, ala MATLAB\n\n    2) as an html style hex string or html color name\n\n    3) as an R,G,B tuple, where R,G,B, range from 0-1\n\n    4) as a string representing a floating point number\n       from 0 to 1, corresponding to shades of gray.\n\n    5) as a special color "Cn", where n is a number 0-9 specifying the\n       nth color in the currently active color cycle.\n\nSee help(colors) for more info.\n'
import matplotlib.pyplot as plt
import numpy as np
matplotlib.pyplot.subplot(111, facecolor='darkslategray')
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 6	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
origin = 'lower'
delta = 0.025
tempResult = arange((- 3.0), 3.01, delta)
	
===================================================================	
module: 5	
----------------------------	

'\nIllustrate the difference between corner_mask=False and corner_mask=True\nfor masked contour plots.\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(7)
	
===================================================================	
module: 5	
----------------------------	

'\nIllustrate the difference between corner_mask=False and corner_mask=True\nfor masked contour plots.\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(10)
	
===================================================================	
module: 11	
----------------------------	

'\nIllustrate simple contour plotting, contours on an image with\na colorbar for the contours, and labelled contours.\n\nSee also contour_image.py.\n'
import matplotlib
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'
delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
module: 12	
----------------------------	

'\nIllustrate simple contour plotting, contours on an image with\na colorbar for the contours, and labelled contours.\n\nSee also contour_image.py.\n'
import matplotlib
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'
delta = 0.025
x = numpy.arange((- 3.0), 3.0, delta)
tempResult = arange((- 2.0), 2.0, delta)
	
===================================================================	
module: 36	
----------------------------	

'\nIllustrate simple contour plotting, contours on an image with\na colorbar for the contours, and labelled contours.\n\nSee also contour_image.py.\n'
import matplotlib
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'
delta = 0.025
x = numpy.arange((- 3.0), 3.0, delta)
y = numpy.arange((- 2.0), 2.0, delta)
(X, Y) = numpy.meshgrid(x, y)
Z1 = matplotlib.mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)
Z2 = matplotlib.mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)
Z = (10.0 * (Z2 - Z1))
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z)
matplotlib.pyplot.clabel(CS, inline=1, fontsize=10)
matplotlib.pyplot.title('Simplest default with labels')
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z)
manual_locations = [((- 1), (- 1.4)), ((- 0.62), (- 0.7)), ((- 2), 0.5), (1.7, 1.2), (2.0, 1.4), (2.4, 1.7)]
matplotlib.pyplot.clabel(CS, inline=1, fontsize=10, manual=manual_locations)
matplotlib.pyplot.title('labels at selected locations')
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z, 6, colors='k')
matplotlib.pyplot.clabel(CS, fontsize=9, inline=1)
matplotlib.pyplot.title('Single color - negative contours dashed')
matplotlib.rcParams['contour.negative_linestyle'] = 'solid'
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z, 6, colors='k')
matplotlib.pyplot.clabel(CS, fontsize=9, inline=1)
matplotlib.pyplot.title('Single color - negative contours solid')
matplotlib.pyplot.figure()
tempResult = arange(0.5, 4, 0.5)
	
===================================================================	
module: 41	
----------------------------	

'\nIllustrate simple contour plotting, contours on an image with\na colorbar for the contours, and labelled contours.\n\nSee also contour_image.py.\n'
import matplotlib
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'
delta = 0.025
x = numpy.arange((- 3.0), 3.0, delta)
y = numpy.arange((- 2.0), 2.0, delta)
(X, Y) = numpy.meshgrid(x, y)
Z1 = matplotlib.mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)
Z2 = matplotlib.mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)
Z = (10.0 * (Z2 - Z1))
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z)
matplotlib.pyplot.clabel(CS, inline=1, fontsize=10)
matplotlib.pyplot.title('Simplest default with labels')
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z)
manual_locations = [((- 1), (- 1.4)), ((- 0.62), (- 0.7)), ((- 2), 0.5), (1.7, 1.2), (2.0, 1.4), (2.4, 1.7)]
matplotlib.pyplot.clabel(CS, inline=1, fontsize=10, manual=manual_locations)
matplotlib.pyplot.title('labels at selected locations')
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z, 6, colors='k')
matplotlib.pyplot.clabel(CS, fontsize=9, inline=1)
matplotlib.pyplot.title('Single color - negative contours dashed')
matplotlib.rcParams['contour.negative_linestyle'] = 'solid'
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z, 6, colors='k')
matplotlib.pyplot.clabel(CS, fontsize=9, inline=1)
matplotlib.pyplot.title('Single color - negative contours solid')
matplotlib.pyplot.figure()
CS = matplotlib.pyplot.contour(X, Y, Z, 6, linewidths=numpy.arange(0.5, 4, 0.5), colors=('r', 'green', 'blue', (1, 1, 0), '#afeeee', '0.5'))
matplotlib.pyplot.clabel(CS, fontsize=9, inline=1)
matplotlib.pyplot.title('Crazy lines')
matplotlib.pyplot.figure()
im = matplotlib.pyplot.imshow(Z, interpolation='bilinear', origin='lower', cmap=matplotlib.cm.gray, extent=((- 3), 3, (- 2), 2))
tempResult = arange((- 1.2), 1.6, 0.2)
	
===================================================================	
module: 8	
----------------------------	

'\nTest combinations of contouring, filled contouring, and image plotting.\nFor contour labelling, see contour_demo.py.\n\nThe emphasis in this demo is on showing how to make contours register\ncorrectly on images, and on how to get both of them oriented as\ndesired.  In particular, note the usage of the "origin" and "extent"\nkeyword arguments to imshow and contour.\n'
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import mlab, cm
delta = 0.5
extent = ((- 3), 4, (- 4), 3)
tempResult = arange((- 3.0), 4.001, delta)
	
===================================================================	
module: 9	
----------------------------	

'\nTest combinations of contouring, filled contouring, and image plotting.\nFor contour labelling, see contour_demo.py.\n\nThe emphasis in this demo is on showing how to make contours register\ncorrectly on images, and on how to get both of them oriented as\ndesired.  In particular, note the usage of the "origin" and "extent"\nkeyword arguments to imshow and contour.\n'
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import mlab, cm
delta = 0.5
extent = ((- 3), 4, (- 4), 3)
x = numpy.arange((- 3.0), 4.001, delta)
tempResult = arange((- 4.0), 3.001, delta)
	
===================================================================	
module: 14	
----------------------------	

'\nTest combinations of contouring, filled contouring, and image plotting.\nFor contour labelling, see contour_demo.py.\n\nThe emphasis in this demo is on showing how to make contours register\ncorrectly on images, and on how to get both of them oriented as\ndesired.  In particular, note the usage of the "origin" and "extent"\nkeyword arguments to imshow and contour.\n'
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import mlab, cm
delta = 0.5
extent = ((- 3), 4, (- 4), 3)
x = numpy.arange((- 3.0), 4.001, delta)
y = numpy.arange((- 4.0), 3.001, delta)
(X, Y) = numpy.meshgrid(x, y)
Z1 = matplotlib.mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)
Z2 = matplotlib.mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)
Z = ((Z1 - Z2) * 10)
tempResult = arange((- 2.0), 1.601, 0.4)
	
===================================================================	
module: 12	
----------------------------	

'\nIllustrate some of the more advanced things that one can do with\ncontour labels.\n\nSee also contour_demo.py.\n'
import matplotlib
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'
delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
module: 13	
----------------------------	

'\nIllustrate some of the more advanced things that one can do with\ncontour labels.\n\nSee also contour_demo.py.\n'
import matplotlib
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.ticker as ticker
import matplotlib.pyplot as plt
matplotlib.rcParams['xtick.direction'] = 'out'
matplotlib.rcParams['ytick.direction'] = 'out'
delta = 0.025
x = numpy.arange((- 3.0), 3.0, delta)
tempResult = arange((- 2.0), 2.0, delta)
	
===================================================================	
module: 7	
----------------------------	

'\nAn example of how to interact with the plotting canvas by connecting\nto move and click events\n'
from __future__ import print_function
import sys
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 1.0, 0.01)
	
===================================================================	
module: 8	
----------------------------	

'\nCompute the cross spectral density of two signals\n'
import numpy as np
import matplotlib.pyplot as plt
(fig, (ax1, ax2)) = matplotlib.pyplot.subplots(2, 1)
fig.subplots_adjust(hspace=0.5)
dt = 0.01
tempResult = arange(0, 30, dt)
	
===================================================================	
module: 47	
----------------------------	

'\nThis example shows how to use matplotlib to provide a data cursor.  It\nuses matplotlib to draw the cursor and may be a slow since this\nrequires redrawing the figure with every mouse move.\n\nFaster cursoring is possible using native GUI drawing, as in\nwxcursor_demo.py.\n\nThe mpldatacursor and mplcursors third-party packages can be used to achieve a\nsimilar effect.  See\n    https://github.com/joferkington/mpldatacursor\n    https://github.com/anntzer/mplcursors\n'
from __future__ import print_function
import matplotlib.pyplot as plt
import numpy as np

class Cursor(object):

    def __init__(self, ax):
        self.ax = ax
        self.lx = ax.axhline(color='k')
        self.ly = ax.axvline(color='k')
        self.txt = ax.text(0.7, 0.9, '', transform=ax.transAxes)

    def mouse_move(self, event):
        if (not event.inaxes):
            return
        (x, y) = (event.xdata, event.ydata)
        self.lx.set_ydata(y)
        self.ly.set_xdata(x)
        self.txt.set_text(('x=%1.2f, y=%1.2f' % (x, y)))
        matplotlib.pyplot.draw()

class SnaptoCursor(object):
    "\n    Like Cursor but the crosshair snaps to the nearest x,y point\n    For simplicity, I'm assuming x is sorted\n    "

    def __init__(self, ax, x, y):
        self.ax = ax
        self.lx = ax.axhline(color='k')
        self.ly = ax.axvline(color='k')
        self.x = x
        self.y = y
        self.txt = ax.text(0.7, 0.9, '', transform=ax.transAxes)

    def mouse_move(self, event):
        if (not event.inaxes):
            return
        (x, y) = (event.xdata, event.ydata)
        indx = numpy.searchsorted(self.x, [x])[0]
        x = self.x[indx]
        y = self.y[indx]
        self.lx.set_ydata(y)
        self.ly.set_xdata(x)
        self.txt.set_text(('x=%1.2f, y=%1.2f' % (x, y)))
        print(('x=%1.2f, y=%1.2f' % (x, y)))
        matplotlib.pyplot.draw()
tempResult = arange(0.0, 1.0, 0.01)
	
===================================================================	
module: 6	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
'\nCreating a colormap from a list of colors\n-----------------------------------------\nCreating a colormap from a list of colors can be done with the `from_list`\nmethod of `LinearSegmentedColormap`. You must pass a list of RGB tuples that\ndefine the mixture of colors from 0 to 1.\n\n\nCreating custom colormaps\n-------------------------\nIt is also possible to create a custom mapping for a colormap. This is\naccomplished by creating dictionary that specifies how the RGB channels\nchange from one end of the cmap to the other.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom\nhalf, green to do the same over the middle half, and blue over the top\nhalf.  Then you would use:\n\ncdict = {\'red\':   ((0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)),\n\n         \'green\': ((0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)),\n\n         \'blue\':  ((0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0))}\n\nIf, as in this example, there are no discontinuities in the r, g, and b\ncomponents, then it is quite simple: the second and third element of\neach tuple, above, is the same--call it "y".  The first element ("x")\ndefines interpolation intervals over the full range of 0 to 1, and it\nmust span that whole range.  In other words, the values of x divide the\n0-to-1 range into a set of segments, and y gives the end-point color\nvalues for each segment.\n\nNow consider the green. cdict[\'green\'] is saying that for\n0 <= x <= 0.25, y is zero; no green.\n0.25 < x <= 0.75, y varies linearly from 0 to 1.\nx > 0.75, y remains at 1, full green.\n\nIf there are discontinuities, then it is a little more complicated.\nLabel the 3 elements in each row in the cdict entry for a given color as\n(x, y0, y1).  Then for values of x between x[i] and x[i+1] the color\nvalue is interpolated between y1[i] and y0[i+1].\n\nGoing back to the cookbook example, look at cdict[\'red\']; because y0 !=\ny1, it is saying that for x from 0 to 0.5, red increases from 0 to 1,\nbut then it jumps down, so that for x from 0.5 to 1, red increases from\n0.7 to 1.  Green ramps from 0 to 1 as x goes from 0 to 0.5, then jumps\nback to 0, and ramps back to 1 as x goes from 0.5 to 1.\n\nrow i:   x  y0  y1\n                /\n               /\nrow i+1: x  y0  y1\n\nAbove is an attempt to show that for x in the range x[i] to x[i+1], the\ninterpolation is between y1[i] and y0[i+1].  So, y0[0] and y1[-1] are\nnever used.\n\n'
tempResult = arange(0, numpy.pi, 0.1)
	
===================================================================	
module: 7	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
'\nCreating a colormap from a list of colors\n-----------------------------------------\nCreating a colormap from a list of colors can be done with the `from_list`\nmethod of `LinearSegmentedColormap`. You must pass a list of RGB tuples that\ndefine the mixture of colors from 0 to 1.\n\n\nCreating custom colormaps\n-------------------------\nIt is also possible to create a custom mapping for a colormap. This is\naccomplished by creating dictionary that specifies how the RGB channels\nchange from one end of the cmap to the other.\n\nExample: suppose you want red to increase from 0 to 1 over the bottom\nhalf, green to do the same over the middle half, and blue over the top\nhalf.  Then you would use:\n\ncdict = {\'red\':   ((0.0,  0.0, 0.0),\n                   (0.5,  1.0, 1.0),\n                   (1.0,  1.0, 1.0)),\n\n         \'green\': ((0.0,  0.0, 0.0),\n                   (0.25, 0.0, 0.0),\n                   (0.75, 1.0, 1.0),\n                   (1.0,  1.0, 1.0)),\n\n         \'blue\':  ((0.0,  0.0, 0.0),\n                   (0.5,  0.0, 0.0),\n                   (1.0,  1.0, 1.0))}\n\nIf, as in this example, there are no discontinuities in the r, g, and b\ncomponents, then it is quite simple: the second and third element of\neach tuple, above, is the same--call it "y".  The first element ("x")\ndefines interpolation intervals over the full range of 0 to 1, and it\nmust span that whole range.  In other words, the values of x divide the\n0-to-1 range into a set of segments, and y gives the end-point color\nvalues for each segment.\n\nNow consider the green. cdict[\'green\'] is saying that for\n0 <= x <= 0.25, y is zero; no green.\n0.25 < x <= 0.75, y varies linearly from 0 to 1.\nx > 0.75, y remains at 1, full green.\n\nIf there are discontinuities, then it is a little more complicated.\nLabel the 3 elements in each row in the cdict entry for a given color as\n(x, y0, y1).  Then for values of x between x[i] and x[i+1] the color\nvalue is interpolated between y1[i] and y0[i+1].\n\nGoing back to the cookbook example, look at cdict[\'red\']; because y0 !=\ny1, it is saying that for x from 0 to 0.5, red increases from 0 to 1,\nbut then it jumps down, so that for x from 0.5 to 1, red increases from\n0.7 to 1.  Green ramps from 0 to 1 as x goes from 0 to 0.5, then jumps\nback to 0, and ramps back to 1 as x goes from 0.5 to 1.\n\nrow i:   x  y0  y1\n                /\n               /\nrow i+1: x  y0  y1\n\nAbove is an attempt to show that for x in the range x[i] to x[i+1], the\ninterpolation is between y1[i] and y0[i+1].  So, y0[0] and y1[-1] are\nnever used.\n\n'
x = numpy.arange(0, numpy.pi, 0.1)
tempResult = arange(0, (2 * numpy.pi), 0.1)
	
===================================================================	
module: 6	
----------------------------	

'\nThe new ticker code was designed to explicitly support user customized\nticking.  The documentation\nhttp://matplotlib.org/matplotlib.ticker.html details this\nprocess.  That code defines a lot of preset tickers but was primarily\ndesigned to be user extensible.\n\nIn this example a user defined function is used to format the ticks in\nmillions of dollars on the y axis\n'
from matplotlib.ticker import FuncFormatter
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(4)
	
===================================================================	
module: 28	
----------------------------	

"\nWhen plotting daily data, a frequent request is to plot the data\nignoring skips, e.g., no extra spaces for weekends.  This is particularly\ncommon in financial time series, when you may have data for M-F and\nnot Sat, Sun and you don't want gaps in the x axis.  The approach is\nto simply use the integer index for the xdata and a custom tick\nFormatter to get the appropriate date string for a given index.\n"
from __future__ import print_function
import numpy as np
from matplotlib.mlab import csv2rec
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook
from matplotlib.ticker import Formatter
datafile = matplotlib.cbook.get_sample_data('msft.csv', asfileobj=False)
print(('loading %s' % datafile))
r = csv2rec(datafile)[(- 40):]

class MyFormatter(Formatter):

    def __init__(self, dates, fmt='%Y-%m-%d'):
        self.dates = dates
        self.fmt = fmt

    def __call__(self, x, pos=0):
        'Return the label for time x at position pos'
        ind = int(numpy.round(x))
        if ((ind >= len(self.dates)) or (ind < 0)):
            return ''
        return self.dates[ind].strftime(self.fmt)
formatter = MyFormatter(r.date)
(fig, ax) = matplotlib.pyplot.subplots()
ax.xaxis.set_major_formatter(formatter)
tempResult = arange(len(r))
	
===================================================================	
filtered_text: 155	
----------------------------	

delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
filtered_text: 156	
----------------------------	

delta = 0.025
x = numpy.arange((- 3.0), 3.0, delta)
tempResult = arange((- 2.0), 2.0, delta)
	
===================================================================	
filtered_text: 162	
----------------------------	

delta = 0.025
x = numpy.arange((- 3.0), 3.0, delta)
y = numpy.arange((- 2.0), 2.0, delta)
(X, Y) = numpy.meshgrid(x, y)
Z1 = matplotlib.mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)
Z2 = matplotlib.mlab.bivariate_normal(X, Y, 1.5, 0.5, 1, 1)
Z = (10.0 * (Z2 - Z1))
im = ax.imshow(Z, interpolation='bilinear', origin='lower', cmap=matplotlib.cm.gray, extent=((- 3), 3, (- 2), 2))
tempResult = arange((- 1.2), 1.6, 0.2)
	
===================================================================	
drop_shadow_patches: 198	
----------------------------	

N = 5
menMeans = (20, 35, 30, 35, 27)
tempResult = arange(N)
	
===================================================================	
module: 23	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.patches import Circle
from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox
from matplotlib.cbook import get_sample_data
if 1:
    (fig, ax) = matplotlib.pyplot.subplots()
    xy = (0.5, 0.7)
    ax.plot(xy[0], xy[1], '.r')
    offsetbox = TextArea('Test 1', minimumdescent=False)
    ab = AnnotationBbox(offsetbox, xy, xybox=((- 20), 40), xycoords='data', boxcoords='offset points', arrowprops=dict(arrowstyle='->'))
    ax.add_artist(ab)
    offsetbox = TextArea('Test', minimumdescent=False)
    ab = AnnotationBbox(offsetbox, xy, xybox=(1.02, xy[1]), xycoords='data', boxcoords=('axes fraction', 'data'), box_alignment=(0.0, 0.5), arrowprops=dict(arrowstyle='->'))
    ax.add_artist(ab)
    xy = [0.3, 0.55]
    da = DrawingArea(20, 20, 0, 0)
    p = Circle((10, 10), 10)
    da.add_artist(p)
    ab = AnnotationBbox(da, xy, xybox=(1.02, xy[1]), xycoords='data', boxcoords=('axes fraction', 'data'), box_alignment=(0.0, 0.5), arrowprops=dict(arrowstyle='->'))
    ax.add_artist(ab)
    tempResult = arange(100)
	
===================================================================	
module: 12	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.image import BboxImage
from matplotlib.transforms import Bbox, TransformedBbox
if (__name__ == '__main__'):
    fig = matplotlib.pyplot.figure(1)
    ax = matplotlib.pyplot.subplot(121)
    txt = ax.text(0.5, 0.5, 'test', size=30, ha='center', color='w')
    kwargs = dict()
    bbox_image = BboxImage(txt.get_window_extent, norm=None, origin=None, clip_on=False, **kwargs)
    tempResult = arange(256)
	
===================================================================	
module: 58	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.image import BboxImage
from matplotlib._png import read_png
import matplotlib.colors
from matplotlib.cbook import get_sample_data

class RibbonBox(object):
    original_image = read_png(get_sample_data('Minduka_Present_Blue_Pack.png', asfileobj=False))
    cut_location = 70
    b_and_h = original_image[:, :, 2]
    color = (original_image[:, :, 2] - original_image[:, :, 0])
    alpha = original_image[:, :, 3]
    nx = original_image.shape[1]

    def __init__(self, color):
        rgb = matplotlib.colors.to_rgba(color)[:3]
        im = numpy.empty(self.original_image.shape, self.original_image.dtype)
        im[:, :, :3] = self.b_and_h[:, :, numpy.newaxis]
        im[:, :, :3] -= (self.color[:, :, numpy.newaxis] * (1.0 - numpy.array(rgb)))
        im[:, :, 3] = self.alpha
        self.im = im

    def get_stretched_image(self, stretch_factor):
        stretch_factor = max(stretch_factor, 1)
        (ny, nx, nch) = self.im.shape
        ny2 = int((ny * stretch_factor))
        stretched_image = numpy.empty((ny2, nx, nch), self.im.dtype)
        cut = self.im[self.cut_location, :, :]
        stretched_image[:, :, :] = cut
        stretched_image[:self.cut_location, :, :] = self.im[:self.cut_location, :, :]
        stretched_image[(- (ny - self.cut_location)):, :, :] = self.im[(- (ny - self.cut_location)):, :, :]
        self._cached_im = stretched_image
        return stretched_image

class RibbonBoxImage(BboxImage):
    zorder = 1

    def __init__(self, bbox, color, cmap=None, norm=None, interpolation=None, origin=None, filternorm=1, filterrad=4.0, resample=False, **kwargs):
        matplotlib.image.BboxImage.__init__(self, bbox, cmap=cmap, norm=norm, interpolation=interpolation, origin=origin, filternorm=filternorm, filterrad=filterrad, resample=resample, **kwargs)
        self._ribbonbox = RibbonBox(color)
        self._cached_ny = None

    def draw(self, renderer, *args, **kwargs):
        bbox = self.get_window_extent(renderer)
        stretch_factor = (bbox.height / bbox.width)
        ny = int((stretch_factor * self._ribbonbox.nx))
        if (self._cached_ny != ny):
            arr = self._ribbonbox.get_stretched_image(stretch_factor)
            self.set_array(arr)
            self._cached_ny = ny
        matplotlib.image.BboxImage.draw(self, renderer, *args, **kwargs)
if 1:
    from matplotlib.transforms import Bbox, TransformedBbox
    from matplotlib.ticker import ScalarFormatter
    (fig, ax) = matplotlib.pyplot.subplots()
    tempResult = arange(2004, 2009)
	
===================================================================	
module: 60	
----------------------------	

import matplotlib.pyplot as plt
from matplotlib.image import BboxImage
import numpy as np
from matplotlib.transforms import IdentityTransform
import matplotlib.patches as mpatches
from matplotlib.offsetbox import AnnotationBbox, AnchoredOffsetbox, AuxTransformBox
from matplotlib.cbook import get_sample_data
from matplotlib.text import TextPath

class PathClippedImagePatch(matplotlib.patches.PathPatch):
    '\n    The given image is used to draw the face of the patch. Internally,\n    it uses BboxImage whose clippath set to the path of the patch.\n\n    FIXME : The result is currently dpi dependent.\n    '

    def __init__(self, path, bbox_image, **kwargs):
        matplotlib.patches.PathPatch.__init__(self, path, **kwargs)
        self._init_bbox_image(bbox_image)

    def set_facecolor(self, color):
        'simply ignore facecolor'
        matplotlib.patches.PathPatch.set_facecolor(self, 'none')

    def _init_bbox_image(self, im):
        bbox_image = BboxImage(self.get_window_extent, norm=None, origin=None)
        bbox_image.set_transform(IdentityTransform())
        bbox_image.set_data(im)
        self.bbox_image = bbox_image

    def draw(self, renderer=None):
        self.bbox_image.set_clip_path(self._path, self.get_transform())
        self.bbox_image.draw(renderer)
        matplotlib.patches.PathPatch.draw(self, renderer)
if 1:
    usetex = matplotlib.pyplot.rcParams['text.usetex']
    fig = matplotlib.pyplot.figure(1)
    ax = matplotlib.pyplot.subplot(211)
    from matplotlib._png import read_png
    fn = get_sample_data('grace_hopper.png', asfileobj=False)
    arr = read_png(fn)
    text_path = TextPath((0, 0), '!?', size=150)
    p = PathClippedImagePatch(text_path, arr, ec='k', transform=IdentityTransform())
    offsetbox = AuxTransformBox(IdentityTransform())
    offsetbox.add_artist(p)
    ao = AnchoredOffsetbox(loc=2, child=offsetbox, frameon=True, borderpad=0.2)
    ax.add_artist(ao)
    from matplotlib.patches import PathPatch
    if usetex:
        r = '\\mbox{textpath supports mathtext \\& \\TeX}'
    else:
        r = 'textpath supports mathtext & TeX'
    text_path = TextPath((0, 0), r, size=20, usetex=usetex)
    p1 = PathPatch(text_path, ec='w', lw=3, fc='w', alpha=0.9, transform=IdentityTransform())
    p2 = PathPatch(text_path, ec='none', fc='k', transform=IdentityTransform())
    offsetbox2 = AuxTransformBox(IdentityTransform())
    offsetbox2.add_artist(p1)
    offsetbox2.add_artist(p2)
    ab = AnnotationBbox(offsetbox2, (0.95, 0.05), xycoords='axes fraction', boxcoords='offset points', box_alignment=(1.0, 0.0), frameon=False)
    ax.add_artist(ab)
    ax.imshow([[0, 1, 2], [1, 2, 3]], cmap=matplotlib.pyplot.cm.gist_gray_r, interpolation='bilinear', aspect='auto')
    ax = matplotlib.pyplot.subplot(212)
    tempResult = arange(256)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.collections import EllipseCollection
tempResult = arange(10)
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.collections import EllipseCollection
x = numpy.arange(10)
tempResult = arange(15)
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.patches import Ellipse
delta = 45.0
tempResult = arange(0, (360 + delta), delta)
	
===================================================================	
module: 5	
----------------------------	

'\nExample: simple line plot.\nShow how to make a plot that has equal aspect ratio\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, (1.0 + 0.01), 0.01)
	
===================================================================	
module: 6	
----------------------------	

'\nIllustration of upper and lower limit symbols on errorbars\n'
import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure(0)
tempResult = arange(10.0)
	
===================================================================	
module: 7	
----------------------------	

'\nIllustration of upper and lower limit symbols on errorbars\n'
import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure(0)
x = numpy.arange(10.0)
tempResult = arange(10.0)
	
===================================================================	
module: 9	
----------------------------	

'\nIllustration of upper and lower limit symbols on errorbars\n'
import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure(0)
x = numpy.arange(10.0)
y = numpy.sin(((numpy.arange(10.0) / 20.0) * numpy.pi))
matplotlib.pyplot.errorbar(x, y, yerr=0.1)
tempResult = arange(10.0)
	
===================================================================	
module: 11	
----------------------------	

'\nIllustration of upper and lower limit symbols on errorbars\n'
import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure(0)
x = numpy.arange(10.0)
y = numpy.sin(((numpy.arange(10.0) / 20.0) * numpy.pi))
matplotlib.pyplot.errorbar(x, y, yerr=0.1)
y = (numpy.sin(((numpy.arange(10.0) / 20.0) * numpy.pi)) + 1)
matplotlib.pyplot.errorbar(x, y, yerr=0.1, uplims=True)
tempResult = arange(10.0)
	
===================================================================	
module: 17	
----------------------------	

'\nIllustration of upper and lower limit symbols on errorbars\n'
import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure(0)
x = numpy.arange(10.0)
y = numpy.sin(((numpy.arange(10.0) / 20.0) * numpy.pi))
matplotlib.pyplot.errorbar(x, y, yerr=0.1)
y = (numpy.sin(((numpy.arange(10.0) / 20.0) * numpy.pi)) + 1)
matplotlib.pyplot.errorbar(x, y, yerr=0.1, uplims=True)
y = (numpy.sin(((numpy.arange(10.0) / 20.0) * numpy.pi)) + 2)
upperlimits = numpy.array(([1, 0] * 5))
lowerlimits = numpy.array(([0, 1] * 5))
matplotlib.pyplot.errorbar(x, y, yerr=0.1, uplims=upperlimits, lolims=lowerlimits)
matplotlib.pyplot.xlim((- 1), 10)
fig = matplotlib.pyplot.figure(1)
tempResult = arange(10.0)
	
===================================================================	
module: 5	
----------------------------	

'\nDemo for the errorevery keyword to show data full accuracy data plots with\nfew errorbars.\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.1, 4, 0.1)
	
===================================================================	
module: 8	
----------------------------	

'\nThis illustrates placing images directly in the figure, with no axes.\n\n'
import numpy as np
import matplotlib
import matplotlib.cm as cm
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
tempResult = arange(10000.0)
	
===================================================================	
module: 7	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
ax1 = fig.add_axes([0.1, 0.1, 0.4, 0.7])
ax2 = fig.add_axes([0.55, 0.1, 0.4, 0.7])
tempResult = arange(0.0, 2.0, 0.02)
	
===================================================================	
module: 10	
----------------------------	

from matplotlib.font_manager import FontProperties
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return (s1 * e1)
tempResult = arange(0.0, 5.0, 0.1)
	
===================================================================	
module: 11	
----------------------------	

from matplotlib.font_manager import FontProperties
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return (s1 * e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
tempResult = arange(0.0, 5.0, 0.02)
	
===================================================================	
module: 12	
----------------------------	

from matplotlib.font_manager import FontProperties
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return (s1 * e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
t2 = numpy.arange(0.0, 5.0, 0.02)
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 6	
----------------------------	

'\nCopy of fill_between.py but using fill_betweenx() instead.\n'
import matplotlib.mlab as mlab
from matplotlib.pyplot import figure, show
import numpy as np
tempResult = arange(0.0, 2, 0.01)
	
===================================================================	
module: 4	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 2, 0.01)
	
===================================================================	
module: 4	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0, (8 * numpy.pi), 0.1)
	
===================================================================	
module: 7	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
theta = numpy.arange(0, (8 * numpy.pi), 0.1)
a = 1
b = 0.2
tempResult = arange(0, (2 * numpy.pi), (numpy.pi / 2.0))
	
===================================================================	
module: 6	
----------------------------	

'\nRecursively find all objects that match some criteria\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.text as text
tempResult = arange(0, 3, 0.02)
	
===================================================================	
module: 7	
----------------------------	

'\nRecursively find all objects that match some criteria\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.text as text
a = numpy.arange(0, 3, 0.02)
tempResult = arange(0, 3, 0.02)
	
===================================================================	
module: 5	
----------------------------	

"\nTo create plots that share a common axes (visually) you can set the\nhspace between the subplots close to zero (do not use zero itself).\nNormally you'll want to turn off the tick labels on all but one of the\naxes.\n\nIn this example the plots share a common xaxis but you can follow the\nsame logic to supply a common y axis.\n"
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 13	
----------------------------	

"\nTo create plots that share a common axes (visually) you can set the\nhspace between the subplots close to zero (do not use zero itself).\nNormally you'll want to turn off the tick labels on all but one of the\naxes.\n\nIn this example the plots share a common xaxis but you can follow the\nsame logic to supply a common y axis.\n"
import matplotlib.pyplot as plt
import numpy as np
t = numpy.arange(0.0, 2.0, 0.01)
s1 = numpy.sin(((2 * numpy.pi) * t))
s2 = numpy.exp((- t))
s3 = (s1 * s2)
f = matplotlib.pyplot.figure()
matplotlib.pyplot.subplots_adjust(hspace=0.001)
ax1 = matplotlib.pyplot.subplot(311)
ax1.plot(t, s1)
tempResult = arange((- 0.9), 1.0, 0.4)
	
===================================================================	
module: 17	
----------------------------	

"\nTo create plots that share a common axes (visually) you can set the\nhspace between the subplots close to zero (do not use zero itself).\nNormally you'll want to turn off the tick labels on all but one of the\naxes.\n\nIn this example the plots share a common xaxis but you can follow the\nsame logic to supply a common y axis.\n"
import matplotlib.pyplot as plt
import numpy as np
t = numpy.arange(0.0, 2.0, 0.01)
s1 = numpy.sin(((2 * numpy.pi) * t))
s2 = numpy.exp((- t))
s3 = (s1 * s2)
f = matplotlib.pyplot.figure()
matplotlib.pyplot.subplots_adjust(hspace=0.001)
ax1 = matplotlib.pyplot.subplot(311)
ax1.plot(t, s1)
matplotlib.pyplot.yticks(numpy.arange((- 0.9), 1.0, 0.4))
matplotlib.pyplot.ylim((- 1), 1)
ax2 = matplotlib.pyplot.subplot(312, sharex=ax1)
ax2.plot(t, s2)
tempResult = arange(0.1, 1.0, 0.2)
	
===================================================================	
module: 21	
----------------------------	

"\nTo create plots that share a common axes (visually) you can set the\nhspace between the subplots close to zero (do not use zero itself).\nNormally you'll want to turn off the tick labels on all but one of the\naxes.\n\nIn this example the plots share a common xaxis but you can follow the\nsame logic to supply a common y axis.\n"
import matplotlib.pyplot as plt
import numpy as np
t = numpy.arange(0.0, 2.0, 0.01)
s1 = numpy.sin(((2 * numpy.pi) * t))
s2 = numpy.exp((- t))
s3 = (s1 * s2)
f = matplotlib.pyplot.figure()
matplotlib.pyplot.subplots_adjust(hspace=0.001)
ax1 = matplotlib.pyplot.subplot(311)
ax1.plot(t, s1)
matplotlib.pyplot.yticks(numpy.arange((- 0.9), 1.0, 0.4))
matplotlib.pyplot.ylim((- 1), 1)
ax2 = matplotlib.pyplot.subplot(312, sharex=ax1)
ax2.plot(t, s2)
matplotlib.pyplot.yticks(numpy.arange(0.1, 1.0, 0.2))
matplotlib.pyplot.ylim(0, 1)
ax3 = matplotlib.pyplot.subplot(313, sharex=ax1)
ax3.plot(t, s3)
tempResult = arange((- 0.9), 1.0, 0.4)
	
===================================================================	
module: 5	
----------------------------	

from __future__ import print_function
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(10)
	
===================================================================	
module: 7	
----------------------------	

'\nhexbin is an axes method or pyplot function that is essentially a\npcolor of a 2-D histogram with hexagonal cells.\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
module: 13	
----------------------------	

'\nThis example demonstrates how to set a hyperlinks on various kinds of elements.\n\nThis currently only works with the SVG backend.\n'
import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
f = matplotlib.pyplot.figure()
s = matplotlib.pyplot.scatter([1, 2, 3], [4, 5, 6])
s.set_urls(['http://www.bbc.co.uk/news', 'http://www.google.com', None])
f.canvas.print_figure('scatter.svg')
f = matplotlib.pyplot.figure()
delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
from matplotlib.path import Path
from matplotlib.patches import PathPatch
delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
module: 7	
----------------------------	

import numpy as np
import matplotlib.cm as cm
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
module: 5	
----------------------------	

'\nYou can specify whether images should be plotted with the array origin\nx[0,0] in the upper left or upper right by using the origin parameter.\nYou can also control the default be setting image.origin in your\nmatplotlibrc file; see http://matplotlib.org/matplotlibrc\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(120)
	
===================================================================	
module: 5	
----------------------------	

'\nYou can use decreasing axes by flipping the normal order of the axis\nlimits\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.01, 5.0, 0.01)
	
===================================================================	
module: 10	
----------------------------	

'\nLayer images above one another using alpha blending\n'
from __future__ import division
import matplotlib.pyplot as plt
import numpy as np

def func3(x, y):
    return ((((1 - (x / 2)) + (x ** 5)) + (y ** 3)) * numpy.exp((- ((x ** 2) + (y ** 2)))))
(dx, dy) = (0.05, 0.05)
tempResult = arange((- 3.0), 3.0, dx)
	
===================================================================	
module: 11	
----------------------------	

'\nLayer images above one another using alpha blending\n'
from __future__ import division
import matplotlib.pyplot as plt
import numpy as np

def func3(x, y):
    return ((((1 - (x / 2)) + (x ** 5)) + (y ** 3)) * numpy.exp((- ((x ** 2) + (y ** 2)))))
(dx, dy) = (0.05, 0.05)
x = numpy.arange((- 3.0), 3.0, dx)
tempResult = arange((- 3.0), 3.0, dy)
	
===================================================================	
module: 4	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 2.0, 0.1)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
t1 = numpy.arange(0.0, 2.0, 0.1)
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
from matplotlib import colors as mcolors
import numpy as np
tempResult = arange(100)
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.collections import LineCollection
N = 50
tempResult = arange(N)
	
===================================================================	
module: 9	
----------------------------	

from __future__ import print_function
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.cbook as cbook
datafile = matplotlib.cbook.get_sample_data('membrane.dat', asfileobj=False)
print('loading', datafile)
x = ((1000 * 0.1) * numpy.fromstring(open(datafile, 'rb').read(), numpy.float32))
tempResult = arange(len(x))
	
===================================================================	
module: 12	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
data = ((3, 1000), (10, 3), (100, 30), (500, 800), (50, 1))
matplotlib.pyplot.xlabel('FOO')
matplotlib.pyplot.ylabel('FOO')
matplotlib.pyplot.title('Testing')
matplotlib.pyplot.yscale('log')
dim = len(data[0])
w = 0.75
dimw = (w / dim)
tempResult = arange(len(data))
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
matplotlib.pyplot.subplots_adjust(hspace=0.4)
tempResult = arange(0.01, 20.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.01
tempResult = arange(dt, 20.0, dt)
	
===================================================================	
module: 9	
----------------------------	

"\nDemonstrate how to use major and minor tickers.\n\nThe two relevant userland classes are Locators and Formatters.\nLocators determine where the ticks are and formatters control the\nformatting of ticks.\n\nMinor ticks are off by default (NullLocator and NullFormatter).  You\ncan turn minor ticks on w/o labels by setting the minor locator.  You\ncan also turn labeling on for the minor ticker by setting the minor\nformatter\n\nMake a plot with major ticks that are multiples of 20 and minor ticks\nthat are multiples of 5.  Label major ticks with %d formatting but\ndon't label minor ticks\n\nThe MultipleLocator ticker class is used to place ticks on multiples of\nsome base.  The FormatStrFormatter uses a string format string (e.g.,\n'%d' or '%1.2f' or '%1.1f cm' ) to format the tick\n\nThe pyplot interface grid command changes the grid settings of the\nmajor ticks of the y and y axis together.  If you want to control the\ngrid of the minor ticks for a given axis, use for example\n\n  ax.xaxis.grid(True, which='minor')\n\nNote, you should not use the same locator between different Axis\nbecause the locator stores references to the Axis data and view limits\n\n"
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import MultipleLocator, FormatStrFormatter
majorLocator = MultipleLocator(20)
majorFormatter = FormatStrFormatter('%d')
minorLocator = MultipleLocator(5)
tempResult = arange(0.0, 100.0, 0.1)
	
===================================================================	
module: 7	
----------------------------	

'\nAutomatic tick selection for major and minor ticks.\n\nUse interactive pan and zoom to see how the tick intervals\nchange. There will be either 4 or 5 minor tick intervals\nper major interval, depending on the major interval.\n'
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import AutoMinorLocator
minorLocator = AutoMinorLocator()
tempResult = arange(0.0, 100.0, 0.01)
	
===================================================================	
module: 27	
----------------------------	

'\nThe techniques here are no longer required with the new support for\nspines in matplotlib -- see\nhttp://matplotlib.org/examples/pylab_examples/spine_placement_demo.html.\n\nThis example should be considered deprecated and is left just for demo\npurposes for folks wanting to make a pseudo-axis\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as lines

def make_xaxis(ax, yloc, offset=0.05, **props):
    (xmin, xmax) = ax.get_xlim()
    locs = [loc for loc in ax.xaxis.get_majorticklocs() if ((loc >= xmin) and (loc <= xmax))]
    (tickline,) = ax.plot(locs, ([yloc] * len(locs)), linestyle='', marker=matplotlib.lines.TICKDOWN, **props)
    (axline,) = ax.plot([xmin, xmax], [yloc, yloc], **props)
    tickline.set_clip_on(False)
    axline.set_clip_on(False)
    for loc in locs:
        ax.text(loc, (yloc - offset), ('%1.1f' % loc), horizontalalignment='center', verticalalignment='top')

def make_yaxis(ax, xloc=0, offset=0.05, **props):
    (ymin, ymax) = ax.get_ylim()
    locs = [loc for loc in ax.yaxis.get_majorticklocs() if ((loc >= ymin) and (loc <= ymax))]
    (tickline,) = ax.plot(([xloc] * len(locs)), locs, linestyle='', marker=matplotlib.lines.TICKLEFT, **props)
    (axline,) = ax.plot([xloc, xloc], [ymin, ymax], **props)
    tickline.set_clip_on(False)
    axline.set_clip_on(False)
    for loc in locs:
        ax.text((xloc - offset), loc, ('%1.1f' % loc), verticalalignment='center', horizontalalignment='right')
props = dict(color='black', linewidth=2, markeredgewidth=2)
tempResult = arange(200.0)
	
===================================================================	
module: 10	
----------------------------	

import matplotlib.pyplot as plt
import matplotlib.path as mpath
import numpy as np
star = matplotlib.path.Path.unit_regular_star(6)
circle = matplotlib.path.Path.unit_circle()
verts = numpy.concatenate([circle.vertices, star.vertices[::(- 1), ...]])
codes = numpy.concatenate([circle.codes, star.codes])
cut_star = matplotlib.path.Path(verts, codes)
tempResult = arange(10)
	
===================================================================	
module: 5	
----------------------------	

'\nPlot lines with points masked out.\n\nThis would typically be used with gappy data, to\nbreak the line at the data gaps.\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0, (2 * numpy.pi), 0.02)
	
===================================================================	
module: 9	
----------------------------	

"\nUse matplotlib's internal LaTeX parser and layout engine.  For true\nlatex rendering, see the text.usetex option\n"
import numpy as np
from matplotlib.pyplot import figure, show
fig = figure()
fig.subplots_adjust(bottom=0.2)
ax = fig.add_subplot(111)
ax.plot([1, 2, 3], 'r')
tempResult = arange(0.0, 3.0, 0.1)
	
===================================================================	
module: 33	
----------------------------	

'Displays a set of subplots with an MRI image, its intensity histogram and\nsome EEG traces.\n'
from __future__ import division, print_function
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook
import matplotlib.cm as cm
from matplotlib.collections import LineCollection
from matplotlib.ticker import MultipleLocator
fig = matplotlib.pyplot.figure('MRI_with_EEG')
dfile = matplotlib.cbook.get_sample_data('s1045.ima.gz')
im = np.fromstring(dfile.read(), np.uint16).astype(float)
im.shape = (256, 256)
dfile.close()
ax0 = fig.add_subplot(2, 2, 1)
ax0.imshow(im, cmap=matplotlib.cm.gray)
ax0.axis('off')
ax1 = fig.add_subplot(2, 2, 2)
im = numpy.ravel(im)
im = im[numpy.nonzero(im)]
im = (im / ((2 ** 16) - 1))
ax1.hist(im, bins=100)
ax1.xaxis.set_major_locator(MultipleLocator(0.4))
ax1.minorticks_on()
ax1.set_yticks([])
ax1.set_xlabel('Intensity (a.u.)')
ax1.set_ylabel('MRI density')
(numSamples, numRows) = (800, 4)
eegfile = matplotlib.cbook.get_sample_data('eeg.dat', asfileobj=False)
print(('Loading EEG %s' % eegfile))
data = numpy.fromfile(eegfile, dtype=float)
data.shape = (numSamples, numRows)
tempResult = arange(numSamples)
	
===================================================================	
module: 37	
----------------------------	

'Displays a set of subplots with an MRI image, its intensity histogram and\nsome EEG traces.\n'
from __future__ import division, print_function
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook
import matplotlib.cm as cm
from matplotlib.collections import LineCollection
from matplotlib.ticker import MultipleLocator
fig = matplotlib.pyplot.figure('MRI_with_EEG')
dfile = matplotlib.cbook.get_sample_data('s1045.ima.gz')
im = np.fromstring(dfile.read(), np.uint16).astype(float)
im.shape = (256, 256)
dfile.close()
ax0 = fig.add_subplot(2, 2, 1)
ax0.imshow(im, cmap=matplotlib.cm.gray)
ax0.axis('off')
ax1 = fig.add_subplot(2, 2, 2)
im = numpy.ravel(im)
im = im[numpy.nonzero(im)]
im = (im / ((2 ** 16) - 1))
ax1.hist(im, bins=100)
ax1.xaxis.set_major_locator(MultipleLocator(0.4))
ax1.minorticks_on()
ax1.set_yticks([])
ax1.set_xlabel('Intensity (a.u.)')
ax1.set_ylabel('MRI density')
(numSamples, numRows) = (800, 4)
eegfile = matplotlib.cbook.get_sample_data('eeg.dat', asfileobj=False)
print(('Loading EEG %s' % eegfile))
data = numpy.fromfile(eegfile, dtype=float)
data.shape = (numSamples, numRows)
t = ((10.0 * numpy.arange(numSamples)) / numSamples)
ticklocs = []
ax2 = fig.add_subplot(2, 1, 2)
ax2.set_xlim(0, 10)
tempResult = arange(10)
	
===================================================================	
module: 7	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
matplotlib.pyplot.figure(figsize=(7, 4))
ax = matplotlib.pyplot.subplot(121)
ax.set_aspect(1)
tempResult = arange(10)
	
===================================================================	
module: 15	
----------------------------	

'\nThis is a demo of creating a pdf file with several pages,\nas well as adding metadata and annotations to pdf files.\n'
import datetime
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt
with PdfPages('multipage_pdf.pdf') as pdf:
    matplotlib.pyplot.figure(figsize=(3, 3))
    matplotlib.pyplot.plot(range(7), [3, 1, 4, 1, 5, 9, 2], 'r-o')
    matplotlib.pyplot.title('Page One')
    pdf.savefig()
    matplotlib.pyplot.close()
    matplotlib.pyplot.rc('text', usetex=True)
    matplotlib.pyplot.figure(figsize=(8, 6))
    tempResult = arange(0, 5, 0.1)
	
===================================================================	
module: 4	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

'\nExample: simple line plots with NaNs inserted.\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.0, (1.0 + 0.01), 0.01)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import OldScalarFormatter, ScalarFormatter
tempResult = arange(0, 1, 0.01)
	
===================================================================	
module: 21	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import OldScalarFormatter, ScalarFormatter
x = numpy.arange(0, 1, 0.01)
(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2, figsize=(6, 6))
fig.text(0.5, 0.975, 'The old formatter', horizontalalignment='center', verticalalignment='top')
ax1.plot(((x * 100000.0) + 10000000000.0), ((x * 1e-10) + 1e-05))
ax1.xaxis.set_major_formatter(OldScalarFormatter())
ax1.yaxis.set_major_formatter(OldScalarFormatter())
ax2.plot((x * 100000.0), (x * 0.0001))
ax2.xaxis.set_major_formatter(OldScalarFormatter())
ax2.yaxis.set_major_formatter(OldScalarFormatter())
ax3.plot((((- x) * 100000.0) - 10000000000.0), (((- x) * 1e-05) - 1e-10))
ax3.xaxis.set_major_formatter(OldScalarFormatter())
ax3.yaxis.set_major_formatter(OldScalarFormatter())
ax4.plot(((- x) * 100000.0), ((- x) * 0.0001))
ax4.xaxis.set_major_formatter(OldScalarFormatter())
ax4.yaxis.set_major_formatter(OldScalarFormatter())
fig.subplots_adjust(wspace=0.7, hspace=0.6)
tempResult = arange(0, 1, 0.01)
	
===================================================================	
module: 37	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import OldScalarFormatter, ScalarFormatter
x = numpy.arange(0, 1, 0.01)
(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2, figsize=(6, 6))
fig.text(0.5, 0.975, 'The old formatter', horizontalalignment='center', verticalalignment='top')
ax1.plot(((x * 100000.0) + 10000000000.0), ((x * 1e-10) + 1e-05))
ax1.xaxis.set_major_formatter(OldScalarFormatter())
ax1.yaxis.set_major_formatter(OldScalarFormatter())
ax2.plot((x * 100000.0), (x * 0.0001))
ax2.xaxis.set_major_formatter(OldScalarFormatter())
ax2.yaxis.set_major_formatter(OldScalarFormatter())
ax3.plot((((- x) * 100000.0) - 10000000000.0), (((- x) * 1e-05) - 1e-10))
ax3.xaxis.set_major_formatter(OldScalarFormatter())
ax3.yaxis.set_major_formatter(OldScalarFormatter())
ax4.plot(((- x) * 100000.0), ((- x) * 0.0001))
ax4.xaxis.set_major_formatter(OldScalarFormatter())
ax4.yaxis.set_major_formatter(OldScalarFormatter())
fig.subplots_adjust(wspace=0.7, hspace=0.6)
x = numpy.arange(0, 1, 0.01)
(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2, figsize=(6, 6))
fig.text(0.5, 0.975, 'The new formatter, default settings', horizontalalignment='center', verticalalignment='top')
ax1.plot(((x * 100000.0) + 10000000000.0), ((x * 1e-10) + 1e-05))
ax1.xaxis.set_major_formatter(ScalarFormatter())
ax1.yaxis.set_major_formatter(ScalarFormatter())
ax2.plot((x * 100000.0), (x * 0.0001))
ax2.xaxis.set_major_formatter(ScalarFormatter())
ax2.yaxis.set_major_formatter(ScalarFormatter())
ax3.plot((((- x) * 100000.0) - 10000000000.0), (((- x) * 1e-05) - 1e-10))
ax3.xaxis.set_major_formatter(ScalarFormatter())
ax3.yaxis.set_major_formatter(ScalarFormatter())
ax4.plot(((- x) * 100000.0), ((- x) * 0.0001))
ax4.xaxis.set_major_formatter(ScalarFormatter())
ax4.yaxis.set_major_formatter(ScalarFormatter())
fig.subplots_adjust(wspace=0.7, hspace=0.6)
tempResult = arange(0, 1, 0.01)
	
===================================================================	
module: 53	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.ticker import OldScalarFormatter, ScalarFormatter
x = numpy.arange(0, 1, 0.01)
(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2, figsize=(6, 6))
fig.text(0.5, 0.975, 'The old formatter', horizontalalignment='center', verticalalignment='top')
ax1.plot(((x * 100000.0) + 10000000000.0), ((x * 1e-10) + 1e-05))
ax1.xaxis.set_major_formatter(OldScalarFormatter())
ax1.yaxis.set_major_formatter(OldScalarFormatter())
ax2.plot((x * 100000.0), (x * 0.0001))
ax2.xaxis.set_major_formatter(OldScalarFormatter())
ax2.yaxis.set_major_formatter(OldScalarFormatter())
ax3.plot((((- x) * 100000.0) - 10000000000.0), (((- x) * 1e-05) - 1e-10))
ax3.xaxis.set_major_formatter(OldScalarFormatter())
ax3.yaxis.set_major_formatter(OldScalarFormatter())
ax4.plot(((- x) * 100000.0), ((- x) * 0.0001))
ax4.xaxis.set_major_formatter(OldScalarFormatter())
ax4.yaxis.set_major_formatter(OldScalarFormatter())
fig.subplots_adjust(wspace=0.7, hspace=0.6)
x = numpy.arange(0, 1, 0.01)
(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2, figsize=(6, 6))
fig.text(0.5, 0.975, 'The new formatter, default settings', horizontalalignment='center', verticalalignment='top')
ax1.plot(((x * 100000.0) + 10000000000.0), ((x * 1e-10) + 1e-05))
ax1.xaxis.set_major_formatter(ScalarFormatter())
ax1.yaxis.set_major_formatter(ScalarFormatter())
ax2.plot((x * 100000.0), (x * 0.0001))
ax2.xaxis.set_major_formatter(ScalarFormatter())
ax2.yaxis.set_major_formatter(ScalarFormatter())
ax3.plot((((- x) * 100000.0) - 10000000000.0), (((- x) * 1e-05) - 1e-10))
ax3.xaxis.set_major_formatter(ScalarFormatter())
ax3.yaxis.set_major_formatter(ScalarFormatter())
ax4.plot(((- x) * 100000.0), ((- x) * 0.0001))
ax4.xaxis.set_major_formatter(ScalarFormatter())
ax4.yaxis.set_major_formatter(ScalarFormatter())
fig.subplots_adjust(wspace=0.7, hspace=0.6)
x = numpy.arange(0, 1, 0.01)
(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2, figsize=(6, 6))
fig.text(0.5, 0.975, 'The new formatter, no numerical offset', horizontalalignment='center', verticalalignment='top')
ax1.plot(((x * 100000.0) + 10000000000.0), ((x * 1e-10) + 1e-05))
ax1.xaxis.set_major_formatter(ScalarFormatter(useOffset=False))
ax1.yaxis.set_major_formatter(ScalarFormatter(useOffset=False))
ax2.plot((x * 100000.0), (x * 0.0001))
ax2.xaxis.set_major_formatter(ScalarFormatter(useOffset=False))
ax2.yaxis.set_major_formatter(ScalarFormatter(useOffset=False))
ax3.plot((((- x) * 100000.0) - 10000000000.0), (((- x) * 1e-05) - 1e-10))
ax3.xaxis.set_major_formatter(ScalarFormatter(useOffset=False))
ax3.yaxis.set_major_formatter(ScalarFormatter(useOffset=False))
ax4.plot(((- x) * 100000.0), ((- x) * 0.0001))
ax4.xaxis.set_major_formatter(ScalarFormatter(useOffset=False))
ax4.yaxis.set_major_formatter(ScalarFormatter(useOffset=False))
fig.subplots_adjust(wspace=0.7, hspace=0.6)
tempResult = arange(0, 1, 0.01)
	
===================================================================	
module: 16	
----------------------------	

import matplotlib.pyplot as plt
import matplotlib.patheffects as PathEffects
import numpy as np
if 1:
    matplotlib.pyplot.figure(1, figsize=(8, 3))
    ax1 = matplotlib.pyplot.subplot(131)
    ax1.imshow([[1, 2], [2, 3]])
    txt = ax1.annotate('test', (1.0, 1.0), (0.0, 0), arrowprops=dict(arrowstyle='->', connectionstyle='angle3', lw=2), size=20, ha='center', path_effects=[matplotlib.patheffects.withStroke(linewidth=3, foreground='w')])
    txt.arrow_patch.set_path_effects([matplotlib.patheffects.Stroke(linewidth=5, foreground='w'), matplotlib.patheffects.Normal()])
    ax1.grid(True, linestyle='-')
    pe = [matplotlib.patheffects.withStroke(linewidth=3, foreground='w')]
    for l in (ax1.get_xgridlines() + ax1.get_ygridlines()):
        l.set_path_effects(pe)
    ax2 = matplotlib.pyplot.subplot(132)
    tempResult = arange(25)
	
===================================================================	
module: 5	
----------------------------	

'\nDemo of a line plot on a polar axis.\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0, 2, 0.01)
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
from matplotlib.pyplot import figure, show, rc
rc('grid', color='#316931', linewidth=1, linestyle='-')
rc('xtick', labelsize=15)
rc('ytick', labelsize=15)
fig = figure(figsize=(8, 8))
ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], projection='polar', facecolor='#d5de9c')
tempResult = arange(0, 3.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.01
tempResult = arange(0, 10, dt)
	
===================================================================	
module: 6	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
dt = (numpy.pi / 100.0)
fs = (1.0 / dt)
tempResult = arange(0, 8, dt)
	
===================================================================	
module: 14	
----------------------------	

"This is a ported version of a MATLAB example from the signal\nprocessing toolbox that showed some difference at one time between\nMatplotlib's and MATLAB's scaling of the PSD.\n\n"
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
prng = numpy.random.RandomState(123456)
fs = 1000
t = numpy.linspace(0, 0.3, 301)
A = np.array([2, 8]).reshape((- 1), 1)
f = np.array([150, 140]).reshape((- 1), 1)
xn = ((A * np.sin((((2 * np.pi) * f) * t))).sum(axis=0) + (5 * prng.randn(*t.shape)))
(fig, (ax0, ax1)) = matplotlib.pyplot.subplots(ncols=2)
fig.subplots_adjust(hspace=0.45, wspace=0.3)
tempResult = arange((- 50), 30, 10)
	
===================================================================	
module: 16	
----------------------------	

"This is a ported version of a MATLAB example from the signal\nprocessing toolbox that showed some difference at one time between\nMatplotlib's and MATLAB's scaling of the PSD.\n\n"
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
prng = numpy.random.RandomState(123456)
fs = 1000
t = numpy.linspace(0, 0.3, 301)
A = np.array([2, 8]).reshape((- 1), 1)
f = np.array([150, 140]).reshape((- 1), 1)
xn = ((A * np.sin((((2 * np.pi) * f) * t))).sum(axis=0) + (5 * prng.randn(*t.shape)))
(fig, (ax0, ax1)) = matplotlib.pyplot.subplots(ncols=2)
fig.subplots_adjust(hspace=0.45, wspace=0.3)
yticks = numpy.arange((- 50), 30, 10)
yrange = (yticks[0], yticks[(- 1)])
tempResult = arange(0, 550, 100)
	
===================================================================	
module: 14	
----------------------------	

"This is a ported version of a MATLAB example from the signal\nprocessing toolbox that showed some difference at one time between\nMatplotlib's and MATLAB's scaling of the PSD.\n\nThis differs from psd_demo3.py in that this uses a complex signal,\nso we can see that complex PSD's work properly\n\n"
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
prng = numpy.random.RandomState(123456)
fs = 1000
t = numpy.linspace(0, 0.3, 301)
A = np.array([2, 8]).reshape((- 1), 1)
f = np.array([150, 140]).reshape((- 1), 1)
xn = ((A * np.exp((((2j * np.pi) * f) * t))).sum(axis=0) + (5 * prng.randn(*t.shape)))
(fig, (ax0, ax1)) = matplotlib.pyplot.subplots(ncols=2)
fig.subplots_adjust(hspace=0.45, wspace=0.3)
tempResult = arange((- 50), 30, 10)
	
===================================================================	
module: 16	
----------------------------	

"This is a ported version of a MATLAB example from the signal\nprocessing toolbox that showed some difference at one time between\nMatplotlib's and MATLAB's scaling of the PSD.\n\nThis differs from psd_demo3.py in that this uses a complex signal,\nso we can see that complex PSD's work properly\n\n"
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
prng = numpy.random.RandomState(123456)
fs = 1000
t = numpy.linspace(0, 0.3, 301)
A = np.array([2, 8]).reshape((- 1), 1)
f = np.array([150, 140]).reshape((- 1), 1)
xn = ((A * np.exp((((2j * np.pi) * f) * t))).sum(axis=0) + (5 * prng.randn(*t.shape)))
(fig, (ax0, ax1)) = matplotlib.pyplot.subplots(ncols=2)
fig.subplots_adjust(hspace=0.45, wspace=0.3)
yticks = numpy.arange((- 50), 30, 10)
yrange = (yticks[0], yticks[(- 1)])
tempResult = arange((- 500), 550, 200)
	
===================================================================	
module: 6	
----------------------------	

'\nDemonstration of quiver and quiverkey functions. This is using the\nnew version coming from the code in quiver.py.\n\nKnown problem: the plot autoscaling does not take into account\nthe arrows, so those on the boundaries are often out of the picture.\nThis is *not* an easy problem to solve in a perfectly general way.\nThe workaround is to manually expand the axes.\n\n'
import matplotlib.pyplot as plt
import numpy as np
from numpy import ma
tempResult = arange(0, (2 * numpy.pi), 0.2)
	
===================================================================	
module: 6	
----------------------------	

'\nDemonstration of quiver and quiverkey functions. This is using the\nnew version coming from the code in quiver.py.\n\nKnown problem: the plot autoscaling does not take into account\nthe arrows, so those on the boundaries are often out of the picture.\nThis is *not* an easy problem to solve in a perfectly general way.\nThe workaround is to manually expand the axes.\n\n'
import matplotlib.pyplot as plt
import numpy as np
from numpy import ma
tempResult = arange(0, (2 * numpy.pi), 0.2)
	
===================================================================	
module: 26	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
x = numpy.random.randn(1000)
y = numpy.random.randn(1000)
nullfmt = NullFormatter()
(left, width) = (0.1, 0.65)
(bottom, height) = (0.1, 0.65)
bottom_h = left_h = ((left + width) + 0.02)
rect_scatter = [left, bottom, width, height]
rect_histx = [left, bottom_h, width, 0.2]
rect_histy = [left_h, bottom, 0.2, height]
matplotlib.pyplot.figure(1, figsize=(8, 8))
axScatter = matplotlib.pyplot.axes(rect_scatter)
axHistx = matplotlib.pyplot.axes(rect_histx)
axHisty = matplotlib.pyplot.axes(rect_histy)
axHistx.xaxis.set_major_formatter(nullfmt)
axHisty.yaxis.set_major_formatter(nullfmt)
axScatter.scatter(x, y)
binwidth = 0.25
xymax = numpy.max([numpy.max(numpy.fabs(x)), numpy.max(numpy.fabs(y))])
lim = ((int((xymax / binwidth)) + 1) * binwidth)
axScatter.set_xlim(((- lim), lim))
axScatter.set_ylim(((- lim), lim))
tempResult = arange((- lim), (lim + binwidth), binwidth)
	
===================================================================	
module: 15	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
N = 100
r0 = 0.6
x = (0.9 * numpy.random.rand(N))
y = (0.9 * numpy.random.rand(N))
area = (numpy.pi * ((10 * numpy.random.rand(N)) ** 2))
c = numpy.sqrt(area)
r = numpy.sqrt(((x * x) + (y * y)))
area1 = numpy.ma.masked_where((r < r0), area)
area2 = numpy.ma.masked_where((r >= r0), area)
matplotlib.pyplot.scatter(x, y, s=area1, marker='^', c=c)
matplotlib.pyplot.scatter(x, y, s=area2, marker='o', c=c)
tempResult = arange(0, (numpy.pi / 2), 0.01)
	
===================================================================	
module: 5	
----------------------------	

from matplotlib import pyplot as plt
import numpy as np
import matplotlib
tempResult = arange(0.0, 50.0, 2.0)
	
===================================================================	
module: 6	
----------------------------	

"\n\nThe pyplot interface allows you to use setp and getp to set and get\nobject properties, as well as to do introspection on the object\n\nset:\n    To set the linestyle of a line to be dashed, you can do\n\n      >>> line, = plt.plot([1,2,3])\n      >>> plt.setp(line, linestyle='--')\n\n    If you want to know the valid types of arguments, you can provide the\n    name of the property you want to set without a value\n\n      >>> plt.setp(line, 'linestyle')\n          linestyle: [ '-' | '--' | '-.' | ':' | 'steps' | 'None' ]\n\n    If you want to see all the properties that can be set, and their\n    possible values, you can do\n\n        >>> plt.setp(line)\n\n    set operates on a single instance or a list of instances.  If you are\n    in query mode introspecting the possible values, only the first\n    instance in the sequence is used.  When actually setting values, all\n    the instances will be set.  e.g., suppose you have a list of two lines,\n    the following will make both lines thicker and red\n\n        >>> x = np.arange(0,1.0,0.01)\n        >>> y1 = np.sin(2*np.pi*x)\n        >>> y2 = np.sin(4*np.pi*x)\n        >>> lines = plt.plot(x, y1, x, y2)\n        >>> plt.setp(lines, linewidth=2, color='r')\n\n\nget:\n\n    get returns the value of a given attribute.  You can use get to query\n    the value of a single attribute\n\n        >>> plt.getp(line, 'linewidth')\n            0.5\n\n    or all the attribute/value pairs\n\n    >>> plt.getp(line)\n        aa = True\n        alpha = 1.0\n        antialiased = True\n        c = b\n        clip_on = True\n        color = b\n        ... long listing skipped ...\n\nAliases:\n\n  To reduce keystrokes in interactive mode, a number of properties\n  have short aliases, e.g., 'lw' for 'linewidth' and 'mec' for\n  'markeredgecolor'.  When calling set or get in introspection mode,\n  these properties will be listed as 'fullname or aliasname', as in\n\n\n\n\n"
from __future__ import print_function
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0, 1.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

'\nYou can share the x or y axis limits for one axis with another by\npassing an axes instance as a sharex or sharey kwarg.\n\nChanging the axis limits on one axes will be reflected automatically\nin the other, and vice-versa, so when you navigate with the toolbar\nthe axes will follow each other on their shared axes.  Ditto for\nchanges in the axis scaling (e.g., log vs linear).  However, it is\npossible to have differences in tick labeling, e.g., you can selectively\nturn off the tick labels on one axes.\n\nThe example below shows how to customize the tick labels on the\nvarious axes.  Shared axes share the tick locator, tick formatter,\nview limits, and transformation (e.g., log, linear).  But the ticklabels\nthemselves do not share properties.  This is a feature and not a bug,\nbecause you may want to make the tick labels smaller on the upper\naxes, e.g., in the example below.\n\nIf you want to turn off the ticklabels for a given axes (e.g., on\nsubplot(211) or subplot(212), you cannot do the standard trick\n\n   setp(ax2, xticklabels=[])\n\nbecause this changes the tick Formatter, which is shared among all\naxes.  But you can alter the visibility of the labels, which is a\nproperty\n\n  setp( ax2.get_xticklabels(), visible=False)\n\n'
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.01, 5.0, 0.01)
	
===================================================================	
module: 4	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.0005
tempResult = arange(0.0, 20.0, dt)
	
===================================================================	
module: 7	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
numpy.random.seed(0)
dt = 0.01
Fs = (1 / dt)
tempResult = arange(0, 10, dt)
	
===================================================================	
module: 8	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt

def fnx():
    return numpy.random.randint(5, 50, 10)
y = numpy.row_stack((fnx(), fnx(), fnx()))
tempResult = arange(10)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
from numpy import ma
import matplotlib.pyplot as plt
tempResult = arange(1, 7, 0.4)
	
===================================================================	
doall: 19	
----------------------------	

tests = stests
matplotlib.pyplot.figure(figsize=(8, ((len(tests) * 1) + 2)))
matplotlib.pyplot.plot([0, 0], 'r')
matplotlib.pyplot.grid(False)
matplotlib.pyplot.axis([0, 3, (- len(tests)), 0])
tempResult = arange(len(tests))
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.01
tempResult = arange((- 50.0), 50.0, dt)
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
dt = 0.01
x = numpy.arange((- 50.0), 50.0, dt)
tempResult = arange(0, 100.0, dt)
	
===================================================================	
module: 21	
----------------------------	

import time
import matplotlib.pyplot as plt
import numpy as np

def get_memory(t):
    'Simulate a function that returns system memory'
    return (100 * (0.5 + (0.5 * numpy.sin(((0.5 * numpy.pi) * t)))))

def get_cpu(t):
    'Simulate a function that returns cpu usage'
    return (100 * (0.5 + (0.5 * numpy.sin(((0.2 * numpy.pi) * (t - 0.25))))))

def get_net(t):
    'Simulate a function that returns network bandwidth'
    return (100 * (0.5 + (0.5 * numpy.sin(((0.7 * numpy.pi) * (t - 0.1))))))

def get_stats(t):
    return (get_memory(t), get_cpu(t), get_net(t))
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(1, 4)
	
===================================================================	
module: 8	
----------------------------	

'\nDemo of table function to display a table within a plot.\n'
import numpy as np
import matplotlib.pyplot as plt
data = [[66386, 174296, 75131, 577908, 32015], [58230, 381139, 78045, 99308, 160454], [89135, 80552, 152558, 497981, 603535], [78415, 81858, 150656, 193263, 69638], [139361, 331509, 343164, 781380, 52269]]
columns = ('Freeze', 'Wind', 'Flood', 'Quake', 'Hail')
rows = [('%d year' % x) for x in (100, 50, 20, 10, 5)]
tempResult = arange(0, 2500, 500)
	
===================================================================	
module: 12	
----------------------------	

'\nDemo of table function to display a table within a plot.\n'
import numpy as np
import matplotlib.pyplot as plt
data = [[66386, 174296, 75131, 577908, 32015], [58230, 381139, 78045, 99308, 160454], [89135, 80552, 152558, 497981, 603535], [78415, 81858, 150656, 193263, 69638], [139361, 331509, 343164, 781380, 52269]]
columns = ('Freeze', 'Wind', 'Flood', 'Quake', 'Hail')
rows = [('%d year' % x) for x in (100, 50, 20, 10, 5)]
values = numpy.arange(0, 2500, 500)
value_increment = 1000
colors = matplotlib.pyplot.cm.BuPu(numpy.linspace(0, 0.5, len(rows)))
n_rows = len(data)
tempResult = arange(len(columns))
	
===================================================================	
module: 10	
----------------------------	

"\nControlling the properties of axis text using handles\n\nSee examples/text_themes.py for a more elegant, pythonic way to control\nfonts.  After all, if we were slaves to MATLAB , we wouldn't be\nusing python!\n"
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.sin(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
tempResult = arange(0.0, 5.0, 0.1)
	
===================================================================	
module: 11	
----------------------------	

"\nControlling the properties of axis text using handles\n\nSee examples/text_themes.py for a more elegant, pythonic way to control\nfonts.  After all, if we were slaves to MATLAB , we wouldn't be\nusing python!\n"
import matplotlib.pyplot as plt
import numpy as np

def f(t):
    s1 = numpy.sin(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
tempResult = arange(0.0, 5.0, 0.02)
	
===================================================================	
module: 18	
----------------------------	

"\nThe way matplotlib does text layout is counter-intuitive to some, so\nthis example is designed to make it a little clearer.  The text is\naligned by it's bounding box (the rectangular box that surrounds the\nink rectangle).  The order of operations is basically rotation then\nalignment, rather than alignment then rotation.  Basically, the text\nis centered at your x,y location, rotated around this point, and then\naligned according to the bounding box of the rotated text.\n\nSo if you specify left, bottom alignment, the bottom left of the\nbounding box of the rotated text will be at the x,y coordinate of the\ntext.\n\nBut a picture is worth a thousand words!\n"
import matplotlib.pyplot as plt
import numpy as np

def addtext(props):
    matplotlib.pyplot.text(0.5, 0.5, 'text 0', props, rotation=0)
    matplotlib.pyplot.text(1.5, 0.5, 'text 45', props, rotation=45)
    matplotlib.pyplot.text(2.5, 0.5, 'text 135', props, rotation=135)
    matplotlib.pyplot.text(3.5, 0.5, 'text 225', props, rotation=225)
    matplotlib.pyplot.text(4.5, 0.5, 'text -45', props, rotation=(- 45))
    matplotlib.pyplot.yticks([0, 0.5, 1])
    matplotlib.pyplot.grid(True)
bbox = {'fc': '0.8', 'pad': 0}
matplotlib.pyplot.subplot(211)
addtext({'ha': 'center', 'va': 'center', 'bbox': bbox})
matplotlib.pyplot.xlim(0, 5)
tempResult = arange(0, 5.1, 0.5)
	
===================================================================	
module: 23	
----------------------------	

"\nThe way matplotlib does text layout is counter-intuitive to some, so\nthis example is designed to make it a little clearer.  The text is\naligned by it's bounding box (the rectangular box that surrounds the\nink rectangle).  The order of operations is basically rotation then\nalignment, rather than alignment then rotation.  Basically, the text\nis centered at your x,y location, rotated around this point, and then\naligned according to the bounding box of the rotated text.\n\nSo if you specify left, bottom alignment, the bottom left of the\nbounding box of the rotated text will be at the x,y coordinate of the\ntext.\n\nBut a picture is worth a thousand words!\n"
import matplotlib.pyplot as plt
import numpy as np

def addtext(props):
    matplotlib.pyplot.text(0.5, 0.5, 'text 0', props, rotation=0)
    matplotlib.pyplot.text(1.5, 0.5, 'text 45', props, rotation=45)
    matplotlib.pyplot.text(2.5, 0.5, 'text 135', props, rotation=135)
    matplotlib.pyplot.text(3.5, 0.5, 'text 225', props, rotation=225)
    matplotlib.pyplot.text(4.5, 0.5, 'text -45', props, rotation=(- 45))
    matplotlib.pyplot.yticks([0, 0.5, 1])
    matplotlib.pyplot.grid(True)
bbox = {'fc': '0.8', 'pad': 0}
matplotlib.pyplot.subplot(211)
addtext({'ha': 'center', 'va': 'center', 'bbox': bbox})
matplotlib.pyplot.xlim(0, 5)
matplotlib.pyplot.xticks(numpy.arange(0, 5.1, 0.5), [])
matplotlib.pyplot.ylabel('center / center')
matplotlib.pyplot.subplot(212)
addtext({'ha': 'left', 'va': 'bottom', 'bbox': bbox})
matplotlib.pyplot.xlim(0, 5)
tempResult = arange(0, 5.1, 0.5)
	
===================================================================	
module: 5	
----------------------------	

"\nText objects in matplotlib are normally rotated with respect to the\nscreen coordinate system (i.e., 45 degrees rotation plots text along a\nline that is in between horizontal and vertical no matter how the axes\nare changed).  However, at times one wants to rotate text with respect\nto something on the plot.  In this case, the correct angle won't be\nthe angle of that object in the plot coordinate system, but the angle\nthat that object APPEARS in the screen coordinate system.  This angle\nis found by transforming the angle from the plot to the screen\ncoordinate system, as shown in the example below.\n"
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0, 10)
	
===================================================================	
module: 5	
----------------------------	

"\nText objects in matplotlib are normally rotated with respect to the\nscreen coordinate system (i.e., 45 degrees rotation plots text along a\nline that is in between horizontal and vertical no matter how the axes\nare changed).  However, at times one wants to rotate text with respect\nto something on the plot.  In this case, the correct angle won't be\nthe angle of that object in the plot coordinate system, but the angle\nthat that object APPEARS in the screen coordinate system.  This angle\nis found by transforming the angle from the plot to the screen\ncoordinate system, as shown in the example below.\n"
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0, 10)
	
===================================================================	
module: 11	
----------------------------	

'\nThis demo is tex_demo.py modified to have unicode. See that file for\nmore information.\n'
from __future__ import unicode_literals
import numpy as np
import matplotlib
matplotlib.rcParams['text.usetex'] = True
matplotlib.rcParams['text.latex.unicode'] = True
import matplotlib.pyplot as plt
matplotlib.pyplot.figure(1, figsize=(6, 4))
ax = matplotlib.pyplot.axes([0.1, 0.1, 0.8, 0.7])
tempResult = arange(0.0, (1.0 + 0.01), 0.01)
	
===================================================================	
module: 7	
----------------------------	

'\nThis illustrates the use of transforms.offset_copy to\nmake a transform that positions a drawing element such as\na text string at a specified offset in screen coordinates\n(dots or inches) relative to a location given in any\ncoordinates.\n\nEvery Artist--the mpl class from which classes such as\nText and Line are derived--has a transform that can be\nset when the Artist is created, such as by the corresponding\npyplot command.  By default this is usually the Axes.transData\ntransform, going from data units to screen dots.  We can\nuse the offset_copy function to make a modified copy of\nthis transform, where the modification consists of an\noffset.\n'
import matplotlib.pyplot as plt
import matplotlib.transforms as mtrans
import numpy as np
from matplotlib.transforms import offset_copy
tempResult = arange(7)
	
===================================================================	
module: 42	
----------------------------	

'\nDemonstrates high-resolution tricontouring of a random set of points ;\na matplotlib.tri.TriAnalyzer is used to improve the plot quality.\n\nThe initial data points and triangular grid for this demo are:\n    - a set of random points is instantiated, inside [-1, 1] x [-1, 1] square\n    - A Delaunay triangulation of these points is then computed, of which a\n    random subset of triangles is masked out by the user (based on\n    *init_mask_frac* parameter). This simulates invalidated data.\n\nThe proposed generic procedure to obtain a high resolution contouring of such\na data set is the following:\n    1) Compute an extended mask with a matplotlib.tri.TriAnalyzer, which will\n    exclude badly shaped (flat) triangles from the border of the\n    triangulation. Apply the mask to the triangulation (using set_mask).\n    2) Refine and interpolate the data using a\n    matplotlib.tri.UniformTriRefiner.\n    3) Plot the refined data with tricontour.\n\n'
from matplotlib.tri import Triangulation, TriAnalyzer, UniformTriRefiner
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

def experiment_res(x, y):
    ' An analytic function representing experiment results '
    x = (2.0 * x)
    r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
    theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
    r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
    theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
    z = (((((4 * (numpy.exp(((r1 / 10) ** 2)) - 1)) * 30.0) * numpy.cos((3 * theta1))) + (((numpy.exp(((r2 / 10) ** 2)) - 1) * 30.0) * numpy.cos((5 * theta2)))) + (2 * ((x ** 2) + (y ** 2))))
    return ((numpy.max(z) - z) / (numpy.max(z) - numpy.min(z)))
n_test = 200
subdiv = 3
init_mask_frac = 0.0
min_circle_ratio = 0.01
random_gen = numpy.random.mtrand.RandomState(seed=127260)
x_test = random_gen.uniform((- 1.0), 1.0, size=n_test)
y_test = random_gen.uniform((- 1.0), 1.0, size=n_test)
z_test = experiment_res(x_test, y_test)
tri = Triangulation(x_test, y_test)
ntri = tri.triangles.shape[0]
mask_init = numpy.zeros(ntri, dtype=numpy.bool)
masked_tri = random_gen.randint(0, ntri, int((ntri * init_mask_frac)))
mask_init[masked_tri] = True
tri.set_mask(mask_init)
mask = TriAnalyzer(tri).get_flat_tri_mask(min_circle_ratio)
tri.set_mask(mask)
refiner = UniformTriRefiner(tri)
(tri_refi, z_test_refi) = refiner.refine_field(z_test, subdiv=subdiv)
z_expected = experiment_res(tri_refi.x, tri_refi.y)
flat_tri = Triangulation(x_test, y_test)
flat_tri.set_mask((~ mask))
plot_tri = True
plot_masked_tri = True
plot_refi_tri = False
plot_expected = False
tempResult = arange(0.0, 1.0, 0.025)
	
===================================================================	
module: 37	
----------------------------	

'\nDemonstrates high-resolution tricontouring on user-defined triangular grids\nwith matplotlib.tri.UniformTriRefiner\n'
import matplotlib.tri as tri
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
import math

def function_z(x, y):
    ' A function of 2 variables '
    r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
    theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
    r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
    theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
    z = (- (((((2 * (numpy.exp(((r1 / 10) ** 2)) - 1)) * 30.0) * numpy.cos((7.0 * theta1))) + (((numpy.exp(((r2 / 10) ** 2)) - 1) * 30.0) * numpy.cos((11.0 * theta2)))) + (0.7 * ((x ** 2) + (y ** 2)))))
    return ((numpy.max(z) - z) / (numpy.max(z) - numpy.min(z)))
n_angles = 20
n_radii = 10
min_radius = 0.15
radii = numpy.linspace(min_radius, 0.95, n_radii)
angles = numpy.linspace(0, (2 * math.pi), n_angles, endpoint=False)
angles = numpy.repeat(angles[(..., numpy.newaxis)], n_radii, axis=1)
angles[:, 1::2] += (math.pi / n_angles)
x = (radii * np.cos(angles)).flatten()
y = (radii * np.sin(angles)).flatten()
z = function_z(x, y)
triang = matplotlib.tri.Triangulation(x, y)
xmid = x[triang.triangles].mean(axis=1)
ymid = y[triang.triangles].mean(axis=1)
mask = numpy.where((((xmid * xmid) + (ymid * ymid)) < (min_radius * min_radius)), 1, 0)
triang.set_mask(mask)
refiner = matplotlib.tri.UniformTriRefiner(triang)
(tri_refi, z_test_refi) = refiner.refine_field(z, subdiv=3)
matplotlib.pyplot.figure()
plt.gca().set_aspect('equal')
matplotlib.pyplot.triplot(triang, lw=0.5, color='white')
tempResult = arange(0.0, 1.0, 0.025)
	
===================================================================	
module: 40	
----------------------------	

'\nDemonstrates computation of gradient with matplotlib.tri.CubicTriInterpolator.\n'
from matplotlib.tri import Triangulation, UniformTriRefiner, CubicTriInterpolator
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np
import math

def dipole_potential(x, y):
    ' The electric dipole potential V '
    r_sq = ((x ** 2) + (y ** 2))
    theta = numpy.arctan2(y, x)
    z = (numpy.cos(theta) / r_sq)
    return ((numpy.max(z) - z) / (numpy.max(z) - numpy.min(z)))
n_angles = 30
n_radii = 10
min_radius = 0.2
radii = numpy.linspace(min_radius, 0.95, n_radii)
angles = numpy.linspace(0, (2 * math.pi), n_angles, endpoint=False)
angles = numpy.repeat(angles[(..., numpy.newaxis)], n_radii, axis=1)
angles[:, 1::2] += (math.pi / n_angles)
x = (radii * np.cos(angles)).flatten()
y = (radii * np.sin(angles)).flatten()
V = dipole_potential(x, y)
triang = Triangulation(x, y)
xmid = x[triang.triangles].mean(axis=1)
ymid = y[triang.triangles].mean(axis=1)
mask = numpy.where((((xmid * xmid) + (ymid * ymid)) < (min_radius * min_radius)), 1, 0)
triang.set_mask(mask)
refiner = UniformTriRefiner(triang)
(tri_refi, z_test_refi) = refiner.refine_field(V, subdiv=3)
tci = CubicTriInterpolator(triang, (- V))
(Ex, Ey) = tci.gradient(triang.x, triang.y)
E_norm = numpy.sqrt(((Ex ** 2) + (Ey ** 2)))
(fig, ax) = matplotlib.pyplot.subplots()
ax.set_aspect('equal')
ax.use_sticky_edges = False
ax.margins(0.07)
ax.triplot(triang, color='0.8')
tempResult = arange(0.0, 1.0, 0.01)
	
===================================================================	
module: 11	
----------------------------	

'\nSmall demonstration of the hlines and vlines plots.\n'
import matplotlib.pyplot as plt
import numpy as np
import numpy.random as rnd

def f(t):
    s1 = numpy.sin(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return (numpy.absolute((s1 * e1)) + 0.05)
tempResult = arange(0.0, 5.0, 0.1)
	
===================================================================	
module: 4	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0, 10, 0.005)
	
===================================================================	
module: 6	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(0.0, 5.0, 0.01)
	
===================================================================	
module: 6	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, polar=True)
tempResult = arange(0, 1, 0.001)
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
fig.subplots_adjust(top=0.8)
ax1 = fig.add_subplot(211)
ax1.set_ylabel('volts')
ax1.set_title('a sine wave')
tempResult = arange(0.0, 1.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
ax = matplotlib.pyplot.subplot(111)
tempResult = arange(0.0, 5.0, 0.01)
	
===================================================================	
module: 4	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
numpy.random.seed(19680801)
y = numpy.random.normal(loc=0.5, scale=0.4, size=1000)
y = y[((y > 0) & (y < 1))]
y.sort()
tempResult = arange(len(y))
	
===================================================================	
module: 4	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.0, 5.0, 0.2)
	
===================================================================	
module: 7	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt

def f(t):
    return (numpy.exp((- t)) * numpy.cos(((2 * numpy.pi) * t)))
tempResult = arange(0.0, 5.0, 0.1)
	
===================================================================	
module: 8	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt

def f(t):
    return (numpy.exp((- t)) * numpy.cos(((2 * numpy.pi) * t)))
t1 = numpy.arange(0.0, 5.0, 0.1)
tempResult = arange(0.0, 5.0, 0.02)
	
===================================================================	
module: 5	
----------------------------	

'\nDemo of TeX rendering.\n\nYou can use TeX to render all of your matplotlib text if the rc\nparameter text.usetex is set.  This works currently on the agg and ps\nbackends, and requires that you have tex and the other dependencies\ndescribed at http://matplotlib.org/users/usetex.html\nproperly installed on your system.  The first time you run a script\nyou will see a lot of output from tex and associated tools.  The next\ntime, the run may be silent, as a lot of the information is cached in\n~/.tex.cache\n\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.0, (1.0 + 0.01), 0.01)
	
===================================================================	
module: 8	
----------------------------	

from mpl_toolkits.mplot3d.axes3d import Axes3D
from matplotlib import cm
import matplotlib.pyplot as plt
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(1, 2, 1, projection='3d')
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 9	
----------------------------	

from mpl_toolkits.mplot3d.axes3d import Axes3D
from matplotlib import cm
import matplotlib.pyplot as plt
import numpy as np
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(1, 2, 1, projection='3d')
X = numpy.arange((- 5), 5, 0.25)
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 5	
----------------------------	

import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import numpy as np
tempResult = arange(0.0, 2, 0.01)
	
===================================================================	
module: 6	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
ax = matplotlib.pyplot.subplot(111)
tempResult = arange(0.0, 1.0, 0.01)
	
===================================================================	
get_demo_image: 9	
----------------------------	

delta = 0.5
extent = ((- 3), 4, (- 4), 3)
tempResult = arange((- 3.0), 4.001, delta)
	
===================================================================	
get_demo_image: 10	
----------------------------	

delta = 0.5
extent = ((- 3), 4, (- 4), 3)
x = numpy.arange((- 3.0), 4.001, delta)
tempResult = arange((- 4.0), 3.001, delta)
	
===================================================================	
module: 7	
----------------------------	

import random
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 8	
----------------------------	

import random
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D
X = numpy.arange((- 5), 5, 0.25)
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
module: 10	
----------------------------	

'\nIllustrate the scale transformations applied to axes, e.g. log, symlog, logit.\n'
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
numpy.random.seed(1)
y = numpy.random.normal(loc=0.5, scale=0.4, size=1000)
y = y[((y > 0) & (y < 1))]
y.sort()
tempResult = arange(len(y))
	
===================================================================	
module: 13	
----------------------------	

import matplotlib.pyplot as plt
import numpy as np
with matplotlib.pyplot.xkcd():
    fig = matplotlib.pyplot.figure()
    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))
    ax.spines['right'].set_color('none')
    ax.spines['top'].set_color('none')
    matplotlib.pyplot.xticks([])
    matplotlib.pyplot.yticks([])
    ax.set_ylim([(- 30), 10])
    data = numpy.ones(100)
    tempResult = arange(30)
	
===================================================================	
module: 35	
----------------------------	

'\n=================================\nDemo of violin plot customization\n=================================\n\nThis example demonstrates how to fully customize violin plots.\nThe first plot shows the default style by providing only\nthe data. The second plot first limits what matplotlib draws\nwith additional kwargs. Then a simplified representation of\na box plot is drawn on top. Lastly, the styles of the artists\nof the violins are modified.\n\nFor more information on violin plots, the scikit-learn docs have a great\nsection: http://scikit-learn.org/stable/modules/density.html\n'
import matplotlib.pyplot as plt
import numpy as np

def adjacent_values(vals, q1, q3):
    upper_adjacent_value = (q3 + ((q3 - q1) * 1.5))
    upper_adjacent_value = numpy.clip(upper_adjacent_value, q3, vals[(- 1)])
    lower_adjacent_value = (q1 - ((q3 - q1) * 1.5))
    lower_adjacent_value = numpy.clip(lower_adjacent_value, vals[0], q1)
    return (lower_adjacent_value, upper_adjacent_value)

def set_axis_style(ax, labels):
    ax.get_xaxis().set_tick_params(direction='out')
    ax.xaxis.set_ticks_position('bottom')
    ax.set_xticks(numpy.arange(1, (len(labels) + 1)))
    ax.set_xticklabels(labels)
    ax.set_xlim(0.25, (len(labels) + 0.75))
    ax.set_xlabel('Sample name')
numpy.random.seed(123)
data = [sorted(numpy.random.normal(0, std, 100)) for std in range(1, 5)]
(fig, (ax1, ax2)) = matplotlib.pyplot.subplots(nrows=1, ncols=2, figsize=(9, 4), sharey=True)
ax1.set_title('Default violin plot')
ax1.set_ylabel('Observed values')
ax1.violinplot(data)
ax2.set_title('Customized violin plot')
parts = ax2.violinplot(data, showmeans=False, showmedians=False, showextrema=False)
for pc in parts['bodies']:
    pc.set_facecolor('#D43F3A')
    pc.set_edgecolor('black')
    pc.set_alpha(1)
(quartile1, medians, quartile3) = numpy.percentile(data, [25, 50, 75], axis=1)
whiskers = numpy.array([adjacent_values(sorted_array, q1, q3) for (sorted_array, q1, q3) in zip(data, quartile1, quartile3)])
(whiskersMin, whiskersMax) = (whiskers[:, 0], whiskers[:, 1])
tempResult = arange(1, (len(medians) + 1))
	
===================================================================	
set_axis_style: 16	
----------------------------	

ax.get_xaxis().set_tick_params(direction='out')
ax.xaxis.set_ticks_position('bottom')
tempResult = arange(1, (len(labels) + 1))
	
===================================================================	
module: 9	
----------------------------	

'\n============================================================\nDemo on creating boxes from error bars using PatchCollection\n============================================================\n\nIn this example, we snazz up a pretty standard error bar plot by adding\na rectangle patch defined by the limits of the bars in both the x- and\ny- directions. To do this, we have to write our own custom function\ncalled ``make_error_boxes``. Close inspection of this function will\nreveal the preferred pattern in writing functions for matplotlib:\n\n  1. an ``Axes`` object is passed directly to the function\n  2. the function operates on the `Axes` methods directly, not through\n     the ``pyplot`` interface\n  3. plotting kwargs that could be abbreviated are spelled out for\n     better code readability in the future (for example we use\n     ``facecolor`` instead of ``fc``)\n  4. the artists returned by the ``Axes`` plotting methods are then\n     returned by the function so that, if desired, their styles\n     can be modified later outside of the function (they are not\n     modified in this example).\n'
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.collections import PatchCollection
from matplotlib.patches import Rectangle
n = 5
numpy.random.seed(10)
tempResult = arange(0, n, 1)
	
===================================================================	
module: 5	
----------------------------	

'\n=============================\nDemo of the errorbar function\n=============================\n\nThis exhibits the most basic use of the error bar method.\nIn this case, constant values are provided for the error\nin both the x- and y-directions.\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.1, 4, 0.5)
	
===================================================================	
module: 5	
----------------------------	

'\n===================================================\nDemo of the different ways of specifying error bars\n===================================================\n\nErrors can be specified as a constant value (as shown in\n`errorbar_demo.py`). However, this example demonstrates\nhow they vary by specifying arrays of error values.\n\nIf the raw ``x`` and ``y`` data have length N, there are two options:\n\nArray of shape (N,):\n    Error varies for each point, but the error values are\n    symmetric (i.e. the lower and upper values are equal).\n\nArray of shape (2, N):\n    Error varies for each point, and the lower and upper limits\n    (in that order) are different (asymmetric case)\n\nIn addition, this example demonstrates how to use log\nscale with error bars.\n'
import numpy as np
import matplotlib.pyplot as plt
tempResult = arange(0.1, 4, 0.5)
	
===================================================================	
module: 13	
----------------------------	

"\n=======================================================\nDemo of how to produce multiple histograms side by side\n=======================================================\n\nThis example plots horizontal histograms of different samples along\na categorical x-axis. Additionally, the histograms are plotted to\nbe symmetrical about their x-position, thus making them very similar\nto violin plots.\n\nTo make this highly specialized plot, we can't use the standard ``hist``\nmethod. Instead we use ``barh`` to draw the horizontal bars directly. The\nvertical positions and lengths of the bars are computed via the\n``np.histogram`` function. The histograms for all the samples are\ncomputed using the same range (min and max values) and number of bins,\nso that the bins for each sample are in the same vertical positions.\n\nSelecting different bin counts and sizes can significantly affect the\nshape of a histogram. The Astropy docs have a great section on how to\nselect these parameters:\nhttp://docs.astropy.org/en/stable/visualization/histogram.html\n"
import numpy as np
import matplotlib.pyplot as plt
numpy.random.seed(0)
number_of_bins = 20
number_of_data_points = 387
labels = ['A', 'B', 'C']
data_sets = [numpy.random.normal(0, 1, number_of_data_points), numpy.random.normal(6, 1, number_of_data_points), numpy.random.normal((- 3), 1, number_of_data_points)]
hist_range = (numpy.min(data_sets), numpy.max(data_sets))
binned_data_sets = [numpy.histogram(d, range=hist_range, bins=number_of_bins)[0] for d in data_sets]
binned_maximums = numpy.max(binned_data_sets, axis=1)
tempResult = arange(0, sum(binned_maximums), numpy.max(binned_maximums))
	
===================================================================	
module: 17	
----------------------------	

'\nThis example demonstrates the "ggplot" style, which adjusts the style to\nemulate ggplot_ (a popular plotting package for R_).\n\nThese settings were shamelessly stolen from [1]_ (with permission).\n\n.. [1] http://www.huyng.com/posts/sane-color-scheme-for-matplotlib/\n\n.. _ggplot: http://ggplot2.org/\n.. _R: https://www.r-project.org/\n\n'
import numpy as np
import matplotlib.pyplot as plt
matplotlib.pyplot.style.use('ggplot')
(fig, axes) = matplotlib.pyplot.subplots(ncols=2, nrows=2)
(ax1, ax2, ax3, ax4) = axes.ravel()
(x, y) = numpy.random.normal(size=(2, 200))
ax1.plot(x, y, 'o')
L = (2 * numpy.pi)
x = numpy.linspace(0, L)
ncolors = len(matplotlib.pyplot.rcParams['axes.prop_cycle'])
shift = numpy.linspace(0, L, ncolors, endpoint=False)
for s in shift:
    ax2.plot(x, numpy.sin((x + s)), '-')
ax2.margins(0)
tempResult = arange(5)
	
===================================================================	
plot_bar_graphs: 27	
----------------------------	

'Plot two bar graphs side by side, with letters as x-tick labels.\n    '
tempResult = arange(nb_samples)
	
===================================================================	
module: 33	
----------------------------	

'\nShow the different tick formatters\n'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

def setup(ax):
    ax.spines['right'].set_color('none')
    ax.spines['left'].set_color('none')
    ax.yaxis.set_major_locator(matplotlib.ticker.NullLocator())
    ax.spines['top'].set_color('none')
    ax.xaxis.set_ticks_position('bottom')
    ax.tick_params(which='major', width=1.0, length=5)
    ax.tick_params(which='minor', width=0.75, length=2.5, labelsize=10)
    ax.set_xlim(0, 5)
    ax.set_ylim(0, 1)
    ax.patch.set_alpha(0.0)
matplotlib.pyplot.figure(figsize=(8, 5))
n = 6
ax = matplotlib.pyplot.subplot(n, 1, 1)
setup(ax)
ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1.0))
ax.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(0.25))
ax.xaxis.set_major_formatter(matplotlib.ticker.NullFormatter())
ax.xaxis.set_minor_formatter(matplotlib.ticker.NullFormatter())
ax.text(0.0, 0.1, 'NullFormatter()', fontsize=16, transform=ax.transAxes)
ax = matplotlib.pyplot.subplot(n, 1, 2)
setup(ax)
ax.xaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1.0))
ax.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(0.25))
majors = ['', '0', '1', '2', '3', '4', '5']
ax.xaxis.set_major_formatter(matplotlib.ticker.FixedFormatter(majors))
tempResult = arange(0, 5, 0.25)
	
===================================================================	
module: 6	
----------------------------	

'\nplot using a variety of cm vs inches conversions.  The example shows\nhow default unit instrospection works (ax1), how various keywords can\nbe used to set the x and y units to override the defaults (ax2, ax3,\nax4) and how one can set the xlimits using scalars (ax3, current units\nassumed) or units (conversions applied to get the numbers to current\nunits)\n\n'
import numpy as np
from basic_units import cm, inch
import matplotlib.pyplot as plt
tempResult = arange(0, 10, 2)
	
===================================================================	
module: 9	
----------------------------	

import numpy as np
from basic_units import cm, inch
import matplotlib.pyplot as plt
N = 5
menMeans = ((150 * cm), (160 * cm), (146 * cm), (172 * cm), (155 * cm))
menStd = ((20 * cm), (30 * cm), (32 * cm), (10 * cm), (20 * cm))
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(N)
	
===================================================================	
module: 10	
----------------------------	

'\nCompare the ellipse generated with arcs versus a polygonal approximation\n'
from basic_units import cm
import numpy as np
from matplotlib import patches
import matplotlib.pyplot as plt
(xcenter, ycenter) = ((0.38 * cm), (0.52 * cm))
(width, height) = ((0.1 * cm), (0.3 * cm))
angle = (- 30)
tempResult = arange(0.0, 360.0, 1.0)
	
===================================================================	
module: 6	
----------------------------	

'\nPlot with radians from the basic_units mockup example package\nThis example shows how the unit class can determine the tick locating,\nformatting and axis labeling.\n'
import numpy as np
from basic_units import radians, degrees, cos
from matplotlib.pyplot import figure, show
tempResult = arange(0, 15, 0.01)
	
===================================================================	
module: 6	
----------------------------	

'\nplot using a variety of cm vs inches conversions.  The example shows\nhow default unit introspection works (ax1), how various keywords can\nbe used to set the x and y units to override the defaults (ax2, ax3,\nax4) and how one can set the xlimits using scalars (ax3, current units\nassumed) or units (conversions applied to get the numbers to current\nunits)\n\n'
from basic_units import cm, inch
import matplotlib.pyplot as plt
import numpy
tempResult = arange(0, 10, 2)
	
===================================================================	
AppForm.get_data2: 41	
----------------------------	

tempResult = arange(20)
	
===================================================================	
PlotPanel.init_plot_data: 36	
----------------------------	

a = self.fig.add_subplot(111)
tempResult = arange(120.0)
	
===================================================================	
PlotPanel.init_plot_data: 37	
----------------------------	

a = self.fig.add_subplot(111)
x = (((numpy.arange(120.0) * 2) * numpy.pi) / 60.0)
tempResult = arange(100.0)
	
===================================================================	
create_figure: 21	
----------------------------	

'\n    Creates a simple example figure.\n    '
fig = Figure()
a = fig.add_subplot(111)
tempResult = arange(0.0, 3.0, 0.01)
	
===================================================================	
FourierDemoWindow.compute: 163	
----------------------------	

tempResult = arange((- 6.0), 6.0, 0.02)
	
===================================================================	
FourierDemoWindow.compute: 164	
----------------------------	

f = numpy.arange((- 6.0), 6.0, 0.02)
tempResult = arange((- 2.0), 2.0, 0.01)
	
===================================================================	
module: 12	
----------------------------	

import matplotlib
matplotlib.use('GTKAgg')
from matplotlib.backends.backend_gtk import DialogLineprops
import numpy as np
import matplotlib.pyplot as plt

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
tempResult = arange(0.0, 5.0, 0.1)
	
===================================================================	
module: 13	
----------------------------	

import matplotlib
matplotlib.use('GTKAgg')
from matplotlib.backends.backend_gtk import DialogLineprops
import numpy as np
import matplotlib.pyplot as plt

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
tempResult = arange(0.0, 5.0, 0.02)
	
===================================================================	
module: 14	
----------------------------	

import matplotlib
matplotlib.use('GTKAgg')
from matplotlib.backends.backend_gtk import DialogLineprops
import numpy as np
import matplotlib.pyplot as plt

def f(t):
    s1 = numpy.cos(((2 * numpy.pi) * t))
    e1 = numpy.exp((- t))
    return numpy.multiply(s1, e1)
t1 = numpy.arange(0.0, 5.0, 0.1)
t2 = numpy.arange(0.0, 5.0, 0.02)
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Button
tempResult = arange(2, 20, 3)
	
===================================================================	
module: 8	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Button
freqs = numpy.arange(2, 20, 3)
(fig, ax) = matplotlib.pyplot.subplots()
matplotlib.pyplot.subplots_adjust(bottom=0.2)
tempResult = arange(0.0, 1.0, 0.001)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import CheckButtons
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import MultiCursor
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 5	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import RadioButtons
tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
module: 7	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import Slider, Button, RadioButtons
(fig, ax) = matplotlib.pyplot.subplots()
matplotlib.pyplot.subplots_adjust(left=0.25, bottom=0.25)
tempResult = arange(0.0, 1.0, 0.001)
	
===================================================================	
module: 8	
----------------------------	

'\nThe SpanSelector is a mouse widget to select a xmin/xmax range and plot the\ndetail view of the selected region in the lower axes\n'
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.widgets import SpanSelector
fig = matplotlib.pyplot.figure(figsize=(8, 6))
ax = fig.add_subplot(211, facecolor='#FFFFCC')
tempResult = arange(0.0, 5.0, 0.01)
	
===================================================================	
BezierSegment.__init__: 87	
----------------------------	

'\n        *control_points* : location of contol points. It needs have a\n         shpae of n * 2, where n is the order of the bezier line. 1<=\n         n <= 3 is supported.\n        '
_o = len(control_points)
tempResult = arange(_o)
	
===================================================================	
index_of: 1478	
----------------------------	

'\n    A helper function to get the index of an input to plot\n    against if x values are not explicitly given.\n\n    Tries to get `y.index` (works if this is a pd.Series), if that\n    fails, return np.arange(y.shape[0]).\n\n    This will be extended in the future to deal with more types of\n    labeled data.\n\n    Parameters\n    ----------\n    y : scalar or array-like\n        The proposed y-value\n\n    Returns\n    -------\n    x, y : ndarray\n       The x and y values to plot.\n    '
try:
    return (y.index.values, y.values)
except AttributeError:
    y = numpy.atleast_1d(y)
    tempResult = arange(y.shape[0], dtype=float)
	
===================================================================	
MemoryMonitor.xy: 1009	
----------------------------	

tempResult = arange(i0, self._n, isub)
	
===================================================================	
unmasked_index_ranges: 1295	
----------------------------	

'\n    Find index ranges where *mask* is *False*.\n\n    *mask* will be flattened if it is not already 1-D.\n\n    Returns Nx2 :class:`numpy.ndarray` with each row the start and stop\n    indices for slices of the compressed :class:`numpy.ndarray`\n    corresponding to each of *N* uninterrupted runs of unmasked\n    values.  If optional argument *compressed* is *False*, it returns\n    the start and stop indices into the original :class:`numpy.ndarray`,\n    not the compressed :class:`numpy.ndarray`.  Returns *None* if there\n    are no unmasked values.\n\n    Example::\n\n      y = ma.array(np.arange(5), mask = [0,0,1,0,0])\n      ii = unmasked_index_ranges(ma.getmaskarray(y))\n      # returns array [[0,2,] [2,4,]]\n\n      y.compressed()[ii[1,0]:ii[1,1]]\n      # returns array [3,4,]\n\n      ii = unmasked_index_ranges(ma.getmaskarray(y), compressed=False)\n      # returns array [[0, 2], [3, 5]]\n\n      y.filled()[ii[1,0]:ii[1,1]]\n      # returns array [3,4,]\n\n    Prior to the transforms refactoring, this was used to support\n    masked arrays in Line2D.\n    '
mask = mask.reshape(mask.size)
m = numpy.concatenate(((1,), mask, (1,)))
tempResult = arange((len(mask) + 1))
	
===================================================================	
ColorbarBase._ticker: 268	
----------------------------	

'\n        Return the sequence of ticks (colorbar data locations),\n        ticklabels (strings), and the corresponding offset string.\n        '
locator = self.locator
formatter = self.formatter
if (locator is None):
    if (self.boundaries is None):
        if isinstance(self.norm, matplotlib.colors.NoNorm):
            nv = len(self._values)
            base = (1 + int((nv / 10)))
            locator = matplotlib.ticker.IndexLocator(base=base, offset=0)
        elif isinstance(self.norm, matplotlib.colors.BoundaryNorm):
            b = self.norm.boundaries
            locator = matplotlib.ticker.FixedLocator(b, nbins=10)
        elif isinstance(self.norm, matplotlib.colors.LogNorm):
            locator = matplotlib.ticker.LogLocator(subs='all')
        elif isinstance(self.norm, matplotlib.colors.SymLogNorm):
            tempResult = arange(1, 10)
	
===================================================================	
ColorbarBase._process_values: 326	
----------------------------	

'\n        Set the :attr:`_boundaries` and :attr:`_values` attributes\n        based on the input boundaries and values.  Input boundaries\n        can be *self.boundaries* or the argument *b*.\n        '
if (b is None):
    b = self.boundaries
if (b is not None):
    self._boundaries = numpy.asarray(b, dtype=float)
    if (self.values is None):
        self._values = (0.5 * (self._boundaries[:(- 1)] + self._boundaries[1:]))
        if isinstance(self.norm, matplotlib.colors.NoNorm):
            self._values = (self._values + 1e-05).astype(numpy.int16)
        return
    self._values = numpy.array(self.values)
    return
if (self.values is not None):
    self._values = numpy.array(self.values)
    if (self.boundaries is None):
        b = numpy.zeros((len(self.values) + 1), 'd')
        b[1:(- 1)] = (0.5 * (self._values[:(- 1)] - self._values[1:]))
        b[0] = ((2.0 * b[1]) - b[2])
        b[(- 1)] = ((2.0 * b[(- 2)]) - b[(- 3)])
        self._boundaries = b
        return
    self._boundaries = numpy.array(self.boundaries)
    return
if isinstance(self.norm, matplotlib.colors.NoNorm):
    b = ((self._uniform_y((self.cmap.N + 1)) * self.cmap.N) - 0.5)
    v = numpy.zeros(((len(b) - 1),), dtype=numpy.int16)
    tempResult = arange(self.cmap.N, dtype=numpy.int16)
	
===================================================================	
ContourLabeler.calc_label_rot_and_inline: 238	
----------------------------	

'\n        This function calculates the appropriate label rotation given\n        the linecontour coordinates in screen units, the index of the\n        label location and the label width.\n\n        It will also break contour and calculate inlining if *lc* is\n        not empty (lc defaults to the empty list if None).  *spacing*\n        is the space around the label in pixels to leave empty.\n\n        Do both of these tasks at once to avoid calling mlab.path_length\n        multiple times, which is relatively costly.\n\n        The method used here involves calculating the path length\n        along the contour in pixel coordinates and then looking\n        approximately label width / 2 away from central point to\n        determine rotation and then to break contour if desired.\n        '
if (lc is None):
    lc = []
hlw = (lw / 2.0)
closed = matplotlib.mlab.is_closed_polygon(slc)
if closed:
    slc = numpy.r_[(slc[ind:(- 1)], slc[:(ind + 1)])]
    if len(lc):
        lc = numpy.r_[(lc[ind:(- 1)], lc[:(ind + 1)])]
    ind = 0
pl = matplotlib.mlab.path_length(slc)
pl = (pl - pl[ind])
xi = numpy.array([(- hlw), hlw])
if closed:
    dp = numpy.array([pl[(- 1)], 0])
else:
    dp = numpy.zeros_like(xi)
ll = matplotlib.mlab.less_simple_linear_interpolation(pl, slc, (dp + xi), extrap=True)
dd = np.diff(ll, axis=0).ravel()
if numpy.all((dd == 0)):
    rotation = 0.0
else:
    rotation = ((numpy.arctan2(dd[1], dd[0]) * 180.0) / numpy.pi)
if self.rightside_up:
    if (rotation > 90):
        rotation = (rotation - 180.0)
    if (rotation < (- 90)):
        rotation = (180.0 + rotation)
nlc = []
if len(lc):
    xi = ((dp + xi) + numpy.array([(- spacing), spacing]))
    tempResult = arange(len(pl))
	
===================================================================	
QuadContourSet._initialize_x_y: 902	
----------------------------	

"\n        Return X, Y arrays such that contour(Z) will match imshow(Z)\n        if origin is not None.\n        The center of pixel Z[i,j] depends on origin:\n        if origin is None, x = j, y = i;\n        if origin is 'lower', x = j + 0.5, y = i + 0.5;\n        if origin is 'upper', x = j + 0.5, y = Nrows - i - 0.5\n        If extent is not None, x and y will be scaled to match,\n        as in imshow.\n        If origin is None and extent is not None, then extent\n        will give the minimum and maximum values of x and y.\n        "
if (z.ndim != 2):
    raise TypeError('Input must be a 2D array.')
else:
    (Ny, Nx) = z.shape
if (self.origin is None):
    if (self.extent is None):
        tempResult = arange(Nx)
	
===================================================================	
QuadContourSet._initialize_x_y: 902	
----------------------------	

"\n        Return X, Y arrays such that contour(Z) will match imshow(Z)\n        if origin is not None.\n        The center of pixel Z[i,j] depends on origin:\n        if origin is None, x = j, y = i;\n        if origin is 'lower', x = j + 0.5, y = i + 0.5;\n        if origin is 'upper', x = j + 0.5, y = Nrows - i - 0.5\n        If extent is not None, x and y will be scaled to match,\n        as in imshow.\n        If origin is None and extent is not None, then extent\n        will give the minimum and maximum values of x and y.\n        "
if (z.ndim != 2):
    raise TypeError('Input must be a 2D array.')
else:
    (Ny, Nx) = z.shape
if (self.origin is None):
    if (self.extent is None):
        tempResult = arange(Ny)
	
===================================================================	
QuadContourSet._initialize_x_y: 914	
----------------------------	

"\n        Return X, Y arrays such that contour(Z) will match imshow(Z)\n        if origin is not None.\n        The center of pixel Z[i,j] depends on origin:\n        if origin is None, x = j, y = i;\n        if origin is 'lower', x = j + 0.5, y = i + 0.5;\n        if origin is 'upper', x = j + 0.5, y = Nrows - i - 0.5\n        If extent is not None, x and y will be scaled to match,\n        as in imshow.\n        If origin is None and extent is not None, then extent\n        will give the minimum and maximum values of x and y.\n        "
if (z.ndim != 2):
    raise TypeError('Input must be a 2D array.')
else:
    (Ny, Nx) = z.shape
if (self.origin is None):
    if (self.extent is None):
        return numpy.meshgrid(numpy.arange(Nx), numpy.arange(Ny))
    else:
        (x0, x1, y0, y1) = self.extent
        x = numpy.linspace(x0, x1, Nx)
        y = numpy.linspace(y0, y1, Ny)
        return numpy.meshgrid(x, y)
if (self.extent is None):
    (x0, x1, y0, y1) = (0, Nx, 0, Ny)
else:
    (x0, x1, y0, y1) = self.extent
dx = (float((x1 - x0)) / Nx)
dy = (float((y1 - y0)) / Ny)
tempResult = arange(Nx)
	
===================================================================	
QuadContourSet._initialize_x_y: 915	
----------------------------	

"\n        Return X, Y arrays such that contour(Z) will match imshow(Z)\n        if origin is not None.\n        The center of pixel Z[i,j] depends on origin:\n        if origin is None, x = j, y = i;\n        if origin is 'lower', x = j + 0.5, y = i + 0.5;\n        if origin is 'upper', x = j + 0.5, y = Nrows - i - 0.5\n        If extent is not None, x and y will be scaled to match,\n        as in imshow.\n        If origin is None and extent is not None, then extent\n        will give the minimum and maximum values of x and y.\n        "
if (z.ndim != 2):
    raise TypeError('Input must be a 2D array.')
else:
    (Ny, Nx) = z.shape
if (self.origin is None):
    if (self.extent is None):
        return numpy.meshgrid(numpy.arange(Nx), numpy.arange(Ny))
    else:
        (x0, x1, y0, y1) = self.extent
        x = numpy.linspace(x0, x1, Nx)
        y = numpy.linspace(y0, y1, Ny)
        return numpy.meshgrid(x, y)
if (self.extent is None):
    (x0, x1, y0, y1) = (0, Nx, 0, Ny)
else:
    (x0, x1, y0, y1) = self.extent
dx = (float((x1 - x0)) / Nx)
dy = (float((y1 - y0)) / Ny)
x = (x0 + ((numpy.arange(Nx) + 0.5) * dx))
tempResult = arange(Ny)
	
===================================================================	
PcolorImage.set_data: 548	
----------------------------	

'\n        Set the grid for the rectangle boundaries, and the data values.\n\n          *x* and *y* are monotonic 1-D ndarrays of lengths N+1 and M+1,\n             respectively, specifying rectangle boundaries.  If None,\n             they will be created as uniform arrays from 0 through N\n             and 0 through M, respectively.\n\n          *A* is an (M,N) ndarray or masked array of values to be\n            colormapped, or a (M,N,3) RGB array, or a (M,N,4) RGBA\n            array.\n\n        '
A = matplotlib.cbook.safe_masked_invalid(A, copy=True)
if (x is None):
    tempResult = arange(0, (A.shape[1] + 1), dtype=numpy.float64)
	
===================================================================	
PcolorImage.set_data: 552	
----------------------------	

'\n        Set the grid for the rectangle boundaries, and the data values.\n\n          *x* and *y* are monotonic 1-D ndarrays of lengths N+1 and M+1,\n             respectively, specifying rectangle boundaries.  If None,\n             they will be created as uniform arrays from 0 through N\n             and 0 through M, respectively.\n\n          *A* is an (M,N) ndarray or masked array of values to be\n            colormapped, or a (M,N,3) RGB array, or a (M,N,4) RGBA\n            array.\n\n        '
A = matplotlib.cbook.safe_masked_invalid(A, copy=True)
if (x is None):
    x = numpy.arange(0, (A.shape[1] + 1), dtype=numpy.float64)
else:
    x = np.array(x, np.float64).ravel()
if (y is None):
    tempResult = arange(0, (A.shape[0] + 1), dtype=numpy.float64)
	
===================================================================	
Line2D.recache: 384	
----------------------------	

if (always or self._invalidx):
    xconv = self.convert_xunits(self._xorig)
    if numpy.ma.isMaskedArray(self._xorig):
        x = ma.asarray(xconv, np.float_).filled(numpy.nan)
    else:
        x = numpy.asarray(xconv, numpy.float_)
    x = x.ravel()
else:
    x = self._x
if (always or self._invalidy):
    yconv = self.convert_yunits(self._yorig)
    if numpy.ma.isMaskedArray(self._yorig):
        y = ma.asarray(yconv, np.float_).filled(numpy.nan)
    else:
        y = numpy.asarray(yconv, numpy.float_)
    y = y.ravel()
else:
    y = self._y
if ((len(x) == 1) and (len(y) > 1)):
    x = (x * numpy.ones(y.shape, numpy.float_))
if ((len(y) == 1) and (len(x) > 1)):
    y = (y * numpy.ones(x.shape, numpy.float_))
if (len(x) != len(y)):
    raise RuntimeError('xdata and ydata must be the same length')
self._xy = numpy.empty((len(x), 2), dtype=numpy.float_)
self._xy[:, 0] = x
self._xy[:, 1] = y
self._x = self._xy[:, 0]
self._y = self._xy[:, 1]
self._subslice = False
if (self.axes and (len(x) > 1000) and self._is_sorted(x) and (self.axes.name == 'rectilinear') and (self.axes.get_xscale() == 'linear') and (self._markevery is None) and (self.get_clip_on() is True)):
    self._subslice = True
    nanmask = numpy.isnan(x)
    if nanmask.any():
        self._x_filled = self._x.copy()
        tempResult = arange(len(x))
	
===================================================================	
_mark_every_path: 105	
----------------------------	

'\n    Helper function that sorts out how to deal the input\n    `markevery` and returns the points where markers should be drawn.\n\n    Takes in the `markevery` value and the line path and returns the\n    sub-sampled path.\n    '
(codes, verts) = (tpath.codes, tpath.vertices)

def _slice_or_none(in_v, slc):
    '\n        Helper function to cope with `codes` being an\n        ndarray or `None`\n        '
    if (in_v is None):
        return None
    return in_v[slc]
if isinstance(markevery, float):
    markevery = (0.0, markevery)
elif isinstance(markevery, int):
    markevery = (0, markevery)
if isinstance(markevery, tuple):
    if (len(markevery) != 2):
        raise ValueError(('`markevery` is a tuple but its len is not 2; markevery=%s' % (markevery,)))
    (start, step) = markevery
    if isinstance(step, int):
        if (not isinstance(start, int)):
            raise ValueError(('`markevery` is a tuple with len 2 and second element is an int, but the first element is not an int; markevery=%s' % (markevery,)))
        return Path(verts[slice(start, None, step)], _slice_or_none(codes, slice(start, None, step)))
    elif isinstance(step, float):
        if (not (isinstance(start, int) or isinstance(start, float))):
            raise ValueError(('`markevery` is a tuple with len 2 and second element is a float, but the first element is not a float or an int; markevery=%s' % (markevery,)))
        disp_coords = affine.transform(tpath.vertices)
        delta = numpy.empty((len(disp_coords), 2), dtype=float)
        delta[0, :] = 0.0
        delta[1:, :] = (disp_coords[1:, :] - disp_coords[:(- 1), :])
        delta = numpy.sum((delta ** 2), axis=1)
        delta = numpy.sqrt(delta)
        delta = numpy.cumsum(delta)
        scale = ax_transform.transform(numpy.array([[0, 0], [1, 1]]))
        scale = numpy.diff(scale, axis=0)
        scale = numpy.sum((scale ** 2))
        scale = numpy.sqrt(scale)
        tempResult = arange((start * scale), delta[(- 1)], (step * scale))
	
===================================================================	
cohere_pairs: 415	
----------------------------	

'\n    Compute the coherence and phase for all pairs *ij*, in *X*.\n\n    *X* is a *numSamples* * *numCols* array\n\n    *ij* is a list of tuples.  Each tuple is a pair of indexes into\n    the columns of X for which you want to compute coherence.  For\n    example, if *X* has 64 columns, and you want to compute all\n    nonredundant pairs, define *ij* as::\n\n      ij = []\n      for i in range(64):\n          for j in range(i+1,64):\n              ij.append( (i,j) )\n\n    *preferSpeedOverMemory* is an optional bool. Defaults to true. If\n    False, limits the caching by only making one, rather than two,\n    complex cache arrays. This is useful if memory becomes critical.\n    Even when *preferSpeedOverMemory* is False, :func:`cohere_pairs`\n    will still give significant performace gains over calling\n    :func:`cohere` for each pair, and will use subtantially less\n    memory than if *preferSpeedOverMemory* is True.  In my tests with\n    a 43000,64 array over all nonredundant pairs,\n    *preferSpeedOverMemory* = True delivered a 33% performance boost\n    on a 1.7GHZ Athlon with 512MB RAM compared with\n    *preferSpeedOverMemory* = False.  But both solutions were more\n    than 10x faster than naively crunching all possible pairs through\n    :func:`cohere`.\n\n    Returns\n    -------\n    Cxy : dictionary of (*i*, *j*) tuples -> coherence vector for\n        that pair.  i.e., ``Cxy[(i,j) = cohere(X[:,i], X[:,j])``.\n        Number of dictionary keys is ``len(ij)``.\n\n    Phase : dictionary of phases of the cross spectral density at\n        each frequency for each pair.  Keys are (*i*, *j*).\n\n    freqs : vector of frequencies, equal in length to either the\n         coherence or phase vectors for any (*i*, *j*) key.\n\n    e.g., to make a coherence Bode plot::\n\n          subplot(211)\n          plot( freqs, Cxy[(12,19)])\n          subplot(212)\n          plot( freqs, Phase[(12,19)])\n\n    For a large number of pairs, :func:`cohere_pairs` can be much more\n    efficient than just calling :func:`cohere` for each pair, because\n    it caches most of the intensive computations.  If :math:`N` is the\n    number of pairs, this function is :math:`O(N)` for most of the\n    heavy lifting, whereas calling cohere for each pair is\n    :math:`O(N^2)`.  However, because of the caching, it is also more\n    memory intensive, making 2 additional complex arrays with\n    approximately the same number of elements as *X*.\n\n    See :file:`test/cohere_pairs_test.py` in the src tree for an\n    example script that shows that this :func:`cohere_pairs` and\n    :func:`cohere` give the same results for a given pair.\n\n    See Also\n    --------\n    :func:`psd`\n        For information about the methods used to compute :math:`P_{xy}`,\n        :math:`P_{xx}` and :math:`P_{yy}`.\n    '
(numRows, numCols) = X.shape
if (numRows < NFFT):
    tmp = X
    X = numpy.zeros((NFFT, numCols), X.dtype)
    X[:numRows, :] = tmp
    del tmp
(numRows, numCols) = X.shape
allColumns = set()
for (i, j) in ij:
    allColumns.add(i)
    allColumns.add(j)
Ncols = len(allColumns)
if numpy.iscomplexobj(X):
    numFreqs = NFFT
else:
    numFreqs = ((NFFT // 2) + 1)
if matplotlib.cbook.iterable(window):
    if (len(window) != NFFT):
        raise ValueError('The length of the window must be equal to NFFT')
    windowVals = window
else:
    windowVals = window(numpy.ones(NFFT, X.dtype))
ind = list(xrange(0, ((numRows - NFFT) + 1), (NFFT - noverlap)))
numSlices = len(ind)
FFTSlices = {}
FFTConjSlices = {}
Pxx = {}
slices = range(numSlices)
normVal = (numpy.linalg.norm(windowVals) ** 2)
for iCol in allColumns:
    progressCallback((i / Ncols), 'Cacheing FFTs')
    Slices = numpy.zeros((numSlices, numFreqs), dtype=numpy.complex_)
    for iSlice in slices:
        thisSlice = X[ind[iSlice]:(ind[iSlice] + NFFT), iCol]
        thisSlice = (windowVals * detrend(thisSlice))
        Slices[iSlice, :] = numpy.fft.fft(thisSlice)[:numFreqs]
    FFTSlices[iCol] = Slices
    if preferSpeedOverMemory:
        FFTConjSlices[iCol] = numpy.conjugate(Slices)
    Pxx[iCol] = numpy.divide(numpy.mean((abs(Slices) ** 2), axis=0), normVal)
del Slices, ind, windowVals
Cxy = {}
Phase = {}
count = 0
N = len(ij)
for (i, j) in ij:
    count += 1
    if ((count % 10) == 0):
        progressCallback((count / N), 'Computing coherences')
    if preferSpeedOverMemory:
        Pxy = (FFTSlices[i] * FFTConjSlices[j])
    else:
        Pxy = (FFTSlices[i] * numpy.conjugate(FFTSlices[j]))
    if (numSlices > 1):
        Pxy = numpy.mean(Pxy, axis=0)
    Pxy /= normVal
    Cxy[(i, j)] = ((abs(Pxy) ** 2) / (Pxx[i] * Pxx[j]))
    Phase[(i, j)] = numpy.arctan2(Pxy.imag, Pxy.real)
tempResult = arange(numFreqs)
	
===================================================================	
rk4: 574	
----------------------------	

'\n    Integrate 1D or ND system of ODEs using 4-th order Runge-Kutta.\n    This is a toy implementation which may be useful if you find\n    yourself stranded on a system w/o scipy.  Otherwise use\n    :func:`scipy.integrate`.\n\n    Parameters\n    ----------\n    y0\n        initial state vector\n\n    t\n        sample times\n\n    derivs\n        returns the derivative of the system and has the\n        signature ``dy = derivs(yi, ti)``\n\n    Examples\n    --------\n\n    A 2D system::\n\n        def derivs6(x,t):\n            d1 =  x[0] + 2*x[1]\n            d2 =  -3*x[0] + 4*x[1]\n            return (d1, d2)\n        dt = 0.0005\n        t = arange(0.0, 2.0, dt)\n        y0 = (1,2)\n        yout = rk4(derivs6, y0, t)\n\n    A 1D system::\n\n        alpha = 2\n        def derivs(x,t):\n            return -alpha*x + exp(-t)\n\n        y0 = 1\n        yout = rk4(derivs, y0, t)\n\n    If you have access to scipy, you should probably be using the\n    scipy.integrate tools rather than this function.\n    '
try:
    Ny = len(y0)
except TypeError:
    yout = numpy.zeros((len(t),), numpy.float_)
else:
    yout = numpy.zeros((len(t), Ny), numpy.float_)
yout[0] = y0
i = 0
tempResult = arange((len(t) - 1))
	
===================================================================	
longest_contiguous_ones: 448	
----------------------------	

'\n    Return the indices of the longest stretch of contiguous ones in *x*,\n    assuming *x* is a vector of zeros and ones.  If there are two\n    equally long stretches, pick the first.\n    '
x = numpy.ravel(x)
if (len(x) == 0):
    return numpy.array([])
ind = (x == 0).nonzero()[0]
if (len(ind) == 0):
    tempResult = arange(len(x))
	
===================================================================	
longest_contiguous_ones: 457	
----------------------------	

'\n    Return the indices of the longest stretch of contiguous ones in *x*,\n    assuming *x* is a vector of zeros and ones.  If there are two\n    equally long stretches, pick the first.\n    '
x = numpy.ravel(x)
if (len(x) == 0):
    return numpy.array([])
ind = (x == 0).nonzero()[0]
if (len(ind) == 0):
    return numpy.arange(len(x))
if (len(ind) == len(x)):
    return numpy.array([])
y = numpy.zeros(((len(x) + 2),), x.dtype)
y[1:(- 1)] = x
dif = numpy.diff(y)
up = (dif == 1).nonzero()[0]
dn = (dif == (- 1)).nonzero()[0]
i = ((dn - up) == max((dn - up))).nonzero()[0][0]
tempResult = arange(up[i], dn[i])
	
===================================================================	
prctile_rank: 546	
----------------------------	

'\n    Return the rank for each element in *x*, return the rank\n    0..len(*p*).  e.g., if *p* = (25, 50, 75), the return value will be a\n    len(*x*) array with values in [0,1,2,3] where 0 indicates the\n    value is less than the 25th percentile, 1 indicates the value is\n    >= the 25th and < 50th percentile, ... and 3 indicates the value\n    is above the 75th percentile cutoff.\n\n    *p* is either an array of percentiles in [0..100] or a scalar which\n    indicates how many quantiles of data you want ranked.\n    '
if (not matplotlib.cbook.iterable(p)):
    tempResult = arange((100.0 / p), 100.0, (100.0 / p))
	
===================================================================	
_spectral_helper: 262	
----------------------------	

'\n    This is a helper function that implements the commonality between the\n    psd, csd, spectrogram and complex, magnitude, angle, and phase spectrums.\n    It is *NOT* meant to be used outside of mlab and may change at any time.\n    '
if (y is None):
    same_data = True
else:
    same_data = (y is x)
if (Fs is None):
    Fs = 2
if (noverlap is None):
    noverlap = 0
if (detrend_func is None):
    detrend_func = detrend_none
if (window is None):
    window = window_hanning
if (NFFT is None):
    NFFT = 256
if ((mode is None) or (mode == 'default')):
    mode = 'psd'
elif (mode not in ['psd', 'complex', 'magnitude', 'angle', 'phase']):
    raise ValueError(("Unknown value for mode %s, must be one of: 'default', 'psd', 'complex', 'magnitude', 'angle', 'phase'" % mode))
if ((not same_data) and (mode != 'psd')):
    raise ValueError("x and y must be equal if mode is not 'psd'")
x = numpy.asarray(x)
if (not same_data):
    y = numpy.asarray(y)
if ((sides is None) or (sides == 'default')):
    if numpy.iscomplexobj(x):
        sides = 'twosided'
    else:
        sides = 'onesided'
elif (sides not in ['onesided', 'twosided']):
    raise ValueError(("Unknown value for sides %s, must be one of: 'default', 'onesided', or 'twosided'" % sides))
if (len(x) < NFFT):
    n = len(x)
    x = numpy.resize(x, (NFFT,))
    x[n:] = 0
if ((not same_data) and (len(y) < NFFT)):
    n = len(y)
    y = numpy.resize(y, (NFFT,))
    y[n:] = 0
if (pad_to is None):
    pad_to = NFFT
if (mode != 'psd'):
    scale_by_freq = False
elif (scale_by_freq is None):
    scale_by_freq = True
if (sides == 'twosided'):
    numFreqs = pad_to
    if (pad_to % 2):
        freqcenter = (((pad_to - 1) // 2) + 1)
    else:
        freqcenter = (pad_to // 2)
    scaling_factor = 1.0
elif (sides == 'onesided'):
    if (pad_to % 2):
        numFreqs = ((pad_to + 1) // 2)
    else:
        numFreqs = ((pad_to // 2) + 1)
    scaling_factor = 2.0
result = stride_windows(x, NFFT, noverlap, axis=0)
result = detrend(result, detrend_func, axis=0)
(result, windowVals) = apply_window(result, window, axis=0, return_window=True)
result = numpy.fft.fft(result, n=pad_to, axis=0)[:numFreqs, :]
freqs = numpy.fft.fftfreq(pad_to, (1 / Fs))[:numFreqs]
if (not same_data):
    resultY = stride_windows(y, NFFT, noverlap)
    resultY = apply_window(resultY, window, axis=0)
    resultY = detrend(resultY, detrend_func, axis=0)
    resultY = numpy.fft.fft(resultY, n=pad_to, axis=0)[:numFreqs, :]
    result = (numpy.conjugate(result) * resultY)
elif (mode == 'psd'):
    result = (numpy.conjugate(result) * result)
elif (mode == 'magnitude'):
    result = numpy.absolute(result)
elif ((mode == 'angle') or (mode == 'phase')):
    result = numpy.angle(result)
elif (mode == 'complex'):
    pass
if (mode == 'psd'):
    if (not (NFFT % 2)):
        slc = slice(1, (- 1), None)
    else:
        slc = slice(1, None, None)
    result[slc] *= scaling_factor
    if scale_by_freq:
        result /= Fs
        result /= (np.abs(windowVals) ** 2).sum()
    else:
        result /= (np.abs(windowVals).sum() ** 2)
tempResult = arange((NFFT / 2), ((len(x) - (NFFT / 2)) + 1), (NFFT - noverlap))
	
===================================================================	
detrend_linear: 115	
----------------------------	

"\n    Return x minus best fit line; 'linear' detrending.\n\n    Parameters\n    ----------\n    y : 0-D or 1-D array or sequence\n        Array or sequence containing the data\n\n    axis : integer\n        The axis along which to take the mean.  See numpy.mean for a\n        description of this argument.\n\n    See Also\n    --------\n    :func:`delinear`\n        This function is the same as :func:`delinear` except for the default\n        *axis*.\n\n    :func:`detrend_mean`\n\n    :func:`detrend_none`\n        :func:`detrend_mean` and :func:`detrend_none` are other detrend\n        algorithms.\n\n    :func:`detrend`\n        :func:`detrend` is a wrapper around all the detrend algorithms.\n    "
y = numpy.asarray(y)
if (y.ndim > 1):
    raise ValueError('y cannot have ndim > 1')
if (not y.ndim):
    return numpy.array(0.0, dtype=y.dtype)
tempResult = arange(y.size, dtype=numpy.float_)
	
===================================================================	
frange: 709	
----------------------------	

"\n    frange([start,] stop[, step, keywords]) -> array of floats\n\n    Return a numpy ndarray containing a progression of floats. Similar to\n    :func:`numpy.arange`, but defaults to a closed interval.\n\n    ``frange(x0, x1)`` returns ``[x0, x0+1, x0+2, ..., x1]``; *start*\n    defaults to 0, and the endpoint *is included*. This behavior is\n    different from that of :func:`range` and\n    :func:`numpy.arange`. This is deliberate, since :func:`frange`\n    will probably be more useful for generating lists of points for\n    function evaluation, and endpoints are often desired in this\n    use. The usual behavior of :func:`range` can be obtained by\n    setting the keyword *closed* = 0, in this case, :func:`frange`\n    basically becomes :func:numpy.arange`.\n\n    When *step* is given, it specifies the increment (or\n    decrement). All arguments can be floating point numbers.\n\n    ``frange(x0,x1,d)`` returns ``[x0,x0+d,x0+2d,...,xfin]`` where\n    *xfin* <= *x1*.\n\n    :func:`frange` can also be called with the keyword *npts*. This\n    sets the number of points the list should contain (and overrides\n    the value *step* might have been given). :func:`numpy.arange`\n    doesn't offer this option.\n\n    Examples::\n\n      >>> frange(3)\n      array([ 0.,  1.,  2.,  3.])\n      >>> frange(3,closed=0)\n      array([ 0.,  1.,  2.])\n      >>> frange(1,6,2)\n      array([1, 3, 5])   or 1,3,5,7, depending on floating point vagueries\n      >>> frange(1,6.5,npts=5)\n      array([ 1.   ,  2.375,  3.75 ,  5.125,  6.5  ])\n    "
kw.setdefault('closed', 1)
endpoint = (kw['closed'] != 0)
if (xfin is None):
    xfin = (xini + 0.0)
    xini = 0.0
if (delta is None):
    delta = 1.0
try:
    npts = kw['npts']
    delta = ((xfin - xini) / float((npts - endpoint)))
except KeyError:
    npts = (int(numpy.round(((xfin - xini) / delta))) + endpoint)
tempResult = arange(npts)
	
===================================================================	
module: 975	
----------------------------	

'\nThe OffsetBox is a simple container artist. The child artist are meant\nto be drawn at a relative position to its parent.  The [VH]Packer,\nDrawingArea and TextArea are derived from the OffsetBox.\n\nThe [VH]Packer automatically adjust the relative postisions of their\nchildren, which should be instances of the OffsetBox. This is used to\nalign similar artists together, e.g., in legend.\n\nThe DrawingArea can contain any Artist as a child. The\nDrawingArea has a fixed width and height. The position of children\nrelative to the parent is fixed.  The TextArea is contains a single\nText instance. The width and height of the TextArea instance is the\nwidth and height of the its child text.\n'
from __future__ import absolute_import, division, print_function, unicode_literals
import six
from six.moves import xrange, zip
import warnings
import matplotlib.transforms as mtransforms
import matplotlib.artist as martist
import matplotlib.text as mtext
import matplotlib.path as mpath
import numpy as np
from matplotlib.transforms import Bbox, BboxBase, TransformedBbox
from matplotlib.font_manager import FontProperties
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch
from matplotlib import rcParams
from matplotlib import docstring
from matplotlib.image import BboxImage
from matplotlib.patches import bbox_artist as mbbox_artist
from matplotlib.text import _AnnotationBase
DEBUG = False

def bbox_artist(*args, **kwargs):
    if DEBUG:
        mbbox_artist(*args, **kwargs)

def _get_packed_offsets(wd_list, total, sep, mode='fixed'):
    "\n    Geiven a list of (width, xdescent) of each boxes, calculate the\n    total width and the x-offset positions of each items according to\n    *mode*. xdescent is analagous to the usual descent, but along the\n    x-direction. xdescent values are currently ignored.\n\n    *wd_list* : list of (width, xdescent) of boxes to be packed.\n    *sep* : spacing between boxes\n    *total* : Intended total length. None if not used.\n    *mode* : packing mode. 'fixed', 'expand', or 'equal'.\n    "
    (w_list, d_list) = list(zip(*wd_list))
    if (mode == 'fixed'):
        offsets_ = numpy.add.accumulate(([0] + [(w + sep) for w in w_list]))
        offsets = offsets_[:(- 1)]
        if (total is None):
            total = (offsets_[(- 1)] - sep)
        return (total, offsets)
    elif (mode == 'expand'):
        if (len(w_list) > 1):
            sep = ((total - sum(w_list)) / (len(w_list) - 1.0))
        else:
            sep = 0.0
        offsets_ = numpy.add.accumulate(([0] + [(w + sep) for w in w_list]))
        offsets = offsets_[:(- 1)]
        return (total, offsets)
    elif (mode == 'equal'):
        maxh = max(w_list)
        if (total is None):
            total = ((maxh + sep) * len(w_list))
        else:
            sep = ((float(total) / len(w_list)) - maxh)
        offsets = numpy.array([((maxh + sep) * i) for i in range(len(w_list))])
        return (total, offsets)
    else:
        raise ValueError(('Unknown mode : %s' % (mode,)))

def _get_aligned_offsets(hd_list, height, align='baseline'):
    "\n    Given a list of (height, descent) of each boxes, align the boxes\n    with *align* and calculate the y-offsets of each boxes.\n    total width and the offset positions of each items according to\n    *mode*. xdescent is analogous to the usual descent, but along the\n    x-direction. xdescent values are currently ignored.\n\n    *hd_list* : list of (width, xdescent) of boxes to be aligned.\n    *sep* : spacing between boxes\n    *height* : Intended total length. None if not used.\n    *align* : align mode. 'baseline', 'top', 'bottom', or 'center'.\n    "
    if (height is None):
        height = max([h for (h, d) in hd_list])
    if (align == 'baseline'):
        height_descent = max([(h - d) for (h, d) in hd_list])
        descent = max([d for (h, d) in hd_list])
        height = (height_descent + descent)
        offsets = [0.0 for (h, d) in hd_list]
    elif (align in ['left', 'top']):
        descent = 0.0
        offsets = [d for (h, d) in hd_list]
    elif (align in ['right', 'bottom']):
        descent = 0.0
        offsets = [((height - h) + d) for (h, d) in hd_list]
    elif (align == 'center'):
        descent = 0.0
        offsets = [(((height - h) * 0.5) + d) for (h, d) in hd_list]
    else:
        raise ValueError(('Unknown Align mode : %s' % (align,)))
    return (height, descent, offsets)

class OffsetBox(matplotlib.artist.Artist):
    '\n    The OffsetBox is a simple container artist. The child artist are meant\n    to be drawn at a relative position to its parent.\n    '

    def __init__(self, *args, **kwargs):
        super(OffsetBox, self).__init__(*args, **kwargs)
        self.set_clip_on(False)
        self._children = []
        self._offset = (0, 0)

    def __getstate__(self):
        state = matplotlib.artist.Artist.__getstate__(self)
        from .cbook import _InstanceMethodPickler
        import inspect
        offset = state['_offset']
        if inspect.ismethod(offset):
            state['_offset'] = _InstanceMethodPickler(offset)
        return state

    def __setstate__(self, state):
        self.__dict__ = state
        from .cbook import _InstanceMethodPickler
        if isinstance(self._offset, _InstanceMethodPickler):
            self._offset = self._offset.get_instancemethod()
        self.stale = True

    def set_figure(self, fig):
        '\n        Set the figure\n\n        accepts a class:`~matplotlib.figure.Figure` instance\n        '
        matplotlib.artist.Artist.set_figure(self, fig)
        for c in self.get_children():
            c.set_figure(fig)

    @matplotlib.artist.Artist.axes.setter
    def axes(self, ax):
        matplotlib.artist.Artist.axes.fset(self, ax)
        for c in self.get_children():
            if (c is not None):
                c.axes = ax

    def contains(self, mouseevent):
        for c in self.get_children():
            (a, b) = c.contains(mouseevent)
            if a:
                return (a, b)
        return (False, {})

    def set_offset(self, xy):
        '\n        Set the offset\n\n        accepts x, y, tuple, or a callable object.\n        '
        self._offset = xy
        self.stale = True

    def get_offset(self, width, height, xdescent, ydescent, renderer):
        '\n        Get the offset\n\n        accepts extent of the box\n        '
        if six.callable(self._offset):
            return self._offset(width, height, xdescent, ydescent, renderer)
        else:
            return self._offset

    def set_width(self, width):
        '\n        Set the width\n\n        accepts float\n        '
        self.width = width
        self.stale = True

    def set_height(self, height):
        '\n        Set the height\n\n        accepts float\n        '
        self.height = height
        self.stale = True

    def get_visible_children(self):
        '\n        Return a list of visible artists it contains.\n        '
        return [c for c in self._children if c.get_visible()]

    def get_children(self):
        '\n        Return a list of artists it contains.\n        '
        return self._children

    def get_extent_offsets(self, renderer):
        raise Exception('')

    def get_extent(self, renderer):
        '\n        Return with, height, xdescent, ydescent of box\n        '
        (w, h, xd, yd, offsets) = self.get_extent_offsets(renderer)
        return (w, h, xd, yd)

    def get_window_extent(self, renderer):
        '\n        get the bounding box in display space.\n        '
        (w, h, xd, yd, offsets) = self.get_extent_offsets(renderer)
        (px, py) = self.get_offset(w, h, xd, yd, renderer)
        return matplotlib.transforms.Bbox.from_bounds((px - xd), (py - yd), w, h)

    def draw(self, renderer):
        '\n        Update the location of children if necessary and draw them\n        to the given *renderer*.\n        '
        (width, height, xdescent, ydescent, offsets) = self.get_extent_offsets(renderer)
        (px, py) = self.get_offset(width, height, xdescent, ydescent, renderer)
        for (c, (ox, oy)) in zip(self.get_visible_children(), offsets):
            c.set_offset(((px + ox), (py + oy)))
            c.draw(renderer)
        bbox_artist(self, renderer, fill=False, props=dict(pad=0.0))
        self.stale = False

class PackerBase(OffsetBox):

    def __init__(self, pad=None, sep=None, width=None, height=None, align=None, mode=None, children=None):
        '\n        Parameters\n        ----------\n        pad : float, optional\n            Boundary pad.\n\n        sep : float, optional\n            Spacing between items.\n\n        width : float, optional\n\n        height : float, optional\n           Width and height of the container box, calculated if\n           `None`.\n\n        align : str, optional\n            Alignment of boxes. Can be one of ``top``, ``bottom``,\n            ``left``, ``right``, ``center`` and ``baseline``\n\n        mode : str, optional\n            Packing mode.\n\n        Notes\n        -----\n        *pad* and *sep* need to given in points and will be scale with\n        the renderer dpi, while *width* and *height* need to be in\n        pixels.\n        '
        super(PackerBase, self).__init__()
        self.height = height
        self.width = width
        self.sep = sep
        self.pad = pad
        self.mode = mode
        self.align = align
        self._children = children

class VPacker(PackerBase):
    '\n    The VPacker has its children packed vertically. It automatically\n    adjust the relative positions of children in the drawing time.\n    '

    def __init__(self, pad=None, sep=None, width=None, height=None, align='baseline', mode='fixed', children=None):
        '\n        Parameters\n        ----------\n        pad : float, optional\n            Boundary pad.\n\n        sep : float, optional\n            Spacing between items.\n\n        width : float, optional\n\n        height : float, optional\n\n            width and height of the container box, calculated if\n            `None`.\n\n        align : str, optional\n            Alignment of boxes.\n\n        mode : str, optional\n            Packing mode.\n\n        Notes\n        -----\n        *pad* and *sep* need to given in points and will be scale with\n        the renderer dpi, while *width* and *height* need to be in\n        pixels.\n        '
        super(VPacker, self).__init__(pad, sep, width, height, align, mode, children)

    def get_extent_offsets(self, renderer):
        '\n        update offset of childrens and return the extents of the box\n        '
        dpicor = renderer.points_to_pixels(1.0)
        pad = (self.pad * dpicor)
        sep = (self.sep * dpicor)
        if (self.width is not None):
            for c in self.get_visible_children():
                if (isinstance(c, PackerBase) and (c.mode == 'expand')):
                    c.set_width(self.width)
        whd_list = [c.get_extent(renderer) for c in self.get_visible_children()]
        whd_list = [(w, h, xd, (h - yd)) for (w, h, xd, yd) in whd_list]
        wd_list = [(w, xd) for (w, h, xd, yd) in whd_list]
        (width, xdescent, xoffsets) = _get_aligned_offsets(wd_list, self.width, self.align)
        pack_list = [(h, yd) for (w, h, xd, yd) in whd_list]
        (height, yoffsets_) = _get_packed_offsets(pack_list, self.height, sep, self.mode)
        yoffsets = (yoffsets_ + [yd for (w, h, xd, yd) in whd_list])
        ydescent = (height - yoffsets[0])
        yoffsets = (height - yoffsets)
        yoffsets = (yoffsets - ydescent)
        return ((width + (2 * pad)), (height + (2 * pad)), (xdescent + pad), (ydescent + pad), list(zip(xoffsets, yoffsets)))

class HPacker(PackerBase):
    '\n    The HPacker has its children packed horizontally. It automatically\n    adjusts the relative positions of children at draw time.\n    '

    def __init__(self, pad=None, sep=None, width=None, height=None, align='baseline', mode='fixed', children=None):
        '\n        Parameters\n        ----------\n        pad : float, optional\n            Boundary pad.\n\n        sep : float, optional\n            Spacing between items.\n\n        width : float, optional\n\n        height : float, optional\n           Width and height of the container box, calculated if\n           `None`.\n\n        align : str\n           Alignment of boxes.\n\n        mode : str\n           Packing mode.\n\n        Notes\n        -----\n        *pad* and *sep* need to given in points and will be scale with\n        the renderer dpi, while *width* and *height* need to be in\n        pixels.\n        '
        super(HPacker, self).__init__(pad, sep, width, height, align, mode, children)

    def get_extent_offsets(self, renderer):
        '\n        update offset of children and return the extents of the box\n        '
        dpicor = renderer.points_to_pixels(1.0)
        pad = (self.pad * dpicor)
        sep = (self.sep * dpicor)
        whd_list = [c.get_extent(renderer) for c in self.get_visible_children()]
        if (not whd_list):
            return ((2 * pad), (2 * pad), pad, pad, [])
        if (self.height is None):
            height_descent = max([(h - yd) for (w, h, xd, yd) in whd_list])
            ydescent = max([yd for (w, h, xd, yd) in whd_list])
            height = (height_descent + ydescent)
        else:
            height = (self.height - (2 * pad))
        hd_list = [(h, yd) for (w, h, xd, yd) in whd_list]
        (height, ydescent, yoffsets) = _get_aligned_offsets(hd_list, self.height, self.align)
        pack_list = [(w, xd) for (w, h, xd, yd) in whd_list]
        (width, xoffsets_) = _get_packed_offsets(pack_list, self.width, sep, self.mode)
        xoffsets = (xoffsets_ + [xd for (w, h, xd, yd) in whd_list])
        xdescent = whd_list[0][2]
        xoffsets = (xoffsets - xdescent)
        return ((width + (2 * pad)), (height + (2 * pad)), (xdescent + pad), (ydescent + pad), list(zip(xoffsets, yoffsets)))

class PaddedBox(OffsetBox):

    def __init__(self, child, pad=None, draw_frame=False, patch_attrs=None):
        '\n        *pad* : boundary pad\n\n        .. note::\n          *pad* need to given in points and will be\n          scale with the renderer dpi, while *width* and *height*\n          need to be in pixels.\n        '
        super(PaddedBox, self).__init__()
        self.pad = pad
        self._children = [child]
        self.patch = FancyBboxPatch(xy=(0.0, 0.0), width=1.0, height=1.0, facecolor='w', edgecolor='k', mutation_scale=1, snap=True)
        self.patch.set_boxstyle('square', pad=0)
        if (patch_attrs is not None):
            self.patch.update(patch_attrs)
        self._drawFrame = draw_frame

    def get_extent_offsets(self, renderer):
        '\n        update offset of childrens and return the extents of the box\n        '
        dpicor = renderer.points_to_pixels(1.0)
        pad = (self.pad * dpicor)
        (w, h, xd, yd) = self._children[0].get_extent(renderer)
        return ((w + (2 * pad)), (h + (2 * pad)), (xd + pad), (yd + pad), [(0, 0)])

    def draw(self, renderer):
        '\n        Update the location of children if necessary and draw them\n        to the given *renderer*.\n        '
        (width, height, xdescent, ydescent, offsets) = self.get_extent_offsets(renderer)
        (px, py) = self.get_offset(width, height, xdescent, ydescent, renderer)
        for (c, (ox, oy)) in zip(self.get_visible_children(), offsets):
            c.set_offset(((px + ox), (py + oy)))
        self.draw_frame(renderer)
        for c in self.get_visible_children():
            c.draw(renderer)
        self.stale = False

    def update_frame(self, bbox, fontsize=None):
        self.patch.set_bounds(bbox.x0, bbox.y0, bbox.width, bbox.height)
        if fontsize:
            self.patch.set_mutation_scale(fontsize)
        self.stale = True

    def draw_frame(self, renderer):
        bbox = self.get_window_extent(renderer)
        self.update_frame(bbox)
        if self._drawFrame:
            self.patch.draw(renderer)

class DrawingArea(OffsetBox):
    '\n    The DrawingArea can contain any Artist as a child. The DrawingArea\n    has a fixed width and height. The position of children relative to\n    the parent is fixed. The children can be clipped at the\n    boundaries of the parent.\n    '

    def __init__(self, width, height, xdescent=0.0, ydescent=0.0, clip=False):
        '\n        *width*, *height* : width and height of the container box.\n        *xdescent*, *ydescent* : descent of the box in x- and y-direction.\n        *clip* : Whether to clip the children\n        '
        super(DrawingArea, self).__init__()
        self.width = width
        self.height = height
        self.xdescent = xdescent
        self.ydescent = ydescent
        self._clip_children = clip
        self.offset_transform = matplotlib.transforms.Affine2D()
        self.offset_transform.clear()
        self.offset_transform.translate(0, 0)
        self.dpi_transform = matplotlib.transforms.Affine2D()

    @property
    def clip_children(self):
        '\n        If the children of this DrawingArea should be clipped\n        by DrawingArea bounding box.\n        '
        return self._clip_children

    @clip_children.setter
    def clip_children(self, val):
        self._clip_children = bool(val)
        self.stale = True

    def get_transform(self):
        '\n        Return the :class:`~matplotlib.transforms.Transform` applied\n        to the children\n        '
        return (self.dpi_transform + self.offset_transform)

    def set_transform(self, t):
        '\n        set_transform is ignored.\n        '
        pass

    def set_offset(self, xy):
        '\n        set offset of the container.\n\n        Accept : tuple of x,y cooridnate in disokay units.\n        '
        self._offset = xy
        self.offset_transform.clear()
        self.offset_transform.translate(xy[0], xy[1])
        self.stale = True

    def get_offset(self):
        '\n        return offset of the container.\n        '
        return self._offset

    def get_window_extent(self, renderer):
        '\n        get the bounding box in display space.\n        '
        (w, h, xd, yd) = self.get_extent(renderer)
        (ox, oy) = self.get_offset()
        return matplotlib.transforms.Bbox.from_bounds((ox - xd), (oy - yd), w, h)

    def get_extent(self, renderer):
        '\n        Return with, height, xdescent, ydescent of box\n        '
        dpi_cor = renderer.points_to_pixels(1.0)
        return ((self.width * dpi_cor), (self.height * dpi_cor), (self.xdescent * dpi_cor), (self.ydescent * dpi_cor))

    def add_artist(self, a):
        'Add any :class:`~matplotlib.artist.Artist` to the container box'
        self._children.append(a)
        if (not a.is_transform_set()):
            a.set_transform(self.get_transform())
        if (self.axes is not None):
            a.axes = self.axes
        fig = self.figure
        if (fig is not None):
            a.set_figure(fig)

    def draw(self, renderer):
        '\n        Draw the children\n        '
        dpi_cor = renderer.points_to_pixels(1.0)
        self.dpi_transform.clear()
        self.dpi_transform.scale(dpi_cor, dpi_cor)
        tpath = matplotlib.transforms.TransformedPath(matplotlib.path.Path([[0, 0], [0, self.height], [self.width, self.height], [self.width, 0]]), self.get_transform())
        for c in self._children:
            if (self._clip_children and (not (c.clipbox or c._clippath))):
                c.set_clip_path(tpath)
            c.draw(renderer)
        bbox_artist(self, renderer, fill=False, props=dict(pad=0.0))
        self.stale = False

class TextArea(OffsetBox):
    '\n    The TextArea is contains a single Text instance. The text is\n    placed at (0,0) with baseline+left alignment. The width and height\n    of the TextArea instance is the width and height of the its child\n    text.\n    '

    def __init__(self, s, textprops=None, multilinebaseline=None, minimumdescent=True):
        '\n        Parameters\n        ----------\n        s : str\n            a string to be displayed.\n\n        textprops : `~matplotlib.font_manager.FontProperties`, optional\n\n        multilinebaseline : bool, optional\n            If `True`, baseline for multiline text is adjusted so that\n            it is (approximatedly) center-aligned with singleline\n            text.\n\n        minimumdescent : bool, optional\n            If `True`, the box has a minimum descent of "p".\n        '
        if (textprops is None):
            textprops = {}
        if ('va' not in textprops):
            textprops['va'] = 'baseline'
        self._text = matplotlib.text.Text(0, 0, s, **textprops)
        OffsetBox.__init__(self)
        self._children = [self._text]
        self.offset_transform = matplotlib.transforms.Affine2D()
        self.offset_transform.clear()
        self.offset_transform.translate(0, 0)
        self._baseline_transform = matplotlib.transforms.Affine2D()
        self._text.set_transform((self.offset_transform + self._baseline_transform))
        self._multilinebaseline = multilinebaseline
        self._minimumdescent = minimumdescent

    def set_text(self, s):
        'Set the text of this area as a string.'
        self._text.set_text(s)
        self.stale = True

    def get_text(self):
        "Returns the string representation of this area's text"
        return self._text.get_text()

    def set_multilinebaseline(self, t):
        '\n        Set multilinebaseline .\n\n        If True, baseline for multiline text is\n        adjusted so that it is (approximatedly) center-aligned with\n        singleline text.\n        '
        self._multilinebaseline = t
        self.stale = True

    def get_multilinebaseline(self):
        '\n        get multilinebaseline .\n        '
        return self._multilinebaseline

    def set_minimumdescent(self, t):
        '\n        Set minimumdescent .\n\n        If True, extent of the single line text is adjusted so that\n        it has minimum descent of "p"\n        '
        self._minimumdescent = t
        self.stale = True

    def get_minimumdescent(self):
        '\n        get minimumdescent.\n        '
        return self._minimumdescent

    def set_transform(self, t):
        '\n        set_transform is ignored.\n        '
        pass

    def set_offset(self, xy):
        '\n        set offset of the container.\n\n        Accept : tuple of x,y coordinates in display units.\n        '
        self._offset = xy
        self.offset_transform.clear()
        self.offset_transform.translate(xy[0], xy[1])
        self.stale = True

    def get_offset(self):
        '\n        return offset of the container.\n        '
        return self._offset

    def get_window_extent(self, renderer):
        '\n        get the bounding box in display space.\n        '
        (w, h, xd, yd) = self.get_extent(renderer)
        (ox, oy) = self.get_offset()
        return matplotlib.transforms.Bbox.from_bounds((ox - xd), (oy - yd), w, h)

    def get_extent(self, renderer):
        (clean_line, ismath) = self._text.is_math_text(self._text._text)
        (_, h_, d_) = renderer.get_text_width_height_descent('lp', self._text._fontproperties, ismath=False)
        (bbox, info, d) = self._text._get_layout(renderer)
        (w, h) = (bbox.width, bbox.height)
        line = info[(- 1)][0]
        self._baseline_transform.clear()
        if ((len(info) > 1) and self._multilinebaseline):
            d_new = ((0.5 * h) - (0.5 * (h_ - d_)))
            self._baseline_transform.translate(0, (d - d_new))
            d = d_new
        else:
            h_d = max((h_ - d_), (h - d))
            if self.get_minimumdescent():
                d = max(d, d_)
            h = (h_d + d)
        return (w, h, 0.0, d)

    def draw(self, renderer):
        '\n        Draw the children\n        '
        self._text.draw(renderer)
        bbox_artist(self, renderer, fill=False, props=dict(pad=0.0))
        self.stale = False

class AuxTransformBox(OffsetBox):
    '\n    Offset Box with the aux_transform . Its children will be\n    transformed with the aux_transform first then will be\n    offseted. The absolute coordinate of the aux_transform is meaning\n    as it will be automatically adjust so that the left-lower corner\n    of the bounding box of children will be set to (0,0) before the\n    offset transform.\n\n    It is similar to drawing area, except that the extent of the box\n    is not predetermined but calculated from the window extent of its\n    children. Furthermore, the extent of the children will be\n    calculated in the transformed coordinate.\n    '

    def __init__(self, aux_transform):
        self.aux_transform = aux_transform
        OffsetBox.__init__(self)
        self.offset_transform = matplotlib.transforms.Affine2D()
        self.offset_transform.clear()
        self.offset_transform.translate(0, 0)
        self.ref_offset_transform = matplotlib.transforms.Affine2D()
        self.ref_offset_transform.clear()

    def add_artist(self, a):
        'Add any :class:`~matplotlib.artist.Artist` to the container box'
        self._children.append(a)
        a.set_transform(self.get_transform())
        self.stale = True

    def get_transform(self):
        '\n        Return the :class:`~matplotlib.transforms.Transform` applied\n        to the children\n        '
        return ((self.aux_transform + self.ref_offset_transform) + self.offset_transform)

    def set_transform(self, t):
        '\n        set_transform is ignored.\n        '
        pass

    def set_offset(self, xy):
        '\n        set offset of the container.\n\n        Accept : tuple of x,y coordinate in disokay units.\n        '
        self._offset = xy
        self.offset_transform.clear()
        self.offset_transform.translate(xy[0], xy[1])
        self.stale = True

    def get_offset(self):
        '\n        return offset of the container.\n        '
        return self._offset

    def get_window_extent(self, renderer):
        '\n        get the bounding box in display space.\n        '
        (w, h, xd, yd) = self.get_extent(renderer)
        (ox, oy) = self.get_offset()
        return matplotlib.transforms.Bbox.from_bounds((ox - xd), (oy - yd), w, h)

    def get_extent(self, renderer):
        _off = self.offset_transform.to_values()
        self.ref_offset_transform.clear()
        self.offset_transform.clear()
        bboxes = [c.get_window_extent(renderer) for c in self._children]
        ub = matplotlib.transforms.Bbox.union(bboxes)
        self.ref_offset_transform.translate((- ub.x0), (- ub.y0))
        mtx = self.offset_transform.matrix_from_values(*_off)
        self.offset_transform.set_matrix(mtx)
        return (ub.width, ub.height, 0.0, 0.0)

    def draw(self, renderer):
        '\n        Draw the children\n        '
        for c in self._children:
            c.draw(renderer)
        bbox_artist(self, renderer, fill=False, props=dict(pad=0.0))
        self.stale = False

class AnchoredOffsetbox(OffsetBox):
    '\n    An offset box placed according to the legend location\n    loc. AnchoredOffsetbox has a single child. When multiple children\n    is needed, use other OffsetBox class to enclose them.  By default,\n    the offset box is anchored against its parent axes. You may\n    explicitly specify the bbox_to_anchor.\n    '
    zorder = 5

    def __init__(self, loc, pad=0.4, borderpad=0.5, child=None, prop=None, frameon=True, bbox_to_anchor=None, bbox_transform=None, **kwargs):
        "\n        loc is a string or an integer specifying the legend location.\n        The valid  location codes are::\n\n        'upper right'  : 1,\n        'upper left'   : 2,\n        'lower left'   : 3,\n        'lower right'  : 4,\n        'right'        : 5,\n        'center left'  : 6,\n        'center right' : 7,\n        'lower center' : 8,\n        'upper center' : 9,\n        'center'       : 10,\n\n        pad : pad around the child for drawing a frame. given in\n          fraction of fontsize.\n\n        borderpad : pad between offsetbox frame and the bbox_to_anchor,\n\n        child : OffsetBox instance that will be anchored.\n\n        prop : font property. This is only used as a reference for paddings.\n\n        frameon : draw a frame box if True.\n\n        bbox_to_anchor : bbox to anchor. Use self.axes.bbox if None.\n\n        bbox_transform : with which the bbox_to_anchor will be transformed.\n\n        "
        super(AnchoredOffsetbox, self).__init__(**kwargs)
        self.set_bbox_to_anchor(bbox_to_anchor, bbox_transform)
        self.set_child(child)
        self.loc = loc
        self.borderpad = borderpad
        self.pad = pad
        if (prop is None):
            self.prop = FontProperties(size=rcParams['legend.fontsize'])
        elif isinstance(prop, dict):
            self.prop = FontProperties(**prop)
            if ('size' not in prop):
                self.prop.set_size(rcParams['legend.fontsize'])
        else:
            self.prop = prop
        self.patch = FancyBboxPatch(xy=(0.0, 0.0), width=1.0, height=1.0, facecolor='w', edgecolor='k', mutation_scale=self.prop.get_size_in_points(), snap=True)
        self.patch.set_boxstyle('square', pad=0)
        self._drawFrame = frameon

    def set_child(self, child):
        'set the child to be anchored'
        self._child = child
        if (child is not None):
            child.axes = self.axes
        self.stale = True

    def get_child(self):
        'return the child'
        return self._child

    def get_children(self):
        'return the list of children'
        return [self._child]

    def get_extent(self, renderer):
        '\n        return the extent of the artist. The extent of the child\n        added with the pad is returned\n        '
        (w, h, xd, yd) = self.get_child().get_extent(renderer)
        fontsize = renderer.points_to_pixels(self.prop.get_size_in_points())
        pad = (self.pad * fontsize)
        return ((w + (2 * pad)), (h + (2 * pad)), (xd + pad), (yd + pad))

    def get_bbox_to_anchor(self):
        '\n        return the bbox that the legend will be anchored\n        '
        if (self._bbox_to_anchor is None):
            return self.axes.bbox
        else:
            transform = self._bbox_to_anchor_transform
            if (transform is None):
                return self._bbox_to_anchor
            else:
                return TransformedBbox(self._bbox_to_anchor, transform)

    def set_bbox_to_anchor(self, bbox, transform=None):
        '\n        set the bbox that the child will be anchored.\n\n        *bbox* can be a Bbox instance, a list of [left, bottom, width,\n        height], or a list of [left, bottom] where the width and\n        height will be assumed to be zero. The bbox will be\n        transformed to display coordinate by the given transform.\n        '
        if ((bbox is None) or isinstance(bbox, BboxBase)):
            self._bbox_to_anchor = bbox
        else:
            try:
                l = len(bbox)
            except TypeError:
                raise ValueError(('Invalid argument for bbox : %s' % str(bbox)))
            if (l == 2):
                bbox = [bbox[0], bbox[1], 0, 0]
            self._bbox_to_anchor = matplotlib.transforms.Bbox.from_bounds(*bbox)
        self._bbox_to_anchor_transform = transform
        self.stale = True

    def get_window_extent(self, renderer):
        '\n        get the bounding box in display space.\n        '
        self._update_offset_func(renderer)
        (w, h, xd, yd) = self.get_extent(renderer)
        (ox, oy) = self.get_offset(w, h, xd, yd, renderer)
        return matplotlib.transforms.Bbox.from_bounds((ox - xd), (oy - yd), w, h)

    def _update_offset_func(self, renderer, fontsize=None):
        '\n        Update the offset func which depends on the dpi of the\n        renderer (because of the padding).\n        '
        if (fontsize is None):
            fontsize = renderer.points_to_pixels(self.prop.get_size_in_points())

        def _offset(w, h, xd, yd, renderer, fontsize=fontsize, self=self):
            bbox = matplotlib.transforms.Bbox.from_bounds(0, 0, w, h)
            borderpad = (self.borderpad * fontsize)
            bbox_to_anchor = self.get_bbox_to_anchor()
            (x0, y0) = self._get_anchored_bbox(self.loc, bbox, bbox_to_anchor, borderpad)
            return ((x0 + xd), (y0 + yd))
        self.set_offset(_offset)

    def update_frame(self, bbox, fontsize=None):
        self.patch.set_bounds(bbox.x0, bbox.y0, bbox.width, bbox.height)
        if fontsize:
            self.patch.set_mutation_scale(fontsize)

    def draw(self, renderer):
        'draw the artist'
        if (not self.get_visible()):
            return
        fontsize = renderer.points_to_pixels(self.prop.get_size_in_points())
        self._update_offset_func(renderer, fontsize)
        if self._drawFrame:
            bbox = self.get_window_extent(renderer)
            self.update_frame(bbox, fontsize)
            self.patch.draw(renderer)
        (width, height, xdescent, ydescent) = self.get_extent(renderer)
        (px, py) = self.get_offset(width, height, xdescent, ydescent, renderer)
        self.get_child().set_offset((px, py))
        self.get_child().draw(renderer)
        self.stale = False

    def _get_anchored_bbox(self, loc, bbox, parentbbox, borderpad):
        '\n        return the position of the bbox anchored at the parentbbox\n        with the loc code, with the borderpad.\n        '
        assert (loc in range(1, 11))
        (BEST, UR, UL, LL, LR, R, CL, CR, LC, UC, C) = list(xrange(11))
        anchor_coefs = {UR: 'NE', UL: 'NW', LL: 'SW', LR: 'SE', R: 'E', CL: 'W', CR: 'E', LC: 'S', UC: 'N', C: 'C'}
        c = anchor_coefs[loc]
        container = parentbbox.padded((- borderpad))
        anchored_box = bbox.anchored(c, container=container)
        return (anchored_box.x0, anchored_box.y0)

class AnchoredText(AnchoredOffsetbox):
    '\n    AnchoredOffsetbox with Text.\n    '

    def __init__(self, s, loc, pad=0.4, borderpad=0.5, prop=None, **kwargs):
        '\n        Parameters\n        ----------\n        s : string\n            Text.\n\n        loc : str\n            Location code.\n\n        pad : float, optional\n            Pad between the text and the frame as fraction of the font\n            size.\n\n        borderpad : float, optional\n            Pad between the frame and the axes (or *bbox_to_anchor*).\n\n        prop : `matplotlib.font_manager.FontProperties`\n            Font properties.\n\n        Notes\n        -----\n        Other keyword parameters of `AnchoredOffsetbox` are also\n        allowed.\n        '
        if (prop is None):
            prop = {}
        propkeys = list(six.iterkeys(prop))
        badkwargs = ('ha', 'horizontalalignment', 'va', 'verticalalignment')
        if (set(badkwargs) & set(propkeys)):
            warnings.warn('Mixing horizontalalignment or verticalalignment with AnchoredText is not supported.')
        self.txt = TextArea(s, textprops=prop, minimumdescent=False)
        fp = self.txt._text.get_fontproperties()
        super(AnchoredText, self).__init__(loc, pad=pad, borderpad=borderpad, child=self.txt, prop=fp, **kwargs)

class OffsetImage(OffsetBox):

    def __init__(self, arr, zoom=1, cmap=None, norm=None, interpolation=None, origin=None, filternorm=1, filterrad=4.0, resample=False, dpi_cor=True, **kwargs):
        OffsetBox.__init__(self)
        self._dpi_cor = dpi_cor
        self.image = BboxImage(bbox=self.get_window_extent, cmap=cmap, norm=norm, interpolation=interpolation, origin=origin, filternorm=filternorm, filterrad=filterrad, resample=resample, **kwargs)
        self._children = [self.image]
        self.set_zoom(zoom)
        self.set_data(arr)

    def set_data(self, arr):
        self._data = numpy.asarray(arr)
        self.image.set_data(self._data)
        self.stale = True

    def get_data(self):
        return self._data

    def set_zoom(self, zoom):
        self._zoom = zoom
        self.stale = True

    def get_zoom(self):
        return self._zoom

    def get_offset(self):
        '\n        return offset of the container.\n        '
        return self._offset

    def get_children(self):
        return [self.image]

    def get_window_extent(self, renderer):
        '\n        get the bounding box in display space.\n        '
        (w, h, xd, yd) = self.get_extent(renderer)
        (ox, oy) = self.get_offset()
        return matplotlib.transforms.Bbox.from_bounds((ox - xd), (oy - yd), w, h)

    def get_extent(self, renderer):
        if self._dpi_cor:
            dpi_cor = renderer.points_to_pixels(1.0)
        else:
            dpi_cor = 1.0
        zoom = self.get_zoom()
        data = self.get_data()
        (ny, nx) = data.shape[:2]
        (w, h) = (((dpi_cor * nx) * zoom), ((dpi_cor * ny) * zoom))
        return (w, h, 0, 0)

    def draw(self, renderer):
        '\n        Draw the children\n        '
        self.image.draw(renderer)
        self.stale = False

class AnnotationBbox(matplotlib.artist.Artist, _AnnotationBase):
    '\n    Annotation-like class, but with offsetbox instead of Text.\n    '
    zorder = 3

    def __str__(self):
        return ('AnnotationBbox(%g,%g)' % (self.xy[0], self.xy[1]))

    @matplotlib.docstring.dedent_interpd
    def __init__(self, offsetbox, xy, xybox=None, xycoords='data', boxcoords=None, frameon=True, pad=0.4, annotation_clip=None, box_alignment=(0.5, 0.5), bboxprops=None, arrowprops=None, fontsize=None, **kwargs):
        '\n        *offsetbox* : OffsetBox instance\n\n        *xycoords* : same as Annotation but can be a tuple of two\n           strings which are interpreted as x and y coordinates.\n\n        *boxcoords* : similar to textcoords as Annotation but can be a\n           tuple of two strings which are interpreted as x and y\n           coordinates.\n\n        *box_alignment* : a tuple of two floats for a vertical and\n           horizontal alignment of the offset box w.r.t. the *boxcoords*.\n           The lower-left corner is (0.0) and upper-right corner is (1.1).\n\n        other parameters are identical to that of Annotation.\n        '
        matplotlib.artist.Artist.__init__(self, **kwargs)
        matplotlib.text._AnnotationBase.__init__(self, xy, xycoords=xycoords, annotation_clip=annotation_clip)
        self.offsetbox = offsetbox
        self.arrowprops = arrowprops
        self.set_fontsize(fontsize)
        if (xybox is None):
            self.xybox = xy
        else:
            self.xybox = xybox
        if (boxcoords is None):
            self.boxcoords = xycoords
        else:
            self.boxcoords = boxcoords
        if (arrowprops is not None):
            self._arrow_relpos = self.arrowprops.pop('relpos', (0.5, 0.5))
            self.arrow_patch = FancyArrowPatch((0, 0), (1, 1), **self.arrowprops)
        else:
            self._arrow_relpos = None
            self.arrow_patch = None
        self._box_alignment = box_alignment
        self.patch = FancyBboxPatch(xy=(0.0, 0.0), width=1.0, height=1.0, facecolor='w', edgecolor='k', mutation_scale=self.prop.get_size_in_points(), snap=True)
        self.patch.set_boxstyle('square', pad=pad)
        if bboxprops:
            self.patch.set(**bboxprops)
        self._drawFrame = frameon

    @property
    def xyann(self):
        return self.xybox

    @xyann.setter
    def xyann(self, xyann):
        self.xybox = xyann
        self.stale = True

    @property
    def anncoords(self):
        return self.boxcoords

    @anncoords.setter
    def anncoords(self, coords):
        self.boxcoords = coords
        self.stale = True

    def contains(self, event):
        (t, tinfo) = self.offsetbox.contains(event)
        return (t, tinfo)

    def get_children(self):
        children = [self.offsetbox, self.patch]
        if self.arrow_patch:
            children.append(self.arrow_patch)
        return children

    def set_figure(self, fig):
        if (self.arrow_patch is not None):
            self.arrow_patch.set_figure(fig)
        self.offsetbox.set_figure(fig)
        matplotlib.artist.Artist.set_figure(self, fig)

    def set_fontsize(self, s=None):
        '\n        set fontsize in points\n        '
        if (s is None):
            s = rcParams['legend.fontsize']
        self.prop = FontProperties(size=s)
        self.stale = True

    def get_fontsize(self, s=None):
        '\n        return fontsize in points\n        '
        return self.prop.get_size_in_points()

    def update_positions(self, renderer):
        '\n        Update the pixel positions of the annotated point and the text.\n        '
        xy_pixel = self._get_position_xy(renderer)
        self._update_position_xybox(renderer, xy_pixel)
        mutation_scale = renderer.points_to_pixels(self.get_fontsize())
        self.patch.set_mutation_scale(mutation_scale)
        if self.arrow_patch:
            self.arrow_patch.set_mutation_scale(mutation_scale)

    def _update_position_xybox(self, renderer, xy_pixel):
        '\n        Update the pixel positions of the annotation text and the arrow\n        patch.\n        '
        (x, y) = self.xybox
        if isinstance(self.boxcoords, tuple):
            (xcoord, ycoord) = self.boxcoords
            (x1, y1) = self._get_xy(renderer, x, y, xcoord)
            (x2, y2) = self._get_xy(renderer, x, y, ycoord)
            (ox0, oy0) = (x1, y2)
        else:
            (ox0, oy0) = self._get_xy(renderer, x, y, self.boxcoords)
        (w, h, xd, yd) = self.offsetbox.get_extent(renderer)
        (_fw, _fh) = self._box_alignment
        self.offsetbox.set_offset((((ox0 - (_fw * w)) + xd), ((oy0 - (_fh * h)) + yd)))
        bbox = self.offsetbox.get_window_extent(renderer)
        self.patch.set_bounds(bbox.x0, bbox.y0, bbox.width, bbox.height)
        (x, y) = xy_pixel
        (ox1, oy1) = (x, y)
        if self.arrowprops:
            (x0, y0) = (x, y)
            d = self.arrowprops.copy()
            relpos = self._arrow_relpos
            ox0 = (bbox.x0 + (bbox.width * relpos[0]))
            oy0 = (bbox.y0 + (bbox.height * relpos[1]))
            self.arrow_patch.set_positions((ox0, oy0), (ox1, oy1))
            fs = self.prop.get_size_in_points()
            mutation_scale = d.pop('mutation_scale', fs)
            mutation_scale = renderer.points_to_pixels(mutation_scale)
            self.arrow_patch.set_mutation_scale(mutation_scale)
            patchA = d.pop('patchA', self.patch)
            self.arrow_patch.set_patchA(patchA)

    def draw(self, renderer):
        '\n        Draw the :class:`Annotation` object to the given *renderer*.\n        '
        if (renderer is not None):
            self._renderer = renderer
        if (not self.get_visible()):
            return
        xy_pixel = self._get_position_xy(renderer)
        if (not self._check_xy(renderer, xy_pixel)):
            return
        self.update_positions(renderer)
        if (self.arrow_patch is not None):
            if ((self.arrow_patch.figure is None) and (self.figure is not None)):
                self.arrow_patch.figure = self.figure
            self.arrow_patch.draw(renderer)
        if self._drawFrame:
            self.patch.draw(renderer)
        self.offsetbox.draw(renderer)
        self.stale = False

class DraggableBase(object):
    '\n    helper code for a draggable artist (legend, offsetbox)\n    The derived class must override following two method.\n\n      def saveoffset(self):\n          pass\n\n      def update_offset(self, dx, dy):\n          pass\n\n    *saveoffset* is called when the object is picked for dragging and it is\n    meant to save reference position of the artist.\n\n    *update_offset* is called during the dragging. dx and dy is the pixel\n     offset from the point where the mouse drag started.\n\n    Optionally you may override following two methods.\n\n      def artist_picker(self, artist, evt):\n          return self.ref_artist.contains(evt)\n\n      def finalize_offset(self):\n          pass\n\n    *artist_picker* is a picker method that will be\n     used. *finalize_offset* is called when the mouse is released. In\n     current implementaion of DraggableLegend and DraggableAnnotation,\n     *update_offset* places the artists simply in display\n     coordinates. And *finalize_offset* recalculate their position in\n     the normalized axes coordinate and set a relavant attribute.\n\n    '

    def __init__(self, ref_artist, use_blit=False):
        self.ref_artist = ref_artist
        self.got_artist = False
        self.canvas = self.ref_artist.figure.canvas
        self._use_blit = (use_blit and self.canvas.supports_blit)
        c2 = self.canvas.mpl_connect('pick_event', self.on_pick)
        c3 = self.canvas.mpl_connect('button_release_event', self.on_release)
        ref_artist.set_picker(self.artist_picker)
        self.cids = [c2, c3]

    def on_motion(self, evt):
        if self.got_artist:
            dx = (evt.x - self.mouse_x)
            dy = (evt.y - self.mouse_y)
            self.update_offset(dx, dy)
            self.canvas.draw()

    def on_motion_blit(self, evt):
        if self.got_artist:
            dx = (evt.x - self.mouse_x)
            dy = (evt.y - self.mouse_y)
            self.update_offset(dx, dy)
            self.canvas.restore_region(self.background)
            self.ref_artist.draw(self.ref_artist.figure._cachedRenderer)
            self.canvas.blit(self.ref_artist.figure.bbox)

    def on_pick(self, evt):
        if (evt.artist == self.ref_artist):
            self.mouse_x = evt.mouseevent.x
            self.mouse_y = evt.mouseevent.y
            self.got_artist = True
            if self._use_blit:
                self.ref_artist.set_animated(True)
                self.canvas.draw()
                self.background = self.canvas.copy_from_bbox(self.ref_artist.figure.bbox)
                self.ref_artist.draw(self.ref_artist.figure._cachedRenderer)
                self.canvas.blit(self.ref_artist.figure.bbox)
                self._c1 = self.canvas.mpl_connect('motion_notify_event', self.on_motion_blit)
            else:
                self._c1 = self.canvas.mpl_connect('motion_notify_event', self.on_motion)
            self.save_offset()

    def on_release(self, event):
        if self.got_artist:
            self.finalize_offset()
            self.got_artist = False
            self.canvas.mpl_disconnect(self._c1)
            if self._use_blit:
                self.ref_artist.set_animated(False)

    def disconnect(self):
        'disconnect the callbacks'
        for cid in self.cids:
            self.canvas.mpl_disconnect(cid)
        try:
            c1 = self._c1
        except AttributeError:
            pass
        else:
            self.canvas.mpl_disconnect(c1)

    def artist_picker(self, artist, evt):
        return self.ref_artist.contains(evt)

    def save_offset(self):
        pass

    def update_offset(self, dx, dy):
        pass

    def finalize_offset(self):
        pass

class DraggableOffsetBox(DraggableBase):

    def __init__(self, ref_artist, offsetbox, use_blit=False):
        DraggableBase.__init__(self, ref_artist, use_blit=use_blit)
        self.offsetbox = offsetbox

    def save_offset(self):
        offsetbox = self.offsetbox
        renderer = offsetbox.figure._cachedRenderer
        (w, h, xd, yd) = offsetbox.get_extent(renderer)
        offset = offsetbox.get_offset(w, h, xd, yd, renderer)
        (self.offsetbox_x, self.offsetbox_y) = offset
        self.offsetbox.set_offset(offset)

    def update_offset(self, dx, dy):
        loc_in_canvas = ((self.offsetbox_x + dx), (self.offsetbox_y + dy))
        self.offsetbox.set_offset(loc_in_canvas)

    def get_loc_in_canvas(self):
        offsetbox = self.offsetbox
        renderer = offsetbox.figure._cachedRenderer
        (w, h, xd, yd) = offsetbox.get_extent(renderer)
        (ox, oy) = offsetbox._offset
        loc_in_canvas = ((ox - xd), (oy - yd))
        return loc_in_canvas

class DraggableAnnotation(DraggableBase):

    def __init__(self, annotation, use_blit=False):
        DraggableBase.__init__(self, annotation, use_blit=use_blit)
        self.annotation = annotation

    def save_offset(self):
        ann = self.annotation
        (self.ox, self.oy) = ann.get_transform().transform(ann.xyann)

    def update_offset(self, dx, dy):
        ann = self.annotation
        ann.xyann = ann.get_transform().inverted().transform(((self.ox + dx), (self.oy + dy)))
if (__name__ == '__main__'):
    import matplotlib.pyplot as plt
    fig = matplotlib.pyplot.figure(1)
    fig.clf()
    ax = matplotlib.pyplot.subplot(121)
    kwargs = dict()
    tempResult = arange(256)
	
===================================================================	
Path.unit_regular_star: 336	
----------------------------	

'\n        Return a :class:`Path` for a unit regular star\n        with the given numVertices and radius of 1.0, centered at (0,\n        0).\n        '
if (numVertices <= 16):
    path = cls._unit_regular_stars.get((numVertices, innerCircle))
else:
    path = None
if (path is None):
    ns2 = (numVertices * 2)
    tempResult = arange((ns2 + 1))
	
===================================================================	
Path.unit_regular_polygon: 314	
----------------------------	

'\n        Return a :class:`Path` instance for a unit regular\n        polygon with the given *numVertices* and radius of 1.0,\n        centered at (0, 0).\n        '
if (numVertices <= 16):
    path = cls._unit_regular_polygons.get(numVertices)
else:
    path = None
if (path is None):
    tempResult = arange((numVertices + 1))
	
===================================================================	
subplots: 507	
----------------------------	

"\n    Create a figure and a set of subplots\n\n    This utility wrapper makes it convenient to create common layouts of\n    subplots, including the enclosing figure object, in a single call.\n\n    Parameters\n    ----------\n    nrows, ncols : int, optional, default: 1\n        Number of rows/columns of the subplot grid.\n\n    sharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n        Controls sharing of properties among x (`sharex`) or y (`sharey`)\n        axes:\n\n            - True or 'all': x- or y-axis will be shared among all\n              subplots.\n            - False or 'none': each subplot x- or y-axis will be\n              independent.\n            - 'row': each subplot row will share an x- or y-axis.\n            - 'col': each subplot column will share an x- or y-axis.\n\n        When subplots have a shared x-axis along a column, only the x tick\n        labels of the bottom subplot are visible.  Similarly, when subplots\n        have a shared y-axis along a row, only the y tick labels of the first\n        column subplot are visible.\n\n    squeeze : bool, optional, default: True\n        - If True, extra dimensions are squeezed out from the returned Axes\n          object:\n\n            - if only one subplot is constructed (nrows=ncols=1), the\n              resulting single Axes object is returned as a scalar.\n            - for Nx1 or 1xN subplots, the returned object is a 1D numpy\n              object array of Axes objects are returned as numpy 1D arrays.\n            - for NxM, subplots with N>1 and M>1 are returned as a 2D arrays.\n\n        - If False, no squeezing at all is done: the returned Axes object is\n          always a 2D array containing Axes instances, even if it ends up\n          being 1x1.\n\n    subplot_kw : dict, optional\n        Dict with keywords passed to the\n        :meth:`~matplotlib.figure.Figure.add_subplot` call used to create each\n        subplot.\n\n    gridspec_kw : dict, optional\n        Dict with keywords passed to the\n        :class:`~matplotlib.gridspec.GridSpec` constructor used to create the\n        grid the subplots are placed on.\n\n    fig_kw : dict, optional\n        Dict with keywords passed to the :func:`figure` call.  Note that all\n        keywords not recognized above will be automatically included here.\n\n    Returns\n    -------\n    fig : :class:`matplotlib.figure.Figure` object\n\n    ax : Axes object or array of Axes objects.\n\n        ax can be either a single :class:`matplotlib.axes.Axes` object or an\n        array of Axes objects if more than one subplot was created.  The\n        dimensions of the resulting array can be controlled with the squeeze\n        keyword, see above.\n\n    Examples\n    --------\n    First create some toy data:\n\n    >>> x = np.linspace(0, 2*np.pi, 400)\n    >>> y = np.sin(x**2)\n\n    Creates just a figure and only one subplot\n\n    >>> fig, ax = plt.subplots()\n    >>> ax.plot(x, y)\n    >>> ax.set_title('Simple plot')\n\n    Creates two subplots and unpacks the output array immediately\n\n    >>> f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n    >>> ax1.plot(x, y)\n    >>> ax1.set_title('Sharing Y axis')\n    >>> ax2.scatter(x, y)\n\n    Creates four polar axes, and accesses them through the returned array\n\n    >>> fig, axes = plt.subplots(2, 2, subplot_kw=dict(polar=True))\n    >>> axes[0, 0].plot(x, y)\n    >>> axes[1, 1].scatter(x, y)\n\n    Share a X axis with each column of subplots\n\n    >>> plt.subplots(2, 2, sharex='col')\n\n    Share a Y axis with each row of subplots\n\n    >>> plt.subplots(2, 2, sharey='row')\n\n    Share both X and Y axes with all subplots\n\n    >>> plt.subplots(2, 2, sharex='all', sharey='all')\n\n    Note that this is the same as\n\n    >>> plt.subplots(2, 2, sharex=True, sharey=True)\n\n    See Also\n    --------\n    figure\n    subplot\n    "
if isinstance(sharex, bool):
    if sharex:
        sharex = 'all'
    else:
        sharex = 'none'
if isinstance(sharey, bool):
    if sharey:
        sharey = 'all'
    else:
        sharey = 'none'
share_values = ['all', 'row', 'col', 'none']
if (sharex not in share_values):
    if isinstance(sharex, int):
        warnings.warn("sharex argument to subplots() was an integer. Did you intend to use subplot() (without 's')?")
    raise ValueError(('sharex [%s] must be one of %s' % (sharex, share_values)))
if (sharey not in share_values):
    raise ValueError(('sharey [%s] must be one of %s' % (sharey, share_values)))
if (subplot_kw is None):
    subplot_kw = {}
if (gridspec_kw is None):
    gridspec_kw = {}
fig = figure(**fig_kw)
gs = GridSpec(nrows, ncols, **gridspec_kw)
nplots = (nrows * ncols)
axarr = numpy.empty(nplots, dtype=object)
ax0 = fig.add_subplot(gs[(0, 0)], **subplot_kw)
axarr[0] = ax0
(r, c) = numpy.mgrid[:nrows, :ncols]
r = (r.flatten() * ncols)
c = c.flatten()
tempResult = arange(nplots)
	
===================================================================	
Quiver._h_arrows: 394	
----------------------------	

' length is in arrow width units '
minsh = (self.minshaft * self.headlength)
N = len(length)
length = length.reshape(N, 1)
numpy.clip(length, 0, (2 ** 16), out=length)
x = numpy.array([0, (- self.headaxislength), (- self.headlength), 0], numpy.float64)
x = (x + (numpy.array([0, 1, 1, 1]) * length))
y = (0.5 * numpy.array([1, 1, self.headwidth, 0], numpy.float64))
y = numpy.repeat(y[numpy.newaxis, :], N, axis=0)
x0 = numpy.array([0, (minsh - self.headaxislength), (minsh - self.headlength), minsh], numpy.float64)
y0 = (0.5 * numpy.array([1, 1, self.headwidth, 0], numpy.float64))
ii = [0, 1, 2, 3, 2, 1, 0, 0]
X = x.take(ii, 1)
Y = y.take(ii, 1)
Y[:, 3:(- 1)] *= (- 1)
X0 = x0.take(ii)
Y0 = y0.take(ii)
Y0[3:(- 1)] *= (- 1)
shrink = ((length / minsh) if (minsh != 0.0) else 0.0)
X0 = (shrink * X0[numpy.newaxis, :])
Y0 = (shrink * Y0[numpy.newaxis, :])
short = numpy.repeat((length < minsh), 8, axis=1)
matplotlib.cbook._putmask(X, short, X0)
matplotlib.cbook._putmask(Y, short, Y0)
if (self.pivot == 'middle'):
    X -= (0.5 * X[:, 3, numpy.newaxis])
elif (self.pivot == 'tip'):
    X = (X - X[:, 3, numpy.newaxis])
elif (self.pivot != 'tail'):
    raise ValueError("Quiver.pivot must have value in {{'middle', 'tip', 'tail'}} not {0}".format(self.pivot))
tooshort = (length < self.minlength)
if tooshort.any():
    tempResult = arange(0, 8, 1, numpy.float64)
	
===================================================================	
_parse_args: 150	
----------------------------	

(X, Y, U, V, C) = ([None] * 5)
args = list(args)
if ((len(args) == 3) or (len(args) == 5)):
    C = numpy.atleast_1d(args.pop((- 1)))
V = numpy.atleast_1d(args.pop((- 1)))
U = numpy.atleast_1d(args.pop((- 1)))
if (U.ndim == 1):
    (nr, nc) = (1, U.shape[0])
else:
    (nr, nc) = U.shape
if (len(args) == 2):
    (X, Y) = [np.array(a).ravel() for a in args]
    if ((len(X) == nc) and (len(Y) == nr)):
        (X, Y) = [a.ravel() for a in numpy.meshgrid(X, Y)]
else:
    tempResult = arange(nc)
	
===================================================================	
_parse_args: 150	
----------------------------	

(X, Y, U, V, C) = ([None] * 5)
args = list(args)
if ((len(args) == 3) or (len(args) == 5)):
    C = numpy.atleast_1d(args.pop((- 1)))
V = numpy.atleast_1d(args.pop((- 1)))
U = numpy.atleast_1d(args.pop((- 1)))
if (U.ndim == 1):
    (nr, nc) = (1, U.shape[0])
else:
    (nr, nc) = U.shape
if (len(args) == 2):
    (X, Y) = [np.array(a).ravel() for a in args]
    if ((len(X) == nc) and (len(Y) == nr)):
        (X, Y) = [a.ravel() for a in numpy.meshgrid(X, Y)]
else:
    tempResult = arange(nr)
	
===================================================================	
stackplot: 29	
----------------------------	

"Draws a stacked area plot.\n\n    *x* : 1d array of dimension N\n\n    *y* : 2d array of dimension MxN, OR any number 1d arrays each of dimension\n          1xN. The data is assumed to be unstacked. Each of the following\n          calls is legal::\n\n            stackplot(x, y)               # where y is MxN\n            stackplot(x, y1, y2, y3, y4)  # where y1, y2, y3, y4, are all 1xNm\n\n    Keyword arguments:\n\n    *baseline* : ['zero', 'sym', 'wiggle', 'weighted_wiggle']\n                Method used to calculate the baseline. 'zero' is just a\n                simple stacked plot. 'sym' is symmetric around zero and\n                is sometimes called `ThemeRiver`.  'wiggle' minimizes the\n                sum of the squared slopes. 'weighted_wiggle' does the\n                same but weights to account for size of each layer.\n                It is also called `Streamgraph`-layout. More details\n                can be found at http://leebyron.com/streamgraph/.\n\n\n    *labels* : A list or tuple of labels to assign to each data series.\n\n\n    *colors* : A list or tuple of colors. These will be cycled through and\n               used to colour the stacked areas.\n               All other keyword arguments are passed to\n               :func:`~matplotlib.Axes.fill_between`\n\n    Returns *r* : A list of\n    :class:`~matplotlib.collections.PolyCollection`, one for each\n    element in the stacked area plot.\n    "
if (len(args) == 1):
    y = numpy.atleast_2d(*args)
elif (len(args) > 1):
    y = numpy.row_stack(args)
labels = iter(kwargs.pop('labels', []))
colors = kwargs.pop('colors', None)
if (colors is not None):
    axes.set_prop_cycle(cycler('color', colors))
baseline = kwargs.pop('baseline', 'zero')
stack = numpy.cumsum(y, axis=0)
if (baseline == 'zero'):
    first_line = 0.0
elif (baseline == 'sym'):
    first_line = ((- numpy.sum(y, 0)) * 0.5)
    stack += first_line[None, :]
elif (baseline == 'wiggle'):
    m = y.shape[0]
    tempResult = arange(0, m)
	
===================================================================	
SymmetricalLogLocator.tick_values: 1355	
----------------------------	

b = self._base
t = self._linthresh
if (vmax < vmin):
    (vmin, vmax) = (vmax, vmin)
has_a = has_b = has_c = False
if (vmin < (- t)):
    has_a = True
    if (vmax > (- t)):
        has_b = True
        if (vmax > t):
            has_c = True
elif (vmin < 0):
    if (vmax > 0):
        has_b = True
        if (vmax > t):
            has_c = True
    else:
        return [vmin, vmax]
elif (vmin < t):
    if (vmax > t):
        has_b = True
        has_c = True
    else:
        return [vmin, vmax]
else:
    has_c = True

def get_log_range(lo, hi):
    lo = numpy.floor((numpy.log(lo) / numpy.log(b)))
    hi = numpy.ceil((numpy.log(hi) / numpy.log(b)))
    return (lo, hi)
if has_a:
    if has_b:
        a_range = get_log_range(t, ((- vmin) + 1))
    else:
        a_range = get_log_range((- vmax), ((- vmin) + 1))
else:
    a_range = (0, 0)
if has_c:
    if has_b:
        c_range = get_log_range(t, (vmax + 1))
    else:
        c_range = get_log_range(vmin, (vmax + 1))
else:
    c_range = (0, 0)
total_ticks = ((a_range[1] - a_range[0]) + (c_range[1] - c_range[0]))
if has_b:
    total_ticks += 1
stride = max(numpy.floor((float(total_ticks) / (self.numticks - 1))), 1)
decades = []
if has_a:
    tempResult = arange(a_range[0], a_range[1], stride)
	
===================================================================	
SymmetricalLogLocator.tick_values: 1359	
----------------------------	

b = self._base
t = self._linthresh
if (vmax < vmin):
    (vmin, vmax) = (vmax, vmin)
has_a = has_b = has_c = False
if (vmin < (- t)):
    has_a = True
    if (vmax > (- t)):
        has_b = True
        if (vmax > t):
            has_c = True
elif (vmin < 0):
    if (vmax > 0):
        has_b = True
        if (vmax > t):
            has_c = True
    else:
        return [vmin, vmax]
elif (vmin < t):
    if (vmax > t):
        has_b = True
        has_c = True
    else:
        return [vmin, vmax]
else:
    has_c = True

def get_log_range(lo, hi):
    lo = numpy.floor((numpy.log(lo) / numpy.log(b)))
    hi = numpy.ceil((numpy.log(hi) / numpy.log(b)))
    return (lo, hi)
if has_a:
    if has_b:
        a_range = get_log_range(t, ((- vmin) + 1))
    else:
        a_range = get_log_range((- vmax), ((- vmin) + 1))
else:
    a_range = (0, 0)
if has_c:
    if has_b:
        c_range = get_log_range(t, (vmax + 1))
    else:
        c_range = get_log_range(vmin, (vmax + 1))
else:
    c_range = (0, 0)
total_ticks = ((a_range[1] - a_range[0]) + (c_range[1] - c_range[0]))
if has_b:
    total_ticks += 1
stride = max(numpy.floor((float(total_ticks) / (self.numticks - 1))), 1)
decades = []
if has_a:
    decades.extend(((- 1) * (b ** numpy.arange(a_range[0], a_range[1], stride)[::(- 1)])))
if has_b:
    decades.append(0.0)
if has_c:
    tempResult = arange(c_range[0], c_range[1], stride)
	
===================================================================	
SymmetricalLogLocator.tick_values: 1361	
----------------------------	

b = self._base
t = self._linthresh
if (vmax < vmin):
    (vmin, vmax) = (vmax, vmin)
has_a = has_b = has_c = False
if (vmin < (- t)):
    has_a = True
    if (vmax > (- t)):
        has_b = True
        if (vmax > t):
            has_c = True
elif (vmin < 0):
    if (vmax > 0):
        has_b = True
        if (vmax > t):
            has_c = True
    else:
        return [vmin, vmax]
elif (vmin < t):
    if (vmax > t):
        has_b = True
        has_c = True
    else:
        return [vmin, vmax]
else:
    has_c = True

def get_log_range(lo, hi):
    lo = numpy.floor((numpy.log(lo) / numpy.log(b)))
    hi = numpy.ceil((numpy.log(hi) / numpy.log(b)))
    return (lo, hi)
if has_a:
    if has_b:
        a_range = get_log_range(t, ((- vmin) + 1))
    else:
        a_range = get_log_range((- vmax), ((- vmin) + 1))
else:
    a_range = (0, 0)
if has_c:
    if has_b:
        c_range = get_log_range(t, (vmax + 1))
    else:
        c_range = get_log_range(vmin, (vmax + 1))
else:
    c_range = (0, 0)
total_ticks = ((a_range[1] - a_range[0]) + (c_range[1] - c_range[0]))
if has_b:
    total_ticks += 1
stride = max(numpy.floor((float(total_ticks) / (self.numticks - 1))), 1)
decades = []
if has_a:
    decades.extend(((- 1) * (b ** numpy.arange(a_range[0], a_range[1], stride)[::(- 1)])))
if has_b:
    decades.append(0.0)
if has_c:
    decades.extend((b ** numpy.arange(c_range[0], c_range[1], stride)))
if (self._subs is None):
    tempResult = arange(2.0, b)
	
===================================================================	
AutoMinorLocator.__call__: 1511	
----------------------------	

'Return the locations of the ticks'
majorlocs = self.axis.get_majorticklocs()
try:
    majorstep = (majorlocs[1] - majorlocs[0])
except IndexError:
    majorstep = 0
if (self.ndivs is None):
    if (majorstep == 0):
        ndivs = 1
    else:
        x = int(numpy.round((10 ** (numpy.log10(majorstep) % 1))))
        if (x in [1, 5, 10]):
            ndivs = 5
        else:
            ndivs = 4
else:
    ndivs = self.ndivs
minorstep = (majorstep / ndivs)
(vmin, vmax) = self.axis.get_view_interval()
if (vmin > vmax):
    (vmin, vmax) = (vmax, vmin)
if (len(majorlocs) > 0):
    t0 = majorlocs[0]
    tmin = ((((vmin - t0) // minorstep) + 1) * minorstep)
    tmax = ((((vmax - t0) // minorstep) + 1) * minorstep)
    tempResult = arange(tmin, tmax, minorstep)
	
===================================================================	
MultipleLocator.tick_values: 946	
----------------------------	

if (vmax < vmin):
    (vmin, vmax) = (vmax, vmin)
vmin = self._base.ge(vmin)
base = self._base.get_base()
n = (((vmax - vmin) + (0.001 * base)) // base)
tempResult = arange((n + 3))
	
===================================================================	
IndexLocator.tick_values: 783	
----------------------------	

tempResult = arange((vmin + self.offset), (vmax + 1), self._base)
	
===================================================================	
MaxNLocator._raw_ticks: 1063	
----------------------------	

if (self._nbins == 'auto'):
    if (self.axis is not None):
        nbins = max(min(self.axis.get_tick_space(), 9), max(1, (self._min_n_ticks - 1)))
    else:
        nbins = 9
else:
    nbins = self._nbins
(scale, offset) = scale_range(vmin, vmax, nbins)
_vmin = (vmin - offset)
_vmax = (vmax - offset)
raw_step = ((vmax - vmin) / nbins)
steps = (self._extended_steps * scale)
if self._integer:
    igood = ((steps < 1) | (numpy.abs((steps - numpy.round(steps))) < 0.001))
    steps = steps[igood]
istep = numpy.nonzero((steps >= raw_step))[0][0]
if (rcParams['axes.autolimit_mode'] == 'round_numbers'):
    for istep in range(istep, len(steps)):
        step = steps[istep]
        best_vmin = ((_vmin // step) * step)
        best_vmax = (best_vmin + (step * nbins))
        if (best_vmax >= _vmax):
            break
for i in range(istep):
    step = steps[(istep - i)]
    if (self._integer and ((numpy.floor(_vmax) - numpy.ceil(_vmin)) >= (self._min_n_ticks - 1))):
        step = max(1, step)
    best_vmin = ((_vmin // step) * step)
    low = numpy.round((Base(step).le((_vmin - best_vmin)) / step))
    high = numpy.round((Base(step).ge((_vmax - best_vmin)) / step))
    tempResult = arange(low, (high + 1))
	
===================================================================	
LogitLocator.tick_values: 1426	
----------------------------	

if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    raise NotImplementedError('Polar axis cannot be logit scaled yet')
(vmin, vmax) = self.nonsingular(vmin, vmax)
vmin = numpy.log10((vmin / (1 - vmin)))
vmax = numpy.log10((vmax / (1 - vmax)))
decade_min = numpy.floor(vmin)
decade_max = numpy.ceil(vmax)
if (not self.minor):
    ticklocs = []
    if (decade_min <= (- 1)):
        tempResult = arange(decade_min, min(0, (decade_max + 1)))
	
===================================================================	
LogitLocator.tick_values: 1431	
----------------------------	

if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    raise NotImplementedError('Polar axis cannot be logit scaled yet')
(vmin, vmax) = self.nonsingular(vmin, vmax)
vmin = numpy.log10((vmin / (1 - vmin)))
vmax = numpy.log10((vmax / (1 - vmax)))
decade_min = numpy.floor(vmin)
decade_max = numpy.ceil(vmax)
if (not self.minor):
    ticklocs = []
    if (decade_min <= (- 1)):
        expo = numpy.arange(decade_min, min(0, (decade_max + 1)))
        ticklocs.extend(list((10 ** expo)))
    if ((decade_min <= 0) and (decade_max >= 0)):
        ticklocs.append(0.5)
    if (decade_max >= 1):
        tempResult = arange(max(1, decade_min), (decade_max + 1))
	
===================================================================	
LogitLocator.tick_values: 1436	
----------------------------	

if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    raise NotImplementedError('Polar axis cannot be logit scaled yet')
(vmin, vmax) = self.nonsingular(vmin, vmax)
vmin = numpy.log10((vmin / (1 - vmin)))
vmax = numpy.log10((vmax / (1 - vmax)))
decade_min = numpy.floor(vmin)
decade_max = numpy.ceil(vmax)
if (not self.minor):
    ticklocs = []
    if (decade_min <= (- 1)):
        expo = numpy.arange(decade_min, min(0, (decade_max + 1)))
        ticklocs.extend(list((10 ** expo)))
    if ((decade_min <= 0) and (decade_max >= 0)):
        ticklocs.append(0.5)
    if (decade_max >= 1):
        expo = (- numpy.arange(max(1, decade_min), (decade_max + 1)))
        ticklocs.extend(list((1 - (10 ** expo))))
else:
    ticklocs = []
    if (decade_min <= (- 2)):
        tempResult = arange(decade_min, min((- 1), decade_max))
	
===================================================================	
LogitLocator.tick_values: 1437	
----------------------------	

if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    raise NotImplementedError('Polar axis cannot be logit scaled yet')
(vmin, vmax) = self.nonsingular(vmin, vmax)
vmin = numpy.log10((vmin / (1 - vmin)))
vmax = numpy.log10((vmax / (1 - vmax)))
decade_min = numpy.floor(vmin)
decade_max = numpy.ceil(vmax)
if (not self.minor):
    ticklocs = []
    if (decade_min <= (- 1)):
        expo = numpy.arange(decade_min, min(0, (decade_max + 1)))
        ticklocs.extend(list((10 ** expo)))
    if ((decade_min <= 0) and (decade_max >= 0)):
        ticklocs.append(0.5)
    if (decade_max >= 1):
        expo = (- numpy.arange(max(1, decade_min), (decade_max + 1)))
        ticklocs.extend(list((1 - (10 ** expo))))
else:
    ticklocs = []
    if (decade_min <= (- 2)):
        expo = numpy.arange(decade_min, min((- 1), decade_max))
        tempResult = arange(2, 10)
	
===================================================================	
LogitLocator.tick_values: 1442	
----------------------------	

if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    raise NotImplementedError('Polar axis cannot be logit scaled yet')
(vmin, vmax) = self.nonsingular(vmin, vmax)
vmin = numpy.log10((vmin / (1 - vmin)))
vmax = numpy.log10((vmax / (1 - vmax)))
decade_min = numpy.floor(vmin)
decade_max = numpy.ceil(vmax)
if (not self.minor):
    ticklocs = []
    if (decade_min <= (- 1)):
        expo = numpy.arange(decade_min, min(0, (decade_max + 1)))
        ticklocs.extend(list((10 ** expo)))
    if ((decade_min <= 0) and (decade_max >= 0)):
        ticklocs.append(0.5)
    if (decade_max >= 1):
        expo = (- numpy.arange(max(1, decade_min), (decade_max + 1)))
        ticklocs.extend(list((1 - (10 ** expo))))
else:
    ticklocs = []
    if (decade_min <= (- 2)):
        expo = numpy.arange(decade_min, min((- 1), decade_max))
        newticks = np.outer(np.arange(2, 10), (10 ** expo)).ravel()
        ticklocs.extend(list(newticks))
    if ((decade_min <= 0) and (decade_max >= 0)):
        ticklocs.extend([0.2, 0.3, 0.4, 0.6, 0.7, 0.8])
    if (decade_max >= 2):
        tempResult = arange(max(2, decade_min), (decade_max + 1))
	
===================================================================	
LogitLocator.tick_values: 1443	
----------------------------	

if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    raise NotImplementedError('Polar axis cannot be logit scaled yet')
(vmin, vmax) = self.nonsingular(vmin, vmax)
vmin = numpy.log10((vmin / (1 - vmin)))
vmax = numpy.log10((vmax / (1 - vmax)))
decade_min = numpy.floor(vmin)
decade_max = numpy.ceil(vmax)
if (not self.minor):
    ticklocs = []
    if (decade_min <= (- 1)):
        expo = numpy.arange(decade_min, min(0, (decade_max + 1)))
        ticklocs.extend(list((10 ** expo)))
    if ((decade_min <= 0) and (decade_max >= 0)):
        ticklocs.append(0.5)
    if (decade_max >= 1):
        expo = (- numpy.arange(max(1, decade_min), (decade_max + 1)))
        ticklocs.extend(list((1 - (10 ** expo))))
else:
    ticklocs = []
    if (decade_min <= (- 2)):
        expo = numpy.arange(decade_min, min((- 1), decade_max))
        newticks = np.outer(np.arange(2, 10), (10 ** expo)).ravel()
        ticklocs.extend(list(newticks))
    if ((decade_min <= 0) and (decade_max >= 0)):
        ticklocs.extend([0.2, 0.3, 0.4, 0.6, 0.7, 0.8])
    if (decade_max >= 2):
        expo = (- numpy.arange(max(2, decade_min), (decade_max + 1)))
        tempResult = arange(2, 10)
	
===================================================================	
LogLocator.tick_values: 1194	
----------------------------	

if (self.numticks == 'auto'):
    if (self.axis is not None):
        numticks = max(min(self.axis.get_tick_space(), 9), 2)
    else:
        numticks = 9
else:
    numticks = self.numticks
b = self._base
if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    vmax = math.ceil((math.log(vmax) / math.log(b)))
    tempResult = arange((vmax - self.numdecs), vmax)
	
===================================================================	
LogLocator.tick_values: 1215	
----------------------------	

if (self.numticks == 'auto'):
    if (self.axis is not None):
        numticks = max(min(self.axis.get_tick_space(), 9), 2)
    else:
        numticks = 9
else:
    numticks = self.numticks
b = self._base
if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    vmax = math.ceil((math.log(vmax) / math.log(b)))
    decades = numpy.arange((vmax - self.numdecs), vmax)
    ticklocs = (b ** decades)
    return ticklocs
if (vmin <= 0.0):
    if (self.axis is not None):
        vmin = self.axis.get_minpos()
    if ((vmin <= 0.0) or (not numpy.isfinite(vmin))):
        raise ValueError('Data has no positive values, and therefore can not be log-scaled.')
vmin = (math.log(vmin) / math.log(b))
vmax = (math.log(vmax) / math.log(b))
if (vmax < vmin):
    (vmin, vmax) = (vmax, vmin)
numdec = (math.floor(vmax) - math.ceil(vmin))
if matplotlib.cbook.is_string_like(self._subs):
    _first = (2.0 if (self._subs == 'auto') else 1.0)
    if ((numdec > 10) or (b < 3)):
        if (self._subs == 'auto'):
            return numpy.array([])
        else:
            subs = numpy.array([1.0])
    elif ((numdec > 5) and (b >= 6)):
        tempResult = arange(_first, b, 2.0)
	
===================================================================	
LogLocator.tick_values: 1217	
----------------------------	

if (self.numticks == 'auto'):
    if (self.axis is not None):
        numticks = max(min(self.axis.get_tick_space(), 9), 2)
    else:
        numticks = 9
else:
    numticks = self.numticks
b = self._base
if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    vmax = math.ceil((math.log(vmax) / math.log(b)))
    decades = numpy.arange((vmax - self.numdecs), vmax)
    ticklocs = (b ** decades)
    return ticklocs
if (vmin <= 0.0):
    if (self.axis is not None):
        vmin = self.axis.get_minpos()
    if ((vmin <= 0.0) or (not numpy.isfinite(vmin))):
        raise ValueError('Data has no positive values, and therefore can not be log-scaled.')
vmin = (math.log(vmin) / math.log(b))
vmax = (math.log(vmax) / math.log(b))
if (vmax < vmin):
    (vmin, vmax) = (vmax, vmin)
numdec = (math.floor(vmax) - math.ceil(vmin))
if matplotlib.cbook.is_string_like(self._subs):
    _first = (2.0 if (self._subs == 'auto') else 1.0)
    if ((numdec > 10) or (b < 3)):
        if (self._subs == 'auto'):
            return numpy.array([])
        else:
            subs = numpy.array([1.0])
    elif ((numdec > 5) and (b >= 6)):
        subs = numpy.arange(_first, b, 2.0)
    else:
        tempResult = arange(_first, b)
	
===================================================================	
LogLocator.tick_values: 1227	
----------------------------	

if (self.numticks == 'auto'):
    if (self.axis is not None):
        numticks = max(min(self.axis.get_tick_space(), 9), 2)
    else:
        numticks = 9
else:
    numticks = self.numticks
b = self._base
if (hasattr(self.axis, 'axes') and (self.axis.axes.name == 'polar')):
    vmax = math.ceil((math.log(vmax) / math.log(b)))
    decades = numpy.arange((vmax - self.numdecs), vmax)
    ticklocs = (b ** decades)
    return ticklocs
if (vmin <= 0.0):
    if (self.axis is not None):
        vmin = self.axis.get_minpos()
    if ((vmin <= 0.0) or (not numpy.isfinite(vmin))):
        raise ValueError('Data has no positive values, and therefore can not be log-scaled.')
vmin = (math.log(vmin) / math.log(b))
vmax = (math.log(vmax) / math.log(b))
if (vmax < vmin):
    (vmin, vmax) = (vmax, vmin)
numdec = (math.floor(vmax) - math.ceil(vmin))
if matplotlib.cbook.is_string_like(self._subs):
    _first = (2.0 if (self._subs == 'auto') else 1.0)
    if ((numdec > 10) or (b < 3)):
        if (self._subs == 'auto'):
            return numpy.array([])
        else:
            subs = numpy.array([1.0])
    elif ((numdec > 5) and (b >= 6)):
        subs = numpy.arange(_first, b, 2.0)
    else:
        subs = numpy.arange(_first, b)
else:
    subs = self._subs
stride = 1
if rcParams['_internal.classic_mode']:
    while (((numdec / stride) + 1) > numticks):
        stride += 1
else:
    while (((numdec // stride) + 1) > numticks):
        stride += 1
tempResult = arange((math.floor(vmin) - stride), (math.ceil(vmax) + (2 * stride)), stride)
	
===================================================================	
LogFormatter.set_locs: 505	
----------------------------	

'\n        Use axis view limits to control which ticks are labeled.\n\n        The ``locs`` parameter is ignored in the present algorithm.\n\n        '
if numpy.isinf(self.minor_thresholds[0]):
    self._sublabels = None
    return
linthresh = self._linthresh
if (linthresh is None):
    try:
        linthresh = self.axis.get_transform().linthresh
    except AttributeError:
        pass
(vmin, vmax) = self.axis.get_view_interval()
if (vmin > vmax):
    (vmin, vmax) = (vmax, vmin)
if ((linthresh is None) and (vmin <= 0)):
    self._sublabels = set((1,))
    return
b = self._base
if (linthresh is not None):
    numdec = 0
    if (vmin < (- linthresh)):
        rhs = min(vmax, (- linthresh))
        numdec += (math.log((vmin / rhs)) / math.log(b))
    if (vmax > linthresh):
        lhs = max(vmin, linthresh)
        numdec += (math.log((vmax / lhs)) / math.log(b))
else:
    vmin = (math.log(vmin) / math.log(b))
    vmax = (math.log(vmax) / math.log(b))
    numdec = abs((vmax - vmin))
if (numdec > self.minor_thresholds[0]):
    self._sublabels = set((1,))
elif (numdec > self.minor_thresholds[1]):
    c = numpy.logspace(0, 1, ((int(b) // 2) + 1), base=b)
    self._sublabels = set(numpy.round(c))
else:
    tempResult = arange(1, (b + 1))
	
===================================================================	
EllipseSelector.draw_shape: 1276	
----------------------------	

(x1, x2, y1, y2) = extents
(xmin, xmax) = sorted([x1, x2])
(ymin, ymax) = sorted([y1, y2])
center = [(x1 + ((x2 - x1) / 2.0)), (y1 + ((y2 - y1) / 2.0))]
a = ((xmax - xmin) / 2.0)
b = ((ymax - ymin) / 2.0)
if (self.drawtype == 'box'):
    self.to_draw.center = center
    self.to_draw.width = (2 * a)
    self.to_draw.height = (2 * b)
else:
    tempResult = arange(31)
	
===================================================================	
Axes.xcorr: 492	
----------------------------	

"\n        Plot the cross correlation between *x* and *y*.\n\n        The correlation with lag k is defined as sum_n x[n+k] * conj(y[n]).\n\n        Parameters\n        ----------\n\n        x : sequence of scalars of length n\n\n        y : sequence of scalars of length n\n\n        hold : boolean, optional, *deprecated*, default: True\n\n        detrend : callable, optional, default: `mlab.detrend_none`\n            x is detrended by the `detrend` callable. Default is no\n            normalization.\n\n        normed : boolean, optional, default: True\n            if True, input vectors are normalised to unit length.\n\n        usevlines : boolean, optional, default: True\n            if True, Axes.vlines is used to plot the vertical lines from the\n            origin to the acorr. Otherwise, Axes.plot is used.\n\n        maxlags : integer, optional, default: 10\n            number of lags to show. If None, will return all 2 * len(x) - 1\n            lags.\n\n        Returns\n        -------\n        (lags, c, line, b) : where:\n\n          - `lags` are a length 2`maxlags+1 lag vector.\n          - `c` is the 2`maxlags+1 auto correlation vectorI\n          - `line` is a `~matplotlib.lines.Line2D` instance returned by\n            `plot`.\n          - `b` is the x-axis (none, if plot is used).\n\n        Other parameters\n        ----------------\n        linestyle : `~matplotlib.lines.Line2D` prop, optional, default: None\n            Only used if usevlines is False.\n\n        marker : string, optional, default: 'o'\n\n        Notes\n        -----\n        The cross correlation is performed with :func:`numpy.correlate` with\n        `mode` = 2.\n        "
if ('hold' in kwargs):
    warnings.warn("the 'hold' kwarg is deprecated", mplDeprecation)
Nx = len(x)
if (Nx != len(y)):
    raise ValueError('x and y must be equal length')
x = detrend(numpy.asarray(x))
y = detrend(numpy.asarray(y))
c = numpy.correlate(x, y, mode=2)
if normed:
    c /= numpy.sqrt((numpy.dot(x, x) * numpy.dot(y, y)))
if (maxlags is None):
    maxlags = (Nx - 1)
if ((maxlags >= Nx) or (maxlags < 1)):
    raise ValueError(('maglags must be None or strictly positive < %d' % Nx))
tempResult = arange((- maxlags), (maxlags + 1))
	
===================================================================	
Axes.hexbin: 1508	
----------------------------	

"\n        Make a hexagonal binning plot.\n\n        Make a hexagonal binning plot of *x* versus *y*, where *x*,\n        *y* are 1-D sequences of the same length, *N*. If *C* is *None*\n        (the default), this is a histogram of the number of occurences\n        of the observations at (x[i],y[i]).\n\n        If *C* is specified, it specifies values at the coordinate\n        (x[i],y[i]). These values are accumulated for each hexagonal\n        bin and then reduced according to *reduce_C_function*, which\n        defaults to numpy's mean function (np.mean). (If *C* is\n        specified, it must also be a 1-D sequence of the same length\n        as *x* and *y*.)\n\n        Parameters\n        ----------\n        x, y : array or masked array\n\n        C : array or masked array, optional, default is *None*\n\n        gridsize : int or (int, int), optional, default is 100\n            The number of hexagons in the *x*-direction, default is\n            100. The corresponding number of hexagons in the\n            *y*-direction is chosen such that the hexagons are\n            approximately regular. Alternatively, gridsize can be a\n            tuple with two elements specifying the number of hexagons\n            in the *x*-direction and the *y*-direction.\n\n        bins : {'log'} or int or sequence, optional, default is *None*\n            If *None*, no binning is applied; the color of each hexagon\n            directly corresponds to its count value.\n\n            If 'log', use a logarithmic scale for the color\n            map. Internally, :math:`log_{10}(i+1)` is used to\n            determine the hexagon color.\n\n            If an integer, divide the counts in the specified number\n            of bins, and color the hexagons accordingly.\n\n            If a sequence of values, the values of the lower bound of\n            the bins to be used.\n\n        xscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the horizontal axis.\n\n        yscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the vertical axis.\n\n        mincnt : int > 0, optional, default is *None*\n            If not *None*, only display cells with more than *mincnt*\n            number of points in the cell\n\n        marginals : bool, optional, default is *False*\n            if marginals is *True*, plot the marginal density as\n            colormapped rectagles along the bottom of the x-axis and\n            left of the y-axis\n\n        extent : scalar, optional, default is *None*\n            The limits of the bins. The default assigns the limits\n            based on *gridsize*, *x*, *y*, *xscale* and *yscale*.\n\n            If *xscale* or *yscale* is set to 'log', the limits are\n            expected to be the exponent for a power of 10. E.g. for\n            x-limits of 1 and 50 in 'linear' scale and y-limits\n            of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n\n            Order of scalars is (left, right, bottom, top).\n\n        Other parameters\n        ----------------\n        cmap : object, optional, default is *None*\n            a :class:`matplotlib.colors.Colormap` instance. If *None*,\n            defaults to rc ``image.cmap``.\n\n        norm : object, optional, default is *None*\n            :class:`matplotlib.colors.Normalize` instance is used to\n            scale luminance data to 0,1.\n\n        vmin, vmax : scalar, optional, default is *None*\n            *vmin* and *vmax* are used in conjunction with *norm* to\n            normalize luminance data. If *None*, the min and max of the\n            color array *C* are used.  Note if you pass a norm instance\n            your settings for *vmin* and *vmax* will be ignored.\n\n        alpha : scalar between 0 and 1, optional, default is *None*\n            the alpha value for the patches\n\n        linewidths : scalar, optional, default is *None*\n            If *None*, defaults to 1.0.\n\n        edgecolors : {'none'} or mpl color, optional, default is 'none'\n            If 'none', draws the edges in the same color as the fill color.\n            This is the default, as it avoids unsightly unpainted pixels\n            between the hexagons.\n\n            If *None*, draws outlines in the default color.\n\n            If a matplotlib color arg, draws outlines in the specified color.\n\n        Returns\n        -------\n        object\n            a :class:`~matplotlib.collections.PolyCollection` instance; use\n            :meth:`~matplotlib.collections.PolyCollection.get_array` on\n            this :class:`~matplotlib.collections.PolyCollection` to get\n            the counts in each hexagon.\n\n            If *marginals* is *True*, horizontal\n            bar and vertical bar (both PolyCollections) will be attached\n            to the return collection as attributes *hbar* and *vbar*.\n\n        Examples\n        --------\n        .. plot:: mpl_examples/pylab_examples/hexbin_demo.py\n\n        Notes\n        --------\n        The standard descriptions of all the\n        :class:`~matplotlib.collections.Collection` parameters:\n\n            %(Collection)s\n\n        "
if (not self._hold):
    self.cla()
self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
(x, y, C) = matplotlib.cbook.delete_masked_points(x, y, C)
if iterable(gridsize):
    (nx, ny) = gridsize
else:
    nx = gridsize
    ny = int((nx / math.sqrt(3)))
x = numpy.array(x, float)
y = numpy.array(y, float)
if (xscale == 'log'):
    if numpy.any((x <= 0.0)):
        raise ValueError('x contains non-positive values, so can not be log-scaled')
    x = numpy.log10(x)
if (yscale == 'log'):
    if numpy.any((y <= 0.0)):
        raise ValueError('y contains non-positive values, so can not be log-scaled')
    y = numpy.log10(y)
if (extent is not None):
    (xmin, xmax, ymin, ymax) = extent
else:
    (xmin, xmax) = ((numpy.amin(x), numpy.amax(x)) if len(x) else (0, 1))
    (ymin, ymax) = ((numpy.amin(y), numpy.amax(y)) if len(y) else (0, 1))
    (xmin, xmax) = matplotlib.transforms.nonsingular(xmin, xmax, expander=0.1)
    (ymin, ymax) = matplotlib.transforms.nonsingular(ymin, ymax, expander=0.1)
padding = (1e-09 * (xmax - xmin))
xmin -= padding
xmax += padding
sx = ((xmax - xmin) / nx)
sy = ((ymax - ymin) / ny)
if marginals:
    xorig = x.copy()
    yorig = y.copy()
x = ((x - xmin) / sx)
y = ((y - ymin) / sy)
ix1 = np.round(x).astype(int)
iy1 = np.round(y).astype(int)
ix2 = np.floor(x).astype(int)
iy2 = np.floor(y).astype(int)
nx1 = (nx + 1)
ny1 = (ny + 1)
nx2 = nx
ny2 = ny
n = ((nx1 * ny1) + (nx2 * ny2))
d1 = (((x - ix1) ** 2) + (3.0 * ((y - iy1) ** 2)))
d2 = ((((x - ix2) - 0.5) ** 2) + (3.0 * (((y - iy2) - 0.5) ** 2)))
bdist = (d1 < d2)
if (C is None):
    accum = numpy.zeros(n)
    lattice1 = accum[:(nx1 * ny1)]
    lattice2 = accum[(nx1 * ny1):]
    lattice1.shape = (nx1, ny1)
    lattice2.shape = (nx2, ny2)
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])] += 1
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])] += 1
    if (mincnt is not None):
        for i in xrange(nx1):
            for j in xrange(ny1):
                if (lattice1[(i, j)] < mincnt):
                    lattice1[(i, j)] = numpy.nan
        for i in xrange(nx2):
            for j in xrange(ny2):
                if (lattice2[(i, j)] < mincnt):
                    lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
else:
    if (mincnt is None):
        mincnt = 0
    lattice1 = numpy.empty((nx1, ny1), dtype=object)
    for i in xrange(nx1):
        for j in xrange(ny1):
            lattice1[(i, j)] = []
    lattice2 = numpy.empty((nx2, ny2), dtype=object)
    for i in xrange(nx2):
        for j in xrange(ny2):
            lattice2[(i, j)] = []
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])].append(C[i])
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])].append(C[i])
    for i in xrange(nx1):
        for j in xrange(ny1):
            vals = lattice1[(i, j)]
            if (len(vals) > mincnt):
                lattice1[(i, j)] = reduce_C_function(vals)
            else:
                lattice1[(i, j)] = numpy.nan
    for i in xrange(nx2):
        for j in xrange(ny2):
            vals = lattice2[(i, j)]
            if (len(vals) > mincnt):
                lattice2[(i, j)] = reduce_C_function(vals)
            else:
                lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
offsets = numpy.zeros((n, 2), float)
tempResult = arange(nx1)
	
===================================================================	
Axes.hexbin: 1509	
----------------------------	

"\n        Make a hexagonal binning plot.\n\n        Make a hexagonal binning plot of *x* versus *y*, where *x*,\n        *y* are 1-D sequences of the same length, *N*. If *C* is *None*\n        (the default), this is a histogram of the number of occurences\n        of the observations at (x[i],y[i]).\n\n        If *C* is specified, it specifies values at the coordinate\n        (x[i],y[i]). These values are accumulated for each hexagonal\n        bin and then reduced according to *reduce_C_function*, which\n        defaults to numpy's mean function (np.mean). (If *C* is\n        specified, it must also be a 1-D sequence of the same length\n        as *x* and *y*.)\n\n        Parameters\n        ----------\n        x, y : array or masked array\n\n        C : array or masked array, optional, default is *None*\n\n        gridsize : int or (int, int), optional, default is 100\n            The number of hexagons in the *x*-direction, default is\n            100. The corresponding number of hexagons in the\n            *y*-direction is chosen such that the hexagons are\n            approximately regular. Alternatively, gridsize can be a\n            tuple with two elements specifying the number of hexagons\n            in the *x*-direction and the *y*-direction.\n\n        bins : {'log'} or int or sequence, optional, default is *None*\n            If *None*, no binning is applied; the color of each hexagon\n            directly corresponds to its count value.\n\n            If 'log', use a logarithmic scale for the color\n            map. Internally, :math:`log_{10}(i+1)` is used to\n            determine the hexagon color.\n\n            If an integer, divide the counts in the specified number\n            of bins, and color the hexagons accordingly.\n\n            If a sequence of values, the values of the lower bound of\n            the bins to be used.\n\n        xscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the horizontal axis.\n\n        yscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the vertical axis.\n\n        mincnt : int > 0, optional, default is *None*\n            If not *None*, only display cells with more than *mincnt*\n            number of points in the cell\n\n        marginals : bool, optional, default is *False*\n            if marginals is *True*, plot the marginal density as\n            colormapped rectagles along the bottom of the x-axis and\n            left of the y-axis\n\n        extent : scalar, optional, default is *None*\n            The limits of the bins. The default assigns the limits\n            based on *gridsize*, *x*, *y*, *xscale* and *yscale*.\n\n            If *xscale* or *yscale* is set to 'log', the limits are\n            expected to be the exponent for a power of 10. E.g. for\n            x-limits of 1 and 50 in 'linear' scale and y-limits\n            of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n\n            Order of scalars is (left, right, bottom, top).\n\n        Other parameters\n        ----------------\n        cmap : object, optional, default is *None*\n            a :class:`matplotlib.colors.Colormap` instance. If *None*,\n            defaults to rc ``image.cmap``.\n\n        norm : object, optional, default is *None*\n            :class:`matplotlib.colors.Normalize` instance is used to\n            scale luminance data to 0,1.\n\n        vmin, vmax : scalar, optional, default is *None*\n            *vmin* and *vmax* are used in conjunction with *norm* to\n            normalize luminance data. If *None*, the min and max of the\n            color array *C* are used.  Note if you pass a norm instance\n            your settings for *vmin* and *vmax* will be ignored.\n\n        alpha : scalar between 0 and 1, optional, default is *None*\n            the alpha value for the patches\n\n        linewidths : scalar, optional, default is *None*\n            If *None*, defaults to 1.0.\n\n        edgecolors : {'none'} or mpl color, optional, default is 'none'\n            If 'none', draws the edges in the same color as the fill color.\n            This is the default, as it avoids unsightly unpainted pixels\n            between the hexagons.\n\n            If *None*, draws outlines in the default color.\n\n            If a matplotlib color arg, draws outlines in the specified color.\n\n        Returns\n        -------\n        object\n            a :class:`~matplotlib.collections.PolyCollection` instance; use\n            :meth:`~matplotlib.collections.PolyCollection.get_array` on\n            this :class:`~matplotlib.collections.PolyCollection` to get\n            the counts in each hexagon.\n\n            If *marginals* is *True*, horizontal\n            bar and vertical bar (both PolyCollections) will be attached\n            to the return collection as attributes *hbar* and *vbar*.\n\n        Examples\n        --------\n        .. plot:: mpl_examples/pylab_examples/hexbin_demo.py\n\n        Notes\n        --------\n        The standard descriptions of all the\n        :class:`~matplotlib.collections.Collection` parameters:\n\n            %(Collection)s\n\n        "
if (not self._hold):
    self.cla()
self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
(x, y, C) = matplotlib.cbook.delete_masked_points(x, y, C)
if iterable(gridsize):
    (nx, ny) = gridsize
else:
    nx = gridsize
    ny = int((nx / math.sqrt(3)))
x = numpy.array(x, float)
y = numpy.array(y, float)
if (xscale == 'log'):
    if numpy.any((x <= 0.0)):
        raise ValueError('x contains non-positive values, so can not be log-scaled')
    x = numpy.log10(x)
if (yscale == 'log'):
    if numpy.any((y <= 0.0)):
        raise ValueError('y contains non-positive values, so can not be log-scaled')
    y = numpy.log10(y)
if (extent is not None):
    (xmin, xmax, ymin, ymax) = extent
else:
    (xmin, xmax) = ((numpy.amin(x), numpy.amax(x)) if len(x) else (0, 1))
    (ymin, ymax) = ((numpy.amin(y), numpy.amax(y)) if len(y) else (0, 1))
    (xmin, xmax) = matplotlib.transforms.nonsingular(xmin, xmax, expander=0.1)
    (ymin, ymax) = matplotlib.transforms.nonsingular(ymin, ymax, expander=0.1)
padding = (1e-09 * (xmax - xmin))
xmin -= padding
xmax += padding
sx = ((xmax - xmin) / nx)
sy = ((ymax - ymin) / ny)
if marginals:
    xorig = x.copy()
    yorig = y.copy()
x = ((x - xmin) / sx)
y = ((y - ymin) / sy)
ix1 = np.round(x).astype(int)
iy1 = np.round(y).astype(int)
ix2 = np.floor(x).astype(int)
iy2 = np.floor(y).astype(int)
nx1 = (nx + 1)
ny1 = (ny + 1)
nx2 = nx
ny2 = ny
n = ((nx1 * ny1) + (nx2 * ny2))
d1 = (((x - ix1) ** 2) + (3.0 * ((y - iy1) ** 2)))
d2 = ((((x - ix2) - 0.5) ** 2) + (3.0 * (((y - iy2) - 0.5) ** 2)))
bdist = (d1 < d2)
if (C is None):
    accum = numpy.zeros(n)
    lattice1 = accum[:(nx1 * ny1)]
    lattice2 = accum[(nx1 * ny1):]
    lattice1.shape = (nx1, ny1)
    lattice2.shape = (nx2, ny2)
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])] += 1
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])] += 1
    if (mincnt is not None):
        for i in xrange(nx1):
            for j in xrange(ny1):
                if (lattice1[(i, j)] < mincnt):
                    lattice1[(i, j)] = numpy.nan
        for i in xrange(nx2):
            for j in xrange(ny2):
                if (lattice2[(i, j)] < mincnt):
                    lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
else:
    if (mincnt is None):
        mincnt = 0
    lattice1 = numpy.empty((nx1, ny1), dtype=object)
    for i in xrange(nx1):
        for j in xrange(ny1):
            lattice1[(i, j)] = []
    lattice2 = numpy.empty((nx2, ny2), dtype=object)
    for i in xrange(nx2):
        for j in xrange(ny2):
            lattice2[(i, j)] = []
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])].append(C[i])
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])].append(C[i])
    for i in xrange(nx1):
        for j in xrange(ny1):
            vals = lattice1[(i, j)]
            if (len(vals) > mincnt):
                lattice1[(i, j)] = reduce_C_function(vals)
            else:
                lattice1[(i, j)] = numpy.nan
    for i in xrange(nx2):
        for j in xrange(ny2):
            vals = lattice2[(i, j)]
            if (len(vals) > mincnt):
                lattice2[(i, j)] = reduce_C_function(vals)
            else:
                lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
offsets = numpy.zeros((n, 2), float)
offsets[:(nx1 * ny1), 0] = numpy.repeat(numpy.arange(nx1), ny1)
tempResult = arange(ny1)
	
===================================================================	
Axes.hexbin: 1510	
----------------------------	

"\n        Make a hexagonal binning plot.\n\n        Make a hexagonal binning plot of *x* versus *y*, where *x*,\n        *y* are 1-D sequences of the same length, *N*. If *C* is *None*\n        (the default), this is a histogram of the number of occurences\n        of the observations at (x[i],y[i]).\n\n        If *C* is specified, it specifies values at the coordinate\n        (x[i],y[i]). These values are accumulated for each hexagonal\n        bin and then reduced according to *reduce_C_function*, which\n        defaults to numpy's mean function (np.mean). (If *C* is\n        specified, it must also be a 1-D sequence of the same length\n        as *x* and *y*.)\n\n        Parameters\n        ----------\n        x, y : array or masked array\n\n        C : array or masked array, optional, default is *None*\n\n        gridsize : int or (int, int), optional, default is 100\n            The number of hexagons in the *x*-direction, default is\n            100. The corresponding number of hexagons in the\n            *y*-direction is chosen such that the hexagons are\n            approximately regular. Alternatively, gridsize can be a\n            tuple with two elements specifying the number of hexagons\n            in the *x*-direction and the *y*-direction.\n\n        bins : {'log'} or int or sequence, optional, default is *None*\n            If *None*, no binning is applied; the color of each hexagon\n            directly corresponds to its count value.\n\n            If 'log', use a logarithmic scale for the color\n            map. Internally, :math:`log_{10}(i+1)` is used to\n            determine the hexagon color.\n\n            If an integer, divide the counts in the specified number\n            of bins, and color the hexagons accordingly.\n\n            If a sequence of values, the values of the lower bound of\n            the bins to be used.\n\n        xscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the horizontal axis.\n\n        yscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the vertical axis.\n\n        mincnt : int > 0, optional, default is *None*\n            If not *None*, only display cells with more than *mincnt*\n            number of points in the cell\n\n        marginals : bool, optional, default is *False*\n            if marginals is *True*, plot the marginal density as\n            colormapped rectagles along the bottom of the x-axis and\n            left of the y-axis\n\n        extent : scalar, optional, default is *None*\n            The limits of the bins. The default assigns the limits\n            based on *gridsize*, *x*, *y*, *xscale* and *yscale*.\n\n            If *xscale* or *yscale* is set to 'log', the limits are\n            expected to be the exponent for a power of 10. E.g. for\n            x-limits of 1 and 50 in 'linear' scale and y-limits\n            of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n\n            Order of scalars is (left, right, bottom, top).\n\n        Other parameters\n        ----------------\n        cmap : object, optional, default is *None*\n            a :class:`matplotlib.colors.Colormap` instance. If *None*,\n            defaults to rc ``image.cmap``.\n\n        norm : object, optional, default is *None*\n            :class:`matplotlib.colors.Normalize` instance is used to\n            scale luminance data to 0,1.\n\n        vmin, vmax : scalar, optional, default is *None*\n            *vmin* and *vmax* are used in conjunction with *norm* to\n            normalize luminance data. If *None*, the min and max of the\n            color array *C* are used.  Note if you pass a norm instance\n            your settings for *vmin* and *vmax* will be ignored.\n\n        alpha : scalar between 0 and 1, optional, default is *None*\n            the alpha value for the patches\n\n        linewidths : scalar, optional, default is *None*\n            If *None*, defaults to 1.0.\n\n        edgecolors : {'none'} or mpl color, optional, default is 'none'\n            If 'none', draws the edges in the same color as the fill color.\n            This is the default, as it avoids unsightly unpainted pixels\n            between the hexagons.\n\n            If *None*, draws outlines in the default color.\n\n            If a matplotlib color arg, draws outlines in the specified color.\n\n        Returns\n        -------\n        object\n            a :class:`~matplotlib.collections.PolyCollection` instance; use\n            :meth:`~matplotlib.collections.PolyCollection.get_array` on\n            this :class:`~matplotlib.collections.PolyCollection` to get\n            the counts in each hexagon.\n\n            If *marginals* is *True*, horizontal\n            bar and vertical bar (both PolyCollections) will be attached\n            to the return collection as attributes *hbar* and *vbar*.\n\n        Examples\n        --------\n        .. plot:: mpl_examples/pylab_examples/hexbin_demo.py\n\n        Notes\n        --------\n        The standard descriptions of all the\n        :class:`~matplotlib.collections.Collection` parameters:\n\n            %(Collection)s\n\n        "
if (not self._hold):
    self.cla()
self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
(x, y, C) = matplotlib.cbook.delete_masked_points(x, y, C)
if iterable(gridsize):
    (nx, ny) = gridsize
else:
    nx = gridsize
    ny = int((nx / math.sqrt(3)))
x = numpy.array(x, float)
y = numpy.array(y, float)
if (xscale == 'log'):
    if numpy.any((x <= 0.0)):
        raise ValueError('x contains non-positive values, so can not be log-scaled')
    x = numpy.log10(x)
if (yscale == 'log'):
    if numpy.any((y <= 0.0)):
        raise ValueError('y contains non-positive values, so can not be log-scaled')
    y = numpy.log10(y)
if (extent is not None):
    (xmin, xmax, ymin, ymax) = extent
else:
    (xmin, xmax) = ((numpy.amin(x), numpy.amax(x)) if len(x) else (0, 1))
    (ymin, ymax) = ((numpy.amin(y), numpy.amax(y)) if len(y) else (0, 1))
    (xmin, xmax) = matplotlib.transforms.nonsingular(xmin, xmax, expander=0.1)
    (ymin, ymax) = matplotlib.transforms.nonsingular(ymin, ymax, expander=0.1)
padding = (1e-09 * (xmax - xmin))
xmin -= padding
xmax += padding
sx = ((xmax - xmin) / nx)
sy = ((ymax - ymin) / ny)
if marginals:
    xorig = x.copy()
    yorig = y.copy()
x = ((x - xmin) / sx)
y = ((y - ymin) / sy)
ix1 = np.round(x).astype(int)
iy1 = np.round(y).astype(int)
ix2 = np.floor(x).astype(int)
iy2 = np.floor(y).astype(int)
nx1 = (nx + 1)
ny1 = (ny + 1)
nx2 = nx
ny2 = ny
n = ((nx1 * ny1) + (nx2 * ny2))
d1 = (((x - ix1) ** 2) + (3.0 * ((y - iy1) ** 2)))
d2 = ((((x - ix2) - 0.5) ** 2) + (3.0 * (((y - iy2) - 0.5) ** 2)))
bdist = (d1 < d2)
if (C is None):
    accum = numpy.zeros(n)
    lattice1 = accum[:(nx1 * ny1)]
    lattice2 = accum[(nx1 * ny1):]
    lattice1.shape = (nx1, ny1)
    lattice2.shape = (nx2, ny2)
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])] += 1
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])] += 1
    if (mincnt is not None):
        for i in xrange(nx1):
            for j in xrange(ny1):
                if (lattice1[(i, j)] < mincnt):
                    lattice1[(i, j)] = numpy.nan
        for i in xrange(nx2):
            for j in xrange(ny2):
                if (lattice2[(i, j)] < mincnt):
                    lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
else:
    if (mincnt is None):
        mincnt = 0
    lattice1 = numpy.empty((nx1, ny1), dtype=object)
    for i in xrange(nx1):
        for j in xrange(ny1):
            lattice1[(i, j)] = []
    lattice2 = numpy.empty((nx2, ny2), dtype=object)
    for i in xrange(nx2):
        for j in xrange(ny2):
            lattice2[(i, j)] = []
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])].append(C[i])
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])].append(C[i])
    for i in xrange(nx1):
        for j in xrange(ny1):
            vals = lattice1[(i, j)]
            if (len(vals) > mincnt):
                lattice1[(i, j)] = reduce_C_function(vals)
            else:
                lattice1[(i, j)] = numpy.nan
    for i in xrange(nx2):
        for j in xrange(ny2):
            vals = lattice2[(i, j)]
            if (len(vals) > mincnt):
                lattice2[(i, j)] = reduce_C_function(vals)
            else:
                lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
offsets = numpy.zeros((n, 2), float)
offsets[:(nx1 * ny1), 0] = numpy.repeat(numpy.arange(nx1), ny1)
offsets[:(nx1 * ny1), 1] = numpy.tile(numpy.arange(ny1), nx1)
tempResult = arange(nx2)
	
===================================================================	
Axes.hexbin: 1511	
----------------------------	

"\n        Make a hexagonal binning plot.\n\n        Make a hexagonal binning plot of *x* versus *y*, where *x*,\n        *y* are 1-D sequences of the same length, *N*. If *C* is *None*\n        (the default), this is a histogram of the number of occurences\n        of the observations at (x[i],y[i]).\n\n        If *C* is specified, it specifies values at the coordinate\n        (x[i],y[i]). These values are accumulated for each hexagonal\n        bin and then reduced according to *reduce_C_function*, which\n        defaults to numpy's mean function (np.mean). (If *C* is\n        specified, it must also be a 1-D sequence of the same length\n        as *x* and *y*.)\n\n        Parameters\n        ----------\n        x, y : array or masked array\n\n        C : array or masked array, optional, default is *None*\n\n        gridsize : int or (int, int), optional, default is 100\n            The number of hexagons in the *x*-direction, default is\n            100. The corresponding number of hexagons in the\n            *y*-direction is chosen such that the hexagons are\n            approximately regular. Alternatively, gridsize can be a\n            tuple with two elements specifying the number of hexagons\n            in the *x*-direction and the *y*-direction.\n\n        bins : {'log'} or int or sequence, optional, default is *None*\n            If *None*, no binning is applied; the color of each hexagon\n            directly corresponds to its count value.\n\n            If 'log', use a logarithmic scale for the color\n            map. Internally, :math:`log_{10}(i+1)` is used to\n            determine the hexagon color.\n\n            If an integer, divide the counts in the specified number\n            of bins, and color the hexagons accordingly.\n\n            If a sequence of values, the values of the lower bound of\n            the bins to be used.\n\n        xscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the horizontal axis.\n\n        yscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the vertical axis.\n\n        mincnt : int > 0, optional, default is *None*\n            If not *None*, only display cells with more than *mincnt*\n            number of points in the cell\n\n        marginals : bool, optional, default is *False*\n            if marginals is *True*, plot the marginal density as\n            colormapped rectagles along the bottom of the x-axis and\n            left of the y-axis\n\n        extent : scalar, optional, default is *None*\n            The limits of the bins. The default assigns the limits\n            based on *gridsize*, *x*, *y*, *xscale* and *yscale*.\n\n            If *xscale* or *yscale* is set to 'log', the limits are\n            expected to be the exponent for a power of 10. E.g. for\n            x-limits of 1 and 50 in 'linear' scale and y-limits\n            of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n\n            Order of scalars is (left, right, bottom, top).\n\n        Other parameters\n        ----------------\n        cmap : object, optional, default is *None*\n            a :class:`matplotlib.colors.Colormap` instance. If *None*,\n            defaults to rc ``image.cmap``.\n\n        norm : object, optional, default is *None*\n            :class:`matplotlib.colors.Normalize` instance is used to\n            scale luminance data to 0,1.\n\n        vmin, vmax : scalar, optional, default is *None*\n            *vmin* and *vmax* are used in conjunction with *norm* to\n            normalize luminance data. If *None*, the min and max of the\n            color array *C* are used.  Note if you pass a norm instance\n            your settings for *vmin* and *vmax* will be ignored.\n\n        alpha : scalar between 0 and 1, optional, default is *None*\n            the alpha value for the patches\n\n        linewidths : scalar, optional, default is *None*\n            If *None*, defaults to 1.0.\n\n        edgecolors : {'none'} or mpl color, optional, default is 'none'\n            If 'none', draws the edges in the same color as the fill color.\n            This is the default, as it avoids unsightly unpainted pixels\n            between the hexagons.\n\n            If *None*, draws outlines in the default color.\n\n            If a matplotlib color arg, draws outlines in the specified color.\n\n        Returns\n        -------\n        object\n            a :class:`~matplotlib.collections.PolyCollection` instance; use\n            :meth:`~matplotlib.collections.PolyCollection.get_array` on\n            this :class:`~matplotlib.collections.PolyCollection` to get\n            the counts in each hexagon.\n\n            If *marginals* is *True*, horizontal\n            bar and vertical bar (both PolyCollections) will be attached\n            to the return collection as attributes *hbar* and *vbar*.\n\n        Examples\n        --------\n        .. plot:: mpl_examples/pylab_examples/hexbin_demo.py\n\n        Notes\n        --------\n        The standard descriptions of all the\n        :class:`~matplotlib.collections.Collection` parameters:\n\n            %(Collection)s\n\n        "
if (not self._hold):
    self.cla()
self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
(x, y, C) = matplotlib.cbook.delete_masked_points(x, y, C)
if iterable(gridsize):
    (nx, ny) = gridsize
else:
    nx = gridsize
    ny = int((nx / math.sqrt(3)))
x = numpy.array(x, float)
y = numpy.array(y, float)
if (xscale == 'log'):
    if numpy.any((x <= 0.0)):
        raise ValueError('x contains non-positive values, so can not be log-scaled')
    x = numpy.log10(x)
if (yscale == 'log'):
    if numpy.any((y <= 0.0)):
        raise ValueError('y contains non-positive values, so can not be log-scaled')
    y = numpy.log10(y)
if (extent is not None):
    (xmin, xmax, ymin, ymax) = extent
else:
    (xmin, xmax) = ((numpy.amin(x), numpy.amax(x)) if len(x) else (0, 1))
    (ymin, ymax) = ((numpy.amin(y), numpy.amax(y)) if len(y) else (0, 1))
    (xmin, xmax) = matplotlib.transforms.nonsingular(xmin, xmax, expander=0.1)
    (ymin, ymax) = matplotlib.transforms.nonsingular(ymin, ymax, expander=0.1)
padding = (1e-09 * (xmax - xmin))
xmin -= padding
xmax += padding
sx = ((xmax - xmin) / nx)
sy = ((ymax - ymin) / ny)
if marginals:
    xorig = x.copy()
    yorig = y.copy()
x = ((x - xmin) / sx)
y = ((y - ymin) / sy)
ix1 = np.round(x).astype(int)
iy1 = np.round(y).astype(int)
ix2 = np.floor(x).astype(int)
iy2 = np.floor(y).astype(int)
nx1 = (nx + 1)
ny1 = (ny + 1)
nx2 = nx
ny2 = ny
n = ((nx1 * ny1) + (nx2 * ny2))
d1 = (((x - ix1) ** 2) + (3.0 * ((y - iy1) ** 2)))
d2 = ((((x - ix2) - 0.5) ** 2) + (3.0 * (((y - iy2) - 0.5) ** 2)))
bdist = (d1 < d2)
if (C is None):
    accum = numpy.zeros(n)
    lattice1 = accum[:(nx1 * ny1)]
    lattice2 = accum[(nx1 * ny1):]
    lattice1.shape = (nx1, ny1)
    lattice2.shape = (nx2, ny2)
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])] += 1
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])] += 1
    if (mincnt is not None):
        for i in xrange(nx1):
            for j in xrange(ny1):
                if (lattice1[(i, j)] < mincnt):
                    lattice1[(i, j)] = numpy.nan
        for i in xrange(nx2):
            for j in xrange(ny2):
                if (lattice2[(i, j)] < mincnt):
                    lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
else:
    if (mincnt is None):
        mincnt = 0
    lattice1 = numpy.empty((nx1, ny1), dtype=object)
    for i in xrange(nx1):
        for j in xrange(ny1):
            lattice1[(i, j)] = []
    lattice2 = numpy.empty((nx2, ny2), dtype=object)
    for i in xrange(nx2):
        for j in xrange(ny2):
            lattice2[(i, j)] = []
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])].append(C[i])
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])].append(C[i])
    for i in xrange(nx1):
        for j in xrange(ny1):
            vals = lattice1[(i, j)]
            if (len(vals) > mincnt):
                lattice1[(i, j)] = reduce_C_function(vals)
            else:
                lattice1[(i, j)] = numpy.nan
    for i in xrange(nx2):
        for j in xrange(ny2):
            vals = lattice2[(i, j)]
            if (len(vals) > mincnt):
                lattice2[(i, j)] = reduce_C_function(vals)
            else:
                lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
offsets = numpy.zeros((n, 2), float)
offsets[:(nx1 * ny1), 0] = numpy.repeat(numpy.arange(nx1), ny1)
offsets[:(nx1 * ny1), 1] = numpy.tile(numpy.arange(ny1), nx1)
offsets[(nx1 * ny1):, 0] = numpy.repeat((numpy.arange(nx2) + 0.5), ny2)
tempResult = arange(ny2)
	
===================================================================	
Axes.hexbin: 1552	
----------------------------	

"\n        Make a hexagonal binning plot.\n\n        Make a hexagonal binning plot of *x* versus *y*, where *x*,\n        *y* are 1-D sequences of the same length, *N*. If *C* is *None*\n        (the default), this is a histogram of the number of occurences\n        of the observations at (x[i],y[i]).\n\n        If *C* is specified, it specifies values at the coordinate\n        (x[i],y[i]). These values are accumulated for each hexagonal\n        bin and then reduced according to *reduce_C_function*, which\n        defaults to numpy's mean function (np.mean). (If *C* is\n        specified, it must also be a 1-D sequence of the same length\n        as *x* and *y*.)\n\n        Parameters\n        ----------\n        x, y : array or masked array\n\n        C : array or masked array, optional, default is *None*\n\n        gridsize : int or (int, int), optional, default is 100\n            The number of hexagons in the *x*-direction, default is\n            100. The corresponding number of hexagons in the\n            *y*-direction is chosen such that the hexagons are\n            approximately regular. Alternatively, gridsize can be a\n            tuple with two elements specifying the number of hexagons\n            in the *x*-direction and the *y*-direction.\n\n        bins : {'log'} or int or sequence, optional, default is *None*\n            If *None*, no binning is applied; the color of each hexagon\n            directly corresponds to its count value.\n\n            If 'log', use a logarithmic scale for the color\n            map. Internally, :math:`log_{10}(i+1)` is used to\n            determine the hexagon color.\n\n            If an integer, divide the counts in the specified number\n            of bins, and color the hexagons accordingly.\n\n            If a sequence of values, the values of the lower bound of\n            the bins to be used.\n\n        xscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the horizontal axis.\n\n        yscale : {'linear', 'log'}, optional, default is 'linear'\n            Use a linear or log10 scale on the vertical axis.\n\n        mincnt : int > 0, optional, default is *None*\n            If not *None*, only display cells with more than *mincnt*\n            number of points in the cell\n\n        marginals : bool, optional, default is *False*\n            if marginals is *True*, plot the marginal density as\n            colormapped rectagles along the bottom of the x-axis and\n            left of the y-axis\n\n        extent : scalar, optional, default is *None*\n            The limits of the bins. The default assigns the limits\n            based on *gridsize*, *x*, *y*, *xscale* and *yscale*.\n\n            If *xscale* or *yscale* is set to 'log', the limits are\n            expected to be the exponent for a power of 10. E.g. for\n            x-limits of 1 and 50 in 'linear' scale and y-limits\n            of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n\n            Order of scalars is (left, right, bottom, top).\n\n        Other parameters\n        ----------------\n        cmap : object, optional, default is *None*\n            a :class:`matplotlib.colors.Colormap` instance. If *None*,\n            defaults to rc ``image.cmap``.\n\n        norm : object, optional, default is *None*\n            :class:`matplotlib.colors.Normalize` instance is used to\n            scale luminance data to 0,1.\n\n        vmin, vmax : scalar, optional, default is *None*\n            *vmin* and *vmax* are used in conjunction with *norm* to\n            normalize luminance data. If *None*, the min and max of the\n            color array *C* are used.  Note if you pass a norm instance\n            your settings for *vmin* and *vmax* will be ignored.\n\n        alpha : scalar between 0 and 1, optional, default is *None*\n            the alpha value for the patches\n\n        linewidths : scalar, optional, default is *None*\n            If *None*, defaults to 1.0.\n\n        edgecolors : {'none'} or mpl color, optional, default is 'none'\n            If 'none', draws the edges in the same color as the fill color.\n            This is the default, as it avoids unsightly unpainted pixels\n            between the hexagons.\n\n            If *None*, draws outlines in the default color.\n\n            If a matplotlib color arg, draws outlines in the specified color.\n\n        Returns\n        -------\n        object\n            a :class:`~matplotlib.collections.PolyCollection` instance; use\n            :meth:`~matplotlib.collections.PolyCollection.get_array` on\n            this :class:`~matplotlib.collections.PolyCollection` to get\n            the counts in each hexagon.\n\n            If *marginals* is *True*, horizontal\n            bar and vertical bar (both PolyCollections) will be attached\n            to the return collection as attributes *hbar* and *vbar*.\n\n        Examples\n        --------\n        .. plot:: mpl_examples/pylab_examples/hexbin_demo.py\n\n        Notes\n        --------\n        The standard descriptions of all the\n        :class:`~matplotlib.collections.Collection` parameters:\n\n            %(Collection)s\n\n        "
if (not self._hold):
    self.cla()
self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
(x, y, C) = matplotlib.cbook.delete_masked_points(x, y, C)
if iterable(gridsize):
    (nx, ny) = gridsize
else:
    nx = gridsize
    ny = int((nx / math.sqrt(3)))
x = numpy.array(x, float)
y = numpy.array(y, float)
if (xscale == 'log'):
    if numpy.any((x <= 0.0)):
        raise ValueError('x contains non-positive values, so can not be log-scaled')
    x = numpy.log10(x)
if (yscale == 'log'):
    if numpy.any((y <= 0.0)):
        raise ValueError('y contains non-positive values, so can not be log-scaled')
    y = numpy.log10(y)
if (extent is not None):
    (xmin, xmax, ymin, ymax) = extent
else:
    (xmin, xmax) = ((numpy.amin(x), numpy.amax(x)) if len(x) else (0, 1))
    (ymin, ymax) = ((numpy.amin(y), numpy.amax(y)) if len(y) else (0, 1))
    (xmin, xmax) = matplotlib.transforms.nonsingular(xmin, xmax, expander=0.1)
    (ymin, ymax) = matplotlib.transforms.nonsingular(ymin, ymax, expander=0.1)
padding = (1e-09 * (xmax - xmin))
xmin -= padding
xmax += padding
sx = ((xmax - xmin) / nx)
sy = ((ymax - ymin) / ny)
if marginals:
    xorig = x.copy()
    yorig = y.copy()
x = ((x - xmin) / sx)
y = ((y - ymin) / sy)
ix1 = np.round(x).astype(int)
iy1 = np.round(y).astype(int)
ix2 = np.floor(x).astype(int)
iy2 = np.floor(y).astype(int)
nx1 = (nx + 1)
ny1 = (ny + 1)
nx2 = nx
ny2 = ny
n = ((nx1 * ny1) + (nx2 * ny2))
d1 = (((x - ix1) ** 2) + (3.0 * ((y - iy1) ** 2)))
d2 = ((((x - ix2) - 0.5) ** 2) + (3.0 * (((y - iy2) - 0.5) ** 2)))
bdist = (d1 < d2)
if (C is None):
    accum = numpy.zeros(n)
    lattice1 = accum[:(nx1 * ny1)]
    lattice2 = accum[(nx1 * ny1):]
    lattice1.shape = (nx1, ny1)
    lattice2.shape = (nx2, ny2)
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])] += 1
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])] += 1
    if (mincnt is not None):
        for i in xrange(nx1):
            for j in xrange(ny1):
                if (lattice1[(i, j)] < mincnt):
                    lattice1[(i, j)] = numpy.nan
        for i in xrange(nx2):
            for j in xrange(ny2):
                if (lattice2[(i, j)] < mincnt):
                    lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
else:
    if (mincnt is None):
        mincnt = 0
    lattice1 = numpy.empty((nx1, ny1), dtype=object)
    for i in xrange(nx1):
        for j in xrange(ny1):
            lattice1[(i, j)] = []
    lattice2 = numpy.empty((nx2, ny2), dtype=object)
    for i in xrange(nx2):
        for j in xrange(ny2):
            lattice2[(i, j)] = []
    for i in xrange(len(x)):
        if bdist[i]:
            if ((ix1[i] >= 0) and (ix1[i] < nx1) and (iy1[i] >= 0) and (iy1[i] < ny1)):
                lattice1[(ix1[i], iy1[i])].append(C[i])
        elif ((ix2[i] >= 0) and (ix2[i] < nx2) and (iy2[i] >= 0) and (iy2[i] < ny2)):
            lattice2[(ix2[i], iy2[i])].append(C[i])
    for i in xrange(nx1):
        for j in xrange(ny1):
            vals = lattice1[(i, j)]
            if (len(vals) > mincnt):
                lattice1[(i, j)] = reduce_C_function(vals)
            else:
                lattice1[(i, j)] = numpy.nan
    for i in xrange(nx2):
        for j in xrange(ny2):
            vals = lattice2[(i, j)]
            if (len(vals) > mincnt):
                lattice2[(i, j)] = reduce_C_function(vals)
            else:
                lattice2[(i, j)] = numpy.nan
    accum = numpy.hstack((lattice1.astype(float).ravel(), lattice2.astype(float).ravel()))
    good_idxs = (~ numpy.isnan(accum))
offsets = numpy.zeros((n, 2), float)
offsets[:(nx1 * ny1), 0] = numpy.repeat(numpy.arange(nx1), ny1)
offsets[:(nx1 * ny1), 1] = numpy.tile(numpy.arange(ny1), nx1)
offsets[(nx1 * ny1):, 0] = numpy.repeat((numpy.arange(nx2) + 0.5), ny2)
offsets[(nx1 * ny1):, 1] = (numpy.tile(numpy.arange(ny2), nx2) + 0.5)
offsets[:, 0] *= sx
offsets[:, 1] *= sy
offsets[:, 0] += xmin
offsets[:, 1] += ymin
offsets = offsets[good_idxs, :]
accum = accum[good_idxs]
polygon = numpy.zeros((6, 2), float)
polygon[:, 0] = (sx * numpy.array([0.5, 0.5, 0.0, (- 0.5), (- 0.5), 0.0]))
polygon[:, 1] = ((sy * numpy.array([(- 0.5), 0.5, 1.0, 0.5, (- 0.5), (- 1.0)])) / 3.0)
if (edgecolors == 'none'):
    edgecolors = 'face'
if (linewidths is None):
    linewidths = [1.0]
if ((xscale == 'log') or (yscale == 'log')):
    polygons = (numpy.expand_dims(polygon, 0) + numpy.expand_dims(offsets, 1))
    if (xscale == 'log'):
        polygons[:, :, 0] = (10.0 ** polygons[:, :, 0])
        xmin = (10.0 ** xmin)
        xmax = (10.0 ** xmax)
        self.set_xscale(xscale)
    if (yscale == 'log'):
        polygons[:, :, 1] = (10.0 ** polygons[:, :, 1])
        ymin = (10.0 ** ymin)
        ymax = (10.0 ** ymax)
        self.set_yscale(yscale)
    collection = matplotlib.collections.PolyCollection(polygons, edgecolors=edgecolors, linewidths=linewidths)
else:
    collection = matplotlib.collections.PolyCollection([polygon], edgecolors=edgecolors, linewidths=linewidths, offsets=offsets, transOffset=matplotlib.transforms.IdentityTransform(), offset_position='data')
if isinstance(norm, matplotlib.colors.LogNorm):
    if (accum == 0).any():
        accum += 1
if (norm is not None):
    if ((norm.vmin is None) and (norm.vmax is None)):
        norm.autoscale(accum)
if (bins == 'log'):
    accum = numpy.log10((accum + 1))
elif (bins is not None):
    if (not iterable(bins)):
        (minimum, maximum) = (min(accum), max(accum))
        bins -= 1
        tempResult = arange(bins)
	
===================================================================	
Axes._pcolorargs: 1877	
----------------------------	

allmatch = kw.pop('allmatch', False)
if (len(args) == 1):
    C = numpy.asanyarray(args[0])
    (numRows, numCols) = C.shape
    if allmatch:
        tempResult = arange(numCols)
	
===================================================================	
Axes._pcolorargs: 1877	
----------------------------	

allmatch = kw.pop('allmatch', False)
if (len(args) == 1):
    C = numpy.asanyarray(args[0])
    (numRows, numCols) = C.shape
    if allmatch:
        tempResult = arange(numRows)
	
===================================================================	
Axes._pcolorargs: 1879	
----------------------------	

allmatch = kw.pop('allmatch', False)
if (len(args) == 1):
    C = numpy.asanyarray(args[0])
    (numRows, numCols) = C.shape
    if allmatch:
        (X, Y) = numpy.meshgrid(numpy.arange(numCols), numpy.arange(numRows))
    else:
        tempResult = arange((numCols + 1))
	
===================================================================	
Axes._pcolorargs: 1879	
----------------------------	

allmatch = kw.pop('allmatch', False)
if (len(args) == 1):
    C = numpy.asanyarray(args[0])
    (numRows, numCols) = C.shape
    if allmatch:
        (X, Y) = numpy.meshgrid(numpy.arange(numCols), numpy.arange(numRows))
    else:
        tempResult = arange((numRows + 1))
	
===================================================================	
Axes.stem: 730	
----------------------------	

"\n        Create a stem plot.\n\n        Call signatures::\n\n          stem(y, linefmt='b-', markerfmt='bo', basefmt='r-')\n          stem(x, y, linefmt='b-', markerfmt='bo', basefmt='r-')\n\n        A stem plot plots vertical lines (using *linefmt*) at each *x*\n        location from the baseline to *y*, and places a marker there\n        using *markerfmt*.  A horizontal line at 0 is is plotted using\n        *basefmt*.\n\n        If no *x* values are provided, the default is (0, 1, ..., len(y) - 1)\n\n        Return value is a tuple (*markerline*, *stemlines*,\n        *baseline*).\n\n        .. seealso::\n            This\n            `document <http://www.mathworks.com/help/techdoc/ref/stem.html>`_\n            for details.\n\n\n        **Example:**\n\n        .. plot:: mpl_examples/pylab_examples/stem_plot.py\n        "
remember_hold = self._hold
if (not self._hold):
    self.cla()
self._hold = True
y = numpy.asarray(args[0])
args = args[1:]
try:
    second = numpy.asarray(args[0], dtype=numpy.float)
    (x, y) = (y, second)
    args = args[1:]
except (IndexError, ValueError):
    tempResult = arange(len(y))
	
===================================================================	
Axes.csd: 2448	
----------------------------	

"\n        Plot the cross-spectral density.\n\n        Call signature::\n\n          csd(x, y, NFFT=256, Fs=2, Fc=0, detrend=mlab.detrend_none,\n              window=mlab.window_hanning, noverlap=0, pad_to=None,\n              sides='default', scale_by_freq=None, return_line=None, **kwargs)\n\n        The cross spectral density :math:`P_{xy}` by Welch's average\n        periodogram method.  The vectors *x* and *y* are divided into\n        *NFFT* length segments.  Each segment is detrended by function\n        *detrend* and windowed by function *window*.  *noverlap* gives\n        the length of the overlap between segments.  The product of\n        the direct FFTs of *x* and *y* are averaged over each segment\n        to compute :math:`P_{xy}`, with a scaling to correct for power\n        loss due to windowing.\n\n        If len(*x*) < *NFFT* or len(*y*) < *NFFT*, they will be zero\n        padded to *NFFT*.\n\n        Parameters\n        ----------\n        x, y : 1-D arrays or sequences\n            Arrays or sequences containing the data\n\n        %(Spectral)s\n\n        %(PSD)s\n\n        noverlap : integer\n            The number of points of overlap between segments.\n            The default value is 0 (no overlap).\n\n        Fc : integer\n            The center frequency of *x* (defaults to 0), which offsets\n            the x extents of the plot to reflect the frequency range used\n            when a signal is acquired and then filtered and downsampled to\n            baseband.\n\n        return_line : bool\n            Whether to include the line object plotted in the returned values.\n            Default is False.\n\n        **kwargs :\n            Keyword arguments control the :class:`~matplotlib.lines.Line2D`\n            properties:\n\n        %(Line2D)s\n\n        Returns\n        -------\n        Pxy : 1-D array\n            The values for the cross spectrum `P_{xy}` before scaling\n            (complex valued)\n\n        freqs : 1-D array\n            The frequencies corresponding to the elements in *Pxy*\n\n        line : a :class:`~matplotlib.lines.Line2D` instance\n            The line created by this function.\n            Only returned if *return_line* is True.\n\n        Notes\n        -----\n        For plotting, the power is plotted as\n        :math:`10\\log_{10}(P_{xy})` for decibels, though `P_{xy}` itself\n        is returned.\n\n        References\n        ----------\n        Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n        John Wiley & Sons (1986)\n\n        Examples\n        --------\n        .. plot:: mpl_examples/pylab_examples/csd_demo.py\n\n        See Also\n        --------\n        :func:`psd`\n            :func:`psd` is the equivalent to setting y=x.\n        "
if (not self._hold):
    self.cla()
if (Fc is None):
    Fc = 0
(pxy, freqs) = matplotlib.mlab.csd(x=x, y=y, NFFT=NFFT, Fs=Fs, detrend=detrend, window=window, noverlap=noverlap, pad_to=pad_to, sides=sides, scale_by_freq=scale_by_freq)
pxy.shape = (len(freqs),)
freqs += Fc
line = self.plot(freqs, (10 * numpy.log10(numpy.absolute(pxy))), **kwargs)
self.set_xlabel('Frequency')
self.set_ylabel('Cross Spectrum Magnitude (dB)')
self.grid(True)
(vmin, vmax) = self.viewLim.intervaly
intv = (vmax - vmin)
step = (10 * int(numpy.log10(intv)))
tempResult = arange(math.floor(vmin), (math.ceil(vmax) + 1), step)
	
===================================================================	
Axes.psd: 2423	
----------------------------	

"\n        Plot the power spectral density.\n\n        Call signature::\n\n          psd(x, NFFT=256, Fs=2, Fc=0, detrend=mlab.detrend_none,\n              window=mlab.window_hanning, noverlap=0, pad_to=None,\n              sides='default', scale_by_freq=None, return_line=None, **kwargs)\n\n        The power spectral density :math:`P_{xx}` by Welch's average\n        periodogram method.  The vector *x* is divided into *NFFT* length\n        segments.  Each segment is detrended by function *detrend* and\n        windowed by function *window*.  *noverlap* gives the length of\n        the overlap between segments.  The :math:`|\\mathrm{fft}(i)|^2`\n        of each segment :math:`i` are averaged to compute :math:`P_{xx}`,\n        with a scaling to correct for power loss due to windowing.\n\n        If len(*x*) < *NFFT*, it will be zero padded to *NFFT*.\n\n        Parameters\n        ----------\n        x : 1-D array or sequence\n            Array or sequence containing the data\n\n        %(Spectral)s\n\n        %(PSD)s\n\n        noverlap : integer\n            The number of points of overlap between segments.\n            The default value is 0 (no overlap).\n\n        Fc : integer\n            The center frequency of *x* (defaults to 0), which offsets\n            the x extents of the plot to reflect the frequency range used\n            when a signal is acquired and then filtered and downsampled to\n            baseband.\n\n        return_line : bool\n            Whether to include the line object plotted in the returned values.\n            Default is False.\n\n        **kwargs :\n            Keyword arguments control the :class:`~matplotlib.lines.Line2D`\n            properties:\n\n        %(Line2D)s\n\n        Returns\n        -------\n        Pxx : 1-D array\n            The values for the power spectrum `P_{xx}` before scaling\n            (real valued)\n\n        freqs : 1-D array\n            The frequencies corresponding to the elements in *Pxx*\n\n        line : a :class:`~matplotlib.lines.Line2D` instance\n            The line created by this function.\n            Only returned if *return_line* is True.\n\n        Notes\n        -----\n        For plotting, the power is plotted as\n        :math:`10\\log_{10}(P_{xx})` for decibels, though *Pxx* itself\n        is returned.\n\n        References\n        ----------\n        Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n        John Wiley & Sons (1986)\n\n        Examples\n        --------\n        .. plot:: mpl_examples/pylab_examples/psd_demo.py\n\n        See Also\n        --------\n        :func:`specgram`\n            :func:`specgram` differs in the default overlap; in not returning\n            the mean of the segment periodograms; in returning the times of the\n            segments; and in plotting a colormap instead of a line.\n\n        :func:`magnitude_spectrum`\n            :func:`magnitude_spectrum` plots the magnitude spectrum.\n\n        :func:`csd`\n            :func:`csd` plots the spectral density between two signals.\n        "
if (not self._hold):
    self.cla()
if (Fc is None):
    Fc = 0
(pxx, freqs) = matplotlib.mlab.psd(x=x, NFFT=NFFT, Fs=Fs, detrend=detrend, window=window, noverlap=noverlap, pad_to=pad_to, sides=sides, scale_by_freq=scale_by_freq)
pxx.shape = (len(freqs),)
freqs += Fc
if (scale_by_freq in (None, True)):
    psd_units = 'dB/Hz'
else:
    psd_units = 'dB'
line = self.plot(freqs, (10 * numpy.log10(pxx)), **kwargs)
self.set_xlabel('Frequency')
self.set_ylabel(('Power Spectral Density (%s)' % psd_units))
self.grid(True)
(vmin, vmax) = self.viewLim.intervaly
intv = (vmax - vmin)
logi = int(numpy.log10(intv))
if (logi == 0):
    logi = 0.1
step = (10 * logi)
tempResult = arange(math.floor(vmin), (math.ceil(vmax) + 1), step)
	
===================================================================	
Axes.errorbar: 962	
----------------------------	

"\n        Plot an errorbar graph.\n\n        Plot x versus y with error deltas in yerr and xerr.\n        Vertical errorbars are plotted if yerr is not None.\n        Horizontal errorbars are plotted if xerr is not None.\n\n        x, y, xerr, and yerr can all be scalars, which plots a\n        single error bar at x, y.\n\n        Parameters\n        ----------\n        x : scalar\n        y : scalar\n\n        xerr/yerr : scalar or array-like, shape(n,1) or shape(2,n), optional\n            If a scalar number, len(N) array-like object, or an Nx1\n            array-like object, errorbars are drawn at +/-value relative\n            to the data. Default is None.\n\n            If a sequence of shape 2xN, errorbars are drawn at -row1\n            and +row2 relative to the data.\n\n        fmt : plot format string, optional, default: None\n            The plot format symbol. If fmt is 'none' (case-insensitive),\n            only the errorbars are plotted.  This is used for adding\n            errorbars to a bar plot, for example.  Default is '',\n            an empty plot format string; properties are\n            then identical to the defaults for :meth:`plot`.\n\n        ecolor : mpl color, optional, default: None\n            A matplotlib color arg which gives the color the errorbar lines;\n            if None, use the color of the line connecting the markers.\n\n        elinewidth : scalar, optional, default: None\n            The linewidth of the errorbar lines. If None, use the linewidth.\n\n        capsize : scalar, optional, default: None\n            The length of the error bar caps in points; if None, it will\n            take the value from ``errorbar.capsize``\n            :data:`rcParam<matplotlib.rcParams>`.\n\n        capthick : scalar, optional, default: None\n            An alias kwarg to markeredgewidth (a.k.a. - mew). This\n            setting is a more sensible name for the property that\n            controls the thickness of the error bar cap in points. For\n            backwards compatibility, if mew or markeredgewidth are given,\n            then they will over-ride capthick. This may change in future\n            releases.\n\n        barsabove : bool, optional, default: False\n            if True , will plot the errorbars above the plot\n            symbols. Default is below.\n\n        lolims / uplims / xlolims / xuplims : bool, optional, default:None\n            These arguments can be used to indicate that a value gives\n            only upper/lower limits. In that case a caret symbol is\n            used to indicate this. lims-arguments may be of the same\n            type as *xerr* and *yerr*.  To use limits with inverted\n            axes, :meth:`set_xlim` or :meth:`set_ylim` must be called\n            before :meth:`errorbar`.\n\n        errorevery : positive integer, optional, default:1\n            subsamples the errorbars. e.g., if errorevery=5, errorbars for\n            every 5-th datapoint will be plotted. The data plot itself still\n            shows all data points.\n\n        Returns\n        -------\n        plotline : :class:`~matplotlib.lines.Line2D` instance\n            x, y plot markers and/or line\n        caplines : list of :class:`~matplotlib.lines.Line2D` instances\n            error bar cap\n        barlinecols : list of :class:`~matplotlib.collections.LineCollection`\n            horizontal and vertical error ranges.\n\n        Other Parameters\n        ----------------\n        kwargs : All other keyword arguments are passed on to the plot\n            command for the markers. For example, this code makes big red\n            squares with thick green edges::\n\n                x,y,yerr = rand(3,10)\n                errorbar(x, y, yerr, marker='s', mfc='red',\n                         mec='green', ms=20, mew=4)\n\n            where mfc, mec, ms and mew are aliases for the longer\n            property names, markerfacecolor, markeredgecolor, markersize\n            and markeredgewidth.\n\n            valid kwargs for the marker properties are\n\n            %(Line2D)s\n\n        Examples\n        --------\n        .. plot:: mpl_examples/statistics/errorbar_demo.py\n        "
kwargs = matplotlib.cbook.normalize_kwargs(kwargs, _alias_map)
kwargs.setdefault('zorder', 2)
if (errorevery < 1):
    raise ValueError('errorevery has to be a strictly positive integer')
self._process_unit_info(xdata=x, ydata=y, kwargs=kwargs)
if (not self._hold):
    self.cla()
holdstate = self._hold
self._hold = True
if (fmt is None):
    fmt = 'none'
    msg = (('Use of None object as fmt keyword argument to ' + 'suppress plotting of data values is deprecated ') + 'since 1.4; use the string "none" instead.')
    warnings.warn(msg, mplDeprecation, stacklevel=1)
plot_line = (fmt.lower() != 'none')
label = kwargs.pop('label', None)
fmt_style_kwargs = {k: v for (k, v) in zip(('linestyle', 'marker', 'color'), _process_plot_format(fmt)) if (v is not None)}
if (('color' in kwargs) or ('color' in fmt_style_kwargs) or (ecolor is not None)):
    base_style = {}
    if ('color' in kwargs):
        base_style['color'] = kwargs.pop('color')
else:
    base_style = six.next(self._get_lines.prop_cycler)
base_style['label'] = '_nolegend_'
base_style.update(fmt_style_kwargs)
if ('color' not in base_style):
    base_style['color'] = 'C0'
if (ecolor is None):
    ecolor = base_style['color']
if (not iterable(x)):
    x = [x]
if (not iterable(y)):
    y = [y]
if (xerr is not None):
    if (not iterable(xerr)):
        xerr = ([xerr] * len(x))
if (yerr is not None):
    if (not iterable(yerr)):
        yerr = ([yerr] * len(y))
plot_line_style = dict(base_style)
plot_line_style.update(**kwargs)
if barsabove:
    plot_line_style['zorder'] = (kwargs['zorder'] - 0.1)
else:
    plot_line_style['zorder'] = (kwargs['zorder'] + 0.1)
eb_lines_style = dict(base_style)
eb_lines_style.pop('marker', None)
eb_lines_style.pop('linestyle', None)
eb_lines_style['color'] = ecolor
if elinewidth:
    eb_lines_style['linewidth'] = elinewidth
elif ('linewidth' in kwargs):
    eb_lines_style['linewidth'] = kwargs['linewidth']
for key in ('transform', 'alpha', 'zorder', 'rasterized'):
    if (key in kwargs):
        eb_lines_style[key] = kwargs[key]
eb_cap_style = dict(base_style)
eb_cap_style.pop('marker', None)
eb_cap_style.pop('ls', None)
eb_cap_style['linestyle'] = 'none'
if (capsize is None):
    capsize = rcParams['errorbar.capsize']
if (capsize > 0):
    eb_cap_style['markersize'] = (2.0 * capsize)
if (capthick is not None):
    eb_cap_style['markeredgewidth'] = capthick
for key in ('markeredgewidth', 'transform', 'alpha', 'zorder', 'rasterized'):
    if (key in kwargs):
        eb_cap_style[key] = kwargs[key]
eb_cap_style['color'] = ecolor
data_line = None
if plot_line:
    data_line = matplotlib.lines.Line2D(x, y, **plot_line_style)
    self.add_line(data_line)
barcols = []
caplines = []

def _bool_asarray_helper(d, expected):
    if (not iterable(d)):
        return numpy.asarray(([d] * expected), bool)
    else:
        return numpy.asarray(d, bool)
lolims = _bool_asarray_helper(lolims, len(x))
uplims = _bool_asarray_helper(uplims, len(x))
xlolims = _bool_asarray_helper(xlolims, len(x))
xuplims = _bool_asarray_helper(xuplims, len(x))
tempResult = arange(len(x))
	
===================================================================	
RendererAgg.draw_path: 94	
----------------------------	

'\n        Draw the path\n        '
nmax = rcParams['agg.path.chunksize']
npts = path.vertices.shape[0]
if ((nmax > 100) and (npts > nmax) and path.should_simplify and (rgbFace is None) and (gc.get_hatch() is None)):
    nch = numpy.ceil((npts / float(nmax)))
    chsize = int(numpy.ceil((npts / nch)))
    tempResult = arange(0, npts, chsize)
	
===================================================================	
Triangulation.__init__: 28	
----------------------------	

self.x = numpy.asarray(x, dtype=numpy.float64)
self.y = numpy.asarray(y, dtype=numpy.float64)
if ((self.x.shape != self.y.shape) or (len(self.x.shape) != 1)):
    raise ValueError('x,y must be equal-length 1-D arrays')
self.old_shape = self.x.shape
duplicates = self._get_duplicate_point_indices()
if (len(duplicates) > 0):
    warnings.warn('Input data contains duplicate x,y points; some values are ignored.', DuplicatePointWarning)
    tempResult = arange(len(self.x))
	
===================================================================	
PolarAxes.cla: 187	
----------------------------	

matplotlib.axes.Axes.cla(self)
self.title.set_y(1.05)
self.xaxis.set_major_formatter(self.ThetaFormatter())
self.xaxis.isDefault_majfmt = True
tempResult = arange(0.0, 360.0, 45.0)
	
===================================================================	
calculate_rms: 173	
----------------------------	

'Calculate the per-pixel errors, then compute the root mean square error.'
if (expectedImage.shape != actualImage.shape):
    raise ImageComparisonFailure('image sizes do not match expected size: {0} actual size {1}'.format(expectedImage.shape, actualImage.shape))
num_values = numpy.prod(expectedImage.shape)
abs_diff_image = abs((expectedImage - actualImage))
expected_version = distutils.version.LooseVersion('1.6')
found_version = distutils.version.LooseVersion(numpy.__version__)
if (found_version >= expected_version):
    histogram = numpy.bincount(abs_diff_image.ravel(), minlength=256)
else:
    tempResult = arange(257)
	
===================================================================	
calculate_rms: 174	
----------------------------	

'Calculate the per-pixel errors, then compute the root mean square error.'
if (expectedImage.shape != actualImage.shape):
    raise ImageComparisonFailure('image sizes do not match expected size: {0} actual size {1}'.format(expectedImage.shape, actualImage.shape))
num_values = numpy.prod(expectedImage.shape)
abs_diff_image = abs((expectedImage - actualImage))
expected_version = distutils.version.LooseVersion('1.6')
found_version = distutils.version.LooseVersion(numpy.__version__)
if (found_version >= expected_version):
    histogram = numpy.bincount(abs_diff_image.ravel(), minlength=256)
else:
    histogram = numpy.histogram(abs_diff_image, bins=numpy.arange(257))[0]
tempResult = arange(len(histogram))
	
===================================================================	
test_marker_with_nan: 51	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots(1)
steps = 1000
tempResult = arange(steps)
	
===================================================================	
test_remove: 116	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(36)
	
===================================================================	
test_default_edges: 139	
----------------------------	

(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2)
tempResult = arange(10)
	
===================================================================	
test_default_edges: 139	
----------------------------	

(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2)
tempResult = arange(10)
	
===================================================================	
test_default_edges: 139	
----------------------------	

(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2)
tempResult = arange(10)
	
===================================================================	
test_default_edges: 139	
----------------------------	

(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2)
tempResult = arange(10)
	
===================================================================	
test_default_edges: 140	
----------------------------	

(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2)
ax1.plot(numpy.arange(10), numpy.arange(10), 'x', (numpy.arange(10) + 1), numpy.arange(10), 'o')
tempResult = arange(10)
	
===================================================================	
test_default_edges: 140	
----------------------------	

(fig, [[ax1, ax2], [ax3, ax4]]) = matplotlib.pyplot.subplots(2, 2)
ax1.plot(numpy.arange(10), numpy.arange(10), 'x', (numpy.arange(10) + 1), numpy.arange(10), 'o')
tempResult = arange(10)
	
===================================================================	
test_markers_fillstyle_rcparams: 1816	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(7)
	
===================================================================	
test_const_xy: 270	
----------------------------	

fig = matplotlib.pyplot.figure()
matplotlib.pyplot.subplot(311)
tempResult = arange(10)
	
===================================================================	
test_const_xy: 272	
----------------------------	

fig = matplotlib.pyplot.figure()
matplotlib.pyplot.subplot(311)
matplotlib.pyplot.plot(numpy.arange(10), numpy.ones((10,)))
matplotlib.pyplot.subplot(312)
tempResult = arange(10)
	
===================================================================	
test_step_linestyle: 1913	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_scatter_2D: 916	
----------------------------	

tempResult = arange(3)
	
===================================================================	
test_scatter_2D: 917	
----------------------------	

x = numpy.arange(3)
tempResult = arange(2)
	
===================================================================	
test_specgram_angle_freqs: 2091	
----------------------------	

'test axes.specgram in angle mode with sinusoidal stimuli'
n = 1000
Fs = 10.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
fstims2 = [(Fs / 4.7), (Fs / 5.6), (Fs / 11.9)]
NFFT = int(((10 * Fs) / min((fstims1 + fstims2))))
noverlap = int((NFFT / 2))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_eventplot: 1757	
----------------------------	

'\n    test that eventplot produces the correct output\n    '
numpy.random.seed(0)
data1 = np.random.random([32, 20]).tolist()
data2 = np.random.random([6, 20]).tolist()
data = (data1 + data2)
num_datasets = len(data)
colors1 = ([[0, 1, 0.7]] * len(data1))
colors2 = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0.75, 0], [1, 0, 1], [0, 1, 1]]
colors = (colors1 + colors2)
tempResult = arange(0, len(data1))
	
===================================================================	
test_hist_step_log_bottom: 845	
----------------------------	

numpy.random.seed(0)
data = numpy.random.standard_normal(2000)
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.hist(data, bins=10, log=True, histtype='stepfilled', alpha=0.5, color='b')
ax.hist(data, bins=10, log=True, histtype='stepfilled', alpha=0.5, color='g', bottom=0.01)
ax.hist(data, bins=10, log=True, histtype='stepfilled', alpha=0.5, color='r', bottom=0.5)
tempResult = arange(10)
	
===================================================================	
test_magnitude_spectrum_freqs: 2307	
----------------------------	

'test axes.magnitude_spectrum with sinusoidal stimuli'
n = 10000
Fs = 100.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
NFFT = int(((1000 * Fs) / min(fstims1)))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_marker_styles: 1811	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
for (y, marker) in enumerate(sorted(matplotlib.markers.MarkerStyle.markers.keys(), key=(lambda x: (str(type(x)) + str(x))))):
    tempResult = arange(10)
	
===================================================================	
test_errorbar_limits: 1538	
----------------------------	

tempResult = arange(0.5, 5.5, 0.5)
	
===================================================================	
test_psd_freqs: 2197	
----------------------------	

'test axes.psd with sinusoidal stimuli'
n = 10000
Fs = 100.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
fstims2 = [(Fs / 4.7), (Fs / 5.6), (Fs / 11.9)]
NFFT = int(((1000 * Fs) / min((fstims1 + fstims2))))
noverlap = int((NFFT / 2))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_specgram_freqs_phase: 2144	
----------------------------	

'test axes.specgram in phase mode with sinusoidal stimuli'
n = 1000
Fs = 10.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
fstims2 = [(Fs / 4.7), (Fs / 5.6), (Fs / 11.9)]
NFFT = int(((10 * Fs) / min((fstims1 + fstims2))))
noverlap = int((NFFT / 2))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_phase_spectrum_freqs: 2437	
----------------------------	

'test axes.phase_spectrum with sinusoidal stimuli'
n = 10000
Fs = 100.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
NFFT = int(((1000 * Fs) / min(fstims1)))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_arc_ellipse: 638	
----------------------------	

from matplotlib import patches
(xcenter, ycenter) = (0.38, 0.52)
(width, height) = (0.1, 0.3)
angle = (- 30)
tempResult = arange(0.0, 360.0, 1.0)
	
===================================================================	
test_specgram_freqs: 1971	
----------------------------	

'test axes.specgram in default (psd) mode with sinusoidal stimuli'
n = 1000
Fs = 10.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
fstims2 = [(Fs / 4.7), (Fs / 5.6), (Fs / 11.9)]
NFFT = int(((10 * Fs) / min((fstims1 + fstims2))))
noverlap = int((NFFT / 2))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_pcolor_fast_non_uniform: 2824	
----------------------------	

tempResult = arange(6)
	
===================================================================	
test_square_plot: 2807	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_pcolor_datetime_axis: 596	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.datetime.timedelta(days=d)) for d in range(21)])
tempResult = arange(21)
	
===================================================================	
test_pcolor_datetime_axis: 597	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.datetime.timedelta(days=d)) for d in range(21)])
y = numpy.arange(21)
tempResult = arange(20)
	
===================================================================	
test_pcolor_datetime_axis: 597	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.datetime.timedelta(days=d)) for d in range(21)])
y = numpy.arange(21)
tempResult = arange(20)
	
===================================================================	
test_polar_theta_position: 323	
----------------------------	

tempResult = arange(0, 3.0, 0.01)
	
===================================================================	
test_angle_spectrum_freqs: 2385	
----------------------------	

'test axes.angle_spectrum with sinusoidal stimuli'
n = 10000
Fs = 100.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
NFFT = int(((1000 * Fs) / min(fstims1)))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_nonfinite_limits: 427	
----------------------------	

tempResult = arange(0.0, numpy.e, 0.01)
	
===================================================================	
test_specgram_magnitude_freqs: 2030	
----------------------------	

'test axes.specgram in magnitude mode with sinusoidal stimuli'
n = 1000
Fs = 10.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
fstims2 = [(Fs / 4.7), (Fs / 5.6), (Fs / 11.9)]
NFFT = int(((100 * Fs) / min((fstims1 + fstims2))))
noverlap = int((NFFT / 2))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_errorbar_shape: 1527	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.gca()
tempResult = arange(0.1, 4, 0.5)
	
===================================================================	
test_hexbin_extent: 367	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(2000.0)
	
===================================================================	
test_errorbar: 1491	
----------------------------	

tempResult = arange(0.1, 4, 0.5)
	
===================================================================	
test_contour_colorbar: 866	
----------------------------	

(x, y, z) = contour_dat()
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange((- 1.8), 1.801, 0.2)
	
===================================================================	
test_contour_colorbar: 867	
----------------------------	

(x, y, z) = contour_dat()
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
cs = ax.contourf(x, y, z, levels=numpy.arange((- 1.8), 1.801, 0.2), cmap=matplotlib.pyplot.get_cmap('RdBu'), vmin=(- 0.6), vmax=0.6, extend='both')
tempResult = arange((- 2.2), (- 0.599), 0.2)
	
===================================================================	
test_contour_colorbar: 868	
----------------------------	

(x, y, z) = contour_dat()
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
cs = ax.contourf(x, y, z, levels=numpy.arange((- 1.8), 1.801, 0.2), cmap=matplotlib.pyplot.get_cmap('RdBu'), vmin=(- 0.6), vmax=0.6, extend='both')
cs1 = ax.contour(x, y, z, levels=numpy.arange((- 2.2), (- 0.599), 0.2), colors=['y'], linestyles='solid', linewidths=2)
tempResult = arange(0.6, 2.2, 0.2)
	
===================================================================	
test_polar_rmin: 313	
----------------------------	

tempResult = arange(0, 3.0, 0.01)
	
===================================================================	
test_hexbin_pickable: 391	
----------------------------	


class FauxMouseEvent():

    def __init__(self, x, y):
        self.x = x
        self.y = y
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(200.0)
	
===================================================================	
test_autoscale_log_shared: 136	
----------------------------	

tempResult = arange(100, dtype=float)
	
===================================================================	
test_errorbar_inputs_shotgun: 2856	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_boxplot_zorder: 1340	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_pandas_indexing_dates: 3003	
----------------------------	

try:
    import pandas as pd
except ImportError:
    raise SkipTest('Pandas not installed')
tempResult = arange('2005-02', '2005-03', dtype='datetime64[D]')
	
===================================================================	
test_polar_annotations: 177	
----------------------------	

tempResult = arange(0.0, 1.0, 0.001)
	
===================================================================	
test_symlog2: 511	
----------------------------	

tempResult = arange((- 50), 50, 0.001)
	
===================================================================	
test_axes_margins: 2925	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.plot([0, 1, 2, 3])
assert (ax.get_ybound()[0] != 0)
(fig, ax) = matplotlib.pyplot.subplots()
ax.bar([0, 1, 2, 3], [1, 1, 1, 1])
assert (ax.get_ybound()[0] == 0)
(fig, ax) = matplotlib.pyplot.subplots()
ax.barh([0, 1, 2, 3], [1, 1, 1, 1])
assert (ax.get_xbound()[0] == 0)
(fig, ax) = matplotlib.pyplot.subplots()
ax.pcolor(numpy.zeros((10, 10)))
assert (ax.get_xbound() == (0, 10))
assert (ax.get_ybound() == (0, 10))
(fig, ax) = matplotlib.pyplot.subplots()
ax.pcolorfast(numpy.zeros((10, 10)))
assert (ax.get_xbound() == (0, 10))
assert (ax.get_ybound() == (0, 10))
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(10)
	
===================================================================	
test_hist_step_bottom: 1664	
----------------------------	

d1 = numpy.linspace(1, 3, 20)
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(10)
	
===================================================================	
test_shaped_data: 252	
----------------------------	

xdata = numpy.array([[0.53295185, 0.23052951, 0.19057629, 0.66724975, 0.96577916, 0.73136095, 0.60823287, 0.017921, 0.29744742, 0.27164665], [0.2798012, 0.25814229, 0.02818193, 0.12966456, 0.57446277, 0.58167607, 0.71028245, 0.69112737, 0.89923072, 0.99072476], [0.81218578, 0.80464528, 0.76071809, 0.85616314, 0.12757994, 0.94324936, 0.73078663, 0.09658102, 0.60703967, 0.77664978], [0.28332265, 0.81479711, 0.86985333, 0.43797066, 0.32540082, 0.43819229, 0.92230363, 0.49414252, 0.68168256, 0.05922372], [0.10721335, 0.93904142, 0.79163075, 0.73232848, 0.90283839, 0.68408046, 0.25502302, 0.95976614, 0.59214115, 0.13663711], [0.28087456, 0.33127607, 0.15530412, 0.76558121, 0.83389773, 0.03735974, 0.98717738, 0.71432229, 0.54881366, 0.86893953], [0.77995937, 0.995556, 0.29688434, 0.15646162, 0.051848, 0.37161935, 0.12998491, 0.09377296, 0.36882507, 0.36583435], [0.37851836, 0.05315792, 0.63144617, 0.25003433, 0.69586032, 0.11393988, 0.92362096, 0.88045438, 0.93530252, 0.68275072], [0.86486596, 0.83236675, 0.82960664, 0.5779663, 0.25724233, 0.84841095, 0.90862812, 0.64414887, 0.3565272, 0.71026066], [0.01383268, 0.3406093, 0.76084285, 0.70800694, 0.87634056, 0.08213693, 0.54655021, 0.98123181, 0.44080053, 0.86815815]])
tempResult = arange(10)
	
===================================================================	
test_shaped_data: 254	
----------------------------	

xdata = numpy.array([[0.53295185, 0.23052951, 0.19057629, 0.66724975, 0.96577916, 0.73136095, 0.60823287, 0.017921, 0.29744742, 0.27164665], [0.2798012, 0.25814229, 0.02818193, 0.12966456, 0.57446277, 0.58167607, 0.71028245, 0.69112737, 0.89923072, 0.99072476], [0.81218578, 0.80464528, 0.76071809, 0.85616314, 0.12757994, 0.94324936, 0.73078663, 0.09658102, 0.60703967, 0.77664978], [0.28332265, 0.81479711, 0.86985333, 0.43797066, 0.32540082, 0.43819229, 0.92230363, 0.49414252, 0.68168256, 0.05922372], [0.10721335, 0.93904142, 0.79163075, 0.73232848, 0.90283839, 0.68408046, 0.25502302, 0.95976614, 0.59214115, 0.13663711], [0.28087456, 0.33127607, 0.15530412, 0.76558121, 0.83389773, 0.03735974, 0.98717738, 0.71432229, 0.54881366, 0.86893953], [0.77995937, 0.995556, 0.29688434, 0.15646162, 0.051848, 0.37161935, 0.12998491, 0.09377296, 0.36882507, 0.36583435], [0.37851836, 0.05315792, 0.63144617, 0.25003433, 0.69586032, 0.11393988, 0.92362096, 0.88045438, 0.93530252, 0.68275072], [0.86486596, 0.83236675, 0.82960664, 0.5779663, 0.25724233, 0.84841095, 0.90862812, 0.64414887, 0.3565272, 0.71026066], [0.01383268, 0.3406093, 0.76084285, 0.70800694, 0.87634056, 0.08213693, 0.54655021, 0.98123181, 0.44080053, 0.86815815]])
y1 = numpy.arange(10)
y1.shape = (1, 10)
tempResult = arange(10)
	
===================================================================	
test_basic_annotate: 168	
----------------------------	

tempResult = arange(0.0, 5.0, 0.01)
	
===================================================================	
test_loglog: 2900	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(1, 11)
	
===================================================================	
test_fill_between_interpolate: 483	
----------------------------	

tempResult = arange(0.0, 2, 0.02)
	
===================================================================	
test_csd_freqs: 2254	
----------------------------	

'test axes.csd with sinusoidal stimuli'
n = 10000
Fs = 100.0
fstims1 = [(Fs / 4), (Fs / 5), (Fs / 11)]
fstims2 = [(Fs / 4.7), (Fs / 5.6), (Fs / 11.9)]
NFFT = int(((1000 * Fs) / min((fstims1 + fstims2))))
noverlap = int((NFFT / 2))
pad_to = int((2 ** numpy.ceil(numpy.log2(NFFT))))
tempResult = arange(0, n, (1 / Fs))
	
===================================================================	
test_pcolormesh_datetime_axis: 572	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.datetime.timedelta(days=d)) for d in range(21)])
tempResult = arange(21)
	
===================================================================	
test_pcolormesh_datetime_axis: 573	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.datetime.timedelta(days=d)) for d in range(21)])
y = numpy.arange(21)
tempResult = arange(20)
	
===================================================================	
test_pcolormesh_datetime_axis: 573	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.datetime.timedelta(days=d)) for d in range(21)])
y = numpy.arange(21)
tempResult = arange(20)
	
===================================================================	
test_uses_per_path: 18	
----------------------------	

id = matplotlib.transforms.Affine2D()
paths = [matplotlib.path.Path.unit_regular_polygon(i) for i in range(3, 7)]
tforms = [id.rotate(i) for i in range(1, 5)]
tempResult = arange(20)
	
===================================================================	
test_composite_image: 72	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_composite_image: 72	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_composite_image: 67	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_composite_image: 67	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_bold_font_output: 84	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(1, 1, 1)
tempResult = arange(10)
	
===================================================================	
test_bold_font_output: 84	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(1, 1, 1)
tempResult = arange(10)
	
===================================================================	
test_composite_images: 47	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_composite_images: 47	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_bold_font_output_with_none_fonttype: 94	
----------------------------	

matplotlib.pyplot.rcParams['svg.fonttype'] = 'none'
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(1, 1, 1)
tempResult = arange(10)
	
===================================================================	
test_bold_font_output_with_none_fonttype: 94	
----------------------------	

matplotlib.pyplot.rcParams['svg.fonttype'] = 'none'
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(1, 1, 1)
tempResult = arange(10)
	
===================================================================	
test_noscale: 39	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_noscale: 39	
----------------------------	

tempResult = arange((- 5), 5, 1)
	
===================================================================	
test_bbox_inches_tight: 18	
----------------------------	

data = [[66386, 174296, 75131, 577908, 32015], [58230, 381139, 78045, 99308, 160454], [89135, 80552, 152558, 497981, 603535], [78415, 81858, 150656, 193263, 69638], [139361, 331509, 343164, 781380, 52269]]
colLabels = rowLabels = ([''] * 5)
rows = len(data)
tempResult = arange(len(colLabels))
	
===================================================================	
test_to_prestep: 264	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_to_prestep: 265	
----------------------------	

x = numpy.arange(4)
tempResult = arange(4)
	
===================================================================	
test_to_prestep: 266	
----------------------------	

x = numpy.arange(4)
y1 = numpy.arange(4)
tempResult = arange(4)
	
===================================================================	
test_to_poststep: 279	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_to_poststep: 280	
----------------------------	

x = numpy.arange(4)
tempResult = arange(4)
	
===================================================================	
test_to_poststep: 281	
----------------------------	

x = numpy.arange(4)
y1 = numpy.arange(4)
tempResult = arange(4)
	
===================================================================	
Test_delete_masked_points.setUp: 61	
----------------------------	

self.mask1 = [False, False, True, True, False, False]
tempResult = arange(1.0, 7.0)
	
===================================================================	
test_to_midstep: 294	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_to_midstep: 295	
----------------------------	

x = numpy.arange(4)
tempResult = arange(4)
	
===================================================================	
test_to_midstep: 296	
----------------------------	

x = numpy.arange(4)
y1 = numpy.arange(4)
tempResult = arange(4)
	
===================================================================	
test_flatiter: 346	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_is_string_like: 16	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_step_fails: 309	
----------------------------	

tempResult = arange(12)
	
===================================================================	
test_step_fails: 310	
----------------------------	

assert_raises(ValueError, matplotlib.cbook._step_validation, np.arange(12).reshape(3, 4), 'a')
tempResult = arange(12)
	
===================================================================	
test_step_fails: 311	
----------------------------	

assert_raises(ValueError, matplotlib.cbook._step_validation, np.arange(12).reshape(3, 4), 'a')
assert_raises(ValueError, matplotlib.cbook._step_validation, numpy.arange(12), 'a')
tempResult = arange(12)
	
===================================================================	
test_step_fails: 312	
----------------------------	

assert_raises(ValueError, matplotlib.cbook._step_validation, np.arange(12).reshape(3, 4), 'a')
assert_raises(ValueError, matplotlib.cbook._step_validation, numpy.arange(12), 'a')
assert_raises(ValueError, matplotlib.cbook._step_validation, numpy.arange(12))
tempResult = arange(12)
	
===================================================================	
test_step_fails: 312	
----------------------------	

assert_raises(ValueError, matplotlib.cbook._step_validation, np.arange(12).reshape(3, 4), 'a')
assert_raises(ValueError, matplotlib.cbook._step_validation, numpy.arange(12), 'a')
assert_raises(ValueError, matplotlib.cbook._step_validation, numpy.arange(12))
tempResult = arange(3)
	
===================================================================	
test_EllipseCollection: 300	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(4)
	
===================================================================	
test_EllipseCollection: 301	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
x = numpy.arange(4)
tempResult = arange(3)
	
===================================================================	
test_quiver_limits: 274	
----------------------------	

ax = matplotlib.pyplot.axes()
tempResult = arange(8)
	
===================================================================	
test_quiver_limits: 274	
----------------------------	

ax = matplotlib.pyplot.axes()
tempResult = arange(10)
	
===================================================================	
test_colorbar_ticks: 172	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange((- 3.0), 4.001)
	
===================================================================	
test_colorbar_ticks: 173	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
x = numpy.arange((- 3.0), 4.001)
tempResult = arange((- 4.0), 3.001)
	
===================================================================	
test_colorbar_positioning: 70	
----------------------------	

tempResult = arange(1200)
	
===================================================================	
test_gridspec_make_colorbar: 103	
----------------------------	

matplotlib.pyplot.figure()
tempResult = arange(1200)
	
===================================================================	
test_colorbar_single_scatter: 116	
----------------------------	

matplotlib.pyplot.figure()
tempResult = arange(4)
	
===================================================================	
test_colorbar_single_scatter: 118	
----------------------------	

matplotlib.pyplot.figure()
x = numpy.arange(4)
y = x.copy()
tempResult = arange(50, 54)
	
===================================================================	
test_Normalize: 135	
----------------------------	

norm = matplotlib.colors.Normalize()
tempResult = arange((- 10), 10, 1, dtype=numpy.float)
	
===================================================================	
test_contour_manual_labels: 116	
----------------------------	

tempResult = arange(0, 10)
	
===================================================================	
test_contour_manual_labels: 116	
----------------------------	

tempResult = arange(0, 10)
	
===================================================================	
test_contour_datetime_axis: 156	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.timedelta(days=d)) for d in range(20)])
tempResult = arange(20)
	
===================================================================	
test_contour_datetime_axis: 157	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.timedelta(days=d)) for d in range(20)])
y = numpy.arange(20)
tempResult = arange(20)
	
===================================================================	
test_contour_datetime_axis: 157	
----------------------------	

fig = matplotlib.pyplot.figure()
fig.subplots_adjust(hspace=0.4, top=0.98, bottom=0.15)
base = datetime.datetime(2013, 1, 1)
x = numpy.array([(base + datetime.timedelta(days=d)) for d in range(20)])
y = numpy.arange(20)
tempResult = arange(20)
	
===================================================================	
test_contour_shape_mismatch_2: 47	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_contour_shape_mismatch_2: 48	
----------------------------	

x = numpy.arange(10)
tempResult = arange(10)
	
===================================================================	
test_contour_shape_mismatch_1: 35	
----------------------------	

tempResult = arange(9)
	
===================================================================	
test_contour_shape_mismatch_1: 36	
----------------------------	

x = numpy.arange(9)
tempResult = arange(9)
	
===================================================================	
test_contour_shape_1d_valid: 16	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_contour_shape_1d_valid: 17	
----------------------------	

x = numpy.arange(10)
tempResult = arange(9)
	
===================================================================	
test_given_colors_levels_and_extends: 135	
----------------------------	

(_, axes) = matplotlib.pyplot.subplots(2, 4)
tempResult = arange(12)
	
===================================================================	
test_contour_shape_mismatch_3: 59	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_contour_shape_mismatch_3: 60	
----------------------------	

x = numpy.arange(10)
tempResult = arange(10)
	
===================================================================	
test_contourf_symmetric_locator: 218	
----------------------------	

tempResult = arange(12)
	
===================================================================	
test_contour_manual_labels1: 125	
----------------------------	

tempResult = arange(0, 10)
	
===================================================================	
test_contour_manual_labels1: 125	
----------------------------	

tempResult = arange(0, 10)
	
===================================================================	
test_contour_shape_2d_valid: 25	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_contour_shape_2d_valid: 26	
----------------------------	

x = numpy.arange(10)
tempResult = arange(9)
	
===================================================================	
test_labels: 177	
----------------------------	

delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
test_labels: 178	
----------------------------	

delta = 0.025
x = numpy.arange((- 3.0), 3.0, delta)
tempResult = arange((- 2.0), 2.0, delta)
	
===================================================================	
test_fillcycle_basic: 76	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.set_prop_cycle(((cycler('c', ['r', 'g', 'y']) + cycler('hatch', ['xx', 'O', '|-'])) + cycler('linestyle', ['-', '--', ':'])))
tempResult = arange(10)
	
===================================================================	
test_fillcycle_ignore: 92	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.set_prop_cycle(((cycler('color', ['r', 'g', 'y']) + cycler('hatch', ['xx', 'O', '|-'])) + cycler('marker', ['.', '*', 'D'])))
tempResult = arange(10)
	
===================================================================	
test_marker_cycle: 31	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.set_prop_cycle((cycler('c', ['r', 'g', 'y']) + cycler('marker', ['.', '*', 'x'])))
tempResult = arange(10)
	
===================================================================	
test_marker_cycle: 44	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.set_prop_cycle((cycler('c', ['r', 'g', 'y']) + cycler('marker', ['.', '*', 'x'])))
xs = numpy.arange(10)
ys = ((0.25 * xs) + 2)
ax.plot(xs, ys, label='red dot', lw=4, ms=16)
ys = ((0.45 * xs) + 3)
ax.plot(xs, ys, label='green star', lw=4, ms=16)
ys = ((0.65 * xs) + 4)
ax.plot(xs, ys, label='yellow x', lw=4, ms=16)
ys = ((0.85 * xs) + 5)
ax.plot(xs, ys, label='red2 dot', lw=4, ms=16)
ax.legend(loc='upper left')
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.set_prop_cycle(c=numpy.array(['r', 'g', 'y']), marker=iter(['.', '*', 'x']))
tempResult = arange(10)
	
===================================================================	
test_property_collision_fill: 115	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(10)
	
===================================================================	
test_colorcycle_basic: 15	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.set_prop_cycle(cycler('color', ['r', 'g', 'y']))
tempResult = arange(10)
	
===================================================================	
test_property_collision_plot: 108	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_prop_cycle('linewidth', [2, 4])
for c in range(1, 4):
    tempResult = arange(10)
	
===================================================================	
test_property_collision_plot: 108	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_prop_cycle('linewidth', [2, 4])
for c in range(1, 4):
    tempResult = arange(10)
	
===================================================================	
test_property_collision_plot: 109	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_prop_cycle('linewidth', [2, 4])
for c in range(1, 4):
    ax.plot(numpy.arange(10), (c * numpy.arange(10)), lw=0.1, color='k')
tempResult = arange(10)
	
===================================================================	
test_property_collision_plot: 109	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_prop_cycle('linewidth', [2, 4])
for c in range(1, 4):
    ax.plot(numpy.arange(10), (c * numpy.arange(10)), lw=0.1, color='k')
tempResult = arange(10)
	
===================================================================	
test_property_collision_plot: 110	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_prop_cycle('linewidth', [2, 4])
for c in range(1, 4):
    ax.plot(numpy.arange(10), (c * numpy.arange(10)), lw=0.1, color='k')
ax.plot(numpy.arange(10), (4 * numpy.arange(10)), color='k')
tempResult = arange(10)
	
===================================================================	
test_property_collision_plot: 110	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_prop_cycle('linewidth', [2, 4])
for c in range(1, 4):
    ax.plot(numpy.arange(10), (c * numpy.arange(10)), lw=0.1, color='k')
ax.plot(numpy.arange(10), (4 * numpy.arange(10)), color='k')
tempResult = arange(10)
	
===================================================================	
test_linestylecycle_basic: 60	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.set_prop_cycle(cycler('ls', ['-', '--', ':']))
tempResult = arange(10)
	
===================================================================	
test_cursor_data: 138	
----------------------------	

from matplotlib.backend_bases import MouseEvent
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(100)
	
===================================================================	
test_cursor_data: 150	
----------------------------	

from matplotlib.backend_bases import MouseEvent
(fig, ax) = matplotlib.pyplot.subplots()
im = ax.imshow(np.arange(100).reshape(10, 10), origin='upper')
(x, y) = (4, 4)
(xdisp, ydisp) = ax.transData.transform_point([x, y])
event = MouseEvent('motion_notify_event', fig.canvas, xdisp, ydisp)
z = im.get_cursor_data(event)
assert (z == 44), ('Did not get 44, got %d' % z)
(x, y) = (10.1, 4)
(xdisp, ydisp) = ax.transData.transform_point([x, y])
event = MouseEvent('motion_notify_event', fig.canvas, xdisp, ydisp)
z = im.get_cursor_data(event)
assert (z is None), ('Did not get None, got %d' % z)
ax.clear()
tempResult = arange(100)
	
===================================================================	
test_cursor_data: 157	
----------------------------	

from matplotlib.backend_bases import MouseEvent
(fig, ax) = matplotlib.pyplot.subplots()
im = ax.imshow(np.arange(100).reshape(10, 10), origin='upper')
(x, y) = (4, 4)
(xdisp, ydisp) = ax.transData.transform_point([x, y])
event = MouseEvent('motion_notify_event', fig.canvas, xdisp, ydisp)
z = im.get_cursor_data(event)
assert (z == 44), ('Did not get 44, got %d' % z)
(x, y) = (10.1, 4)
(xdisp, ydisp) = ax.transData.transform_point([x, y])
event = MouseEvent('motion_notify_event', fig.canvas, xdisp, ydisp)
z = im.get_cursor_data(event)
assert (z is None), ('Did not get None, got %d' % z)
ax.clear()
im = ax.imshow(np.arange(100).reshape(10, 10), origin='lower')
(x, y) = (4, 4)
(xdisp, ydisp) = ax.transData.transform_point([x, y])
event = MouseEvent('motion_notify_event', fig.canvas, xdisp, ydisp)
z = im.get_cursor_data(event)
assert (z == 44), ('Did not get 44, got %d' % z)
(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(100)
	
===================================================================	
test_image_composite_alpha: 258	
----------------------------	

'\n    Tests that the alpha value is recognized and correctly applied in the\n    process of compositing images together.\n    '
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
arr = numpy.zeros((11, 21, 4))
arr[:, :, 0] = 1
tempResult = arange(0, 1.1, 0.1)
	
===================================================================	
test_image_composite_alpha: 258	
----------------------------	

'\n    Tests that the alpha value is recognized and correctly applied in the\n    process of compositing images together.\n    '
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
arr = numpy.zeros((11, 21, 4))
arr[:, :, 0] = 1
tempResult = arange(0, 1, 0.1)
	
===================================================================	
test_image_composite_alpha: 262	
----------------------------	

'\n    Tests that the alpha value is recognized and correctly applied in the\n    process of compositing images together.\n    '
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
arr = numpy.zeros((11, 21, 4))
arr[:, :, 0] = 1
arr[:, :, 3] = numpy.concatenate((numpy.arange(0, 1.1, 0.1), numpy.arange(0, 1, 0.1)[::(- 1)]))
arr2 = numpy.zeros((21, 11, 4))
arr2[:, :, 0] = 1
arr2[:, :, 1] = 1
tempResult = arange(0, 1.1, 0.1)
	
===================================================================	
test_image_composite_alpha: 262	
----------------------------	

'\n    Tests that the alpha value is recognized and correctly applied in the\n    process of compositing images together.\n    '
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
arr = numpy.zeros((11, 21, 4))
arr[:, :, 0] = 1
arr[:, :, 3] = numpy.concatenate((numpy.arange(0, 1.1, 0.1), numpy.arange(0, 1, 0.1)[::(- 1)]))
arr2 = numpy.zeros((21, 11, 4))
arr2[:, :, 0] = 1
arr2[:, :, 1] = 1
tempResult = arange(0, 1, 0.1)
	
===================================================================	
test_no_interpolation_origin: 207	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(211)
tempResult = arange(100)
	
===================================================================	
test_no_interpolation_origin: 209	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(211)
ax.imshow(np.arange(100).reshape((2, 50)), origin='lower', interpolation='none')
ax = fig.add_subplot(212)
tempResult = arange(100)
	
===================================================================	
test_figureimage_setdata: 384	
----------------------------	

fig = matplotlib.pyplot.gcf()
im = FigureImage(fig)
tempResult = arange(12, dtype=numpy.float64)
	
===================================================================	
test_bbox_image_inverted: 293	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_pcolorimage_setdata: 393	
----------------------------	

ax = matplotlib.pyplot.gca()
im = PcolorImage(ax)
tempResult = arange(3, dtype=numpy.float64)
	
===================================================================	
test_pcolorimage_setdata: 394	
----------------------------	

ax = matplotlib.pyplot.gca()
im = PcolorImage(ax)
x = numpy.arange(3, dtype=numpy.float64)
tempResult = arange(4, dtype=numpy.float64)
	
===================================================================	
test_pcolorimage_setdata: 395	
----------------------------	

ax = matplotlib.pyplot.gca()
im = PcolorImage(ax)
x = numpy.arange(3, dtype=numpy.float64)
y = numpy.arange(4, dtype=numpy.float64)
tempResult = arange(6, dtype=numpy.float64)
	
===================================================================	
test_image_edges: 226	
----------------------------	

f = matplotlib.pyplot.figure(figsize=[1, 1])
ax = f.add_axes([0, 0, 1, 1], frameon=False)
tempResult = arange(12)
	
===================================================================	
test_rotate_image: 438	
----------------------------	

delta = 0.25
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
test_zoom_and_clip_upper_origin: 323	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_axesimage_setdata: 375	
----------------------------	

ax = matplotlib.pyplot.gca()
im = AxesImage(ax)
tempResult = arange(12, dtype=numpy.float64)
	
===================================================================	
test_imshow: 197	
----------------------------	

import numpy as np
import matplotlib.pyplot as plt
fig = matplotlib.pyplot.figure()
tempResult = arange(100)
	
===================================================================	
test_mask_image_over_under: 480	
----------------------------	

delta = 0.025
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
test_imshow_endianess: 511	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_image_interps: 35	
----------------------------	

'make the basic nearest, bilinear and bicubic interps'
tempResult = arange(100)
	
===================================================================	
test_nonuniformimage_setdata: 364	
----------------------------	

ax = matplotlib.pyplot.gca()
im = NonUniformImage(ax)
tempResult = arange(3, dtype=numpy.float64)
	
===================================================================	
test_nonuniformimage_setdata: 365	
----------------------------	

ax = matplotlib.pyplot.gca()
im = NonUniformImage(ax)
x = numpy.arange(3, dtype=numpy.float64)
tempResult = arange(4, dtype=numpy.float64)
	
===================================================================	
test_nonuniformimage_setdata: 366	
----------------------------	

ax = matplotlib.pyplot.gca()
im = NonUniformImage(ax)
x = numpy.arange(3, dtype=numpy.float64)
y = numpy.arange(4, dtype=numpy.float64)
tempResult = arange(12, dtype=numpy.float64)
	
===================================================================	
test_figimage: 68	
----------------------------	

'test the figimage method'
for suppressComposite in (False, True):
    fig = matplotlib.pyplot.figure(figsize=(2, 2), dpi=100)
    fig.suppressComposite = suppressComposite
    tempResult = arange(100.0)
	
===================================================================	
test_figimage: 68	
----------------------------	

'test the figimage method'
for suppressComposite in (False, True):
    fig = matplotlib.pyplot.figure(figsize=(2, 2), dpi=100)
    fig.suppressComposite = suppressComposite
    tempResult = arange(100.0)
	
===================================================================	
test_image_composite_background: 245	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(12)
	
===================================================================	
test_legend_auto1: 22	
----------------------------	

'Test automatic legend placement'
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(100)
	
===================================================================	
test_legend_expand: 116	
----------------------------	

'Test expand mode'
legend_modes = [None, 'expand']
(fig, axes_list) = matplotlib.pyplot.subplots(len(legend_modes), 1)
tempResult = arange(100)
	
===================================================================	
test_labels_first: 62	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(10)
	
===================================================================	
test_labels_first: 64	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
ax.plot(numpy.arange(10), '-o', label=1)
ax.plot((numpy.ones(10) * 5), ':x', label='x')
tempResult = arange(20, 10, (- 1))
	
===================================================================	
test_legend_auto2: 32	
----------------------------	

'Test automatic legend placement'
fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111)
tempResult = arange(100)
	
===================================================================	
gaussian_kde_tests.test_kde_integer_input: 1968	
----------------------------	

'Regression test for #1181.'
tempResult = arange(5)
	
===================================================================	
stride_testcase.test_stride_windows_n_lt_1_ValueError: 124	
----------------------------	

tempResult = arange(10)
	
===================================================================	
stride_testcase.test_stride_windows_n1_noverlap0_axis1: 185	
----------------------------	

tempResult = arange(10)
	
===================================================================	
window_testcase.check_window_apply_repeat: 353	
----------------------------	

'This is an adaptation of the original window application\n        algorithm.  This is here to test to make sure the new implementation\n        has the same result'
step = (NFFT - noverlap)
tempResult = arange(0, ((len(x) - NFFT) + 1), step)
	
===================================================================	
stride_testcase.test_stride_windows_n32_noverlap0_axis1_unflatten: 258	
----------------------------	

n = 32
tempResult = arange(n)
	
===================================================================	
stride_testcase.test_stride_windows_noverlap_eq_n_ValueError: 116	
----------------------------	

tempResult = arange(10)
	
===================================================================	
stride_testcase.test_stride_repeat_n5_axis1: 167	
----------------------------	

tempResult = arange(10)
	
===================================================================	
stride_testcase.test_stride_windows_n15_noverlap2_axis0: 212	
----------------------------	

tempResult = arange(100)
	
===================================================================	
stride_testcase.test_stride_windows_n5_noverlap0_axis0: 194	
----------------------------	

tempResult = arange(100)
	
===================================================================	
window_testcase.setUp: 345	
----------------------------	

numpy.random.seed(0)
self.n = 1000
tempResult = arange(0.0, self.n)
	
===================================================================	
stride_testcase.test_stride_windows_n5_noverlap0_axis1: 203	
----------------------------	

tempResult = arange(100)
	
===================================================================	
window_testcase.test_apply_window_hanning_2D_stack_windows_axis1_unflatten: 563	
----------------------------	

n = 32
tempResult = arange(n)
	
===================================================================	
stride_testcase.test_stride_windows_n13_noverlapn3_axis0: 230	
----------------------------	

tempResult = arange(100)
	
===================================================================	
stride_testcase.test_stride_repeat_n_lt_1_ValueError: 140	
----------------------------	

tempResult = arange(10)
	
===================================================================	
stride_testcase.test_stride_windows_n15_noverlap2_axis1: 221	
----------------------------	

tempResult = arange(100)
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_diff_dim: 2072	
----------------------------	

"Test the evaluate method when the dim's of dataset and points are\n        different dimensions"
tempResult = arange(3, 10, 2)
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_diff_dim: 2074	
----------------------------	

"Test the evaluate method when the dim's of dataset and points are\n        different dimensions"
x1 = numpy.arange(3, 10, 2)
kde = matplotlib.mlab.GaussianKDE(x1)
tempResult = arange(3, 12, 2)
	
===================================================================	
stride_testcase.test_stride_windows_2D_ValueError: 104	
----------------------------	

tempResult = arange(10)
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_dim_and_num: 2090	
----------------------------	

' Tests if evaluated against a one by one array'
tempResult = arange(3, 10, 2)
	
===================================================================	
spectral_testcase_nosig_real_onesided.test_psd_detrend_linear_str_trend: 1343	
----------------------------	

if (self.NFFT_density is None):
    return
freqs = self.freqs_density
tempResult = arange(self.NFFT_density)
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_equal_dim_and_num_lt: 2106	
----------------------------	

'Test when line 3810 fails'
tempResult = arange(3, 10, 2)
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_equal_dim_and_num_lt: 2107	
----------------------------	

'Test when line 3810 fails'
x1 = numpy.arange(3, 10, 2)
tempResult = arange(3, 8, 2)
	
===================================================================	
spectral_testcase_nosig_real_onesided.test_psd_window_hanning: 1363	
----------------------------	

if (self.NFFT_density is None):
    return
freqs = self.freqs_density
tempResult = arange(self.NFFT_density)
	
===================================================================	
stride_testcase.test_stride_repeat_n1_axis0: 144	
----------------------------	

tempResult = arange(10)
	
===================================================================	
spectral_testcase_nosig_real_onesided.test_psd_detrend_linear_func_trend: 1323	
----------------------------	

if (self.NFFT_density is None):
    return
freqs = self.freqs_density
tempResult = arange(self.NFFT_density)
	
===================================================================	
stride_testcase.test_stride_windows_n13_noverlapn3_axis1: 239	
----------------------------	

tempResult = arange(100)
	
===================================================================	
spectral_testcase_nosig_real_onesided.createStim: 1089	
----------------------------	

Fs = 100.0
tempResult = arange(0, 10, (1 / Fs))
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_point_dim_not_one: 2099	
----------------------------	

'Test'
tempResult = arange(3, 10, 2)
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_point_dim_not_one: 2100	
----------------------------	

'Test'
x1 = numpy.arange(3, 10, 2)
tempResult = arange(3, 10, 2)
	
===================================================================	
gaussian_kde_evaluate_tests.test_evaluate_point_dim_not_one: 2100	
----------------------------	

'Test'
x1 = numpy.arange(3, 10, 2)
tempResult = arange(3, 10, 2)
	
===================================================================	
stride_testcase.test_stride_windows_noverlap_gt_n_ValueError: 112	
----------------------------	

tempResult = arange(10)
	
===================================================================	
spectral_testcase_nosig_real_onesided.test_psd_window_hanning_detrend_linear: 1388	
----------------------------	

if (self.NFFT_density is None):
    return
freqs = self.freqs_density
tempResult = arange(self.NFFT_density)
	
===================================================================	
window_testcase.test_apply_window_hanning_2D_stack_axis1: 536	
----------------------------	

tempResult = arange(32)
	
===================================================================	
stride_testcase.test_stride_repeat_n5_axis0: 158	
----------------------------	

tempResult = arange(10)
	
===================================================================	
window_testcase.test_apply_window_hanning_2D_stack_windows_axis1: 549	
----------------------------	

tempResult = arange(32)
	
===================================================================	
stride_testcase.test_stride_windows_n32_noverlap0_axis0_unflatten: 249	
----------------------------	

n = 32
tempResult = arange(n)
	
===================================================================	
stride_testcase.test_stride_windows_n1_noverlap0_axis0: 176	
----------------------------	

tempResult = arange(10)
	
===================================================================	
stride_testcase.test_stride_repeat_n1_axis1: 151	
----------------------------	

tempResult = arange(10)
	
===================================================================	
stride_testcase.test_stride_windows_n_gt_lenx_ValueError: 120	
----------------------------	

tempResult = arange(10)
	
===================================================================	
stride_testcase.calc_window_target: 96	
----------------------------	

'This is an adaptation of the original window extraction\n        algorithm.  This is here to test to make sure the new implementation\n        has the same result'
step = (NFFT - noverlap)
tempResult = arange(0, ((len(x) - NFFT) + 1), step)
	
===================================================================	
stride_testcase.test_stride_repeat_2D_ValueError: 128	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_marker_paths_pdf: 78	
----------------------------	

N = 7
tempResult = arange(N)
	
===================================================================	
test_log_transform_with_zero: 56	
----------------------------	

tempResult = arange((- 10), 10)
	
===================================================================	
test_patheffect2: 28	
----------------------------	

ax2 = matplotlib.pyplot.subplot(111)
tempResult = arange(25)
	
===================================================================	
test_image: 136	
----------------------------	

from matplotlib.backends.backend_agg import new_figure_manager
manager = new_figure_manager(1000)
fig = manager.canvas.figure
ax = fig.add_subplot(1, 1, 1)
tempResult = arange(12)
	
===================================================================	
test_complete: 78	
----------------------------	

fig = matplotlib.pyplot.figure('Figure with a label?', figsize=(10, 6))
matplotlib.pyplot.suptitle('Can you fit any more in a figure?')
tempResult = arange(8)
	
===================================================================	
test_complete: 78	
----------------------------	

fig = matplotlib.pyplot.figure('Figure with a label?', figsize=(10, 6))
matplotlib.pyplot.suptitle('Can you fit any more in a figure?')
tempResult = arange(10)
	
===================================================================	
draw_quiver: 12	
----------------------------	

tempResult = arange(0, (2 * numpy.pi), 1)
	
===================================================================	
draw_quiver: 12	
----------------------------	

tempResult = arange(0, (2 * numpy.pi), 1)
	
===================================================================	
test_bad_masked_sizes: 108	
----------------------------	

'Test error handling when given differing sized masked arrays'
tempResult = arange(3)
	
===================================================================	
test_bad_masked_sizes: 109	
----------------------------	

'Test error handling when given differing sized masked arrays'
x = numpy.arange(3)
tempResult = arange(3)
	
===================================================================	
test_zero_headlength: 50	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(10)
	
===================================================================	
test_zero_headlength: 50	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(10)
	
===================================================================	
test_no_warnings: 39	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(15)
	
===================================================================	
test_no_warnings: 39	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
tempResult = arange(10)
	
===================================================================	
test_validators: 144	
----------------------------	

tempResult = arange(15)
	
===================================================================	
test_validators: 144	
----------------------------	

tempResult = arange(15)
	
===================================================================	
test_log_scatter: 26	
----------------------------	

'Issue #1799'
(fig, ax) = matplotlib.pyplot.subplots(1)
tempResult = arange(10)
	
===================================================================	
test_log_scatter: 27	
----------------------------	

'Issue #1799'
(fig, ax) = matplotlib.pyplot.subplots(1)
x = numpy.arange(10)
tempResult = arange(10)
	
===================================================================	
test_overflow: 26	
----------------------------	

x = numpy.array([1.0, 2.0, 3.0, 200000.0])
tempResult = arange(len(x))
	
===================================================================	
test_fft_peaks: 88	
----------------------------	

fig = matplotlib.pyplot.figure()
tempResult = arange(65536)
	
===================================================================	
test_clipping: 16	
----------------------------	

tempResult = arange(0.0, 2.0, 0.01)
	
===================================================================	
test_throw_rendering_complexity_exceeded: 117	
----------------------------	

matplotlib.pyplot.rcParams['path.simplify'] = False
tempResult = arange(200000)
	
===================================================================	
test_label_without_ticks: 50	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(1, 1, 1)
matplotlib.pyplot.subplots_adjust(left=0.3, bottom=0.3)
tempResult = arange(10)
	
===================================================================	
test_subplots_offsettext: 60	
----------------------------	

tempResult = arange(0, 10000000000.0, 1000000000.0)
	
===================================================================	
test_subplots_offsettext: 61	
----------------------------	

x = numpy.arange(0, 10000000000.0, 1000000000.0)
tempResult = arange(0, 100, 10)
	
===================================================================	
test_LogFormatter_sublabel: 148	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_xscale('log')
ax.xaxis.set_major_locator(matplotlib.ticker.LogLocator(base=10, subs=[]))
tempResult = arange(2, 10)
	
===================================================================	
test_LogFormatter_sublabel: 164	
----------------------------	

(fig, ax) = matplotlib.pyplot.subplots()
ax.set_xscale('log')
ax.xaxis.set_major_locator(matplotlib.ticker.LogLocator(base=10, subs=[]))
ax.xaxis.set_minor_locator(matplotlib.ticker.LogLocator(base=10, subs=numpy.arange(2, 10)))
ax.xaxis.set_major_formatter(matplotlib.ticker.LogFormatter(labelOnlyBase=True))
ax.xaxis.set_minor_formatter(matplotlib.ticker.LogFormatter(labelOnlyBase=False))
ax.set_xlim(1, 10000.0)
fmt = ax.xaxis.get_major_formatter()
fmt.set_locs(ax.xaxis.get_majorticklocs())
show_major_labels = [(fmt(x) != '') for x in ax.xaxis.get_majorticklocs()]
assert numpy.all(show_major_labels)
_sub_labels(ax.xaxis, subs=[])
ax.set_xlim(1, 800)
_sub_labels(ax.xaxis, subs=[])
ax.set_xlim(1, 80)
_sub_labels(ax.xaxis, subs=[])
ax.set_xlim(1, 8)
_sub_labels(ax.xaxis, subs=[2, 3, 4, 6])
ax.set_xlim(0.5, 0.9)
tempResult = arange(2, 10, dtype=int)
	
===================================================================	
test_LogFormatterExponent: 182	
----------------------------	


class FakeAxis(object):
    'Allow Formatter to be called without having a "full" plot set up.'

    def __init__(self, vmin=1, vmax=10):
        self.vmin = vmin
        self.vmax = vmax

    def get_view_interval(self):
        return (self.vmin, self.vmax)
tempResult = arange((- 3), 4, dtype=float)
	
===================================================================	
test_tight_layout5: 69	
----------------------------	

'Test tight_layout for image'
fig = matplotlib.pyplot.figure()
ax = matplotlib.pyplot.subplot(111)
tempResult = arange(100)
	
===================================================================	
test_pre_transform_plotting: 72	
----------------------------	

ax = matplotlib.pyplot.axes()
times10 = mtrans.Affine2D().scale(10)
tempResult = arange(48)
	
===================================================================	
test_pre_transform_plotting: 73	
----------------------------	

ax = matplotlib.pyplot.axes()
times10 = mtrans.Affine2D().scale(10)
ax.contourf(np.arange(48).reshape(6, 8), transform=(times10 + ax.transData))
tempResult = arange(48)
	
===================================================================	
test_delaunay_duplicate_points: 47	
----------------------------	

npoints = 10
duplicate = 7
duplicate_of = 3
numpy.random.seed(23)
x = numpy.random.random(npoints)
y = numpy.random.random(npoints)
x[duplicate] = x[duplicate_of]
y[duplicate] = y[duplicate_of]
triang = matplotlib.tri.Triangulation(x, y)
tempResult = arange(npoints)
	
===================================================================	
test_triinterp: 172	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_triinterp: 172	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_trifinder: 110	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_trifinder: 110	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_delaunay: 35	
----------------------------	

nx = 5
ny = 4
(x, y) = numpy.meshgrid(numpy.linspace(0.0, 1.0, nx), numpy.linspace(0.0, 1.0, ny))
x = x.ravel()
y = y.ravel()
npoints = (nx * ny)
ntriangles = ((2 * (nx - 1)) * (ny - 1))
nedges = (((((3 * nx) * ny) - (2 * nx)) - (2 * ny)) + 1)
triang = matplotlib.tri.Triangulation(x, y)
assert_array_almost_equal(triang.x, x)
assert_array_almost_equal(triang.y, y)
assert_equal(len(triang.triangles), ntriangles)
assert_equal(numpy.min(triang.triangles), 0)
assert_equal(numpy.max(triang.triangles), (npoints - 1))
assert_equal(len(triang.edges), nedges)
assert_equal(numpy.min(triang.edges), 0)
assert_equal(numpy.max(triang.edges), (npoints - 1))
neighbors = triang.neighbors
triang._neighbors = None
assert_array_equal(triang.neighbors, neighbors)
tempResult = arange(npoints)
	
===================================================================	
test_tri_smooth_contouring: 490	
----------------------------	

n_angles = 20
n_radii = 10
min_radius = 0.15

def z(x, y):
    r1 = numpy.sqrt((((0.5 - x) ** 2) + ((0.5 - y) ** 2)))
    theta1 = numpy.arctan2((0.5 - x), (0.5 - y))
    r2 = numpy.sqrt(((((- x) - 0.2) ** 2) + (((- y) - 0.2) ** 2)))
    theta2 = numpy.arctan2(((- x) - 0.2), ((- y) - 0.2))
    z = (- (((((2 * (numpy.exp(((r1 / 10) ** 2)) - 1)) * 30.0) * numpy.cos((7.0 * theta1))) + (((numpy.exp(((r2 / 10) ** 2)) - 1) * 30.0) * numpy.cos((11.0 * theta2)))) + (0.7 * ((x ** 2) + (y ** 2)))))
    return ((numpy.max(z) - z) / (numpy.max(z) - numpy.min(z)))
radii = numpy.linspace(min_radius, 0.95, n_radii)
angles = numpy.linspace((0 + n_angles), ((2 * numpy.pi) + n_angles), n_angles, endpoint=False)
angles = numpy.repeat(angles[(..., numpy.newaxis)], n_radii, axis=1)
angles[:, 1::2] += (numpy.pi / n_angles)
x0 = (radii * np.cos(angles)).flatten()
y0 = (radii * np.sin(angles)).flatten()
triang0 = matplotlib.tri.Triangulation(x0, y0)
z0 = z(x0, y0)
xmid = x0[triang0.triangles].mean(axis=1)
ymid = y0[triang0.triangles].mean(axis=1)
mask = numpy.where((((xmid * xmid) + (ymid * ymid)) < (min_radius * min_radius)), 1, 0)
triang0.set_mask(mask)
refiner = matplotlib.tri.UniformTriRefiner(triang0)
(tri_refi, z_test_refi) = refiner.refine_field(z0, subdiv=4)
tempResult = arange(0.0, 1.0, 0.025)
	
===================================================================	
poisson_sparse_matrix: 305	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
tempResult = arange(l, dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 305	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
tempResult = arange((l - 1), dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 305	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
tempResult = arange(1, l, dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 305	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
tempResult = arange((l - n), dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 305	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
tempResult = arange(n, l, dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 306	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
rows = numpy.concatenate([numpy.arange(l, dtype=numpy.int32), numpy.arange((l - 1), dtype=numpy.int32), numpy.arange(1, l, dtype=numpy.int32), numpy.arange((l - n), dtype=numpy.int32), numpy.arange(n, l, dtype=numpy.int32)])
tempResult = arange(l, dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 306	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
rows = numpy.concatenate([numpy.arange(l, dtype=numpy.int32), numpy.arange((l - 1), dtype=numpy.int32), numpy.arange(1, l, dtype=numpy.int32), numpy.arange((l - n), dtype=numpy.int32), numpy.arange(n, l, dtype=numpy.int32)])
tempResult = arange(1, l, dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 306	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
rows = numpy.concatenate([numpy.arange(l, dtype=numpy.int32), numpy.arange((l - 1), dtype=numpy.int32), numpy.arange(1, l, dtype=numpy.int32), numpy.arange((l - n), dtype=numpy.int32), numpy.arange(n, l, dtype=numpy.int32)])
tempResult = arange((l - 1), dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 306	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
rows = numpy.concatenate([numpy.arange(l, dtype=numpy.int32), numpy.arange((l - 1), dtype=numpy.int32), numpy.arange(1, l, dtype=numpy.int32), numpy.arange((l - n), dtype=numpy.int32), numpy.arange(n, l, dtype=numpy.int32)])
tempResult = arange(n, l, dtype=numpy.int32)
	
===================================================================	
poisson_sparse_matrix: 306	
----------------------------	

'\n        Sparse Poisson matrix.\n\n        Returns the sparse matrix in coo format resulting from the\n        discretisation of the 2-dimensional Poisson equation according to a\n        finite difference numerical scheme on a uniform (n, m) grid.\n        Size of the matrix: (n*m, n*m)\n        '
l = (m * n)
rows = numpy.concatenate([numpy.arange(l, dtype=numpy.int32), numpy.arange((l - 1), dtype=numpy.int32), numpy.arange(1, l, dtype=numpy.int32), numpy.arange((l - n), dtype=numpy.int32), numpy.arange(n, l, dtype=numpy.int32)])
tempResult = arange((l - n), dtype=numpy.int32)
	
===================================================================	
Get no callers of function numpy.arange at line 526 col 13.	
===================================================================	
_ReducedHCT_Element.get_Kff_and_Ff: 321	
----------------------------	

"\n        Builds K and F for the following elliptic formulation:\n        minimization of curvature energy with value of function at node\n        imposed and derivatives 'free'.\n        Builds the global Kff matrix in cco format.\n        Builds the full Ff vec Ff = - Kfc x Uc\n\n        Parameters\n        ----------\n        *J* is a (N x 2 x 2) array of jacobian matrices (jacobian matrix at\n        triangle first apex)\n        *ecc* is a (N x 3 x 1) array (array of column-matrices) of triangle\n        eccentricities\n        *triangles* is a (N x 3) array of nodes indexes.\n        *Uc* is (N x 3) array of imposed displacements at nodes\n\n        Returns\n        -------\n        (Kff_rows, Kff_cols, Kff_vals) Kff matrix in coo format - Duplicate\n        (row, col) entries must be summed.\n        Ff: force vector - dim npts * 3\n        "
ntri = numpy.size(ecc, 0)
tempResult = arange(ntri, dtype=numpy.int32)
	
===================================================================	
_roll_vectorized: 640	
----------------------------	

'\n    Rolls an array of matrices along an axis according to an array of indices\n    *roll_indices*\n    *axis* can be either 0 (rolls rows) or 1 (rolls columns).\n    '
assert (axis in [0, 1])
ndim = M.ndim
assert (ndim == 3)
ndim_roll = roll_indices.ndim
assert (ndim_roll == 1)
sh = M.shape
(r, c) = sh[(- 2):]
assert (sh[0] == roll_indices.shape[0])
tempResult = arange(sh[0], dtype=numpy.int32)
	
===================================================================	
UniformTriRefiner._refine_triangulation_once: 77	
----------------------------	

'\n        This function refines a matplotlib.tri *triangulation* by splitting\n        each triangle into 4 child-masked_triangles built on the edges midside\n        nodes.\n        The masked triangles, if present, are also splitted but their children\n        returned masked.\n\n        If *ancestors* is not provided, returns only a new triangulation:\n        child_triangulation.\n\n        If the array-like key table *ancestor* is given, it shall be of shape\n        (ntri,) where ntri is the number of *triangulation* masked_triangles.\n        In this case, the function returns\n        (child_triangulation, child_ancestors)\n        child_ancestors is defined so that the 4 child masked_triangles share\n        the same index as their father: child_ancestors.shape = (4 * ntri,).\n\n        '
x = triangulation.x
y = triangulation.y
neighbors = triangulation.neighbors
triangles = triangulation.triangles
npts = numpy.shape(x)[0]
ntri = numpy.shape(triangles)[0]
if (ancestors is not None):
    ancestors = numpy.asarray(ancestors)
    if (numpy.shape(ancestors) != (ntri,)):
        raise ValueError('Incompatible shapes provide for triangulation.masked_triangles and ancestors: {0} and {1}'.format(numpy.shape(triangles), numpy.shape(ancestors)))
borders = numpy.sum((neighbors == (- 1)))
added_pts = (((3 * ntri) + borders) // 2)
refi_npts = (npts + added_pts)
refi_x = numpy.zeros(refi_npts)
refi_y = numpy.zeros(refi_npts)
refi_x[:npts] = x
refi_y[:npts] = y
tempResult = arange(ntri, dtype=numpy.int32)
	
===================================================================	
UniformTriRefiner._refine_triangulation_once: 77	
----------------------------	

'\n        This function refines a matplotlib.tri *triangulation* by splitting\n        each triangle into 4 child-masked_triangles built on the edges midside\n        nodes.\n        The masked triangles, if present, are also splitted but their children\n        returned masked.\n\n        If *ancestors* is not provided, returns only a new triangulation:\n        child_triangulation.\n\n        If the array-like key table *ancestor* is given, it shall be of shape\n        (ntri,) where ntri is the number of *triangulation* masked_triangles.\n        In this case, the function returns\n        (child_triangulation, child_ancestors)\n        child_ancestors is defined so that the 4 child masked_triangles share\n        the same index as their father: child_ancestors.shape = (4 * ntri,).\n\n        '
x = triangulation.x
y = triangulation.y
neighbors = triangulation.neighbors
triangles = triangulation.triangles
npts = numpy.shape(x)[0]
ntri = numpy.shape(triangles)[0]
if (ancestors is not None):
    ancestors = numpy.asarray(ancestors)
    if (numpy.shape(ancestors) != (ntri,)):
        raise ValueError('Incompatible shapes provide for triangulation.masked_triangles and ancestors: {0} and {1}'.format(numpy.shape(triangles), numpy.shape(ancestors)))
borders = numpy.sum((neighbors == (- 1)))
added_pts = (((3 * ntri) + borders) // 2)
refi_npts = (npts + added_pts)
refi_x = numpy.zeros(refi_npts)
refi_y = numpy.zeros(refi_npts)
refi_x[:npts] = x
refi_y[:npts] = y
tempResult = arange(ntri, dtype=numpy.int32)
	
===================================================================	
UniformTriRefiner._refine_triangulation_once: 77	
----------------------------	

'\n        This function refines a matplotlib.tri *triangulation* by splitting\n        each triangle into 4 child-masked_triangles built on the edges midside\n        nodes.\n        The masked triangles, if present, are also splitted but their children\n        returned masked.\n\n        If *ancestors* is not provided, returns only a new triangulation:\n        child_triangulation.\n\n        If the array-like key table *ancestor* is given, it shall be of shape\n        (ntri,) where ntri is the number of *triangulation* masked_triangles.\n        In this case, the function returns\n        (child_triangulation, child_ancestors)\n        child_ancestors is defined so that the 4 child masked_triangles share\n        the same index as their father: child_ancestors.shape = (4 * ntri,).\n\n        '
x = triangulation.x
y = triangulation.y
neighbors = triangulation.neighbors
triangles = triangulation.triangles
npts = numpy.shape(x)[0]
ntri = numpy.shape(triangles)[0]
if (ancestors is not None):
    ancestors = numpy.asarray(ancestors)
    if (numpy.shape(ancestors) != (ntri,)):
        raise ValueError('Incompatible shapes provide for triangulation.masked_triangles and ancestors: {0} and {1}'.format(numpy.shape(triangles), numpy.shape(ancestors)))
borders = numpy.sum((neighbors == (- 1)))
added_pts = (((3 * ntri) + borders) // 2)
refi_npts = (npts + added_pts)
refi_x = numpy.zeros(refi_npts)
refi_y = numpy.zeros(refi_npts)
refi_x[:npts] = x
refi_y[:npts] = y
tempResult = arange(ntri, dtype=numpy.int32)
	
===================================================================	
UniformTriRefiner._refine_triangulation_once: 94	
----------------------------	

'\n        This function refines a matplotlib.tri *triangulation* by splitting\n        each triangle into 4 child-masked_triangles built on the edges midside\n        nodes.\n        The masked triangles, if present, are also splitted but their children\n        returned masked.\n\n        If *ancestors* is not provided, returns only a new triangulation:\n        child_triangulation.\n\n        If the array-like key table *ancestor* is given, it shall be of shape\n        (ntri,) where ntri is the number of *triangulation* masked_triangles.\n        In this case, the function returns\n        (child_triangulation, child_ancestors)\n        child_ancestors is defined so that the 4 child masked_triangles share\n        the same index as their father: child_ancestors.shape = (4 * ntri,).\n\n        '
x = triangulation.x
y = triangulation.y
neighbors = triangulation.neighbors
triangles = triangulation.triangles
npts = numpy.shape(x)[0]
ntri = numpy.shape(triangles)[0]
if (ancestors is not None):
    ancestors = numpy.asarray(ancestors)
    if (numpy.shape(ancestors) != (ntri,)):
        raise ValueError('Incompatible shapes provide for triangulation.masked_triangles and ancestors: {0} and {1}'.format(numpy.shape(triangles), numpy.shape(ancestors)))
borders = numpy.sum((neighbors == (- 1)))
added_pts = (((3 * ntri) + borders) // 2)
refi_npts = (npts + added_pts)
refi_x = numpy.zeros(refi_npts)
refi_y = numpy.zeros(refi_npts)
refi_x[:npts] = x
refi_y[:npts] = y
edge_elems = numpy.ravel(numpy.vstack([numpy.arange(ntri, dtype=numpy.int32), numpy.arange(ntri, dtype=numpy.int32), numpy.arange(ntri, dtype=numpy.int32)]))
edge_apexes = numpy.ravel(numpy.vstack([numpy.zeros(ntri, dtype=numpy.int32), numpy.ones(ntri, dtype=numpy.int32), (numpy.ones(ntri, dtype=numpy.int32) * 2)]))
edge_neighbors = neighbors[(edge_elems, edge_apexes)]
mask_masters = (edge_elems > edge_neighbors)
masters = edge_elems[mask_masters]
apex_masters = edge_apexes[mask_masters]
x_add = ((x[triangles[(masters, apex_masters)]] + x[triangles[(masters, ((apex_masters + 1) % 3))]]) * 0.5)
y_add = ((y[triangles[(masters, apex_masters)]] + y[triangles[(masters, ((apex_masters + 1) % 3))]]) * 0.5)
refi_x[npts:] = x_add
refi_y[npts:] = y_add
new_pt_corner = triangles
new_pt_midside = numpy.empty([ntri, 3], dtype=numpy.int32)
cum_sum = npts
for imid in range(3):
    mask_st_loc = (imid == apex_masters)
    n_masters_loc = numpy.sum(mask_st_loc)
    elem_masters_loc = masters[mask_st_loc]
    tempResult = arange(n_masters_loc, dtype=numpy.int32)
	
===================================================================	
UniformTriRefiner.refine_triangulation: 27	
----------------------------	

'\n        Computes an uniformly refined triangulation *refi_triangulation* of\n        the encapsulated :attr:`triangulation`.\n\n        This function refines the encapsulated triangulation by splitting each\n        father triangle into 4 child sub-triangles built on the edges midside\n        nodes, recursively (level of recursion *subdiv*).\n        In the end, each triangle is hence divided into ``4**subdiv``\n        child triangles.\n        The default value for *subdiv* is 3 resulting in 64 refined\n        subtriangles for each triangle of the initial triangulation.\n\n        Parameters\n        ----------\n        return_tri_index : boolean, optional\n            Boolean indicating whether an index table indicating the father\n            triangle index of each point will be returned. Default value\n            False.\n        subdiv : integer, optional\n            Recursion level for the subdivision. Defaults value 3.\n            Each triangle will be divided into ``4**subdiv`` child triangles.\n\n        Returns\n        -------\n        refi_triangulation : :class:`~matplotlib.tri.Triangulation`\n            The returned refined triangulation\n        found_index : array-like of integers\n            Index of the initial triangulation containing triangle, for each\n            point of *refi_triangulation*.\n            Returned only if *return_tri_index* is set to True.\n\n        '
refi_triangulation = self._triangulation
ntri = refi_triangulation.triangles.shape[0]
tempResult = arange(ntri, dtype=numpy.int32)
	
===================================================================	
TriAnalyzer.get_flat_tri_mask: 68	
----------------------------	

'\n        Eliminates excessively flat border triangles from the triangulation.\n\n        Returns a mask *new_mask* which allows to clean the encapsulated\n        triangulation from its border-located flat triangles\n        (according to their :meth:`circle_ratios`).\n        This mask is meant to be subsequently applied to the triangulation\n        using :func:`matplotlib.tri.Triangulation.set_mask` .\n        *new_mask* is an extension of the initial triangulation mask\n        in the sense that an initially masked triangle will remain masked.\n\n        The *new_mask* array is computed recursively ; at each step flat\n        triangles are removed only if they share a side with the current\n        mesh border. Thus no new holes in the triangulated domain will be\n        created.\n\n        Parameters\n        ----------\n        min_circle_ratio : float, optional\n            Border triangles with incircle/circumcircle radii ratio r/R will\n            be removed if r/R < *min_circle_ratio*. Default value: 0.01\n        rescale : boolean, optional\n            If True, a rescaling will first be internally performed (based on\n            :attr:`scale_factors` ), so that the (unmasked) triangles fit\n            exactly inside a unit square mesh. This rescaling accounts for the\n            difference of scale which might exist between the 2 axis. Default\n            (and recommended) value is True.\n\n        Returns\n        -------\n        new_mask : array-like of booleans\n            Mask to apply to encapsulated triangulation.\n            All the initially masked triangles remain masked in the\n            *new_mask*.\n\n        Notes\n        -----\n        The rationale behind this function is that a Delaunay\n        triangulation - of an unstructured set of points - sometimes contains\n        almost flat triangles at its border, leading to artifacts in plots\n        (especially for high-resolution contouring).\n        Masked with computed *new_mask*, the encapsulated\n        triangulation would contain no more unmasked border triangles\n        with a circle ratio below *min_circle_ratio*, thus improving the\n        mesh quality for subsequent plots or interpolation.\n\n        Examples\n        --------\n        Please refer to the following illustrating example:\n\n        .. plot:: mpl_examples/pylab_examples/tricontour_smooth_delaunay.py\n\n        '
ntri = self._triangulation.triangles.shape[0]
mask_bad_ratio = (self.circle_ratios(rescale) < min_circle_ratio)
current_mask = self._triangulation.mask
if (current_mask is None):
    current_mask = numpy.zeros(ntri, dtype=numpy.bool)
valid_neighbors = numpy.copy(self._triangulation.neighbors)
tempResult = arange(ntri, dtype=numpy.int32)
	
===================================================================	
TriAnalyzer._total_to_compress_renum: 108	
----------------------------	

'\n        Parameters\n        ----------\n        mask : 1d boolean array or None\n            mask\n        n : integer\n            length of the mask. Useful only id mask can be None\n\n        Returns\n        -------\n        renum : integer array\n            array so that (`valid_array` being a compressed array\n            based on a `masked_array` with mask *mask*) :\n\n                  - For all i such as mask[i] = False:\n                    valid_array[renum[i]] = masked_array[i]\n                  - For all i such as mask[i] = True:\n                    renum[i] = -1 (invalid value)\n\n        '
if (n is None):
    n = numpy.size(mask)
if (mask is not None):
    renum = (- numpy.ones(n, dtype=numpy.int32))
    tempResult = arange(n, dtype=numpy.int32)
	
===================================================================	
TriAnalyzer._total_to_compress_renum: 109	
----------------------------	

'\n        Parameters\n        ----------\n        mask : 1d boolean array or None\n            mask\n        n : integer\n            length of the mask. Useful only id mask can be None\n\n        Returns\n        -------\n        renum : integer array\n            array so that (`valid_array` being a compressed array\n            based on a `masked_array` with mask *mask*) :\n\n                  - For all i such as mask[i] = False:\n                    valid_array[renum[i]] = masked_array[i]\n                  - For all i such as mask[i] = True:\n                    renum[i] = -1 (invalid value)\n\n        '
if (n is None):
    n = numpy.size(mask)
if (mask is not None):
    renum = (- numpy.ones(n, dtype=numpy.int32))
    valid = np.arange(n, dtype=np.int32).compress((~ mask), axis=0)
    tempResult = arange(numpy.size(valid, 0), dtype=numpy.int32)
	
===================================================================	
TriAnalyzer._total_to_compress_renum: 112	
----------------------------	

'\n        Parameters\n        ----------\n        mask : 1d boolean array or None\n            mask\n        n : integer\n            length of the mask. Useful only id mask can be None\n\n        Returns\n        -------\n        renum : integer array\n            array so that (`valid_array` being a compressed array\n            based on a `masked_array` with mask *mask*) :\n\n                  - For all i such as mask[i] = False:\n                    valid_array[renum[i]] = masked_array[i]\n                  - For all i such as mask[i] = True:\n                    renum[i] = -1 (invalid value)\n\n        '
if (n is None):
    n = numpy.size(mask)
if (mask is not None):
    renum = (- numpy.ones(n, dtype=numpy.int32))
    valid = np.arange(n, dtype=np.int32).compress((~ mask), axis=0)
    renum[valid] = numpy.arange(numpy.size(valid, 0), dtype=numpy.int32)
    return renum
else:
    tempResult = arange(n, dtype=numpy.int32)
	
===================================================================	
rec2excel: 63	
----------------------------	

'\n    save record array r to excel xlwt worksheet ws\n    starting at rownum.  if ws is string like, assume it is a\n    filename and save to it\n\n    start writing at rownum, colnum\n\n    formatd is a dictionary mapping dtype name -> mlab.Format instances\n\n    nanstr is the string that mpl will put into excel for np.nan value\n    The next rownum after writing is returned\n    '
autosave = False
if matplotlib.cbook.is_string_like(ws):
    filename = ws
    wb = xlwt.Workbook()
    ws = wb.add_sheet('worksheet')
    autosave = True
if (formatd is None):
    formatd = dict()
formats = []
font = xlwt.Font()
font.bold = True
stylehdr = xlwt.XFStyle()
stylehdr.font = font
for (i, name) in enumerate(r.dtype.names):
    dt = r.dtype[name]
    format = formatd.get(name)
    if (format is None):
        format = matplotlib.mlab.defaultformatd.get(dt.type, matplotlib.mlab.FormatObj())
    format = xlformat_factory(format)
    ws.write(rownum, (colnum + i), name, stylehdr)
    formats.append(format)
rownum += 1
tempResult = arange(len(r.dtype.names))
	
===================================================================	
ColorbarBase._process_values: 313	
----------------------------	

'\n        Set the :attr:`_boundaries` and :attr:`_values` attributes\n        based on the input boundaries and values.  Input boundaries\n        can be *self.boundaries* or the argument *b*.\n        '
if (b is None):
    b = self.boundaries
if (b is not None):
    self._boundaries = numpy.asarray(b, dtype=float)
    if (self.values is None):
        self._values = (0.5 * (self._boundaries[:(- 1)] + self._boundaries[1:]))
        if isinstance(self.norm, matplotlib.colors.NoNorm):
            self._values = (self._values + 1e-05).astype(numpy.int16)
        return
    self._values = numpy.array(self.values)
    return
if (self.values is not None):
    self._values = numpy.array(self.values)
    if (self.boundaries is None):
        b = numpy.zeros((len(self.values) + 1), 'd')
        b[1:(- 1)] = (0.5 * (self._values[:(- 1)] - self._values[1:]))
        b[0] = ((2.0 * b[1]) - b[2])
        b[(- 1)] = ((2.0 * b[(- 2)]) - b[(- 3)])
        self._boundaries = b
        return
    self._boundaries = numpy.array(self.boundaries)
    return
if isinstance(self.norm, matplotlib.colors.NoNorm):
    b = ((self._uniform_y((self.cmap.N + 1)) * self.cmap.N) - 0.5)
    v = numpy.zeros(((len(b) - 1),), dtype=numpy.int16)
    tempResult = arange(self.cmap.N, dtype=numpy.int16)
	
===================================================================	
ParasiteAxesAuxTransBase._pcolor: 93	
----------------------------	

if (len(XYC) == 1):
    C = XYC[0]
    (ny, nx) = C.shape
    tempResult = arange((- 0.5), nx, 1.0)
	
===================================================================	
ParasiteAxesAuxTransBase._pcolor: 94	
----------------------------	

if (len(XYC) == 1):
    C = XYC[0]
    (ny, nx) = C.shape
    gx = numpy.arange((- 0.5), nx, 1.0)
    tempResult = arange((- 0.5), ny, 1.0)
	
===================================================================	
ParasiteAxesAuxTransBase._contour: 121	
----------------------------	

if (len(XYCL) <= 2):
    C = XYCL[0]
    (ny, nx) = C.shape
    tempResult = arange(0.0, nx, 1.0)
	
===================================================================	
ParasiteAxesAuxTransBase._contour: 122	
----------------------------	

if (len(XYCL) <= 2):
    C = XYCL[0]
    (ny, nx) = C.shape
    gx = numpy.arange(0.0, nx, 1.0)
    tempResult = arange(0.0, ny, 1.0)
	
===================================================================	
select_step: 76	
----------------------------	

if (v1 > v2):
    (v1, v2) = (v2, v1)
dv = (float((v2 - v1)) / nv)
if hour:
    _select_step = select_step_hour
    cycle = 24.0
else:
    _select_step = select_step_degree
    cycle = 360.0
if (dv > (1.0 / threshold_factor)):
    (step, factor) = _select_step(dv)
else:
    (step, factor) = select_step_sub((dv * threshold_factor))
    factor = (factor * threshold_factor)
(f1, f2, fstep) = ((v1 * factor), (v2 * factor), (step / factor))
tempResult = arange(math.floor((f1 / step)), (math.ceil((f2 / step)) + 0.5), 1, dtype='i')
	
===================================================================	
select_step: 81	
----------------------------	

if (v1 > v2):
    (v1, v2) = (v2, v1)
dv = (float((v2 - v1)) / nv)
if hour:
    _select_step = select_step_hour
    cycle = 24.0
else:
    _select_step = select_step_degree
    cycle = 360.0
if (dv > (1.0 / threshold_factor)):
    (step, factor) = _select_step(dv)
else:
    (step, factor) = select_step_sub((dv * threshold_factor))
    factor = (factor * threshold_factor)
(f1, f2, fstep) = ((v1 * factor), (v2 * factor), (step / factor))
levs = (numpy.arange(math.floor((f1 / step)), (math.ceil((f2 / step)) + 0.5), 1, dtype='i') * step)
n = len(levs)
if ((factor == 1.0) and (levs[(- 1)] >= (levs[0] + cycle))):
    nv = int((cycle / step))
    if include_last:
        tempResult = arange(0, (nv + 1), 1)
	
===================================================================	
select_step: 83	
----------------------------	

if (v1 > v2):
    (v1, v2) = (v2, v1)
dv = (float((v2 - v1)) / nv)
if hour:
    _select_step = select_step_hour
    cycle = 24.0
else:
    _select_step = select_step_degree
    cycle = 360.0
if (dv > (1.0 / threshold_factor)):
    (step, factor) = _select_step(dv)
else:
    (step, factor) = select_step_sub((dv * threshold_factor))
    factor = (factor * threshold_factor)
(f1, f2, fstep) = ((v1 * factor), (v2 * factor), (step / factor))
levs = (numpy.arange(math.floor((f1 / step)), (math.ceil((f2 / step)) + 0.5), 1, dtype='i') * step)
n = len(levs)
if ((factor == 1.0) and (levs[(- 1)] >= (levs[0] + cycle))):
    nv = int((cycle / step))
    if include_last:
        levs = (levs[0] + (numpy.arange(0, (nv + 1), 1) * step))
    else:
        tempResult = arange(0, nv, 1)
	
===================================================================	
module: 472	
----------------------------	

'\nAxislines includes modified implementation of the Axes class. The\nbiggest difference is that the artists responsible to draw axis line,\nticks, ticklabel and axis labels are separated out from the mpl\'s Axis\nclass, which are much more than artists in the original\nmpl. Originally, this change was motivated to support curvilinear\ngrid. Here are a few reasons that I came up with new axes class.\n\n\n * "top" and "bottom" x-axis (or "left" and "right" y-axis) can have\n   different ticks (tick locations and labels). This is not possible\n   with the current mpl, although some twin axes trick can help.\n\n * Curvilinear grid.\n\n * angled ticks.\n\nIn the new axes class, xaxis and yaxis is set to not visible by\ndefault, and new set of artist (AxisArtist) are defined to draw axis\nline, ticks, ticklabels and axis label. Axes.axis attribute serves as\na dictionary of these artists, i.e., ax.axis["left"] is a AxisArtist\ninstance responsible to draw left y-axis. The default Axes.axis contains\n"bottom", "left", "top" and "right".\n\nAxisArtist can be considered as a container artist and\nhas following children artists which will draw ticks, labels, etc.\n\n * line\n * major_ticks, major_ticklabels\n * minor_ticks, minor_ticklabels\n * offsetText\n * label\n\nNote that these are separate artists from Axis class of the\noriginal mpl, thus most of tick-related command in the original mpl\nwon\'t work, although some effort has made to work with. For example,\ncolor and markerwidth of the ax.axis["bottom"].major_ticks will follow\nthose of Axes.xaxis unless explicitly specified.\n\nIn addition to AxisArtist, the Axes will have *gridlines* attribute,\nwhich obviously draws grid lines. The gridlines needs to be separated\nfrom the axis as some gridlines can never pass any axis.\n\n'
from __future__ import absolute_import, division, print_function, unicode_literals
import six
import matplotlib.axes as maxes
import matplotlib.artist as martist
import matplotlib.text as mtext
import matplotlib.font_manager as font_manager
from matplotlib.path import Path
from matplotlib.transforms import Affine2D, ScaledTranslation, IdentityTransform, TransformedPath, Bbox
from matplotlib.collections import LineCollection
from matplotlib import rcParams
from matplotlib.artist import allow_rasterization
import warnings
import numpy as np
import matplotlib.lines as mlines
from .axisline_style import AxislineStyle
from .axis_artist import AxisArtist, GridlinesCollection

class AxisArtistHelper(object):
    '\n    AxisArtistHelper should define\n    following method with given APIs. Note that the first axes argument\n    will be axes attribute of the caller artist.\n\n\n        # LINE (spinal line?)\n\n        def get_line(self, axes):\n            # path : Path\n            return path\n\n        def get_line_transform(self, axes):\n            # ...\n            # trans : transform\n            return trans\n\n        # LABEL\n\n        def get_label_pos(self, axes):\n            # x, y : position\n            return (x, y), trans\n\n\n        def get_label_offset_transform(self,                 axes,\n                pad_points, fontprops, renderer,\n                bboxes,\n                ):\n            # va : vertical alignment\n            # ha : horizontal alignment\n            # a : angle\n            return trans, va, ha, a\n\n        # TICK\n\n        def get_tick_transform(self, axes):\n            return trans\n\n        def get_tick_iterators(self, axes):\n            # iter : iterable object that yields (c, angle, l) where\n            # c, angle, l is position, tick angle, and label\n\n            return iter_major, iter_minor\n\n\n        '

    class _Base(object):
        '\n        Base class for axis helper.\n        '

        def __init__(self):
            '\n            '
            (self.delta1, self.delta2) = (1e-05, 1e-05)

        def update_lim(self, axes):
            pass

    class Fixed(_Base):
        '\n        Helper class for a fixed (in the axes coordinate) axis.\n        '
        _default_passthru_pt = dict(left=(0, 0), right=(1, 0), bottom=(0, 0), top=(0, 1))

        def __init__(self, loc, nth_coord=None):
            '\n            nth_coord = along which coordinate value varies\n            in 2d, nth_coord = 0 ->  x axis, nth_coord = 1 -> y axis\n            '
            self._loc = loc
            if (loc not in ['left', 'right', 'bottom', 'top']):
                raise ValueError(('%s' % loc))
            if (nth_coord is None):
                if (loc in ['left', 'right']):
                    nth_coord = 1
                elif (loc in ['bottom', 'top']):
                    nth_coord = 0
            self.nth_coord = nth_coord
            super(AxisArtistHelper.Fixed, self).__init__()
            self.passthru_pt = self._default_passthru_pt[loc]
            _verts = numpy.array([[0.0, 0.0], [1.0, 1.0]])
            fixed_coord = (1 - nth_coord)
            _verts[:, fixed_coord] = self.passthru_pt[fixed_coord]
            self._path = Path(_verts)

        def get_nth_coord(self):
            return self.nth_coord

        def get_line(self, axes):
            return self._path

        def get_line_transform(self, axes):
            return axes.transAxes

        def get_axislabel_transform(self, axes):
            return axes.transAxes

        def get_axislabel_pos_angle(self, axes):
            '\n            label reference position in transAxes.\n\n            get_label_transform() returns a transform of (transAxes+offset)\n            '
            loc = self._loc
            (pos, angle_tangent) = dict(left=((0.0, 0.5), 90), right=((1.0, 0.5), 90), bottom=((0.5, 0.0), 0), top=((0.5, 1.0), 0))[loc]
            return (pos, angle_tangent)

        def get_tick_transform(self, axes):
            trans_tick = [axes.get_xaxis_transform(), axes.get_yaxis_transform()][self.nth_coord]
            return trans_tick

    class Floating(_Base):

        def __init__(self, nth_coord, value):
            self.nth_coord = nth_coord
            self._value = value
            super(AxisArtistHelper.Floating, self).__init__()

        def get_nth_coord(self):
            return self.nth_coord

        def get_line(self, axes):
            raise RuntimeError('get_line method should be defined by the derived class')

class AxisArtistHelperRectlinear(object):

    class Fixed(AxisArtistHelper.Fixed):

        def __init__(self, axes, loc, nth_coord=None):
            '\n            nth_coord = along which coordinate value varies\n            in 2d, nth_coord = 0 ->  x axis, nth_coord = 1 -> y axis\n            '
            super(AxisArtistHelperRectlinear.Fixed, self).__init__(loc, nth_coord)
            self.axis = [axes.xaxis, axes.yaxis][self.nth_coord]

        def get_tick_iterators(self, axes):
            'tick_loc, tick_angle, tick_label'
            loc = self._loc
            if (loc in ['bottom', 'top']):
                (angle_normal, angle_tangent) = (90, 0)
            else:
                (angle_normal, angle_tangent) = (0, 90)
            major = self.axis.major
            majorLocs = major.locator()
            major.formatter.set_locs(majorLocs)
            majorLabels = [major.formatter(val, i) for (i, val) in enumerate(majorLocs)]
            minor = self.axis.minor
            minorLocs = minor.locator()
            minor.formatter.set_locs(minorLocs)
            minorLabels = [minor.formatter(val, i) for (i, val) in enumerate(minorLocs)]
            trans_tick = self.get_tick_transform(axes)
            tr2ax = (trans_tick + axes.transAxes.inverted())

            def _f(locs, labels):
                for (x, l) in zip(locs, labels):
                    c = list(self.passthru_pt)
                    c[self.nth_coord] = x
                    c2 = tr2ax.transform_point(c)
                    if ((0.0 - self.delta1) <= c2[self.nth_coord] <= (1.0 + self.delta2)):
                        (yield (c, angle_normal, angle_tangent, l))
            return (_f(majorLocs, majorLabels), _f(minorLocs, minorLabels))

    class Floating(AxisArtistHelper.Floating):

        def __init__(self, axes, nth_coord, passingthrough_point, axis_direction='bottom'):
            super(AxisArtistHelperRectlinear.Floating, self).__init__(nth_coord, passingthrough_point)
            self._axis_direction = axis_direction
            self.axis = [axes.xaxis, axes.yaxis][self.nth_coord]

        def get_line(self, axes):
            _verts = numpy.array([[0.0, 0.0], [1.0, 1.0]])
            fixed_coord = (1 - self.nth_coord)
            trans_passingthrough_point = (axes.transData + axes.transAxes.inverted())
            p = trans_passingthrough_point.transform_point([self._value, self._value])
            _verts[:, fixed_coord] = p[fixed_coord]
            return Path(_verts)

        def get_line_transform(self, axes):
            return axes.transAxes

        def get_axislabel_transform(self, axes):
            return axes.transAxes

        def get_axislabel_pos_angle(self, axes):
            '\n            label reference position in transAxes.\n\n            get_label_transform() returns a transform of (transAxes+offset)\n            '
            loc = self._axis_direction
            if (self.nth_coord == 0):
                angle = 0
            else:
                angle = 90
            _verts = [0.5, 0.5]
            fixed_coord = (1 - self.nth_coord)
            trans_passingthrough_point = (axes.transData + axes.transAxes.inverted())
            p = trans_passingthrough_point.transform_point([self._value, self._value])
            _verts[fixed_coord] = p[fixed_coord]
            if (not (0.0 <= _verts[fixed_coord] <= 1.0)):
                return (None, None)
            else:
                return (_verts, angle)

        def get_tick_transform(self, axes):
            return axes.transData

        def get_tick_iterators(self, axes):
            'tick_loc, tick_angle, tick_label'
            loc = self._axis_direction
            if (loc in ['bottom', 'top']):
                (angle_normal, angle_tangent) = (90, 0)
            else:
                (angle_normal, angle_tangent) = (0, 90)
            if (self.nth_coord == 0):
                (angle_normal, angle_tangent) = (90, 0)
            else:
                (angle_normal, angle_tangent) = (0, 90)
            major = self.axis.major
            majorLocs = major.locator()
            major.formatter.set_locs(majorLocs)
            majorLabels = [major.formatter(val, i) for (i, val) in enumerate(majorLocs)]
            minor = self.axis.minor
            minorLocs = minor.locator()
            minor.formatter.set_locs(minorLocs)
            minorLabels = [minor.formatter(val, i) for (i, val) in enumerate(minorLocs)]
            tr2ax = (axes.transData + axes.transAxes.inverted())

            def _f(locs, labels):
                for (x, l) in zip(locs, labels):
                    c = [self._value, self._value]
                    c[self.nth_coord] = x
                    (c1, c2) = tr2ax.transform_point(c)
                    if ((0.0 <= c1 <= 1.0) and (0.0 <= c2 <= 1.0)):
                        if ((0.0 - self.delta1) <= [c1, c2][self.nth_coord] <= (1.0 + self.delta2)):
                            (yield (c, angle_normal, angle_tangent, l))
            return (_f(majorLocs, majorLabels), _f(minorLocs, minorLabels))

class GridHelperBase(object):

    def __init__(self):
        self._force_update = True
        self._old_limits = None
        super(GridHelperBase, self).__init__()

    def update_lim(self, axes):
        (x1, x2) = axes.get_xlim()
        (y1, y2) = axes.get_ylim()
        if (self._force_update or (self._old_limits != (x1, x2, y1, y2))):
            self._update(x1, x2, y1, y2)
            self._force_update = False
            self._old_limits = (x1, x2, y1, y2)

    def _update(self, x1, x2, y1, y2):
        pass

    def invalidate(self):
        self._force_update = True

    def valid(self):
        return (not self._force_update)

    def get_gridlines(self, which, axis):
        '\n        Return list of grid lines as a list of paths (list of points).\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n        '
        return []

    def new_gridlines(self, ax):
        '\n        Create and return a new GridlineCollection instance.\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n\n        '
        gridlines = GridlinesCollection(None, transform=ax.transData, colors=rcParams['grid.color'], linestyles=rcParams['grid.linestyle'], linewidths=rcParams['grid.linewidth'])
        ax._set_artist_props(gridlines)
        gridlines.set_grid_helper(self)
        ax.axes._set_artist_props(gridlines)
        return gridlines

class GridHelperRectlinear(GridHelperBase):

    def __init__(self, axes):
        super(GridHelperRectlinear, self).__init__()
        self.axes = axes

    def new_fixed_axis(self, loc, nth_coord=None, axis_direction=None, offset=None, axes=None):
        if (axes is None):
            warnings.warn("'new_fixed_axis' explicitly requires the axes keyword.")
            axes = self.axes
        _helper = AxisArtistHelperRectlinear.Fixed(axes, loc, nth_coord)
        if (axis_direction is None):
            axis_direction = loc
        axisline = AxisArtist(axes, _helper, offset=offset, axis_direction=axis_direction)
        return axisline

    def new_floating_axis(self, nth_coord, value, axis_direction='bottom', axes=None):
        if (axes is None):
            warnings.warn("'new_floating_axis' explicitly requires the axes keyword.")
            axes = self.axes
        passthrough_point = (value, value)
        transform = axes.transData
        _helper = AxisArtistHelperRectlinear.Floating(axes, nth_coord, value, axis_direction)
        axisline = AxisArtist(axes, _helper)
        axisline.line.set_clip_on(True)
        axisline.line.set_clip_box(axisline.axes.bbox)
        return axisline

    def get_gridlines(self, which='major', axis='both'):
        '\n        return list of gridline coordinates in data coordinates.\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n        '
        gridlines = []
        if (axis in ['both', 'x']):
            locs = []
            (y1, y2) = self.axes.get_ylim()
            if (which in ['both', 'major']):
                locs.extend(self.axes.xaxis.major.locator())
            if (which in ['both', 'minor']):
                locs.extend(self.axes.xaxis.minor.locator())
            for x in locs:
                gridlines.append([[x, x], [y1, y2]])
        if (axis in ['both', 'y']):
            (x1, x2) = self.axes.get_xlim()
            locs = []
            if self.axes.yaxis._gridOnMajor:
                locs.extend(self.axes.yaxis.major.locator())
            if self.axes.yaxis._gridOnMinor:
                locs.extend(self.axes.yaxis.minor.locator())
            for y in locs:
                gridlines.append([[x1, x2], [y, y]])
        return gridlines

class SimpleChainedObjects(object):

    def __init__(self, objects):
        self._objects = objects

    def __getattr__(self, k):
        _a = SimpleChainedObjects([getattr(a, k) for a in self._objects])
        return _a

    def __call__(self, *kl, **kwargs):
        for m in self._objects:
            m(*kl, **kwargs)

class Axes(matplotlib.axes.Axes):

    class AxisDict(dict):

        def __init__(self, axes):
            self.axes = axes
            super(Axes.AxisDict, self).__init__()

        def __getitem__(self, k):
            if isinstance(k, tuple):
                r = SimpleChainedObjects([dict.__getitem__(self, k1) for k1 in k])
                return r
            elif isinstance(k, slice):
                if ((k.start == None) and (k.stop == None) and (k.step == None)):
                    r = SimpleChainedObjects(list(six.itervalues(self)))
                    return r
                else:
                    raise ValueError('Unsupported slice')
            else:
                return dict.__getitem__(self, k)

        def __call__(self, *v, **kwargs):
            return matplotlib.axes.Axes.axis(self.axes, *v, **kwargs)

    def __init__(self, *kl, **kw):
        helper = kw.pop('grid_helper', None)
        self._axisline_on = True
        if helper:
            self._grid_helper = helper
        else:
            self._grid_helper = GridHelperRectlinear(self)
        super(Axes, self).__init__(*kl, **kw)
        self.toggle_axisline(True)

    def toggle_axisline(self, b=None):
        if (b is None):
            b = (not self._axisline_on)
        if b:
            self._axisline_on = True
            for s in self.spines.values():
                s.set_visible(False)
            self.xaxis.set_visible(False)
            self.yaxis.set_visible(False)
        else:
            self._axisline_on = False
            for s in self.spines.values():
                s.set_visible(True)
            self.xaxis.set_visible(True)
            self.yaxis.set_visible(True)

    def _init_axis(self):
        super(Axes, self)._init_axis()

    def _init_axis_artists(self, axes=None):
        if (axes is None):
            axes = self
        self._axislines = self.AxisDict(self)
        new_fixed_axis = self.get_grid_helper().new_fixed_axis
        for loc in ['bottom', 'top', 'left', 'right']:
            self._axislines[loc] = new_fixed_axis(loc=loc, axes=axes, axis_direction=loc)
        for axisline in [self._axislines['top'], self._axislines['right']]:
            axisline.label.set_visible(False)
            axisline.major_ticklabels.set_visible(False)
            axisline.minor_ticklabels.set_visible(False)

    def _get_axislines(self):
        return self._axislines
    axis = property(_get_axislines)

    def new_gridlines(self, grid_helper=None):
        '\n        Create and return a new GridlineCollection instance.\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n\n        '
        if (grid_helper is None):
            grid_helper = self.get_grid_helper()
        gridlines = grid_helper.new_gridlines(self)
        return gridlines

    def _init_gridlines(self, grid_helper=None):
        gridlines = self.new_gridlines(grid_helper)
        self.gridlines = gridlines

    def cla(self):
        self._init_gridlines()
        super(Axes, self).cla()
        self.gridlines.set_clip_path(self.axes.patch)
        self._init_axis_artists()

    def get_grid_helper(self):
        return self._grid_helper

    def grid(self, b=None, which='major', axis='both', **kwargs):
        '\n        Toggle the gridlines, and optionally set the properties of the lines.\n        '
        super(Axes, self).grid(b, which=which, axis=axis, **kwargs)
        if (not self._axisline_on):
            return
        if (b is None):
            if (self.axes.xaxis._gridOnMinor or self.axes.xaxis._gridOnMajor or self.axes.yaxis._gridOnMinor or self.axes.yaxis._gridOnMajor):
                b = True
            else:
                b = False
        self.gridlines.set_which(which)
        self.gridlines.set_axis(axis)
        self.gridlines.set_visible(b)
        if len(kwargs):
            matplotlib.artist.setp(self.gridlines, **kwargs)

    def get_children(self):
        if self._axisline_on:
            children = (list(six.itervalues(self._axislines)) + [self.gridlines])
        else:
            children = []
        children.extend(super(Axes, self).get_children())
        return children

    def invalidate_grid_helper(self):
        self._grid_helper.invalidate()

    def new_fixed_axis(self, loc, offset=None):
        gh = self.get_grid_helper()
        axis = gh.new_fixed_axis(loc, nth_coord=None, axis_direction=None, offset=offset, axes=self)
        return axis

    def new_floating_axis(self, nth_coord, value, axis_direction='bottom'):
        gh = self.get_grid_helper()
        axis = gh.new_floating_axis(nth_coord, value, axis_direction=axis_direction, axes=self)
        return axis

    def draw(self, renderer, inframe=False):
        if (not self._axisline_on):
            super(Axes, self).draw(renderer, inframe)
            return
        orig_artists = self.artists
        self.artists = ((self.artists + list(self._axislines.values())) + [self.gridlines])
        super(Axes, self).draw(renderer, inframe)
        self.artists = orig_artists

    def get_tightbbox(self, renderer, call_axes_locator=True):
        bb0 = super(Axes, self).get_tightbbox(renderer, call_axes_locator)
        if (not self._axisline_on):
            return bb0
        bb = [bb0]
        for axisline in list(six.itervalues(self._axislines)):
            if (not axisline.get_visible()):
                continue
            bb.append(axisline.get_tightbbox(renderer))
        _bbox = matplotlib.transforms.Bbox.union([b for b in bb if (b and ((b.width != 0) or (b.height != 0)))])
        return _bbox
Subplot = matplotlib.axes.subplot_class_factory(Axes)

class AxesZero(Axes):

    def __init__(self, *kl, **kw):
        super(AxesZero, self).__init__(*kl, **kw)

    def _init_axis_artists(self):
        super(AxesZero, self)._init_axis_artists()
        new_floating_axis = self._grid_helper.new_floating_axis
        xaxis_zero = new_floating_axis(nth_coord=0, value=0.0, axis_direction='bottom', axes=self)
        xaxis_zero.line.set_clip_path(self.patch)
        xaxis_zero.set_visible(False)
        self._axislines['xzero'] = xaxis_zero
        yaxis_zero = new_floating_axis(nth_coord=1, value=0.0, axis_direction='left', axes=self)
        yaxis_zero.line.set_clip_path(self.patch)
        yaxis_zero.set_visible(False)
        self._axislines['yzero'] = yaxis_zero
SubplotZero = matplotlib.axes.subplot_class_factory(AxesZero)
if 0:
    import matplotlib.pyplot as plt
    fig = matplotlib.pyplot.figure(1, (4, 3))
    ax = SubplotZero(fig, 1, 1, 1)
    fig.add_subplot(ax)
    ax.axis['xzero'].set_visible(True)
    ax.axis['xzero'].label.set_text('Axis Zero')
    for n in ['top', 'right']:
        ax.axis[n].set_visible(False)
    tempResult = arange(0, (2 * numpy.pi), 0.01)
	
===================================================================	
module: 482	
----------------------------	

'\nAxislines includes modified implementation of the Axes class. The\nbiggest difference is that the artists responsible to draw axis line,\nticks, ticklabel and axis labels are separated out from the mpl\'s Axis\nclass, which are much more than artists in the original\nmpl. Originally, this change was motivated to support curvilinear\ngrid. Here are a few reasons that I came up with new axes class.\n\n\n * "top" and "bottom" x-axis (or "left" and "right" y-axis) can have\n   different ticks (tick locations and labels). This is not possible\n   with the current mpl, although some twin axes trick can help.\n\n * Curvilinear grid.\n\n * angled ticks.\n\nIn the new axes class, xaxis and yaxis is set to not visible by\ndefault, and new set of artist (AxisArtist) are defined to draw axis\nline, ticks, ticklabels and axis label. Axes.axis attribute serves as\na dictionary of these artists, i.e., ax.axis["left"] is a AxisArtist\ninstance responsible to draw left y-axis. The default Axes.axis contains\n"bottom", "left", "top" and "right".\n\nAxisArtist can be considered as a container artist and\nhas following children artists which will draw ticks, labels, etc.\n\n * line\n * major_ticks, major_ticklabels\n * minor_ticks, minor_ticklabels\n * offsetText\n * label\n\nNote that these are separate artists from Axis class of the\noriginal mpl, thus most of tick-related command in the original mpl\nwon\'t work, although some effort has made to work with. For example,\ncolor and markerwidth of the ax.axis["bottom"].major_ticks will follow\nthose of Axes.xaxis unless explicitly specified.\n\nIn addition to AxisArtist, the Axes will have *gridlines* attribute,\nwhich obviously draws grid lines. The gridlines needs to be separated\nfrom the axis as some gridlines can never pass any axis.\n\n'
from __future__ import absolute_import, division, print_function, unicode_literals
import six
import matplotlib.axes as maxes
import matplotlib.artist as martist
import matplotlib.text as mtext
import matplotlib.font_manager as font_manager
from matplotlib.path import Path
from matplotlib.transforms import Affine2D, ScaledTranslation, IdentityTransform, TransformedPath, Bbox
from matplotlib.collections import LineCollection
from matplotlib import rcParams
from matplotlib.artist import allow_rasterization
import warnings
import numpy as np
import matplotlib.lines as mlines
from .axisline_style import AxislineStyle
from .axis_artist import AxisArtist, GridlinesCollection

class AxisArtistHelper(object):
    '\n    AxisArtistHelper should define\n    following method with given APIs. Note that the first axes argument\n    will be axes attribute of the caller artist.\n\n\n        # LINE (spinal line?)\n\n        def get_line(self, axes):\n            # path : Path\n            return path\n\n        def get_line_transform(self, axes):\n            # ...\n            # trans : transform\n            return trans\n\n        # LABEL\n\n        def get_label_pos(self, axes):\n            # x, y : position\n            return (x, y), trans\n\n\n        def get_label_offset_transform(self,                 axes,\n                pad_points, fontprops, renderer,\n                bboxes,\n                ):\n            # va : vertical alignment\n            # ha : horizontal alignment\n            # a : angle\n            return trans, va, ha, a\n\n        # TICK\n\n        def get_tick_transform(self, axes):\n            return trans\n\n        def get_tick_iterators(self, axes):\n            # iter : iterable object that yields (c, angle, l) where\n            # c, angle, l is position, tick angle, and label\n\n            return iter_major, iter_minor\n\n\n        '

    class _Base(object):
        '\n        Base class for axis helper.\n        '

        def __init__(self):
            '\n            '
            (self.delta1, self.delta2) = (1e-05, 1e-05)

        def update_lim(self, axes):
            pass

    class Fixed(_Base):
        '\n        Helper class for a fixed (in the axes coordinate) axis.\n        '
        _default_passthru_pt = dict(left=(0, 0), right=(1, 0), bottom=(0, 0), top=(0, 1))

        def __init__(self, loc, nth_coord=None):
            '\n            nth_coord = along which coordinate value varies\n            in 2d, nth_coord = 0 ->  x axis, nth_coord = 1 -> y axis\n            '
            self._loc = loc
            if (loc not in ['left', 'right', 'bottom', 'top']):
                raise ValueError(('%s' % loc))
            if (nth_coord is None):
                if (loc in ['left', 'right']):
                    nth_coord = 1
                elif (loc in ['bottom', 'top']):
                    nth_coord = 0
            self.nth_coord = nth_coord
            super(AxisArtistHelper.Fixed, self).__init__()
            self.passthru_pt = self._default_passthru_pt[loc]
            _verts = numpy.array([[0.0, 0.0], [1.0, 1.0]])
            fixed_coord = (1 - nth_coord)
            _verts[:, fixed_coord] = self.passthru_pt[fixed_coord]
            self._path = Path(_verts)

        def get_nth_coord(self):
            return self.nth_coord

        def get_line(self, axes):
            return self._path

        def get_line_transform(self, axes):
            return axes.transAxes

        def get_axislabel_transform(self, axes):
            return axes.transAxes

        def get_axislabel_pos_angle(self, axes):
            '\n            label reference position in transAxes.\n\n            get_label_transform() returns a transform of (transAxes+offset)\n            '
            loc = self._loc
            (pos, angle_tangent) = dict(left=((0.0, 0.5), 90), right=((1.0, 0.5), 90), bottom=((0.5, 0.0), 0), top=((0.5, 1.0), 0))[loc]
            return (pos, angle_tangent)

        def get_tick_transform(self, axes):
            trans_tick = [axes.get_xaxis_transform(), axes.get_yaxis_transform()][self.nth_coord]
            return trans_tick

    class Floating(_Base):

        def __init__(self, nth_coord, value):
            self.nth_coord = nth_coord
            self._value = value
            super(AxisArtistHelper.Floating, self).__init__()

        def get_nth_coord(self):
            return self.nth_coord

        def get_line(self, axes):
            raise RuntimeError('get_line method should be defined by the derived class')

class AxisArtistHelperRectlinear(object):

    class Fixed(AxisArtistHelper.Fixed):

        def __init__(self, axes, loc, nth_coord=None):
            '\n            nth_coord = along which coordinate value varies\n            in 2d, nth_coord = 0 ->  x axis, nth_coord = 1 -> y axis\n            '
            super(AxisArtistHelperRectlinear.Fixed, self).__init__(loc, nth_coord)
            self.axis = [axes.xaxis, axes.yaxis][self.nth_coord]

        def get_tick_iterators(self, axes):
            'tick_loc, tick_angle, tick_label'
            loc = self._loc
            if (loc in ['bottom', 'top']):
                (angle_normal, angle_tangent) = (90, 0)
            else:
                (angle_normal, angle_tangent) = (0, 90)
            major = self.axis.major
            majorLocs = major.locator()
            major.formatter.set_locs(majorLocs)
            majorLabels = [major.formatter(val, i) for (i, val) in enumerate(majorLocs)]
            minor = self.axis.minor
            minorLocs = minor.locator()
            minor.formatter.set_locs(minorLocs)
            minorLabels = [minor.formatter(val, i) for (i, val) in enumerate(minorLocs)]
            trans_tick = self.get_tick_transform(axes)
            tr2ax = (trans_tick + axes.transAxes.inverted())

            def _f(locs, labels):
                for (x, l) in zip(locs, labels):
                    c = list(self.passthru_pt)
                    c[self.nth_coord] = x
                    c2 = tr2ax.transform_point(c)
                    if ((0.0 - self.delta1) <= c2[self.nth_coord] <= (1.0 + self.delta2)):
                        (yield (c, angle_normal, angle_tangent, l))
            return (_f(majorLocs, majorLabels), _f(minorLocs, minorLabels))

    class Floating(AxisArtistHelper.Floating):

        def __init__(self, axes, nth_coord, passingthrough_point, axis_direction='bottom'):
            super(AxisArtistHelperRectlinear.Floating, self).__init__(nth_coord, passingthrough_point)
            self._axis_direction = axis_direction
            self.axis = [axes.xaxis, axes.yaxis][self.nth_coord]

        def get_line(self, axes):
            _verts = numpy.array([[0.0, 0.0], [1.0, 1.0]])
            fixed_coord = (1 - self.nth_coord)
            trans_passingthrough_point = (axes.transData + axes.transAxes.inverted())
            p = trans_passingthrough_point.transform_point([self._value, self._value])
            _verts[:, fixed_coord] = p[fixed_coord]
            return Path(_verts)

        def get_line_transform(self, axes):
            return axes.transAxes

        def get_axislabel_transform(self, axes):
            return axes.transAxes

        def get_axislabel_pos_angle(self, axes):
            '\n            label reference position in transAxes.\n\n            get_label_transform() returns a transform of (transAxes+offset)\n            '
            loc = self._axis_direction
            if (self.nth_coord == 0):
                angle = 0
            else:
                angle = 90
            _verts = [0.5, 0.5]
            fixed_coord = (1 - self.nth_coord)
            trans_passingthrough_point = (axes.transData + axes.transAxes.inverted())
            p = trans_passingthrough_point.transform_point([self._value, self._value])
            _verts[fixed_coord] = p[fixed_coord]
            if (not (0.0 <= _verts[fixed_coord] <= 1.0)):
                return (None, None)
            else:
                return (_verts, angle)

        def get_tick_transform(self, axes):
            return axes.transData

        def get_tick_iterators(self, axes):
            'tick_loc, tick_angle, tick_label'
            loc = self._axis_direction
            if (loc in ['bottom', 'top']):
                (angle_normal, angle_tangent) = (90, 0)
            else:
                (angle_normal, angle_tangent) = (0, 90)
            if (self.nth_coord == 0):
                (angle_normal, angle_tangent) = (90, 0)
            else:
                (angle_normal, angle_tangent) = (0, 90)
            major = self.axis.major
            majorLocs = major.locator()
            major.formatter.set_locs(majorLocs)
            majorLabels = [major.formatter(val, i) for (i, val) in enumerate(majorLocs)]
            minor = self.axis.minor
            minorLocs = minor.locator()
            minor.formatter.set_locs(minorLocs)
            minorLabels = [minor.formatter(val, i) for (i, val) in enumerate(minorLocs)]
            tr2ax = (axes.transData + axes.transAxes.inverted())

            def _f(locs, labels):
                for (x, l) in zip(locs, labels):
                    c = [self._value, self._value]
                    c[self.nth_coord] = x
                    (c1, c2) = tr2ax.transform_point(c)
                    if ((0.0 <= c1 <= 1.0) and (0.0 <= c2 <= 1.0)):
                        if ((0.0 - self.delta1) <= [c1, c2][self.nth_coord] <= (1.0 + self.delta2)):
                            (yield (c, angle_normal, angle_tangent, l))
            return (_f(majorLocs, majorLabels), _f(minorLocs, minorLabels))

class GridHelperBase(object):

    def __init__(self):
        self._force_update = True
        self._old_limits = None
        super(GridHelperBase, self).__init__()

    def update_lim(self, axes):
        (x1, x2) = axes.get_xlim()
        (y1, y2) = axes.get_ylim()
        if (self._force_update or (self._old_limits != (x1, x2, y1, y2))):
            self._update(x1, x2, y1, y2)
            self._force_update = False
            self._old_limits = (x1, x2, y1, y2)

    def _update(self, x1, x2, y1, y2):
        pass

    def invalidate(self):
        self._force_update = True

    def valid(self):
        return (not self._force_update)

    def get_gridlines(self, which, axis):
        '\n        Return list of grid lines as a list of paths (list of points).\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n        '
        return []

    def new_gridlines(self, ax):
        '\n        Create and return a new GridlineCollection instance.\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n\n        '
        gridlines = GridlinesCollection(None, transform=ax.transData, colors=rcParams['grid.color'], linestyles=rcParams['grid.linestyle'], linewidths=rcParams['grid.linewidth'])
        ax._set_artist_props(gridlines)
        gridlines.set_grid_helper(self)
        ax.axes._set_artist_props(gridlines)
        return gridlines

class GridHelperRectlinear(GridHelperBase):

    def __init__(self, axes):
        super(GridHelperRectlinear, self).__init__()
        self.axes = axes

    def new_fixed_axis(self, loc, nth_coord=None, axis_direction=None, offset=None, axes=None):
        if (axes is None):
            warnings.warn("'new_fixed_axis' explicitly requires the axes keyword.")
            axes = self.axes
        _helper = AxisArtistHelperRectlinear.Fixed(axes, loc, nth_coord)
        if (axis_direction is None):
            axis_direction = loc
        axisline = AxisArtist(axes, _helper, offset=offset, axis_direction=axis_direction)
        return axisline

    def new_floating_axis(self, nth_coord, value, axis_direction='bottom', axes=None):
        if (axes is None):
            warnings.warn("'new_floating_axis' explicitly requires the axes keyword.")
            axes = self.axes
        passthrough_point = (value, value)
        transform = axes.transData
        _helper = AxisArtistHelperRectlinear.Floating(axes, nth_coord, value, axis_direction)
        axisline = AxisArtist(axes, _helper)
        axisline.line.set_clip_on(True)
        axisline.line.set_clip_box(axisline.axes.bbox)
        return axisline

    def get_gridlines(self, which='major', axis='both'):
        '\n        return list of gridline coordinates in data coordinates.\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n        '
        gridlines = []
        if (axis in ['both', 'x']):
            locs = []
            (y1, y2) = self.axes.get_ylim()
            if (which in ['both', 'major']):
                locs.extend(self.axes.xaxis.major.locator())
            if (which in ['both', 'minor']):
                locs.extend(self.axes.xaxis.minor.locator())
            for x in locs:
                gridlines.append([[x, x], [y1, y2]])
        if (axis in ['both', 'y']):
            (x1, x2) = self.axes.get_xlim()
            locs = []
            if self.axes.yaxis._gridOnMajor:
                locs.extend(self.axes.yaxis.major.locator())
            if self.axes.yaxis._gridOnMinor:
                locs.extend(self.axes.yaxis.minor.locator())
            for y in locs:
                gridlines.append([[x1, x2], [y, y]])
        return gridlines

class SimpleChainedObjects(object):

    def __init__(self, objects):
        self._objects = objects

    def __getattr__(self, k):
        _a = SimpleChainedObjects([getattr(a, k) for a in self._objects])
        return _a

    def __call__(self, *kl, **kwargs):
        for m in self._objects:
            m(*kl, **kwargs)

class Axes(matplotlib.axes.Axes):

    class AxisDict(dict):

        def __init__(self, axes):
            self.axes = axes
            super(Axes.AxisDict, self).__init__()

        def __getitem__(self, k):
            if isinstance(k, tuple):
                r = SimpleChainedObjects([dict.__getitem__(self, k1) for k1 in k])
                return r
            elif isinstance(k, slice):
                if ((k.start == None) and (k.stop == None) and (k.step == None)):
                    r = SimpleChainedObjects(list(six.itervalues(self)))
                    return r
                else:
                    raise ValueError('Unsupported slice')
            else:
                return dict.__getitem__(self, k)

        def __call__(self, *v, **kwargs):
            return matplotlib.axes.Axes.axis(self.axes, *v, **kwargs)

    def __init__(self, *kl, **kw):
        helper = kw.pop('grid_helper', None)
        self._axisline_on = True
        if helper:
            self._grid_helper = helper
        else:
            self._grid_helper = GridHelperRectlinear(self)
        super(Axes, self).__init__(*kl, **kw)
        self.toggle_axisline(True)

    def toggle_axisline(self, b=None):
        if (b is None):
            b = (not self._axisline_on)
        if b:
            self._axisline_on = True
            for s in self.spines.values():
                s.set_visible(False)
            self.xaxis.set_visible(False)
            self.yaxis.set_visible(False)
        else:
            self._axisline_on = False
            for s in self.spines.values():
                s.set_visible(True)
            self.xaxis.set_visible(True)
            self.yaxis.set_visible(True)

    def _init_axis(self):
        super(Axes, self)._init_axis()

    def _init_axis_artists(self, axes=None):
        if (axes is None):
            axes = self
        self._axislines = self.AxisDict(self)
        new_fixed_axis = self.get_grid_helper().new_fixed_axis
        for loc in ['bottom', 'top', 'left', 'right']:
            self._axislines[loc] = new_fixed_axis(loc=loc, axes=axes, axis_direction=loc)
        for axisline in [self._axislines['top'], self._axislines['right']]:
            axisline.label.set_visible(False)
            axisline.major_ticklabels.set_visible(False)
            axisline.minor_ticklabels.set_visible(False)

    def _get_axislines(self):
        return self._axislines
    axis = property(_get_axislines)

    def new_gridlines(self, grid_helper=None):
        '\n        Create and return a new GridlineCollection instance.\n\n        *which* : "major" or "minor"\n        *axis* : "both", "x" or "y"\n\n        '
        if (grid_helper is None):
            grid_helper = self.get_grid_helper()
        gridlines = grid_helper.new_gridlines(self)
        return gridlines

    def _init_gridlines(self, grid_helper=None):
        gridlines = self.new_gridlines(grid_helper)
        self.gridlines = gridlines

    def cla(self):
        self._init_gridlines()
        super(Axes, self).cla()
        self.gridlines.set_clip_path(self.axes.patch)
        self._init_axis_artists()

    def get_grid_helper(self):
        return self._grid_helper

    def grid(self, b=None, which='major', axis='both', **kwargs):
        '\n        Toggle the gridlines, and optionally set the properties of the lines.\n        '
        super(Axes, self).grid(b, which=which, axis=axis, **kwargs)
        if (not self._axisline_on):
            return
        if (b is None):
            if (self.axes.xaxis._gridOnMinor or self.axes.xaxis._gridOnMajor or self.axes.yaxis._gridOnMinor or self.axes.yaxis._gridOnMajor):
                b = True
            else:
                b = False
        self.gridlines.set_which(which)
        self.gridlines.set_axis(axis)
        self.gridlines.set_visible(b)
        if len(kwargs):
            matplotlib.artist.setp(self.gridlines, **kwargs)

    def get_children(self):
        if self._axisline_on:
            children = (list(six.itervalues(self._axislines)) + [self.gridlines])
        else:
            children = []
        children.extend(super(Axes, self).get_children())
        return children

    def invalidate_grid_helper(self):
        self._grid_helper.invalidate()

    def new_fixed_axis(self, loc, offset=None):
        gh = self.get_grid_helper()
        axis = gh.new_fixed_axis(loc, nth_coord=None, axis_direction=None, offset=offset, axes=self)
        return axis

    def new_floating_axis(self, nth_coord, value, axis_direction='bottom'):
        gh = self.get_grid_helper()
        axis = gh.new_floating_axis(nth_coord, value, axis_direction=axis_direction, axes=self)
        return axis

    def draw(self, renderer, inframe=False):
        if (not self._axisline_on):
            super(Axes, self).draw(renderer, inframe)
            return
        orig_artists = self.artists
        self.artists = ((self.artists + list(self._axislines.values())) + [self.gridlines])
        super(Axes, self).draw(renderer, inframe)
        self.artists = orig_artists

    def get_tightbbox(self, renderer, call_axes_locator=True):
        bb0 = super(Axes, self).get_tightbbox(renderer, call_axes_locator)
        if (not self._axisline_on):
            return bb0
        bb = [bb0]
        for axisline in list(six.itervalues(self._axislines)):
            if (not axisline.get_visible()):
                continue
            bb.append(axisline.get_tightbbox(renderer))
        _bbox = matplotlib.transforms.Bbox.union([b for b in bb if (b and ((b.width != 0) or (b.height != 0)))])
        return _bbox
Subplot = matplotlib.axes.subplot_class_factory(Axes)

class AxesZero(Axes):

    def __init__(self, *kl, **kw):
        super(AxesZero, self).__init__(*kl, **kw)

    def _init_axis_artists(self):
        super(AxesZero, self)._init_axis_artists()
        new_floating_axis = self._grid_helper.new_floating_axis
        xaxis_zero = new_floating_axis(nth_coord=0, value=0.0, axis_direction='bottom', axes=self)
        xaxis_zero.line.set_clip_path(self.patch)
        xaxis_zero.set_visible(False)
        self._axislines['xzero'] = xaxis_zero
        yaxis_zero = new_floating_axis(nth_coord=1, value=0.0, axis_direction='left', axes=self)
        yaxis_zero.line.set_clip_path(self.patch)
        yaxis_zero.set_visible(False)
        self._axislines['yzero'] = yaxis_zero
SubplotZero = matplotlib.axes.subplot_class_factory(AxesZero)
if 0:
    import matplotlib.pyplot as plt
    fig = matplotlib.pyplot.figure(1, (4, 3))
    ax = SubplotZero(fig, 1, 1, 1)
    fig.add_subplot(ax)
    ax.axis['xzero'].set_visible(True)
    ax.axis['xzero'].label.set_text('Axis Zero')
    for n in ['top', 'right']:
        ax.axis[n].set_visible(False)
    xx = numpy.arange(0, (2 * numpy.pi), 0.01)
    ax.plot(xx, numpy.sin(xx))
    ax.set_ylabel('Test')
    matplotlib.pyplot.draw()
    matplotlib.pyplot.show()
if (__name__ == '__main__'):
    import matplotlib.pyplot as plt
    fig = matplotlib.pyplot.figure(1, (4, 3))
    ax = Subplot(fig, 1, 1, 1)
    fig.add_subplot(ax)
    tempResult = arange(0, (2 * numpy.pi), 0.01)
	
===================================================================	
module: 96	
----------------------------	

from __future__ import absolute_import, division, print_function, unicode_literals
import six
from six.moves import zip
import numpy as np
from math import degrees
import math
import warnings

def atan2(dy, dx):
    if ((dx == 0) and (dx == 0)):
        warnings.warn('dx and dy is 0')
        return 0
    else:
        return math.atan2(dy, dx)

def clip(xlines, ylines, x0, clip='right', xdir=True, ydir=True):
    clipped_xlines = []
    clipped_ylines = []
    _pos_angles = []
    if xdir:
        xsign = 1
    else:
        xsign = (- 1)
    if ydir:
        ysign = 1
    else:
        ysign = (- 1)
    for (x, y) in zip(xlines, ylines):
        if (clip in ['up', 'right']):
            b = (x < x0).astype('i')
            db = (b[1:] - b[:(- 1)])
        else:
            b = (x > x0).astype('i')
            db = (b[1:] - b[:(- 1)])
        if b[0]:
            ns = 0
        else:
            ns = (- 1)
        (segx, segy) = ([], [])
        for (i,) in numpy.argwhere((db != 0)):
            c = db[i]
            if (c == (- 1)):
                dx = (x0 - x[i])
                dy = ((y[(i + 1)] - y[i]) * (dx / (x[(i + 1)] - x[i])))
                y0 = (y[i] + dy)
                clipped_xlines.append(numpy.concatenate([segx, x[ns:(i + 1)], [x0]]))
                clipped_ylines.append(numpy.concatenate([segy, y[ns:(i + 1)], [y0]]))
                ns = (- 1)
                (segx, segy) = ([], [])
                if ((dx == 0.0) and (dy == 0)):
                    dx = (x[(i + 1)] - x[i])
                    dy = (y[(i + 1)] - y[i])
                a = degrees(atan2((ysign * dy), (xsign * dx)))
                _pos_angles.append((x0, y0, a))
            elif (c == 1):
                dx = (x0 - x[i])
                dy = ((y[(i + 1)] - y[i]) * (dx / (x[(i + 1)] - x[i])))
                y0 = (y[i] + dy)
                (segx, segy) = ([x0], [y0])
                ns = (i + 1)
                if ((dx == 0.0) and (dy == 0)):
                    dx = (x[(i + 1)] - x[i])
                    dy = (y[(i + 1)] - y[i])
                a = degrees(atan2((ysign * dy), (xsign * dx)))
                _pos_angles.append((x0, y0, a))
        if (ns != (- 1)):
            clipped_xlines.append(numpy.concatenate([segx, x[ns:]]))
            clipped_ylines.append(numpy.concatenate([segy, y[ns:]]))
    return (clipped_xlines, clipped_ylines, _pos_angles)

def clip_line_to_rect(xline, yline, bbox):
    (x0, y0, x1, y1) = bbox.extents
    xdir = (x1 > x0)
    ydir = (y1 > y0)
    if (x1 > x0):
        (lx1, ly1, c_right_) = clip([xline], [yline], x1, clip='right', xdir=xdir, ydir=ydir)
        (lx2, ly2, c_left_) = clip(lx1, ly1, x0, clip='left', xdir=xdir, ydir=ydir)
    else:
        (lx1, ly1, c_right_) = clip([xline], [yline], x0, clip='right', xdir=xdir, ydir=ydir)
        (lx2, ly2, c_left_) = clip(lx1, ly1, x1, clip='left', xdir=xdir, ydir=ydir)
    if (y1 > y0):
        (ly3, lx3, c_top_) = clip(ly2, lx2, y1, clip='right', xdir=ydir, ydir=xdir)
        (ly4, lx4, c_bottom_) = clip(ly3, lx3, y0, clip='left', xdir=ydir, ydir=xdir)
    else:
        (ly3, lx3, c_top_) = clip(ly2, lx2, y0, clip='right', xdir=ydir, ydir=xdir)
        (ly4, lx4, c_bottom_) = clip(ly3, lx3, y1, clip='left', xdir=ydir, ydir=xdir)
    c_left = [((x, y), (((a + 90) % 180) - 90)) for (x, y, a) in c_left_ if bbox.containsy(y)]
    c_bottom = [((x, y), ((90 - a) % 180)) for (y, x, a) in c_bottom_ if bbox.containsx(x)]
    c_right = [((x, y), (((a + 90) % 180) + 90)) for (x, y, a) in c_right_ if bbox.containsy(y)]
    c_top = [((x, y), (((90 - a) % 180) + 180)) for (y, x, a) in c_top_ if bbox.containsx(x)]
    return (list(zip(lx4, ly4)), [c_left, c_bottom, c_right, c_top])
if (__name__ == '__main__'):
    import matplotlib.pyplot as plt
    x = numpy.array([(- 3), (- 2), (- 1), 0.0, 1, 2, 3, 2, 1, 0, (- 1), (- 2), (- 3), 5])
    tempResult = arange(len(x))
	
===================================================================	
get_test_data: 1440	
----------------------------	

'\n    Return a tuple X, Y, Z with a test data set.\n    '
from matplotlib.mlab import bivariate_normal
tempResult = arange((- 3.0), 3.0, delta)
	
===================================================================	
test_divider_append_axes: 28	
----------------------------	

numpy.random.seed(0)
x = numpy.random.randn(1000)
y = numpy.random.randn(1000)
(fig, axScatter) = matplotlib.pyplot.subplots()
axScatter.scatter(x, y)
divider = make_axes_locatable(axScatter)
axHistbot = divider.append_axes('bottom', 1.2, pad=0.1, sharex=axScatter)
axHistright = divider.append_axes('right', 1.2, pad=0.1, sharey=axScatter)
axHistleft = divider.append_axes('left', 1.2, pad=0.1, sharey=axScatter)
axHisttop = divider.append_axes('top', 1.2, pad=0.1, sharex=axScatter)
binwidth = 0.25
xymax = numpy.max([numpy.max(numpy.fabs(x)), numpy.max(numpy.fabs(y))])
lim = ((int((xymax / binwidth)) + 1) * binwidth)
tempResult = arange((- lim), (lim + binwidth), binwidth)
	
===================================================================	
test_surface3d: 117	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
test_surface3d: 118	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
X = numpy.arange((- 5), 5, 0.25)
tempResult = arange((- 5), 5, 0.25)
	
===================================================================	
test_scatter3d_color: 110	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
tempResult = arange(10)
	
===================================================================	
test_scatter3d_color: 110	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
tempResult = arange(10)
	
===================================================================	
test_scatter3d_color: 110	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
tempResult = arange(10)
	
===================================================================	
test_scatter3d_color: 111	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(numpy.arange(10), numpy.arange(10), numpy.arange(10), color='r', marker='o')
tempResult = arange(10, 20)
	
===================================================================	
test_scatter3d_color: 111	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(numpy.arange(10), numpy.arange(10), numpy.arange(10), color='r', marker='o')
tempResult = arange(10, 20)
	
===================================================================	
test_scatter3d_color: 111	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(numpy.arange(10), numpy.arange(10), numpy.arange(10), color='r', marker='o')
tempResult = arange(10, 20)
	
===================================================================	
test_bar3d: 14	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
for (c, z) in zip(['r', 'g', 'b', 'y'], [30, 20, 10, 0]):
    tempResult = arange(20)
	
===================================================================	
test_bar3d: 15	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
for (c, z) in zip(['r', 'g', 'b', 'y'], [30, 20, 10, 0]):
    xs = numpy.arange(20)
    tempResult = arange(20)
	
===================================================================	
test_contourf3d_fill: 60	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 2), 2, 0.25)
	
===================================================================	
test_contourf3d_fill: 60	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.gca(projection='3d')
tempResult = arange((- 2), 2, 0.25)
	
===================================================================	
test_scatter3d: 103	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
tempResult = arange(10)
	
===================================================================	
test_scatter3d: 103	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
tempResult = arange(10)
	
===================================================================	
test_scatter3d: 103	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
tempResult = arange(10)
	
===================================================================	
test_scatter3d: 104	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(numpy.arange(10), numpy.arange(10), numpy.arange(10), c='r', marker='o')
tempResult = arange(10, 20)
	
===================================================================	
test_scatter3d: 104	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(numpy.arange(10), numpy.arange(10), numpy.arange(10), c='r', marker='o')
tempResult = arange(10, 20)
	
===================================================================	
test_scatter3d: 104	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(numpy.arange(10), numpy.arange(10), numpy.arange(10), c='r', marker='o')
tempResult = arange(10, 20)
	
===================================================================	
Get no callers of function numpy.arange at line 86 col 9.	
===================================================================	
Get no callers of function numpy.arange at line 87 col 9.	
===================================================================	
Get no callers of function numpy.arange at line 93 col 25.	
===================================================================	
Get no callers of function numpy.arange at line 93 col 52.	
===================================================================	
test_bar3d_dflt_smoke: 24	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
tempResult = arange(4)
	
===================================================================	
test_bar3d_dflt_smoke: 25	
----------------------------	

fig = matplotlib.pyplot.figure()
ax = fig.add_subplot(111, projection='3d')
x = numpy.arange(4)
tempResult = arange(5)
	
===================================================================	
make_matplotlib_icon: 44	
----------------------------	

fig = matplotlib.pyplot.figure(figsize=(1, 1))
fig.patch.set_alpha(0.0)
ax = fig.add_axes([0.025, 0.025, 0.95, 0.95], projection='polar')
ax.set_axisbelow(True)
N = 7
arc = (2.0 * numpy.pi)
tempResult = arange(0.0, arc, (arc / N))
	
===================================================================	
make_matplotlib_icon: 54	
----------------------------	

fig = matplotlib.pyplot.figure(figsize=(1, 1))
fig.patch.set_alpha(0.0)
ax = fig.add_axes([0.025, 0.025, 0.95, 0.95], projection='polar')
ax.set_axisbelow(True)
N = 7
arc = (2.0 * numpy.pi)
theta = numpy.arange(0.0, arc, (arc / N))
radii = (10 * numpy.array([0.2, 0.6, 0.8, 0.7, 0.4, 0.5, 0.8]))
width = ((numpy.pi / 4) * numpy.array([0.4, 0.4, 0.6, 0.8, 0.2, 0.5, 0.3]))
bars = ax.bar(theta, radii, width=width, bottom=0.0, linewidth=1, edgecolor='k')
for (r, bar) in zip(radii, bars):
    bar.set_facecolor(matplotlib.cm.jet((r / 10.0)))
for label in (ax.get_xticklabels() + ax.get_yticklabels()):
    label.set_visible(False)
for line in (ax.get_ygridlines() + ax.get_xgridlines()):
    line.set_lw(0.0)
tempResult = arange(1, 9, 2)
	
===================================================================	
module: 15	
----------------------------	

from __future__ import print_function
import gc
import matplotlib
matplotlib.use('PDF')
from matplotlib.cbook import report_memory
import numpy as np
import matplotlib.pyplot as plt
rand = numpy.random.rand
(indStart, indEnd) = (200, 401)
(mem_size, coll_count) = ([], [])
for i in range(indEnd):
    fig = matplotlib.pyplot.figure(1)
    fig.clf()
    tempResult = arange(0.0, 2.0, 0.01)
	
***************************************************	
ipython_ipython-6.1.0: 0	
***************************************************	
pandas_pandas-0.19.2: 987	
===================================================================	
value_counts: 170	
----------------------------	

"\n    Compute a histogram of the counts of non-null values.\n\n    Parameters\n    ----------\n    values : ndarray (1-d)\n    sort : boolean, default True\n        Sort by values\n    ascending : boolean, default False\n        Sort in ascending order\n    normalize: boolean, default False\n        If True then compute a relative histogram\n    bins : integer, optional\n        Rather than count values, group them into half-open bins,\n        convenience for pd.cut, only works with numeric data\n    dropna : boolean, default True\n        Don't include counts of NaN\n\n    Returns\n    -------\n    value_counts : Series\n\n    "
from pandas.core.series import Series
name = getattr(values, 'name', None)
if (bins is not None):
    try:
        from pandas.tools.tile import cut
        values = Series(values).values
        (cat, bins) = cut(values, bins, retbins=True)
    except TypeError:
        raise TypeError('bins argument only works with numeric data.')
    values = cat.codes
if (is_extension_type(values) and (not is_datetimetz(values))):
    result = Series(values).values.value_counts(dropna=dropna)
    result.name = name
    counts = result.values
else:
    (keys, counts) = _value_counts_arraylike(values, dropna=dropna)
    from pandas import Index, Series
    if (not isinstance(keys, Index)):
        keys = Index(keys)
    result = Series(counts, index=keys, name=name)
if (bins is not None):
    tempResult = arange(len(cat.categories))
	
===================================================================	
take_2d_multi: 546	
----------------------------	

'\n    Specialized Cython take which sets NaN values in one pass\n    '
if ((indexer is None) or ((indexer[0] is None) and (indexer[1] is None))):
    tempResult = arange(arr.shape[0], dtype=numpy.int64)
	
===================================================================	
take_2d_multi: 547	
----------------------------	

'\n    Specialized Cython take which sets NaN values in one pass\n    '
if ((indexer is None) or ((indexer[0] is None) and (indexer[1] is None))):
    row_idx = numpy.arange(arr.shape[0], dtype=numpy.int64)
    tempResult = arange(arr.shape[1], dtype=numpy.int64)
	
===================================================================	
take_2d_multi: 553	
----------------------------	

'\n    Specialized Cython take which sets NaN values in one pass\n    '
if ((indexer is None) or ((indexer[0] is None) and (indexer[1] is None))):
    row_idx = numpy.arange(arr.shape[0], dtype=numpy.int64)
    col_idx = numpy.arange(arr.shape[1], dtype=numpy.int64)
    indexer = (row_idx, col_idx)
    (dtype, fill_value) = (arr.dtype, arr.dtype.type())
else:
    (row_idx, col_idx) = indexer
    if (row_idx is None):
        tempResult = arange(arr.shape[0], dtype=numpy.int64)
	
===================================================================	
take_2d_multi: 557	
----------------------------	

'\n    Specialized Cython take which sets NaN values in one pass\n    '
if ((indexer is None) or ((indexer[0] is None) and (indexer[1] is None))):
    row_idx = numpy.arange(arr.shape[0], dtype=numpy.int64)
    col_idx = numpy.arange(arr.shape[1], dtype=numpy.int64)
    indexer = (row_idx, col_idx)
    (dtype, fill_value) = (arr.dtype, arr.dtype.type())
else:
    (row_idx, col_idx) = indexer
    if (row_idx is None):
        row_idx = numpy.arange(arr.shape[0], dtype=numpy.int64)
    else:
        row_idx = _ensure_int64(row_idx)
    if (col_idx is None):
        tempResult = arange(arr.shape[1], dtype=numpy.int64)
	
===================================================================	
safe_sort: 109	
----------------------------	

'\n    Sort ``values`` and reorder corresponding ``labels``.\n    ``values`` should be unique if ``labels`` is not None.\n    Safe for use with mixed types (int, str), orders ints before strs.\n\n    .. versionadded:: 0.19.0\n\n    Parameters\n    ----------\n    values : list-like\n        Sequence; must be unique if ``labels`` is not None.\n    labels : list_like\n        Indices to ``values``. All out of bound indices are treated as\n        "not found" and will be masked with ``na_sentinel``.\n    na_sentinel : int, default -1\n        Value in ``labels`` to mark "not found".\n        Ignored when ``labels`` is None.\n    assume_unique : bool, default False\n        When True, ``values`` are assumed to be unique, which can speed up\n        the calculation. Ignored when ``labels`` is None.\n\n    Returns\n    -------\n    ordered : ndarray\n        Sorted ``values``\n    new_labels : ndarray\n        Reordered ``labels``; returned when ``labels`` is not None.\n\n    Raises\n    ------\n    TypeError\n        * If ``values`` is not list-like or if ``labels`` is neither None\n        nor list-like\n        * If ``values`` cannot be sorted\n    ValueError\n        * If ``labels`` is not None and ``values`` contain duplicates.\n    '
if (not is_list_like(values)):
    raise TypeError('Only list-like objects are allowed to be passed tosafe_sort as values')
values = numpy.array(values, copy=False)

def sort_mixed(values):
    str_pos = numpy.array([isinstance(x, string_types) for x in values], dtype=bool)
    nums = numpy.sort(values[(~ str_pos)])
    strs = numpy.sort(values[str_pos])
    return _ensure_object(numpy.concatenate([nums, strs]))
sorter = None
if (pandas.compat.PY3 and (pandas.lib.infer_dtype(values) == 'mixed-integer')):
    ordered = sort_mixed(values)
else:
    try:
        sorter = values.argsort()
        ordered = values.take(sorter)
    except TypeError:
        ordered = sort_mixed(values)
if (labels is None):
    return ordered
if (not is_list_like(labels)):
    raise TypeError('Only list-like objects or None are allowed to bepassed to safe_sort as labels')
labels = _ensure_platform_int(numpy.asarray(labels))
from pandas import Index
if ((not assume_unique) and (not Index(values).is_unique)):
    raise ValueError('values should be unique if labels is not None')
if (sorter is None):
    ((hash_klass, _), values) = _get_data_algo(values, _hashtables)
    t = hash_klass(len(values))
    t.map_locations(values)
    sorter = _ensure_platform_int(t.lookup(ordered))
reverse_indexer = numpy.empty(len(sorter), dtype=numpy.int_)
tempResult = arange(len(sorter))
	
===================================================================	
take_nd: 497	
----------------------------	

'\n    Specialized Cython take which sets NaN values in one pass\n\n    Parameters\n    ----------\n    arr : ndarray\n        Input array\n    indexer : ndarray\n        1-D array of indices to take, subarrays corresponding to -1 value\n        indicies are filed with fill_value\n    axis : int, default 0\n        Axis to take from\n    out : ndarray or None, default None\n        Optional output array, must be appropriate type to hold input and\n        fill_value together, if indexer has any -1 value entries; call\n        _maybe_promote to determine this type for any fill_value\n    fill_value : any, default np.nan\n        Fill value to replace -1 values with\n    mask_info : tuple of (ndarray, boolean)\n        If provided, value should correspond to:\n            (indexer != -1, (indexer != -1).any())\n        If not provided, it will be computed internally if necessary\n    allow_fill : boolean, default True\n        If False, indexer is assumed to contain no -1 values so no filling\n        will be done.  This short-circuits computation of a mask.  Result is\n        undefined if allow_fill == False and -1 is present in indexer.\n    '
if is_categorical(arr):
    return arr.take_nd(indexer, fill_value=fill_value, allow_fill=allow_fill)
elif is_datetimetz(arr):
    return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)
if (indexer is None):
    tempResult = arange(arr.shape[axis], dtype=numpy.int64)
	
===================================================================	
Categorical.value_counts: 448	
----------------------------	

"\n        Returns a Series containing counts of each category.\n\n        Every category will have an entry, even those with a count of 0.\n\n        Parameters\n        ----------\n        dropna : boolean, default True\n            Don't include counts of NaN, even if NaN is a category.\n\n        Returns\n        -------\n        counts : Series\n        "
from numpy import bincount
from pandas.types.missing import isnull
from pandas.core.series import Series
from pandas.core.index import CategoricalIndex
obj = (self.remove_categories([numpy.nan]) if (dropna and isnull(self.categories).any()) else self)
(code, cat) = (obj._codes, obj.categories)
(ncat, mask) = (len(cat), (0 <= code))
tempResult = arange(ncat)
	
===================================================================	
NDFrame.interpolate: 1464	
----------------------------	

'\n        Interpolate values according to different methods.\n        '
if (self.ndim > 2):
    raise NotImplementedError('Interpolate has not been implemented on Panel and Panel 4D objects.')
if (axis == 0):
    ax = self._info_axis_name
    _maybe_transposed_self = self
elif (axis == 1):
    _maybe_transposed_self = self.T
    ax = 1
else:
    _maybe_transposed_self = self
ax = _maybe_transposed_self._get_axis_number(ax)
if (_maybe_transposed_self.ndim == 2):
    alt_ax = (1 - ax)
else:
    alt_ax = ax
if (isinstance(_maybe_transposed_self.index, MultiIndex) and (method != 'linear')):
    raise ValueError('Only `method=linear` interpolation is supported on MultiIndexes.')
if (_maybe_transposed_self._data.get_dtype_counts().get('object') == len(_maybe_transposed_self.T)):
    raise TypeError('Cannot interpolate with all NaNs.')
if (method == 'linear'):
    tempResult = arange(len(_maybe_transposed_self._get_axis(alt_ax)))
	
===================================================================	
_nargsort: 2499	
----------------------------	

'\n    This is intended to be a drop-in replacement for np.argsort which\n    handles NaNs. It adds ascending and na_position parameters.\n    GH #6399, #5231\n    '
if is_categorical_dtype(items):
    return items.argsort(ascending=ascending)
items = numpy.asanyarray(items)
tempResult = arange(len(items))
	
===================================================================	
SeriesGroupBy.value_counts: 1722	
----------------------------	

from functools import partial
from pandas.tools.tile import cut
from pandas.tools.merge import _get_join_indexers
if ((bins is not None) and (not numpy.iterable(bins))):
    return self.apply(pandas.core.series.Series.value_counts, normalize=normalize, sort=sort, ascending=ascending, bins=bins)
(ids, _, _) = self.grouper.group_info
val = self.obj.get_values()
mask = (ids != (- 1))
(ids, val) = (ids[mask], val[mask])
if (bins is None):
    (lab, lev) = pandas.core.algorithms.factorize(val, sort=True)
else:
    (cat, bins) = cut(val, bins, retbins=True)
    (lab, lev, dropna) = (cat.codes, bins[:(- 1)], False)
sorter = numpy.lexsort((lab, ids))
(ids, lab) = (ids[sorter], lab[sorter])
idx = numpy.r_[(0, (1 + numpy.nonzero((ids[1:] != ids[:(- 1)]))[0]))]
inc = numpy.r_[(True, (lab[1:] != lab[:(- 1)]))]
inc[idx] = True
out = numpy.diff(numpy.nonzero(numpy.r_[(inc, True)])[0])
rep = partial(numpy.repeat, repeats=numpy.add.reduceat(inc, idx))
labels = (list(map(rep, self.grouper.recons_labels)) + [lab[inc]])
levels = ([ping.group_index for ping in self.grouper.groupings] + [lev])
names = (self.grouper.names + [self.name])
if dropna:
    mask = (labels[(- 1)] != (- 1))
    if mask.all():
        dropna = False
    else:
        (out, labels) = (out[mask], [label[mask] for label in labels])
if normalize:
    out = out.astype('float')
    d = numpy.diff(numpy.r_[(idx, len(ids))])
    if dropna:
        m = ids[(lab == (- 1))]
        if _np_version_under1p8:
            (mi, ml) = pandas.core.algorithms.factorize(m)
            d[ml] = (d[ml] - numpy.bincount(mi))
        else:
            numpy.add.at(d, m, (- 1))
        acc = rep(d)[mask]
    else:
        acc = rep(d)
    out /= acc
if (sort and (bins is None)):
    cat = (ids[inc][mask] if dropna else ids[inc])
    sorter = numpy.lexsort(((out if ascending else (- out)), cat))
    (out, labels[(- 1)]) = (out[sorter], labels[(- 1)][sorter])
if (bins is None):
    mi = MultiIndex(levels=levels, labels=labels, names=names, verify_integrity=False)
    if is_integer_dtype(out):
        out = _ensure_int64(out)
    return Series(out, index=mi, name=self.name)
diff = numpy.zeros(len(out), dtype='bool')
for lab in labels[:(- 1)]:
    diff |= numpy.r_[(True, (lab[1:] != lab[:(- 1)]))]
(ncat, nbin) = (diff.sum(), len(levels[(- 1)]))
tempResult = arange(ncat)
	
===================================================================	
SeriesGroupBy.value_counts: 1722	
----------------------------	

from functools import partial
from pandas.tools.tile import cut
from pandas.tools.merge import _get_join_indexers
if ((bins is not None) and (not numpy.iterable(bins))):
    return self.apply(pandas.core.series.Series.value_counts, normalize=normalize, sort=sort, ascending=ascending, bins=bins)
(ids, _, _) = self.grouper.group_info
val = self.obj.get_values()
mask = (ids != (- 1))
(ids, val) = (ids[mask], val[mask])
if (bins is None):
    (lab, lev) = pandas.core.algorithms.factorize(val, sort=True)
else:
    (cat, bins) = cut(val, bins, retbins=True)
    (lab, lev, dropna) = (cat.codes, bins[:(- 1)], False)
sorter = numpy.lexsort((lab, ids))
(ids, lab) = (ids[sorter], lab[sorter])
idx = numpy.r_[(0, (1 + numpy.nonzero((ids[1:] != ids[:(- 1)]))[0]))]
inc = numpy.r_[(True, (lab[1:] != lab[:(- 1)]))]
inc[idx] = True
out = numpy.diff(numpy.nonzero(numpy.r_[(inc, True)])[0])
rep = partial(numpy.repeat, repeats=numpy.add.reduceat(inc, idx))
labels = (list(map(rep, self.grouper.recons_labels)) + [lab[inc]])
levels = ([ping.group_index for ping in self.grouper.groupings] + [lev])
names = (self.grouper.names + [self.name])
if dropna:
    mask = (labels[(- 1)] != (- 1))
    if mask.all():
        dropna = False
    else:
        (out, labels) = (out[mask], [label[mask] for label in labels])
if normalize:
    out = out.astype('float')
    d = numpy.diff(numpy.r_[(idx, len(ids))])
    if dropna:
        m = ids[(lab == (- 1))]
        if _np_version_under1p8:
            (mi, ml) = pandas.core.algorithms.factorize(m)
            d[ml] = (d[ml] - numpy.bincount(mi))
        else:
            numpy.add.at(d, m, (- 1))
        acc = rep(d)[mask]
    else:
        acc = rep(d)
    out /= acc
if (sort and (bins is None)):
    cat = (ids[inc][mask] if dropna else ids[inc])
    sorter = numpy.lexsort(((out if ascending else (- out)), cat))
    (out, labels[(- 1)]) = (out[sorter], labels[(- 1)][sorter])
if (bins is None):
    mi = MultiIndex(levels=levels, labels=labels, names=names, verify_integrity=False)
    if is_integer_dtype(out):
        out = _ensure_int64(out)
    return Series(out, index=mi, name=self.name)
diff = numpy.zeros(len(out), dtype='bool')
for lab in labels[:(- 1)]:
    diff |= numpy.r_[(True, (lab[1:] != lab[:(- 1)]))]
(ncat, nbin) = (diff.sum(), len(levels[(- 1)]))
tempResult = arange(nbin)
	
===================================================================	
DataFrameGroupBy._wrap_agged_blocks: 2167	
----------------------------	

if (not self.as_index):
    tempResult = arange(blocks[0].values.shape[1])
	
===================================================================	
_reorder_by_uniques: 2570	
----------------------------	

sorter = uniques.argsort()
reverse_indexer = numpy.empty(len(sorter), dtype=numpy.int64)
tempResult = arange(len(sorter))
	
===================================================================	
BaseGrouper._get_compressed_labels: 911	
----------------------------	

all_labels = [ping.labels for ping in self.groupings]
if (len(all_labels) > 1):
    group_index = get_group_index(all_labels, self.shape, sort=True, xnull=True)
    return _compress_group_index(group_index, sort=self.sort)
ping = self.groupings[0]
tempResult = arange(len(ping.group_index))
	
===================================================================	
_GroupBy._cumcount_array: 408	
----------------------------	

'\n        Parameters\n        ----------\n        ascending : bool, default True\n            If False, number in reverse, from length of group - 1 to 0.\n\n        Note\n        ----\n        this is currently implementing sort=False\n        (though the default is sort=True) for groupby in general\n        '
(ids, _, ngroups) = self.grouper.group_info
sorter = _get_group_index_sorter(ids, ngroups)
(ids, count) = (ids[sorter], len(ids))
if (count == 0):
    return numpy.empty(0, dtype=numpy.int64)
run = numpy.r_[(True, (ids[:(- 1)] != ids[1:]))]
rep = numpy.diff(numpy.r_[(numpy.nonzero(run)[0], count)])
out = (~ run).cumsum()
if ascending:
    out -= numpy.repeat(out[run], rep)
else:
    out = (numpy.repeat(out[numpy.r_[(run[1:], True)]], rep) - out)
rev = numpy.empty(count, dtype=numpy.intp)
tempResult = arange(count, dtype=numpy.intp)
	
===================================================================	
NDFrameGroupBy.aggregate: 1827	
----------------------------	

_level = kwargs.pop('_level', None)
(result, how) = self._aggregate(arg, *args, _level=_level, **kwargs)
if (how is None):
    return result
if (result is None):
    if (self.grouper.nkeys > 1):
        return self._python_agg_general(arg, *args, **kwargs)
    else:
        try:
            assert ((not args) and (not kwargs))
            result = self._aggregate_multiple_funcs([arg], _level=_level)
            result.columns = Index(result.columns.levels[0], name=self._selected_obj.columns.name)
        except:
            result = self._aggregate_generic(arg, *args, **kwargs)
if (not self.as_index):
    self._insert_inaxis_grouper_inplace(result)
    tempResult = arange(len(result))
	
===================================================================	
Grouping.__init__: 1250	
----------------------------	

self.name = name
self.level = level
self.grouper = _convert_grouper(index, grouper)
self.index = index
self.sort = sort
self.obj = obj
self.in_axis = in_axis
if (isinstance(grouper, (Series, Index)) and (name is None)):
    self.name = grouper.name
if isinstance(grouper, MultiIndex):
    self.grouper = grouper.values
self._should_compress = True
if (level is not None):
    if (not isinstance(level, int)):
        if (level not in index.names):
            raise AssertionError(('Level %s not in index' % str(level)))
        level = index.names.index(level)
    if (self.name is None):
        self.name = index.names[level]
    (self.grouper, self._labels, self._group_index) = index._get_grouper_for_level(self.grouper, level)
elif ((self.grouper is None) and (self.name is not None)):
    self.grouper = self.obj[self.name]
elif isinstance(self.grouper, (list, tuple)):
    self.grouper = pandas.core.common._asarray_tuplesafe(self.grouper)
elif is_categorical_dtype(self.grouper):
    if self.sort:
        if (not self.grouper.ordered):
            pass
    else:
        cat = self.grouper.unique()
        self.grouper = self.grouper.reorder_categories(cat.categories)
    self._labels = self.grouper.codes
    c = self.grouper.categories
    tempResult = arange(len(c))
	
===================================================================	
BinGrouper.group_info: 1174	
----------------------------	

ngroups = self.ngroups
tempResult = arange(ngroups)
	
===================================================================	
BinGrouper.group_info: 1178	
----------------------------	

ngroups = self.ngroups
obs_group_ids = numpy.arange(ngroups)
rep = numpy.diff(numpy.r_[(0, self.bins)])
rep = _ensure_platform_int(rep)
if (ngroups == len(self.bins)):
    tempResult = arange(ngroups)
	
===================================================================	
BinGrouper.group_info: 1180	
----------------------------	

ngroups = self.ngroups
obs_group_ids = numpy.arange(ngroups)
rep = numpy.diff(numpy.r_[(0, self.bins)])
rep = _ensure_platform_int(rep)
if (ngroups == len(self.bins)):
    comp_ids = numpy.repeat(numpy.arange(ngroups), rep)
else:
    tempResult = arange(ngroups)
	
===================================================================	
_NDFrameIndexer._convert_to_indexer: 710	
----------------------------	

'\n        Convert indexing key into something we can use to do actual fancy\n        indexing on an ndarray\n\n        Examples\n        ix[:5] -> slice(0, 5)\n        ix[[1,2,3]] -> [1,2,3]\n        ix[[\'foo\', \'bar\', \'baz\']] -> [i, j, k] (indices of foo, bar, baz)\n\n        Going by Zen of Python?\n        "In the face of ambiguity, refuse the temptation to guess."\n        raise AmbiguousIndexError with integer labels?\n        - No, prefer label-based indexing\n        '
labels = self.obj._get_axis(axis)
if isinstance(obj, slice):
    return self._convert_slice_indexer(obj, axis)
try:
    obj = self._convert_scalar_indexer(obj, axis)
except TypeError:
    if is_setter:
        pass
is_int_index = labels.is_integer()
is_int_positional = (is_integer(obj) and (not is_int_index))
try:
    return labels.get_loc(obj)
except LookupError:
    if (isinstance(obj, tuple) and isinstance(labels, MultiIndex)):
        if (is_setter and (len(obj) == labels.nlevels)):
            return {'key': obj}
        raise
except TypeError:
    pass
except ValueError:
    if (not is_int_positional):
        raise
if is_int_positional:
    if is_setter:
        if (self.name == 'loc'):
            return {'key': obj}
        if ((obj >= self.obj.shape[axis]) and (not isinstance(labels, MultiIndex))):
            raise ValueError('cannot set by positional indexing with enlargement')
    return obj
if is_nested_tuple(obj, labels):
    return labels.get_locs(obj)
elif is_list_like_indexer(obj):
    if is_bool_indexer(obj):
        obj = check_bool_indexer(labels, obj)
        (inds,) = obj.nonzero()
        return inds
    else:
        if isinstance(obj, Index):
            objarr = obj
        else:
            objarr = _asarray_tuplesafe(obj)
        indexer = labels._convert_list_indexer(objarr, kind=self.name)
        if (indexer is not None):
            return indexer
        if (isinstance(labels, MultiIndex) and (not isinstance(objarr[0], tuple))):
            level = 0
            (_, indexer) = labels.reindex(objarr, level=level)
            if (indexer is None):
                tempResult = arange(len(labels))
	
===================================================================	
SparseBlock.shift: 1706	
----------------------------	

' shift the block by periods '
N = len(self.values.T)
indexer = numpy.zeros(N, dtype=int)
if (periods > 0):
    tempResult = arange((N - periods))
	
===================================================================	
SparseBlock.shift: 1708	
----------------------------	

' shift the block by periods '
N = len(self.values.T)
indexer = numpy.zeros(N, dtype=int)
if (periods > 0):
    indexer[periods:] = numpy.arange((N - periods))
else:
    tempResult = arange((- periods), N)
	
===================================================================	
BlockManager.get: 2289	
----------------------------	

'\n        Return values for selected item (ndarray or BlockManager).\n        '
if self.items.is_unique:
    if (not isnull(item)):
        loc = self.items.get_loc(item)
    else:
        tempResult = arange(len(self.items))
	
===================================================================	
form_blocks: 2775	
----------------------------	

float_items = []
complex_items = []
int_items = []
bool_items = []
object_items = []
sparse_items = []
datetime_items = []
datetime_tz_items = []
cat_items = []
extra_locs = []
names_idx = Index(names)
if names_idx.equals(axes[0]):
    tempResult = arange(len(names_idx))
	
===================================================================	
DatetimeTZBlock.shift: 1602	
----------------------------	

' shift the block by periods '
N = len(self)
indexer = numpy.zeros(N, dtype=int)
if (periods > 0):
    tempResult = arange((N - periods))
	
===================================================================	
DatetimeTZBlock.shift: 1604	
----------------------------	

' shift the block by periods '
N = len(self)
indexer = numpy.zeros(N, dtype=int)
if (periods > 0):
    indexer[periods:] = numpy.arange((N - periods))
else:
    tempResult = arange((- periods), N)
	
===================================================================	
Block.quantile: 861	
----------------------------	

"\n        compute the quantiles of the\n\n        Parameters\n        ----------\n        qs: a scalar or list of the quantiles to be computed\n        interpolation: type of interpolation, default 'linear'\n        axis: axis to compute, default 0\n\n        Returns\n        -------\n        tuple of (axis, block)\n\n        "
if _np_version_under1p9:
    if (interpolation != 'linear'):
        raise ValueError('Interpolation methods other than linear are not supported in numpy < 1.9.')
kw = {}
if (not _np_version_under1p9):
    kw.update({'interpolation': interpolation})
values = self.get_values()
(values, _, _, _) = self._try_coerce_args(values, values)

def _nanpercentile1D(values, mask, q, **kw):
    values = values[(~ mask)]
    if (len(values) == 0):
        if is_scalar(q):
            return self._na_value
        else:
            return numpy.array(([self._na_value] * len(q)), dtype=values.dtype)
    return numpy.percentile(values, q, **kw)

def _nanpercentile(values, q, axis, **kw):
    mask = isnull(self.values)
    if ((not is_scalar(mask)) and mask.any()):
        if (self.ndim == 1):
            return _nanpercentile1D(values, mask, q, **kw)
        else:
            if (mask.ndim < values.ndim):
                mask = mask.reshape(values.shape)
            if (axis == 0):
                values = values.T
                mask = mask.T
            result = [_nanpercentile1D(val, m, q, **kw) for (val, m) in zip(list(values), list(mask))]
            result = np.array(result, dtype=values.dtype, copy=False).T
            return result
    else:
        return numpy.percentile(values, q, axis=axis, **kw)
from pandas import Float64Index
is_empty = (values.shape[axis] == 0)
if is_list_like(qs):
    ax = Float64Index(qs)
    if is_empty:
        if (self.ndim == 1):
            result = self._na_value
        else:
            result = np.repeat(np.array(([self._na_value] * len(qs))), len(values)).reshape(len(values), len(qs))
    else:
        try:
            result = _nanpercentile(values, (numpy.array(qs) * 100), axis=axis, **kw)
        except ValueError:
            result = [_nanpercentile(values, (q * 100), axis=axis, **kw) for q in qs]
        result = numpy.array(result, copy=False)
        if (self.ndim > 1):
            result = result.T
else:
    if (self.ndim == 1):
        ax = Float64Index([qs])
    else:
        ax = mgr.axes[0]
    if is_empty:
        if (self.ndim == 1):
            result = self._na_value
        else:
            result = numpy.array(([self._na_value] * len(self)))
    else:
        result = _nanpercentile(values, (qs * 100), axis=axis, **kw)
ndim = (getattr(result, 'ndim', None) or 0)
result = self._try_coerce_result(result)
if is_scalar(result):
    return (ax, self.make_block_scalar(result))
tempResult = arange(len(result))
	
===================================================================	
BlockManager.set: 2391	
----------------------------	

'\n        Set new item in-place. Does not consolidate. Adds new Block if not\n        contained in the current set of items\n        if check, then validate that we are not setting the same data in-place\n        '
value_is_extension_type = is_extension_type(value)
if value_is_extension_type:

    def value_getitem(placement):
        return value
else:
    if (value.ndim == (self.ndim - 1)):
        value = _safe_reshape(value, ((1,) + value.shape))

        def value_getitem(placement):
            return value
    else:

        def value_getitem(placement):
            return value[placement.indexer]
    if (value.shape[1:] != self.shape[1:]):
        raise AssertionError('Shape of new values must be compatible with manager shape')
try:
    loc = self.items.get_loc(item)
except KeyError:
    self.insert(len(self.items), item, value)
    return
if isinstance(loc, int):
    loc = [loc]
blknos = self._blknos[loc]
blklocs = self._blklocs[loc].copy()
unfit_mgr_locs = []
unfit_val_locs = []
removed_blknos = []
for (blkno, val_locs) in _get_blkno_placements(blknos, len(self.blocks), group=True):
    blk = self.blocks[blkno]
    blk_locs = blklocs[val_locs.indexer]
    if blk.should_store(value):
        blk.set(blk_locs, value_getitem(val_locs), check=check)
    else:
        unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])
        unfit_val_locs.append(val_locs)
        if (len(val_locs) == len(blk.mgr_locs)):
            removed_blknos.append(blkno)
        else:
            self._blklocs[blk.mgr_locs.indexer] = (- 1)
            blk.delete(blk_locs)
            tempResult = arange(len(blk))
	
===================================================================	
BlockManager.set: 2397	
----------------------------	

'\n        Set new item in-place. Does not consolidate. Adds new Block if not\n        contained in the current set of items\n        if check, then validate that we are not setting the same data in-place\n        '
value_is_extension_type = is_extension_type(value)
if value_is_extension_type:

    def value_getitem(placement):
        return value
else:
    if (value.ndim == (self.ndim - 1)):
        value = _safe_reshape(value, ((1,) + value.shape))

        def value_getitem(placement):
            return value
    else:

        def value_getitem(placement):
            return value[placement.indexer]
    if (value.shape[1:] != self.shape[1:]):
        raise AssertionError('Shape of new values must be compatible with manager shape')
try:
    loc = self.items.get_loc(item)
except KeyError:
    self.insert(len(self.items), item, value)
    return
if isinstance(loc, int):
    loc = [loc]
blknos = self._blknos[loc]
blklocs = self._blklocs[loc].copy()
unfit_mgr_locs = []
unfit_val_locs = []
removed_blknos = []
for (blkno, val_locs) in _get_blkno_placements(blknos, len(self.blocks), group=True):
    blk = self.blocks[blkno]
    blk_locs = blklocs[val_locs.indexer]
    if blk.should_store(value):
        blk.set(blk_locs, value_getitem(val_locs), check=check)
    else:
        unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])
        unfit_val_locs.append(val_locs)
        if (len(val_locs) == len(blk.mgr_locs)):
            removed_blknos.append(blkno)
        else:
            self._blklocs[blk.mgr_locs.indexer] = (- 1)
            blk.delete(blk_locs)
            self._blklocs[blk.mgr_locs.indexer] = numpy.arange(len(blk))
if len(removed_blknos):
    is_deleted = numpy.zeros(self.nblocks, dtype=numpy.bool_)
    is_deleted[removed_blknos] = True
    new_blknos = numpy.empty(self.nblocks, dtype=numpy.int64)
    new_blknos.fill((- 1))
    tempResult = arange((self.nblocks - len(removed_blknos)))
	
===================================================================	
BlockManager.set: 2406	
----------------------------	

'\n        Set new item in-place. Does not consolidate. Adds new Block if not\n        contained in the current set of items\n        if check, then validate that we are not setting the same data in-place\n        '
value_is_extension_type = is_extension_type(value)
if value_is_extension_type:

    def value_getitem(placement):
        return value
else:
    if (value.ndim == (self.ndim - 1)):
        value = _safe_reshape(value, ((1,) + value.shape))

        def value_getitem(placement):
            return value
    else:

        def value_getitem(placement):
            return value[placement.indexer]
    if (value.shape[1:] != self.shape[1:]):
        raise AssertionError('Shape of new values must be compatible with manager shape')
try:
    loc = self.items.get_loc(item)
except KeyError:
    self.insert(len(self.items), item, value)
    return
if isinstance(loc, int):
    loc = [loc]
blknos = self._blknos[loc]
blklocs = self._blklocs[loc].copy()
unfit_mgr_locs = []
unfit_val_locs = []
removed_blknos = []
for (blkno, val_locs) in _get_blkno_placements(blknos, len(self.blocks), group=True):
    blk = self.blocks[blkno]
    blk_locs = blklocs[val_locs.indexer]
    if blk.should_store(value):
        blk.set(blk_locs, value_getitem(val_locs), check=check)
    else:
        unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])
        unfit_val_locs.append(val_locs)
        if (len(val_locs) == len(blk.mgr_locs)):
            removed_blknos.append(blkno)
        else:
            self._blklocs[blk.mgr_locs.indexer] = (- 1)
            blk.delete(blk_locs)
            self._blklocs[blk.mgr_locs.indexer] = numpy.arange(len(blk))
if len(removed_blknos):
    is_deleted = numpy.zeros(self.nblocks, dtype=numpy.bool_)
    is_deleted[removed_blknos] = True
    new_blknos = numpy.empty(self.nblocks, dtype=numpy.int64)
    new_blknos.fill((- 1))
    new_blknos[(~ is_deleted)] = numpy.arange((self.nblocks - len(removed_blknos)))
    self._blknos = pandas.core.algorithms.take_1d(new_blknos, self._blknos, axis=0, allow_fill=False)
    self.blocks = tuple((blk for (i, blk) in enumerate(self.blocks) if (i not in set(removed_blknos))))
if unfit_val_locs:
    unfit_mgr_locs = numpy.concatenate(unfit_mgr_locs)
    unfit_count = len(unfit_mgr_locs)
    new_blocks = []
    if value_is_extension_type:
        new_blocks.extend((make_block(values=value.copy(), ndim=self.ndim, placement=slice(mgr_loc, (mgr_loc + 1))) for mgr_loc in unfit_mgr_locs))
        tempResult = arange(unfit_count)
	
===================================================================	
BlockManager.set: 2412	
----------------------------	

'\n        Set new item in-place. Does not consolidate. Adds new Block if not\n        contained in the current set of items\n        if check, then validate that we are not setting the same data in-place\n        '
value_is_extension_type = is_extension_type(value)
if value_is_extension_type:

    def value_getitem(placement):
        return value
else:
    if (value.ndim == (self.ndim - 1)):
        value = _safe_reshape(value, ((1,) + value.shape))

        def value_getitem(placement):
            return value
    else:

        def value_getitem(placement):
            return value[placement.indexer]
    if (value.shape[1:] != self.shape[1:]):
        raise AssertionError('Shape of new values must be compatible with manager shape')
try:
    loc = self.items.get_loc(item)
except KeyError:
    self.insert(len(self.items), item, value)
    return
if isinstance(loc, int):
    loc = [loc]
blknos = self._blknos[loc]
blklocs = self._blklocs[loc].copy()
unfit_mgr_locs = []
unfit_val_locs = []
removed_blknos = []
for (blkno, val_locs) in _get_blkno_placements(blknos, len(self.blocks), group=True):
    blk = self.blocks[blkno]
    blk_locs = blklocs[val_locs.indexer]
    if blk.should_store(value):
        blk.set(blk_locs, value_getitem(val_locs), check=check)
    else:
        unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])
        unfit_val_locs.append(val_locs)
        if (len(val_locs) == len(blk.mgr_locs)):
            removed_blknos.append(blkno)
        else:
            self._blklocs[blk.mgr_locs.indexer] = (- 1)
            blk.delete(blk_locs)
            self._blklocs[blk.mgr_locs.indexer] = numpy.arange(len(blk))
if len(removed_blknos):
    is_deleted = numpy.zeros(self.nblocks, dtype=numpy.bool_)
    is_deleted[removed_blknos] = True
    new_blknos = numpy.empty(self.nblocks, dtype=numpy.int64)
    new_blknos.fill((- 1))
    new_blknos[(~ is_deleted)] = numpy.arange((self.nblocks - len(removed_blknos)))
    self._blknos = pandas.core.algorithms.take_1d(new_blknos, self._blknos, axis=0, allow_fill=False)
    self.blocks = tuple((blk for (i, blk) in enumerate(self.blocks) if (i not in set(removed_blknos))))
if unfit_val_locs:
    unfit_mgr_locs = numpy.concatenate(unfit_mgr_locs)
    unfit_count = len(unfit_mgr_locs)
    new_blocks = []
    if value_is_extension_type:
        new_blocks.extend((make_block(values=value.copy(), ndim=self.ndim, placement=slice(mgr_loc, (mgr_loc + 1))) for mgr_loc in unfit_mgr_locs))
        self._blknos[unfit_mgr_locs] = (numpy.arange(unfit_count) + len(self.blocks))
        self._blklocs[unfit_mgr_locs] = 0
    else:
        unfit_val_items = unfit_val_locs[0].append(unfit_val_locs[1:])
        new_blocks.append(make_block(values=value_getitem(unfit_val_items), ndim=self.ndim, placement=unfit_mgr_locs))
        self._blknos[unfit_mgr_locs] = len(self.blocks)
        tempResult = arange(unfit_count)
	
===================================================================	
BlockManager.reduction: 2002	
----------------------------	

'\n        iterate over the blocks, collect and create a new block manager.\n        This routine is intended for reduction type operations and\n        will do inference on the generated blocks.\n\n        Parameters\n        ----------\n        f: the callable or function name to operate on at the block level\n        axis: reduction axis, default 0\n        consolidate: boolean, default True. Join together blocks having same\n            dtype\n        transposed: boolean, default False\n            we are holding transposed data\n\n        Returns\n        -------\n        Block Manager (new object)\n\n        '
if consolidate:
    self._consolidate_inplace()
(axes, blocks) = ([], [])
for b in self.blocks:
    kwargs['mgr'] = self
    (axe, block) = getattr(b, f)(axis=axis, **kwargs)
    axes.append(axe)
    blocks.append(block)
ndim = set([b.ndim for b in blocks])
if (2 in ndim):
    new_axes = list(self.axes)
    if (len(blocks) > 1):
        new_axes[1] = axes[0]
        for (b, sb) in zip(blocks, self.blocks):
            b.mgr_locs = sb.mgr_locs
    else:
        new_axes[axis] = Index(numpy.concatenate([ax.values for ax in axes]))
    if transposed:
        new_axes = new_axes[::(- 1)]
        tempResult = arange(b.shape[1])
	
===================================================================	
BlockManager.reduction: 2019	
----------------------------	

'\n        iterate over the blocks, collect and create a new block manager.\n        This routine is intended for reduction type operations and\n        will do inference on the generated blocks.\n\n        Parameters\n        ----------\n        f: the callable or function name to operate on at the block level\n        axis: reduction axis, default 0\n        consolidate: boolean, default True. Join together blocks having same\n            dtype\n        transposed: boolean, default False\n            we are holding transposed data\n\n        Returns\n        -------\n        Block Manager (new object)\n\n        '
if consolidate:
    self._consolidate_inplace()
(axes, blocks) = ([], [])
for b in self.blocks:
    kwargs['mgr'] = self
    (axe, block) = getattr(b, f)(axis=axis, **kwargs)
    axes.append(axe)
    blocks.append(block)
ndim = set([b.ndim for b in blocks])
if (2 in ndim):
    new_axes = list(self.axes)
    if (len(blocks) > 1):
        new_axes[1] = axes[0]
        for (b, sb) in zip(blocks, self.blocks):
            b.mgr_locs = sb.mgr_locs
    else:
        new_axes[axis] = Index(numpy.concatenate([ax.values for ax in axes]))
    if transposed:
        new_axes = new_axes[::(- 1)]
        blocks = [b.make_block(b.values.T, placement=numpy.arange(b.shape[1])) for b in blocks]
    return self.__class__(blocks, new_axes)
if ((0 in ndim) and (1 not in ndim)):
    values = numpy.array([b.values for b in blocks])
    if (len(values) == 1):
        return values.item()
    blocks = [make_block(values, ndim=1)]
    axes = Index([ax[0] for ax in axes])
values = pandas.types.concat._concat_compat([b.values for b in blocks])
if (len(self.blocks) > 1):
    indexer = numpy.empty(len(self.axes[0]), dtype=numpy.intp)
    i = 0
    for b in self.blocks:
        for j in b.mgr_locs:
            indexer[j] = i
            i = (i + 1)
    values = values.take(indexer)
tempResult = arange(len(values))
	
===================================================================	
BlockManager._rebuild_blknos_and_blklocs: 1842	
----------------------------	

'\n        Update mgr._blknos / mgr._blklocs.\n        '
new_blknos = numpy.empty(self.shape[0], dtype=numpy.int64)
new_blklocs = numpy.empty(self.shape[0], dtype=numpy.int64)
new_blknos.fill((- 1))
new_blklocs.fill((- 1))
for (blkno, blk) in enumerate(self.blocks):
    rl = blk.mgr_locs
    new_blknos[rl.indexer] = blkno
    tempResult = arange(len(rl))
	
===================================================================	
BlockManager.take: 2521	
----------------------------	

'\n        Take items along any axis.\n        '
self._consolidate_inplace()
tempResult = arange(indexer.start, indexer.stop, indexer.step, dtype='int64')
	
===================================================================	
nanmedian: 248	
----------------------------	

(values, mask, dtype, dtype_max) = _get_values(values, skipna)

def get_median(x):
    mask = notnull(x)
    if ((not skipna) and (not mask.all())):
        return numpy.nan
    return pandas.algos.median(_values_from_object(x[mask]))
if (not is_float_dtype(values)):
    values = values.astype('f8')
    values[mask] = numpy.nan
if (axis is None):
    values = values.ravel()
notempty = values.size
if (values.ndim > 1):
    if notempty:
        return _wrap_results(numpy.apply_along_axis(get_median, axis, values), dtype)
    shp = numpy.array(values.shape)
    tempResult = arange(values.ndim)
	
===================================================================	
Panel._extract_axis: 791	
----------------------------	

index = None
if (len(data) == 0):
    index = Index([])
elif (len(data) > 0):
    raw_lengths = []
    indexes = []
have_raw_arrays = False
have_frames = False
for v in data.values():
    if isinstance(v, self._constructor_sliced):
        have_frames = True
        indexes.append(v._get_axis(axis))
    elif (v is not None):
        have_raw_arrays = True
        raw_lengths.append(v.shape[axis])
if have_frames:
    index = _get_combined_index(indexes, intersect=intersect)
if have_raw_arrays:
    lengths = list(set(raw_lengths))
    if (len(lengths) > 1):
        raise ValueError(('ndarrays must match shape on axis %d' % axis))
    if have_frames:
        if (lengths[0] != len(index)):
            raise AssertionError('Length of data and index must match')
    else:
        tempResult = arange(lengths[0])
	
===================================================================	
Panel.construct_index_parts: 483	
----------------------------	

levels = [idx]
if major:
    tempResult = arange(N)
	
===================================================================	
Panel.construct_index_parts: 486	
----------------------------	

levels = [idx]
if major:
    labels = [np.arange(N).repeat(K)[selector]]
    names = (idx.name or 'major')
else:
    tempResult = arange(K)
	
===================================================================	
_stack_multi_columns: 395	
----------------------------	


def _convert_level_number(level_num, columns):
    '\n        Logic for converting the level number to something we can safely pass\n        to swaplevel:\n\n        We generally want to convert the level number into a level name, except\n        when columns do not have names, in which case we must leave as a level\n        number\n        '
    if (level_num in columns.names):
        return columns.names[level_num]
    elif (columns.names[level_num] is None):
        return level_num
    else:
        return columns.names[level_num]
this = frame.copy()
if (level_num != (frame.columns.nlevels - 1)):
    roll_columns = this.columns
    for i in range(level_num, (frame.columns.nlevels - 1)):
        lev1 = _convert_level_number(i, roll_columns)
        lev2 = _convert_level_number((i + 1), roll_columns)
        roll_columns = roll_columns.swaplevel(lev1, lev2)
    this.columns = roll_columns
if (not this.columns.is_lexsorted()):
    level_to_sort = _convert_level_number(0, this.columns)
    this = this.sortlevel(level_to_sort, axis=1)
if (len(frame.columns.levels) > 2):
    tuples = list(zip(*[lev.take(lab) for (lev, lab) in zip(this.columns.levels[:(- 1)], this.columns.labels[:(- 1)])]))
    unique_groups = [key for (key, _) in itertools.groupby(tuples)]
    new_names = this.columns.names[:(- 1)]
    new_columns = pandas.core.index.MultiIndex.from_tuples(unique_groups, names=new_names)
else:
    new_columns = unique_groups = this.columns.levels[0]
new_data = {}
level_vals = this.columns.levels[(- 1)]
level_labels = sorted(set(this.columns.labels[(- 1)]))
level_vals_used = level_vals[level_labels]
levsize = len(level_labels)
drop_cols = []
for key in unique_groups:
    loc = this.columns.get_loc(key)
    slice_len = (loc.stop - loc.start)
    if (slice_len == 0):
        drop_cols.append(key)
        continue
    elif (slice_len != levsize):
        chunk = this.ix[:, this.columns[loc]]
        chunk.columns = level_vals.take(chunk.columns.labels[(- 1)])
        value_slice = chunk.reindex(columns=level_vals_used).values
    elif frame._is_mixed_type:
        value_slice = this.ix[:, this.columns[loc]].values
    else:
        value_slice = this.values[:, loc]
    new_data[key] = value_slice.ravel()
if (len(drop_cols) > 0):
    new_columns = new_columns.difference(drop_cols)
N = len(this)
if isinstance(this.index, MultiIndex):
    new_levels = list(this.index.levels)
    new_names = list(this.index.names)
    new_labels = [lab.repeat(levsize) for lab in this.index.labels]
else:
    new_levels = [this.index]
    tempResult = arange(N)
	
===================================================================	
get_empty_Frame: 544	
----------------------------	

if isinstance(data, Series):
    index = data.index
else:
    tempResult = arange(len(data))
	
===================================================================	
factorize: 284	
----------------------------	

if index.is_unique:
    tempResult = arange(len(index))
	
===================================================================	
_Unstacker.get_new_columns: 127	
----------------------------	

if (self.value_columns is None):
    if (self.lift == 0):
        return self.removed_level
    lev = self.removed_level
    return lev.insert(0, _get_na_value(lev.dtype.type))
stride = (len(self.removed_level) + self.lift)
width = len(self.value_columns)
tempResult = arange(width)
	
===================================================================	
_Unstacker.get_new_columns: 136	
----------------------------	

if (self.value_columns is None):
    if (self.lift == 0):
        return self.removed_level
    lev = self.removed_level
    return lev.insert(0, _get_na_value(lev.dtype.type))
stride = (len(self.removed_level) + self.lift)
width = len(self.value_columns)
propagator = numpy.repeat(numpy.arange(width), stride)
if isinstance(self.value_columns, MultiIndex):
    new_levels = (self.value_columns.levels + (self.removed_level,))
    new_names = (self.value_columns.names + (self.removed_name,))
    new_labels = [lab.take(propagator) for lab in self.value_columns.labels]
else:
    new_levels = [self.value_columns, self.removed_level]
    new_names = [self.value_columns.name, self.removed_name]
    new_labels = [propagator]
tempResult = arange(stride)
	
===================================================================	
_Unstacker._make_selectors: 80	
----------------------------	

new_levels = self.new_index_levels
remaining_labels = self.sorted_labels[:(- 1)]
level_sizes = [len(x) for x in new_levels]
(comp_index, obs_ids) = get_compressed_ids(remaining_labels, level_sizes)
ngroups = len(obs_ids)
comp_index = _ensure_platform_int(comp_index)
stride = (self.index.levshape[self.level] + self.lift)
self.full_shape = (ngroups, stride)
selector = ((self.sorted_labels[(- 1)] + (stride * comp_index)) + self.lift)
mask = numpy.zeros(numpy.prod(self.full_shape), dtype=bool)
mask.put(selector, True)
if (mask.sum() < len(self.index)):
    raise ValueError('Index contains duplicate entries, cannot reshape')
self.group_index = comp_index
self.mask = mask
self.unique_groups = obs_ids
tempResult = arange(ngroups)
	
===================================================================	
Index._reindex_non_unique: 1438	
----------------------------	

"\n        *this is an internal non-public method*\n\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        "
target = _ensure_index(target)
(indexer, missing) = self.get_indexer_non_unique(target)
check = (indexer != (- 1))
new_labels = self.take(indexer[check])
new_indexer = None
if len(missing):
    tempResult = arange(len(indexer))
	
===================================================================	
Index._reindex_non_unique: 1448	
----------------------------	

"\n        *this is an internal non-public method*\n\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        "
target = _ensure_index(target)
(indexer, missing) = self.get_indexer_non_unique(target)
check = (indexer != (- 1))
new_labels = self.take(indexer[check])
new_indexer = None
if len(missing):
    l = numpy.arange(len(indexer))
    missing = _ensure_platform_int(missing)
    missing_labels = target.take(missing)
    missing_indexer = _ensure_int64(l[(~ check)])
    cur_labels = self.take(indexer[check]).values
    cur_indexer = _ensure_int64(l[check])
    new_labels = numpy.empty(tuple([len(indexer)]), dtype=object)
    new_labels[cur_indexer] = cur_labels
    new_labels[missing_indexer] = missing_labels
    if target.is_unique:
        tempResult = arange(len(indexer))
	
===================================================================	
Index._reindex_non_unique: 1449	
----------------------------	

"\n        *this is an internal non-public method*\n\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        "
target = _ensure_index(target)
(indexer, missing) = self.get_indexer_non_unique(target)
check = (indexer != (- 1))
new_labels = self.take(indexer[check])
new_indexer = None
if len(missing):
    l = numpy.arange(len(indexer))
    missing = _ensure_platform_int(missing)
    missing_labels = target.take(missing)
    missing_indexer = _ensure_int64(l[(~ check)])
    cur_labels = self.take(indexer[check]).values
    cur_indexer = _ensure_int64(l[check])
    new_labels = numpy.empty(tuple([len(indexer)]), dtype=object)
    new_labels[cur_indexer] = cur_labels
    new_labels[missing_indexer] = missing_labels
    if target.is_unique:
        new_indexer = numpy.arange(len(indexer))
        tempResult = arange(len(cur_labels))
	
===================================================================	
Index._reindex_non_unique: 1454	
----------------------------	

"\n        *this is an internal non-public method*\n\n        Create a new index with target's values (move/add/delete values as\n        necessary) use with non-unique Index and a possibly non-unique target\n\n        Parameters\n        ----------\n        target : an iterable\n\n        Returns\n        -------\n        new_index : pd.Index\n            Resulting index\n        indexer : np.ndarray or None\n            Indices of output values in original index\n\n        "
target = _ensure_index(target)
(indexer, missing) = self.get_indexer_non_unique(target)
check = (indexer != (- 1))
new_labels = self.take(indexer[check])
new_indexer = None
if len(missing):
    l = numpy.arange(len(indexer))
    missing = _ensure_platform_int(missing)
    missing_labels = target.take(missing)
    missing_indexer = _ensure_int64(l[(~ check)])
    cur_labels = self.take(indexer[check]).values
    cur_indexer = _ensure_int64(l[check])
    new_labels = numpy.empty(tuple([len(indexer)]), dtype=object)
    new_labels[cur_indexer] = cur_labels
    new_labels[missing_indexer] = missing_labels
    if target.is_unique:
        new_indexer = numpy.arange(len(indexer))
        new_indexer[cur_indexer] = numpy.arange(len(cur_labels))
        new_indexer[missing_indexer] = (- 1)
    else:
        indexer = indexer.values
        indexer[(~ check)] = 0
        tempResult = arange(len(self.take(indexer)))
	
===================================================================	
Index._join_level: 1618	
----------------------------	

'\n        The join method *only* affects the level of the resulting\n        MultiIndex. Otherwise it just exactly aligns the Index data to the\n        labels of the level in the MultiIndex. If `keep_order` == True, the\n        order of the data indexed by the MultiIndex will not be changed;\n        otherwise, it will tie out with `other`.\n        '
from pandas.algos import groupsort_indexer
from .multi import MultiIndex

def _get_leaf_sorter(labels):
    '\n            returns sorter for the inner most level while preserving the\n            order of higher levels\n            '
    if (labels[0].size == 0):
        return numpy.empty(0, dtype='int64')
    if (len(labels) == 1):
        lab = _ensure_int64(labels[0])
        (sorter, _) = groupsort_indexer(lab, (1 + lab.max()))
        return sorter
    tic = (labels[0][:(- 1)] != labels[0][1:])
    for lab in labels[1:(- 1)]:
        tic |= (lab[:(- 1)] != lab[1:])
    starts = np.hstack(([True], tic, [True])).nonzero()[0]
    lab = _ensure_int64(labels[(- 1)])
    return pandas.lib.get_level_sorter(lab, _ensure_int64(starts))
if (isinstance(self, MultiIndex) and isinstance(other, MultiIndex)):
    raise TypeError('Join on level between two MultiIndex objects is ambiguous')
(left, right) = (self, other)
flip_order = (not isinstance(self, MultiIndex))
if flip_order:
    (left, right) = (right, left)
    how = {'right': 'left', 'left': 'right'}.get(how, how)
level = left._get_level_number(level)
old_level = left.levels[level]
if (not right.is_unique):
    raise NotImplementedError('Index._join_level on non-unique index is not implemented')
(new_level, left_lev_indexer, right_lev_indexer) = old_level.join(right, how=how, return_indexers=True)
if (left_lev_indexer is None):
    if (keep_order or (len(left) == 0)):
        left_indexer = None
        join_index = left
    else:
        left_indexer = _get_leaf_sorter(left.labels[:(level + 1)])
        join_index = left[left_indexer]
else:
    left_lev_indexer = _ensure_int64(left_lev_indexer)
    rev_indexer = pandas.lib.get_reverse_indexer(left_lev_indexer, len(old_level))
    new_lev_labels = pandas.core.algorithms.take_nd(rev_indexer, left.labels[level], allow_fill=False)
    new_labels = list(left.labels)
    new_labels[level] = new_lev_labels
    new_levels = list(left.levels)
    new_levels[level] = new_level
    if keep_order:
        tempResult = arange(len(left), dtype=numpy.intp)
	
===================================================================	
Index.asof_locs: 1017	
----------------------------	

'\n        where : array of timestamps\n        mask : array of booleans where data is not NA\n\n        '
locs = self.values[mask].searchsorted(where.values, side='right')
locs = numpy.where((locs > 0), (locs - 1), 0)
tempResult = arange(len(self))
	
===================================================================	
Index.symmetric_difference: 1185	
----------------------------	

"\n        Compute the symmetric difference of two Index objects.\n        It's sorted if sorting is possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n        result_name : str\n\n        Returns\n        -------\n        symmetric_difference : Index\n\n        Notes\n        -----\n        ``symmetric_difference`` contains elements that appear in either\n        ``idx1`` or ``idx2`` but not both. Equivalent to the Index created by\n        ``idx1.difference(idx2) | idx2.difference(idx1)`` with duplicates\n        dropped.\n\n        Examples\n        --------\n        >>> idx1 = Index([1, 2, 3, 4])\n        >>> idx2 = Index([2, 3, 4, 5])\n        >>> idx1.symmetric_difference(idx2)\n        Int64Index([1, 5], dtype='int64')\n\n        You can also use the ``^`` operator:\n\n        >>> idx1 ^ idx2\n        Int64Index([1, 5], dtype='int64')\n        "
self._assert_can_do_setop(other)
(other, result_name_update) = self._convert_can_do_setop(other)
if (result_name is None):
    result_name = result_name_update
this = self._get_unique_index()
other = other._get_unique_index()
indexer = this.get_indexer(other)
common_indexer = indexer.take((indexer != (- 1)).nonzero()[0])
tempResult = arange(this.size)
	
===================================================================	
Index.difference: 1167	
----------------------------	

"\n        Return a new Index with elements from the index that are not in\n        `other`.\n\n        This is the set difference of two Index objects.\n        It's sorted if sorting is possible.\n\n        Parameters\n        ----------\n        other : Index or array-like\n\n        Returns\n        -------\n        difference : Index\n\n        Examples\n        --------\n\n        >>> idx1 = pd.Index([1, 2, 3, 4])\n        >>> idx2 = pd.Index([3, 4, 5, 6])\n        >>> idx1.difference(idx2)\n        Int64Index([1, 2], dtype='int64')\n\n        "
self._assert_can_do_setop(other)
if self.equals(other):
    return Index([], name=self.name)
(other, result_name) = self._convert_can_do_setop(other)
this = self._get_unique_index()
indexer = this.get_indexer(other)
indexer = indexer.take((indexer != (- 1)).nonzero()[0])
tempResult = arange(this.size)
	
===================================================================	
CategoricalIndex._reindex_non_unique: 257	
----------------------------	

" reindex from a non-unique; which CategoricalIndex's are almost\n        always\n        "
(new_target, indexer) = self.reindex(target)
new_indexer = None
check = (indexer == (- 1))
if check.any():
    tempResult = arange(len(self.take(indexer)))
	
===================================================================	
MultiIndex.get_loc: 930	
----------------------------	

'\n        Get integer location, slice or boolean mask for requested label or\n        tuple.  If the key is past the lexsort depth, the return may be a\n        boolean mask array, otherwise it is always a slice or int.\n\n        Parameters\n        ----------\n        key : label or tuple\n        method : None\n\n        Returns\n        -------\n        loc : int, slice object or boolean mask\n        '
if (method is not None):
    raise NotImplementedError('only the default get_loc method is currently supported for MultiIndex')

def _maybe_to_slice(loc):
    'convert integer indexer to boolean mask or slice if possible'
    if ((not isinstance(loc, numpy.ndarray)) or (loc.dtype != 'int64')):
        return loc
    loc = pandas.lib.maybe_indices_to_slice(loc, len(self))
    if isinstance(loc, slice):
        return loc
    mask = numpy.empty(len(self), dtype='bool')
    mask.fill(False)
    mask[loc] = True
    return mask
if (not isinstance(key, tuple)):
    loc = self._get_level_indexer(key, level=0)
    return _maybe_to_slice(loc)
keylen = len(key)
if (self.nlevels < keylen):
    raise KeyError('Key length ({0}) exceeds index depth ({1})'.format(keylen, self.nlevels))
if ((keylen == self.nlevels) and self.is_unique):

    def _maybe_str_to_time_stamp(key, lev):
        if (lev.is_all_dates and (not isinstance(key, Timestamp))):
            try:
                return Timestamp(key, tz=getattr(lev, 'tz', None))
            except Exception:
                pass
        return key
    key = _values_from_object(key)
    key = tuple(map(_maybe_str_to_time_stamp, key, self.levels))
    return self._engine.get_loc(key)
i = self.lexsort_depth
(lead_key, follow_key) = (key[:i], key[i:])
(start, stop) = (self.slice_locs(lead_key, lead_key) if lead_key else (0, len(self)))
if (start == stop):
    raise KeyError(key)
if (not follow_key):
    return slice(start, stop)
warnings.warn('indexing past lexsort depth may impact performance.', PerformanceWarning, stacklevel=10)
tempResult = arange(start, stop, dtype='int64')
	
===================================================================	
MultiIndex.convert_indexer: 1029	
----------------------------	

tempResult = arange(start, stop, step)
	
===================================================================	
MultiIndex._update_indexer: 1096	
----------------------------	

if (indexer is None):
    tempResult = arange(n)
	
===================================================================	
MultiIndex._bounds: 1256	
----------------------------	

'\n        Return or compute and return slice points for level 0, assuming\n        sortedness\n        '
if (self.__bounds is None):
    tempResult = arange(len(self.levels[0]))
	
===================================================================	
RangeIndex.argsort: 190	
----------------------------	

'\n        Returns the indices that would sort the index and its\n        underlying data.\n\n        Returns\n        -------\n        argsorted : numpy array\n\n        See also\n        --------\n        numpy.ndarray.argsort\n        '
pandas.compat.numpy.function.validate_argsort(args, kwargs)
if (self._step > 0):
    tempResult = arange(len(self))
	
===================================================================	
RangeIndex.argsort: 192	
----------------------------	

'\n        Returns the indices that would sort the index and its\n        underlying data.\n\n        Returns\n        -------\n        argsorted : numpy array\n\n        See also\n        --------\n        numpy.ndarray.argsort\n        '
pandas.compat.numpy.function.validate_argsort(args, kwargs)
if (self._step > 0):
    return numpy.arange(len(self))
else:
    tempResult = arange((len(self) - 1), (- 1), (- 1))
	
===================================================================	
RangeIndex._data: 113	
----------------------------	

tempResult = arange(self._start, self._stop, self._step, dtype=numpy.int64)
	
===================================================================	
Selection.__init__: 2894	
----------------------------	
self.table = table
self.where = where
self.start = start
self.stop = stop
self.condition = None
self.filter = None
self.terms = None
self.coordinates = None
if is_list_like(where):
    try:
        inferred = lib.infer_dtype(where)
        if ((inferred == 'integer') or (inferred == 'boolean')):
            where = np.asarray(where)
            if (where.dtype == np.bool_):
                (start, stop) = (self.start, self.stop)
                if (start is None):
                    start = 0
                if (stop is None):
                    stop = self.table.nrows
                tempResult = arange(start, stop)	
===================================================================	
AppendableTable.write_data: 2422	
----------------------------	

' we form the data into a 2-d including indexes,values,mask\n            write chunk-by-chunk '
names = self.dtype.names
nrows = self.nrows_expected
masks = []
if dropna:
    for a in self.values_axes:
        mask = isnull(a.data).all(axis=0)
        if isinstance(mask, numpy.ndarray):
            masks.append(mask.astype('u1', copy=False))
if len(masks):
    mask = masks[0]
    for m in masks[1:]:
        mask = (mask & m)
    mask = mask.ravel()
else:
    mask = None
indexes = [a.cvalues for a in self.index_axes]
nindexes = len(indexes)
bindexes = []
for (i, idx) in enumerate(indexes):
    if ((i > 0) and (i < nindexes)):
        repeater = numpy.prod([indexes[bi].shape[0] for bi in range(0, i)])
        idx = numpy.tile(idx, repeater)
    if (i < (nindexes - 1)):
        repeater = numpy.prod([indexes[bi].shape[0] for bi in range((i + 1), nindexes)])
        idx = numpy.repeat(idx, repeater)
    bindexes.append(idx)
values = [a.take_data() for a in self.values_axes]
tempResult = arange(v.ndim)
	
===================================================================	
AppendableFrameTable.read: 2542	
----------------------------	

if (not self.read_axes(where=where, **kwargs)):
    return None
info = (self.info.get(self.non_index_axes[0][0], dict()) if len(self.non_index_axes) else dict())
index = self.index_axes[0].values
frames = []
for a in self.values_axes:
    if (info.get('type') == 'MultiIndex'):
        cols = pandas.MultiIndex.from_tuples(a.values)
    else:
        cols = Index(a.values)
    names = info.get('names')
    if (names is not None):
        cols.set_names(names, inplace=True)
    if self.is_transposed:
        values = a.cvalues
        index_ = cols
        cols_ = Index(index, name=getattr(index, 'name', None))
    else:
        values = a.cvalues.T
        index_ = Index(index, name=getattr(index, 'name', None))
        cols_ = cols
    if ((values.ndim == 1) and isinstance(values, numpy.ndarray)):
        values = values.reshape((1, values.shape[0]))
    tempResult = arange(len(cols_))
	
===================================================================	
LegacyTable.read: 2330	
----------------------------	

'we have n indexable columns, with an arbitrary number of data\n        axes\n        '
if (not self.read_axes(where=where, **kwargs)):
    return None
lst_vals = [a.values for a in self.index_axes]
(labels, levels) = _factorize_from_iterables(lst_vals)
labels = list(labels)
levels = list(levels)
N = [len(lvl) for lvl in levels]
key = _factor_indexer(N[1:], labels)
objs = []
if (len(unique(key)) == len(key)):
    (sorter, _) = pandas.algos.groupsort_indexer(_ensure_int64(key), numpy.prod(N))
    sorter = _ensure_platform_int(sorter)
    for c in self.values_axes:
        sorted_values = c.take_data().take(sorter, axis=0)
        if (sorted_values.ndim == 1):
            sorted_values = sorted_values.reshape((sorted_values.shape[0], 1))
        take_labels = [l.take(sorter) for l in labels]
        items = Index(c.values)
        tempResult = arange(len(items))
	
===================================================================	
Selection.select_coords: 2940	
----------------------------	

'\n        generate the selection\n        '
(start, stop) = (self.start, self.stop)
nrows = self.table.nrows
if (start is None):
    start = 0
elif (start < 0):
    start += nrows
if (self.stop is None):
    stop = nrows
elif (stop < 0):
    stop += nrows
if (self.condition is not None):
    return self.table.table.get_where_list(self.condition.format(), start=start, stop=stop, sort=True)
elif (self.coordinates is not None):
    return self.coordinates
tempResult = arange(start, stop)
	
===================================================================	
GenericIndexCol.convert: 935	
----------------------------	

' set the values from this selection: take = take ownership '
tempResult = arange(self.table.nrows)
	
===================================================================	
StataValueLabel.__init__: 273	
----------------------------	

self.labname = catarray.name
categories = catarray.cat.categories
tempResult = arange(len(categories))
	
===================================================================	
StataReader.read: 819	
----------------------------	

if ((self.nobs == 0) and (nrows is None)):
    self._can_read_value_labels = True
    self._data_read = True
    self.close()
    return DataFrame(columns=self.varlist)
if (convert_dates is None):
    convert_dates = self._convert_dates
if (convert_categoricals is None):
    convert_categoricals = self._convert_categoricals
if (convert_missing is None):
    convert_missing = self._convert_missing
if (preserve_dtypes is None):
    preserve_dtypes = self._preserve_dtypes
if (columns is None):
    columns = self._columns
if (order_categoricals is None):
    order_categoricals = self._order_categoricals
if (nrows is None):
    nrows = self.nobs
if ((self.format_version >= 117) and (self._dtype is None)):
    self._can_read_value_labels = True
    self._read_strls()
if (self._dtype is None):
    dtype = []
    for (i, typ) in enumerate(self.typlist):
        if (typ in self.NUMPY_TYPE_MAP):
            dtype.append((('s' + str(i)), (self.byteorder + self.NUMPY_TYPE_MAP[typ])))
        else:
            dtype.append((('s' + str(i)), ('S' + str(typ))))
    dtype = numpy.dtype(dtype)
    self._dtype = dtype
dtype = self._dtype
max_read_len = ((self.nobs - self._lines_read) * dtype.itemsize)
read_len = (nrows * dtype.itemsize)
read_len = min(read_len, max_read_len)
if (read_len <= 0):
    if convert_categoricals:
        self._read_value_labels()
    self.close()
    raise StopIteration
offset = (self._lines_read * dtype.itemsize)
self.path_or_buf.seek((self.data_location + offset))
read_lines = min(nrows, (self.nobs - self._lines_read))
data = numpy.frombuffer(self.path_or_buf.read(read_len), dtype=dtype, count=read_lines)
self._lines_read += read_lines
if (self._lines_read == self.nobs):
    self._can_read_value_labels = True
    self._data_read = True
if (self.byteorder != self._native_byteorder):
    data = data.byteswap().newbyteorder()
if convert_categoricals:
    self._read_value_labels()
if (len(data) == 0):
    data = DataFrame(columns=self.varlist, index=index)
else:
    data = pandas.core.frame.DataFrame.from_records(data, index=index)
    data.columns = self.varlist
if (index is None):
    tempResult = arange((self._lines_read - read_lines), self._lines_read)
	
===================================================================	
_create_sp_series: 16	
----------------------------	

nan = numpy.nan
tempResult = arange(15, dtype=numpy.float64)
	
===================================================================	
_create_sp_tsseries: 25	
----------------------------	

nan = numpy.nan
tempResult = arange(15, dtype=numpy.float64)
	
===================================================================	
_create_sp_frame: 35	
----------------------------	

nan = numpy.nan
tempResult = arange(10)
	
===================================================================	
create_data: 43	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
tempResult = arange(10)
	
===================================================================	
create_data: 45	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
tempResult = arange(10)
	
===================================================================	
create_data: 45	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
tempResult = arange(5)
	
===================================================================	
create_data: 45	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
tempResult = arange(5)
	
===================================================================	
create_data: 48	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
series = dict(float=Series(data['A']), int=Series(data['B']), mixed=Series(data['E']), ts=Series(np.arange(10).astype(numpy.int64), index=date_range('20130101', periods=10)), mi=Series(np.arange(5).astype(numpy.float64), index=pandas.MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=['one', 'two'])), dup=Series(np.arange(5).astype(numpy.float64), index=['A', 'B', 'C', 'D', 'A']), cat=Series(Categorical(['foo', 'bar', 'baz'])), dt=Series(date_range('20130101', periods=5)), dt_tz=Series(date_range('20130101', periods=5, tz='US/Eastern')), period=Series(([Period('2000Q1')] * 5)))
mixed_dup_df = DataFrame(data)
mixed_dup_df.columns = list('ABCDA')
tempResult = arange(5)
	
===================================================================	
create_data: 48	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
series = dict(float=Series(data['A']), int=Series(data['B']), mixed=Series(data['E']), ts=Series(np.arange(10).astype(numpy.int64), index=date_range('20130101', periods=10)), mi=Series(np.arange(5).astype(numpy.float64), index=pandas.MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=['one', 'two'])), dup=Series(np.arange(5).astype(numpy.float64), index=['A', 'B', 'C', 'D', 'A']), cat=Series(Categorical(['foo', 'bar', 'baz'])), dt=Series(date_range('20130101', periods=5)), dt_tz=Series(date_range('20130101', periods=5, tz='US/Eastern')), period=Series(([Period('2000Q1')] * 5)))
mixed_dup_df = DataFrame(data)
mixed_dup_df.columns = list('ABCDA')
tempResult = arange(5)
	
===================================================================	
create_data: 48	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
series = dict(float=Series(data['A']), int=Series(data['B']), mixed=Series(data['E']), ts=Series(np.arange(10).astype(numpy.int64), index=date_range('20130101', periods=10)), mi=Series(np.arange(5).astype(numpy.float64), index=pandas.MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=['one', 'two'])), dup=Series(np.arange(5).astype(numpy.float64), index=['A', 'B', 'C', 'D', 'A']), cat=Series(Categorical(['foo', 'bar', 'baz'])), dt=Series(date_range('20130101', periods=5)), dt_tz=Series(date_range('20130101', periods=5, tz='US/Eastern')), period=Series(([Period('2000Q1')] * 5)))
mixed_dup_df = DataFrame(data)
mixed_dup_df.columns = list('ABCDA')
tempResult = arange(15)
	
===================================================================	
create_data: 48	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
series = dict(float=Series(data['A']), int=Series(data['B']), mixed=Series(data['E']), ts=Series(np.arange(10).astype(numpy.int64), index=date_range('20130101', periods=10)), mi=Series(np.arange(5).astype(numpy.float64), index=pandas.MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=['one', 'two'])), dup=Series(np.arange(5).astype(numpy.float64), index=['A', 'B', 'C', 'D', 'A']), cat=Series(Categorical(['foo', 'bar', 'baz'])), dt=Series(date_range('20130101', periods=5)), dt_tz=Series(date_range('20130101', periods=5, tz='US/Eastern')), period=Series(([Period('2000Q1')] * 5)))
mixed_dup_df = DataFrame(data)
mixed_dup_df.columns = list('ABCDA')
tempResult = arange(3)
	
===================================================================	
create_data: 51	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
series = dict(float=Series(data['A']), int=Series(data['B']), mixed=Series(data['E']), ts=Series(np.arange(10).astype(numpy.int64), index=date_range('20130101', periods=10)), mi=Series(np.arange(5).astype(numpy.float64), index=pandas.MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=['one', 'two'])), dup=Series(np.arange(5).astype(numpy.float64), index=['A', 'B', 'C', 'D', 'A']), cat=Series(Categorical(['foo', 'bar', 'baz'])), dt=Series(date_range('20130101', periods=5)), dt_tz=Series(date_range('20130101', periods=5, tz='US/Eastern')), period=Series(([Period('2000Q1')] * 5)))
mixed_dup_df = DataFrame(data)
mixed_dup_df.columns = list('ABCDA')
frame = dict(float=DataFrame({'A': series['float'], 'B': (series['float'] + 1)}), int=DataFrame({'A': series['int'], 'B': (series['int'] + 1)}), mixed=DataFrame({k: data[k] for k in ['A', 'B', 'C', 'D']}), mi=DataFrame({'A': np.arange(5).astype(numpy.float64), 'B': np.arange(5).astype(numpy.int64)}, index=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'baz'], ['one', 'two', 'one', 'two', 'three']])), names=['first', 'second'])), dup=DataFrame(np.arange(15).reshape(5, 3).astype(numpy.float64), columns=['A', 'B', 'A']), cat_onecol=DataFrame({'A': Categorical(['foo', 'bar'])}), cat_and_float=DataFrame({'A': Categorical(['foo', 'bar', 'baz']), 'B': np.arange(3).astype(numpy.int64)}), mixed_dup=mixed_dup_df, dt_mixed_tzs=DataFrame({'A': Timestamp('20130102', tz='US/Eastern'), 'B': Timestamp('20130603', tz='CET')}, index=range(5)))
mixed_dup_panel = Panel({'ItemA': frame['float'], 'ItemB': frame['int']})
mixed_dup_panel.items = ['ItemA', 'ItemA']
tempResult = arange(30)
	
===================================================================	
create_data: 52	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
series = dict(float=Series(data['A']), int=Series(data['B']), mixed=Series(data['E']), ts=Series(np.arange(10).astype(numpy.int64), index=date_range('20130101', periods=10)), mi=Series(np.arange(5).astype(numpy.float64), index=pandas.MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=['one', 'two'])), dup=Series(np.arange(5).astype(numpy.float64), index=['A', 'B', 'C', 'D', 'A']), cat=Series(Categorical(['foo', 'bar', 'baz'])), dt=Series(date_range('20130101', periods=5)), dt_tz=Series(date_range('20130101', periods=5, tz='US/Eastern')), period=Series(([Period('2000Q1')] * 5)))
mixed_dup_df = DataFrame(data)
mixed_dup_df.columns = list('ABCDA')
frame = dict(float=DataFrame({'A': series['float'], 'B': (series['float'] + 1)}), int=DataFrame({'A': series['int'], 'B': (series['int'] + 1)}), mixed=DataFrame({k: data[k] for k in ['A', 'B', 'C', 'D']}), mi=DataFrame({'A': np.arange(5).astype(numpy.float64), 'B': np.arange(5).astype(numpy.int64)}, index=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'baz'], ['one', 'two', 'one', 'two', 'three']])), names=['first', 'second'])), dup=DataFrame(np.arange(15).reshape(5, 3).astype(numpy.float64), columns=['A', 'B', 'A']), cat_onecol=DataFrame({'A': Categorical(['foo', 'bar'])}), cat_and_float=DataFrame({'A': Categorical(['foo', 'bar', 'baz']), 'B': np.arange(3).astype(numpy.int64)}), mixed_dup=mixed_dup_df, dt_mixed_tzs=DataFrame({'A': Timestamp('20130102', tz='US/Eastern'), 'B': Timestamp('20130603', tz='CET')}, index=range(5)))
mixed_dup_panel = Panel({'ItemA': frame['float'], 'ItemB': frame['int']})
mixed_dup_panel.items = ['ItemA', 'ItemA']
panel = dict(float=Panel({'ItemA': frame['float'], 'ItemB': (frame['float'] + 1)}), dup=Panel(np.arange(30).reshape(3, 5, 2).astype(numpy.float64), items=['A', 'B', 'A']), mixed_dup=mixed_dup_panel)
tempResult = arange(1000)
	
===================================================================	
create_data: 52	
----------------------------	

' create the pickle/msgpack data '
data = {'A': [0.0, 1.0, 2.0, 3.0, numpy.nan], 'B': [0, 1, 0, 1, 0], 'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'], 'D': date_range('1/1/2009', periods=5), 'E': [0.0, 1, Timestamp('20100101'), 'foo', 2.0]}
scalars = dict(timestamp=Timestamp('20130101'), period=Period('2012', 'M'))
index = dict(int=Index(numpy.arange(10)), date=date_range('20130101', periods=10), period=period_range('2013-01-01', freq='M', periods=10))
mi = dict(reg2=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])), names=['first', 'second']))
series = dict(float=Series(data['A']), int=Series(data['B']), mixed=Series(data['E']), ts=Series(np.arange(10).astype(numpy.int64), index=date_range('20130101', periods=10)), mi=Series(np.arange(5).astype(numpy.float64), index=pandas.MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=['one', 'two'])), dup=Series(np.arange(5).astype(numpy.float64), index=['A', 'B', 'C', 'D', 'A']), cat=Series(Categorical(['foo', 'bar', 'baz'])), dt=Series(date_range('20130101', periods=5)), dt_tz=Series(date_range('20130101', periods=5, tz='US/Eastern')), period=Series(([Period('2000Q1')] * 5)))
mixed_dup_df = DataFrame(data)
mixed_dup_df.columns = list('ABCDA')
frame = dict(float=DataFrame({'A': series['float'], 'B': (series['float'] + 1)}), int=DataFrame({'A': series['int'], 'B': (series['int'] + 1)}), mixed=DataFrame({k: data[k] for k in ['A', 'B', 'C', 'D']}), mi=DataFrame({'A': np.arange(5).astype(numpy.float64), 'B': np.arange(5).astype(numpy.int64)}, index=pandas.MultiIndex.from_tuples(tuple(zip(*[['bar', 'bar', 'baz', 'baz', 'baz'], ['one', 'two', 'one', 'two', 'three']])), names=['first', 'second'])), dup=DataFrame(np.arange(15).reshape(5, 3).astype(numpy.float64), columns=['A', 'B', 'A']), cat_onecol=DataFrame({'A': Categorical(['foo', 'bar'])}), cat_and_float=DataFrame({'A': Categorical(['foo', 'bar', 'baz']), 'B': np.arange(3).astype(numpy.int64)}), mixed_dup=mixed_dup_df, dt_mixed_tzs=DataFrame({'A': Timestamp('20130102', tz='US/Eastern'), 'B': Timestamp('20130603', tz='CET')}, index=range(5)))
mixed_dup_panel = Panel({'ItemA': frame['float'], 'ItemB': frame['int']})
mixed_dup_panel.items = ['ItemA', 'ItemA']
panel = dict(float=Panel({'ItemA': frame['float'], 'ItemB': (frame['float'] + 1)}), dup=Panel(np.arange(30).reshape(3, 5, 2).astype(numpy.float64), items=['A', 'B', 'A']), mixed_dup=mixed_dup_panel)
tempResult = arange(10000)
	
===================================================================	
TestClipboard.setUpClass: 26	
----------------------------	

super(TestClipboard, cls).setUpClass()
cls.data = {}
cls.data['string'] = mkdf(5, 3, c_idx_type='s', r_idx_type='i', c_idx_names=[None], r_idx_names=[None])
cls.data['int'] = mkdf(5, 3, data_gen_f=(lambda *args: randint(2)), c_idx_type='s', r_idx_type='i', c_idx_names=[None], r_idx_names=[None])
cls.data['float'] = mkdf(5, 3, data_gen_f=(lambda r, c: (float(r) + 0.01)), c_idx_type='s', r_idx_type='i', c_idx_names=[None], r_idx_names=[None])
tempResult = arange(1.0, 6.0)
	
===================================================================	
TestClipboard.setUpClass: 26	
----------------------------	

super(TestClipboard, cls).setUpClass()
cls.data = {}
cls.data['string'] = mkdf(5, 3, c_idx_type='s', r_idx_type='i', c_idx_names=[None], r_idx_names=[None])
cls.data['int'] = mkdf(5, 3, data_gen_f=(lambda *args: randint(2)), c_idx_type='s', r_idx_type='i', c_idx_names=[None], r_idx_names=[None])
cls.data['float'] = mkdf(5, 3, data_gen_f=(lambda r, c: (float(r) + 0.01)), c_idx_type='s', r_idx_type='i', c_idx_names=[None], r_idx_names=[None])
tempResult = arange(1, 6)
	
===================================================================	
ExcelWriterBase.test_to_excel_multiindex_dates: 860	
----------------------------	

_skip_if_no_xlrd()
tsframe = self.tsframe.copy()
tempResult = arange(len(tsframe.index))
	
===================================================================	
ExcelWriterBase.test_to_excel_multiindex_cols: 840	
----------------------------	

_skip_if_no_xlrd()
frame = self.frame
tempResult = arange((len(frame.index) * 2))
	
===================================================================	
ExcelWriterBase.test_to_excel_multiindex: 817	
----------------------------	

_skip_if_no_xlrd()
frame = self.frame
tempResult = arange((len(frame.index) * 2))
	
===================================================================	
TestReadHtml.test_skiprows_ndarray: 173	
----------------------------	

tempResult = arange(2)
	
===================================================================	
TestReadHtml.test_skiprows_ndarray: 174	
----------------------------	

df1 = self.read_html(self.spam_data, '.*Water.*', skiprows=numpy.arange(2))
tempResult = arange(2)
	
===================================================================	
TestEncoding.setUp: 526	
----------------------------	

super(TestEncoding, self).setUp()
tempResult = arange(1000, dtype=numpy.int32)
	
===================================================================	
TestCompression.setUp: 393	
----------------------------	

try:
    from sqlalchemy import create_engine
    self._create_sql_engine = create_engine
except ImportError:
    self._SQLALCHEMY_INSTALLED = False
else:
    self._SQLALCHEMY_INSTALLED = True
super(TestCompression, self).setUp()
tempResult = arange(1000, dtype=numpy.float64)
	
===================================================================	
TestCompression.setUp: 393	
----------------------------	

try:
    from sqlalchemy import create_engine
    self._create_sql_engine = create_engine
except ImportError:
    self._SQLALCHEMY_INSTALLED = False
else:
    self._SQLALCHEMY_INSTALLED = True
super(TestCompression, self).setUp()
tempResult = arange(1000, dtype=numpy.int32)
	
===================================================================	
TestHDFStore.test_append_raise: 1318	
----------------------------	

with ensure_clean_store(self.path) as store:
    df = pandas.util.testing.makeDataFrame()
    df['invalid'] = ([['a']] * len(df))
    self.assertEqual(df.dtypes['invalid'], numpy.object_)
    self.assertRaises(TypeError, store.append, 'df', df)
    df['invalid2'] = ([['a']] * len(df))
    df['invalid3'] = ([['a']] * len(df))
    self.assertRaises(TypeError, store.append, 'df', df)
    df = pandas.util.testing.makeDataFrame()
    s = Series(datetime.datetime.datetime(2001, 1, 2), index=df.index)
    s = s.astype(object)
    s[0:5] = numpy.nan
    df['invalid'] = s
    self.assertEqual(df.dtypes['invalid'], numpy.object_)
    self.assertRaises(TypeError, store.append, 'df', df)
    tempResult = arange(10)
	
===================================================================	
TestHDFStore.test_append_raise: 1319	
----------------------------	

with ensure_clean_store(self.path) as store:
    df = pandas.util.testing.makeDataFrame()
    df['invalid'] = ([['a']] * len(df))
    self.assertEqual(df.dtypes['invalid'], numpy.object_)
    self.assertRaises(TypeError, store.append, 'df', df)
    df['invalid2'] = ([['a']] * len(df))
    df['invalid3'] = ([['a']] * len(df))
    self.assertRaises(TypeError, store.append, 'df', df)
    df = pandas.util.testing.makeDataFrame()
    s = Series(datetime.datetime.datetime(2001, 1, 2), index=df.index)
    s = s.astype(object)
    s[0:5] = numpy.nan
    df['invalid'] = s
    self.assertEqual(df.dtypes['invalid'], numpy.object_)
    self.assertRaises(TypeError, store.append, 'df', df)
    self.assertRaises(TypeError, store.append, 'df', numpy.arange(10))
    tempResult = arange(10)
	
===================================================================	
TestHDFStore.test_column_multiindex: 1162	
----------------------------	

index = pandas.MultiIndex.from_tuples([('A', 'a'), ('A', 'b'), ('B', 'a'), ('B', 'b')], names=['first', 'second'])
tempResult = arange(12)
	
===================================================================	
TestHDFStore.test_column_multiindex: 1177	
----------------------------	

index = pandas.MultiIndex.from_tuples([('A', 'a'), ('A', 'b'), ('B', 'a'), ('B', 'b')], names=['first', 'second'])
df = DataFrame(np.arange(12).reshape(3, 4), columns=index)
expected = df.copy()
if isinstance(expected.index, RangeIndex):
    expected.index = Int64Index(expected.index)
with ensure_clean_store(self.path) as store:
    store.put('df', df)
    pandas.util.testing.assert_frame_equal(store['df'], expected, check_index_type=True, check_column_type=True)
    store.put('df1', df, format='table')
    pandas.util.testing.assert_frame_equal(store['df1'], expected, check_index_type=True, check_column_type=True)
    self.assertRaises(ValueError, store.put, 'df2', df, format='table', data_columns=['A'])
    self.assertRaises(ValueError, store.put, 'df3', df, format='table', data_columns=True)
with ensure_clean_store(self.path) as store:
    store.append('df2', df)
    store.append('df2', df)
    pandas.util.testing.assert_frame_equal(store['df2'], concat((df, df)))
tempResult = arange(12)
	
===================================================================	
TestHDFStore.test_select_dtypes: 2086	
----------------------------	

with ensure_clean_store(self.path) as store:
    df = DataFrame(dict(ts=bdate_range('2012-01-01', periods=300), A=numpy.random.randn(300)))
    _maybe_remove(store, 'df')
    store.append('df', df, data_columns=['ts', 'A'])
    result = store.select('df', [Term("ts>=Timestamp('2012-02-01')")])
    expected = df[(df.ts >= Timestamp('2012-02-01'))]
    pandas.util.testing.assert_frame_equal(expected, result)
    df = DataFrame(numpy.random.randn(5, 2), columns=['A', 'B'])
    df['object'] = 'foo'
    df.ix[4:5, 'object'] = 'bar'
    df['boolv'] = (df['A'] > 0)
    _maybe_remove(store, 'df')
    store.append('df', df, data_columns=True)
    expected = df[(df.boolv == True)].reindex(columns=['A', 'boolv'])
    for v in [True, 'true', 1]:
        result = store.select('df', Term(('boolv == %s' % str(v))), columns=['A', 'boolv'])
        pandas.util.testing.assert_frame_equal(expected, result)
    expected = df[(df.boolv == False)].reindex(columns=['A', 'boolv'])
    for v in [False, 'false', 0]:
        result = store.select('df', Term(('boolv == %s' % str(v))), columns=['A', 'boolv'])
        pandas.util.testing.assert_frame_equal(expected, result)
    df = DataFrame(dict(A=numpy.random.rand(20), B=numpy.random.rand(20)))
    _maybe_remove(store, 'df_int')
    store.append('df_int', df)
    result = store.select('df_int', [Term('index<10'), Term("columns=['A']")])
    expected = df.reindex(index=list(df.index)[0:10], columns=['A'])
    pandas.util.testing.assert_frame_equal(expected, result)
    tempResult = arange(20, dtype='f8')
	
===================================================================	
TestHDFStore.test_append_series: 620	
----------------------------	

with ensure_clean_store(self.path) as store:
    ss = pandas.util.testing.makeStringSeries()
    ts = pandas.util.testing.makeTimeSeries()
    tempResult = arange(100)
	
===================================================================	
TestHDFStore.test_append_series: 641	
----------------------------	

with ensure_clean_store(self.path) as store:
    ss = pandas.util.testing.makeStringSeries()
    ts = pandas.util.testing.makeTimeSeries()
    ns = Series(numpy.arange(100))
    store.append('ss', ss)
    result = store['ss']
    pandas.util.testing.assert_series_equal(result, ss)
    self.assertIsNone(result.name)
    store.append('ts', ts)
    result = store['ts']
    pandas.util.testing.assert_series_equal(result, ts)
    self.assertIsNone(result.name)
    ns.name = 'foo'
    store.append('ns', ns)
    result = store['ns']
    pandas.util.testing.assert_series_equal(result, ns)
    self.assertEqual(result.name, ns.name)
    expected = ns[(ns > 60)]
    result = store.select('ns', Term('foo>60'))
    pandas.util.testing.assert_series_equal(result, expected)
    expected = ns[((ns > 70) & (ns.index < 90))]
    result = store.select('ns', [Term('foo>70'), Term('index<90')])
    pandas.util.testing.assert_series_equal(result, expected)
    mi = DataFrame(numpy.random.randn(5, 1), columns=['A'])
    tempResult = arange(len(mi))
	
===================================================================	
TestHDFStore.test_tuple_index: 1784	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestHDFStore.test_append_all_nans: 741	
----------------------------	

with ensure_clean_store(self.path) as store:
    tempResult = arange(20)
	
===================================================================	
TestHDFStore.test_append_all_nans: 761	
----------------------------	

with ensure_clean_store(self.path) as store:
    df = DataFrame({'A1': numpy.random.randn(20), 'A2': numpy.random.randn(20)}, index=numpy.arange(20))
    df.ix[0:15, :] = numpy.nan
    _maybe_remove(store, 'df')
    store.append('df', df[:10], dropna=True)
    store.append('df', df[10:], dropna=True)
    pandas.util.testing.assert_frame_equal(store['df'], df[(- 4):])
    _maybe_remove(store, 'df2')
    store.append('df2', df[:10], dropna=False)
    store.append('df2', df[10:], dropna=False)
    pandas.util.testing.assert_frame_equal(store['df2'], df)
    pandas.set_option('io.hdf.dropna_table', False)
    _maybe_remove(store, 'df3')
    store.append('df3', df[:10])
    store.append('df3', df[10:])
    pandas.util.testing.assert_frame_equal(store['df3'], df)
    pandas.set_option('io.hdf.dropna_table', True)
    _maybe_remove(store, 'df4')
    store.append('df4', df[:10])
    store.append('df4', df[10:])
    pandas.util.testing.assert_frame_equal(store['df4'], df[(- 4):])
    tempResult = arange(20)
	
===================================================================	
TestHDFStore.test_append_all_nans: 771	
----------------------------	

with ensure_clean_store(self.path) as store:
    df = DataFrame({'A1': numpy.random.randn(20), 'A2': numpy.random.randn(20)}, index=numpy.arange(20))
    df.ix[0:15, :] = numpy.nan
    _maybe_remove(store, 'df')
    store.append('df', df[:10], dropna=True)
    store.append('df', df[10:], dropna=True)
    pandas.util.testing.assert_frame_equal(store['df'], df[(- 4):])
    _maybe_remove(store, 'df2')
    store.append('df2', df[:10], dropna=False)
    store.append('df2', df[10:], dropna=False)
    pandas.util.testing.assert_frame_equal(store['df2'], df)
    pandas.set_option('io.hdf.dropna_table', False)
    _maybe_remove(store, 'df3')
    store.append('df3', df[:10])
    store.append('df3', df[10:])
    pandas.util.testing.assert_frame_equal(store['df3'], df)
    pandas.set_option('io.hdf.dropna_table', True)
    _maybe_remove(store, 'df4')
    store.append('df4', df[:10])
    store.append('df4', df[10:])
    pandas.util.testing.assert_frame_equal(store['df4'], df[(- 4):])
    df = DataFrame({'A1': numpy.random.randn(20), 'A2': numpy.random.randn(20), 'B': 'foo', 'C': 'bar'}, index=numpy.arange(20))
    df.ix[0:15, :] = numpy.nan
    _maybe_remove(store, 'df')
    store.append('df', df[:10], dropna=True)
    store.append('df', df[10:], dropna=True)
    pandas.util.testing.assert_frame_equal(store['df'], df)
    _maybe_remove(store, 'df2')
    store.append('df2', df[:10], dropna=False)
    store.append('df2', df[10:], dropna=False)
    pandas.util.testing.assert_frame_equal(store['df2'], df)
    tempResult = arange(20)
	
===================================================================	
TestHDFStore.test_append_misc: 1251	
----------------------------	

with ensure_clean_store(self.path) as store:
    with pandas.util.testing.assert_produces_warning(FutureWarning, check_stacklevel=False):
        p4d = pandas.util.testing.makePanel4D()
        self.assertRaises(TypeError, store.put, 'p4d', p4d)
        self.assertRaises(TypeError, store.put, 'abc', None)
        self.assertRaises(TypeError, store.put, 'abc', '123')
        self.assertRaises(TypeError, store.put, 'abc', 123)
        tempResult = arange(5)
	
===================================================================	
TestHDFStore.test_remove_startstop: 1551	
----------------------------	

with ensure_clean_store(self.path) as store:
    wp = pandas.util.testing.makePanel(30)
    _maybe_remove(store, 'wp1')
    store.put('wp1', wp, format='t')
    n = store.remove('wp1', start=32)
    self.assertTrue((n == (120 - 32)))
    result = store.select('wp1')
    expected = wp.reindex(major_axis=wp.major_axis[:(32 // 4)])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp2')
    store.put('wp2', wp, format='t')
    n = store.remove('wp2', start=(- 32))
    self.assertTrue((n == 32))
    result = store.select('wp2')
    expected = wp.reindex(major_axis=wp.major_axis[:((- 32) // 4)])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp3')
    store.put('wp3', wp, format='t')
    n = store.remove('wp3', stop=32)
    self.assertTrue((n == 32))
    result = store.select('wp3')
    expected = wp.reindex(major_axis=wp.major_axis[(32 // 4):])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp4')
    store.put('wp4', wp, format='t')
    n = store.remove('wp4', stop=(- 32))
    self.assertTrue((n == (120 - 32)))
    result = store.select('wp4')
    expected = wp.reindex(major_axis=wp.major_axis[((- 32) // 4):])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp5')
    store.put('wp5', wp, format='t')
    n = store.remove('wp5', start=16, stop=(- 16))
    self.assertTrue((n == (120 - 32)))
    result = store.select('wp5')
    expected = wp.reindex(major_axis=wp.major_axis[:(16 // 4)].union(wp.major_axis[((- 16) // 4):]))
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp6')
    store.put('wp6', wp, format='t')
    n = store.remove('wp6', start=16, stop=16)
    self.assertTrue((n == 0))
    result = store.select('wp6')
    expected = wp.reindex(major_axis=wp.major_axis)
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp7')
    tempResult = arange(0, 30, 3)
	
===================================================================	
TestHDFStore.test_remove_startstop: 1557	
----------------------------	

with ensure_clean_store(self.path) as store:
    wp = pandas.util.testing.makePanel(30)
    _maybe_remove(store, 'wp1')
    store.put('wp1', wp, format='t')
    n = store.remove('wp1', start=32)
    self.assertTrue((n == (120 - 32)))
    result = store.select('wp1')
    expected = wp.reindex(major_axis=wp.major_axis[:(32 // 4)])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp2')
    store.put('wp2', wp, format='t')
    n = store.remove('wp2', start=(- 32))
    self.assertTrue((n == 32))
    result = store.select('wp2')
    expected = wp.reindex(major_axis=wp.major_axis[:((- 32) // 4)])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp3')
    store.put('wp3', wp, format='t')
    n = store.remove('wp3', stop=32)
    self.assertTrue((n == 32))
    result = store.select('wp3')
    expected = wp.reindex(major_axis=wp.major_axis[(32 // 4):])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp4')
    store.put('wp4', wp, format='t')
    n = store.remove('wp4', stop=(- 32))
    self.assertTrue((n == (120 - 32)))
    result = store.select('wp4')
    expected = wp.reindex(major_axis=wp.major_axis[((- 32) // 4):])
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp5')
    store.put('wp5', wp, format='t')
    n = store.remove('wp5', start=16, stop=(- 16))
    self.assertTrue((n == (120 - 32)))
    result = store.select('wp5')
    expected = wp.reindex(major_axis=wp.major_axis[:(16 // 4)].union(wp.major_axis[((- 16) // 4):]))
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp6')
    store.put('wp6', wp, format='t')
    n = store.remove('wp6', start=16, stop=16)
    self.assertTrue((n == 0))
    result = store.select('wp6')
    expected = wp.reindex(major_axis=wp.major_axis)
    assert_panel_equal(result, expected)
    _maybe_remove(store, 'wp7')
    date = wp.major_axis.take(numpy.arange(0, 30, 3))
    crit = Term('major_axis=date')
    store.put('wp7', wp, format='t')
    n = store.remove('wp7', where=[crit], stop=80)
    self.assertTrue((n == 28))
    result = store.select('wp7')
    tempResult = arange(0, 20, 3)
	
===================================================================	
TestHDFStore.test_coordinates: 2517	
----------------------------	

df = pandas.util.testing.makeTimeDataFrame()
with ensure_clean_store(self.path) as store:
    _maybe_remove(store, 'df')
    store.append('df', df)
    c = store.select_as_coordinates('df')
    tempResult = arange(len(df.index))
	
===================================================================	
TestHDFStore.test_coordinates: 2522	
----------------------------	

df = pandas.util.testing.makeTimeDataFrame()
with ensure_clean_store(self.path) as store:
    _maybe_remove(store, 'df')
    store.append('df', df)
    c = store.select_as_coordinates('df')
    assert (c.values == np.arange(len(df.index))).all()
    _maybe_remove(store, 'df')
    df = DataFrame(dict(A=lrange(5), B=lrange(5)))
    store.append('df', df)
    c = store.select_as_coordinates('df', ['index<3'])
    tempResult = arange(3)
	
===================================================================	
TestHDFStore.test_coordinates: 2527	
----------------------------	

df = pandas.util.testing.makeTimeDataFrame()
with ensure_clean_store(self.path) as store:
    _maybe_remove(store, 'df')
    store.append('df', df)
    c = store.select_as_coordinates('df')
    assert (c.values == np.arange(len(df.index))).all()
    _maybe_remove(store, 'df')
    df = DataFrame(dict(A=lrange(5), B=lrange(5)))
    store.append('df', df)
    c = store.select_as_coordinates('df', ['index<3'])
    assert (c.values == np.arange(3)).all()
    result = store.select('df', where=c)
    expected = df.ix[0:2, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    c = store.select_as_coordinates('df', ['index>=3', 'index<=4'])
    tempResult = arange(2)
	
===================================================================	
TestHDFStore.test_coordinates: 2555	
----------------------------	

df = pandas.util.testing.makeTimeDataFrame()
with ensure_clean_store(self.path) as store:
    _maybe_remove(store, 'df')
    store.append('df', df)
    c = store.select_as_coordinates('df')
    assert (c.values == np.arange(len(df.index))).all()
    _maybe_remove(store, 'df')
    df = DataFrame(dict(A=lrange(5), B=lrange(5)))
    store.append('df', df)
    c = store.select_as_coordinates('df', ['index<3'])
    assert (c.values == np.arange(3)).all()
    result = store.select('df', where=c)
    expected = df.ix[0:2, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    c = store.select_as_coordinates('df', ['index>=3', 'index<=4'])
    assert (c.values == (np.arange(2) + 3)).all()
    result = store.select('df', where=c)
    expected = df.ix[3:4, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    self.assertIsInstance(c, Index)
    _maybe_remove(store, 'df1')
    _maybe_remove(store, 'df2')
    df1 = pandas.util.testing.makeTimeDataFrame()
    df2 = tm.makeTimeDataFrame().rename(columns=(lambda x: ('%s_2' % x)))
    store.append('df1', df1, data_columns=['A', 'B'])
    store.append('df2', df2)
    c = store.select_as_coordinates('df1', ['A>0', 'B>0'])
    df1_result = store.select('df1', c)
    df2_result = store.select('df2', c)
    result = concat([df1_result, df2_result], axis=1)
    expected = concat([df1, df2], axis=1)
    expected = expected[((expected.A > 0) & (expected.B > 0))]
    pandas.util.testing.assert_frame_equal(result, expected)
with ensure_clean_store(self.path) as store:
    df = DataFrame(numpy.random.randn(1000, 2), index=date_range('20000101', periods=1000))
    store.append('df', df)
    c = store.select_column('df', 'index')
    where = c[(DatetimeIndex(c).month == 5)].index
    expected = df.iloc[where]
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    tempResult = arange(len(df), dtype='float64')
	
===================================================================	
TestHDFStore.test_coordinates: 2556	
----------------------------	

df = pandas.util.testing.makeTimeDataFrame()
with ensure_clean_store(self.path) as store:
    _maybe_remove(store, 'df')
    store.append('df', df)
    c = store.select_as_coordinates('df')
    assert (c.values == np.arange(len(df.index))).all()
    _maybe_remove(store, 'df')
    df = DataFrame(dict(A=lrange(5), B=lrange(5)))
    store.append('df', df)
    c = store.select_as_coordinates('df', ['index<3'])
    assert (c.values == np.arange(3)).all()
    result = store.select('df', where=c)
    expected = df.ix[0:2, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    c = store.select_as_coordinates('df', ['index>=3', 'index<=4'])
    assert (c.values == (np.arange(2) + 3)).all()
    result = store.select('df', where=c)
    expected = df.ix[3:4, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    self.assertIsInstance(c, Index)
    _maybe_remove(store, 'df1')
    _maybe_remove(store, 'df2')
    df1 = pandas.util.testing.makeTimeDataFrame()
    df2 = tm.makeTimeDataFrame().rename(columns=(lambda x: ('%s_2' % x)))
    store.append('df1', df1, data_columns=['A', 'B'])
    store.append('df2', df2)
    c = store.select_as_coordinates('df1', ['A>0', 'B>0'])
    df1_result = store.select('df1', c)
    df2_result = store.select('df2', c)
    result = concat([df1_result, df2_result], axis=1)
    expected = concat([df1, df2], axis=1)
    expected = expected[((expected.A > 0) & (expected.B > 0))]
    pandas.util.testing.assert_frame_equal(result, expected)
with ensure_clean_store(self.path) as store:
    df = DataFrame(numpy.random.randn(1000, 2), index=date_range('20000101', periods=1000))
    store.append('df', df)
    c = store.select_column('df', 'index')
    where = c[(DatetimeIndex(c).month == 5)].index
    expected = df.iloc[where]
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    self.assertRaises(ValueError, store.select, 'df', where=numpy.arange(len(df), dtype='float64'))
    tempResult = arange((len(df) + 1))
	
===================================================================	
TestHDFStore.test_coordinates: 2557	
----------------------------	

df = pandas.util.testing.makeTimeDataFrame()
with ensure_clean_store(self.path) as store:
    _maybe_remove(store, 'df')
    store.append('df', df)
    c = store.select_as_coordinates('df')
    assert (c.values == np.arange(len(df.index))).all()
    _maybe_remove(store, 'df')
    df = DataFrame(dict(A=lrange(5), B=lrange(5)))
    store.append('df', df)
    c = store.select_as_coordinates('df', ['index<3'])
    assert (c.values == np.arange(3)).all()
    result = store.select('df', where=c)
    expected = df.ix[0:2, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    c = store.select_as_coordinates('df', ['index>=3', 'index<=4'])
    assert (c.values == (np.arange(2) + 3)).all()
    result = store.select('df', where=c)
    expected = df.ix[3:4, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    self.assertIsInstance(c, Index)
    _maybe_remove(store, 'df1')
    _maybe_remove(store, 'df2')
    df1 = pandas.util.testing.makeTimeDataFrame()
    df2 = tm.makeTimeDataFrame().rename(columns=(lambda x: ('%s_2' % x)))
    store.append('df1', df1, data_columns=['A', 'B'])
    store.append('df2', df2)
    c = store.select_as_coordinates('df1', ['A>0', 'B>0'])
    df1_result = store.select('df1', c)
    df2_result = store.select('df2', c)
    result = concat([df1_result, df2_result], axis=1)
    expected = concat([df1, df2], axis=1)
    expected = expected[((expected.A > 0) & (expected.B > 0))]
    pandas.util.testing.assert_frame_equal(result, expected)
with ensure_clean_store(self.path) as store:
    df = DataFrame(numpy.random.randn(1000, 2), index=date_range('20000101', periods=1000))
    store.append('df', df)
    c = store.select_column('df', 'index')
    where = c[(DatetimeIndex(c).month == 5)].index
    expected = df.iloc[where]
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    self.assertRaises(ValueError, store.select, 'df', where=numpy.arange(len(df), dtype='float64'))
    self.assertRaises(ValueError, store.select, 'df', where=numpy.arange((len(df) + 1)))
    tempResult = arange(len(df))
	
===================================================================	
TestHDFStore.test_coordinates: 2558	
----------------------------	

df = pandas.util.testing.makeTimeDataFrame()
with ensure_clean_store(self.path) as store:
    _maybe_remove(store, 'df')
    store.append('df', df)
    c = store.select_as_coordinates('df')
    assert (c.values == np.arange(len(df.index))).all()
    _maybe_remove(store, 'df')
    df = DataFrame(dict(A=lrange(5), B=lrange(5)))
    store.append('df', df)
    c = store.select_as_coordinates('df', ['index<3'])
    assert (c.values == np.arange(3)).all()
    result = store.select('df', where=c)
    expected = df.ix[0:2, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    c = store.select_as_coordinates('df', ['index>=3', 'index<=4'])
    assert (c.values == (np.arange(2) + 3)).all()
    result = store.select('df', where=c)
    expected = df.ix[3:4, :]
    pandas.util.testing.assert_frame_equal(result, expected)
    self.assertIsInstance(c, Index)
    _maybe_remove(store, 'df1')
    _maybe_remove(store, 'df2')
    df1 = pandas.util.testing.makeTimeDataFrame()
    df2 = tm.makeTimeDataFrame().rename(columns=(lambda x: ('%s_2' % x)))
    store.append('df1', df1, data_columns=['A', 'B'])
    store.append('df2', df2)
    c = store.select_as_coordinates('df1', ['A>0', 'B>0'])
    df1_result = store.select('df1', c)
    df2_result = store.select('df2', c)
    result = concat([df1_result, df2_result], axis=1)
    expected = concat([df1, df2], axis=1)
    expected = expected[((expected.A > 0) & (expected.B > 0))]
    pandas.util.testing.assert_frame_equal(result, expected)
with ensure_clean_store(self.path) as store:
    df = DataFrame(numpy.random.randn(1000, 2), index=date_range('20000101', periods=1000))
    store.append('df', df)
    c = store.select_column('df', 'index')
    where = c[(DatetimeIndex(c).month == 5)].index
    expected = df.iloc[where]
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    result = store.select('df', where=where)
    pandas.util.testing.assert_frame_equal(result, expected)
    self.assertRaises(ValueError, store.select, 'df', where=numpy.arange(len(df), dtype='float64'))
    self.assertRaises(ValueError, store.select, 'df', where=numpy.arange((len(df) + 1)))
    self.assertRaises(ValueError, store.select, 'df', where=numpy.arange(len(df)), start=5)
    tempResult = arange(len(df))
	
===================================================================	
TestHDFStore.test_append: 608	
----------------------------	

with ensure_clean_store(self.path) as store:
    df = pandas.util.testing.makeTimeDataFrame()
    _maybe_remove(store, 'df1')
    store.append('df1', df[:10])
    store.append('df1', df[10:])
    pandas.util.testing.assert_frame_equal(store['df1'], df)
    _maybe_remove(store, 'df2')
    store.put('df2', df[:10], format='table')
    store.append('df2', df[10:])
    pandas.util.testing.assert_frame_equal(store['df2'], df)
    _maybe_remove(store, 'df3')
    store.append('/df3', df[:10])
    store.append('/df3', df[10:])
    pandas.util.testing.assert_frame_equal(store['df3'], df)
    with pandas.util.testing.assert_produces_warning(expected_warning=tables.NaturalNameWarning):
        _maybe_remove(store, '/df3 foo')
        store.append('/df3 foo', df[:10])
        store.append('/df3 foo', df[10:])
        pandas.util.testing.assert_frame_equal(store['df3 foo'], df)
    wp = pandas.util.testing.makePanel()
    _maybe_remove(store, 'wp1')
    store.append('wp1', wp.ix[:, :10, :])
    store.append('wp1', wp.ix[:, 10:, :])
    assert_panel_equal(store['wp1'], wp)
    with pandas.util.testing.assert_produces_warning(FutureWarning, check_stacklevel=False):
        p4d = pandas.util.testing.makePanel4D()
        _maybe_remove(store, 'p4d')
        store.append('p4d', p4d.ix[:, :, :10, :])
        store.append('p4d', p4d.ix[:, :, 10:, :])
        assert_panel4d_equal(store['p4d'], p4d)
        _maybe_remove(store, 'p4d')
        store.append('p4d', p4d.ix[:, :, :10, :], axes=['items', 'major_axis', 'minor_axis'])
        store.append('p4d', p4d.ix[:, :, 10:, :], axes=['items', 'major_axis', 'minor_axis'])
        assert_panel4d_equal(store['p4d'], p4d)
        p4d2 = p4d.copy()
        p4d2['l4'] = p4d['l1']
        p4d2['l5'] = p4d['l1']
        _maybe_remove(store, 'p4d2')
        store.append('p4d2', p4d2, axes=['items', 'major_axis', 'minor_axis'])
        assert_panel4d_equal(store['p4d2'], p4d2)
    _maybe_remove(store, 'wp1')
    wp_append1 = wp.ix[:, :10, :]
    store.append('wp1', wp_append1)
    wp_append2 = wp.ix[:, 10:, :].reindex(items=wp.items[::(- 1)])
    store.append('wp1', wp_append2)
    assert_panel_equal(store['wp1'], wp)
    df = DataFrame(data=[[1, 2], [0, 1], [1, 2], [0, 0]])
    df['mixed_column'] = 'testing'
    df.ix[(2, 'mixed_column')] = numpy.nan
    _maybe_remove(store, 'df')
    store.append('df', df)
    pandas.util.testing.assert_frame_equal(store['df'], df)
    tempResult = arange(5)
	
===================================================================	
TestHDFStore.test_put_string_index: 497	
----------------------------	

with ensure_clean_store(self.path) as store:
    index = Index([('I am a very long string index: %s' % i) for i in range(20)])
    tempResult = arange(20)
	
===================================================================	
TestHDFStore.test_put_string_index: 504	
----------------------------	

with ensure_clean_store(self.path) as store:
    index = Index([('I am a very long string index: %s' % i) for i in range(20)])
    s = Series(numpy.arange(20), index=index)
    df = DataFrame({'A': s, 'B': s})
    store['a'] = s
    pandas.util.testing.assert_series_equal(store['a'], s)
    store['b'] = df
    pandas.util.testing.assert_frame_equal(store['b'], df)
    index = Index((['abcdefghijklmnopqrstuvwxyz1234567890'] + [('I am a very long string index: %s' % i) for i in range(20)]))
    tempResult = arange(21)
	
===================================================================	
TestHDFStore.test_append_some_nans: 714	
----------------------------	

with ensure_clean_store(self.path) as store:
    tempResult = arange(20)
	
===================================================================	
_TestSQLAlchemy.test_datetime: 779	
----------------------------	

tempResult = arange(3.0)
	
===================================================================	
_TestSQLApi.test_to_sql_series: 286	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
_TestSQLAlchemy.test_datetime_NaT: 794	
----------------------------	

tempResult = arange(3.0)
	
===================================================================	
TestStata.test_nan_to_missing_value: 319	
----------------------------	

tempResult = arange(4.0)
	
===================================================================	
TestStata.test_nan_to_missing_value: 320	
----------------------------	

s1 = Series(numpy.arange(4.0), dtype=numpy.float32)
tempResult = arange(4.0)
	
===================================================================	
TestStata.test_categorical_order: 630	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestStata.test_categorical_order: 630	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestStata.test_categorical_order: 630	
----------------------------	

tempResult = arange(0, 5)
	
===================================================================	
TestStata.test_categorical_order: 630	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestStata.test_categorical_order: 630	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestStata.test_no_index: 333	
----------------------------	

columns = ['x', 'y']
tempResult = arange(10.0)
	
===================================================================	
TestStata.test_categorical_sorting: 652	
----------------------------	

parsed_115 = read_stata(self.dta20_115)
parsed_117 = read_stata(self.dta20_117)
parsed_115 = parsed_115.sort_values('srh')
parsed_117 = parsed_117.sort_values('srh')
tempResult = arange(parsed_115.shape[0])
	
===================================================================	
TestStata.test_categorical_sorting: 653	
----------------------------	

parsed_115 = read_stata(self.dta20_115)
parsed_117 = read_stata(self.dta20_117)
parsed_115 = parsed_115.sort_values('srh')
parsed_117 = parsed_117.sort_values('srh')
parsed_115.index = numpy.arange(parsed_115.shape[0])
tempResult = arange(parsed_117.shape[0])
	
===================================================================	
TestStata.test_numeric_column_names: 306	
----------------------------	

tempResult = arange(25.0)
	
===================================================================	
module: 21	
----------------------------	

import nose
from pandas.compat import range, lrange, StringIO, OrderedDict
import os
import numpy as np
from pandas import Series, DataFrame, DatetimeIndex, Timestamp, read_json, compat
from datetime import timedelta
import pandas as pd
from pandas.util.testing import assert_almost_equal, assert_frame_equal, assert_series_equal, network, ensure_clean, assert_index_equal
import pandas.util.testing as tm
_seriesd = pandas.util.testing.getSeriesData()
_tsd = pandas.util.testing.getTimeSeriesData()
_frame = DataFrame(_seriesd)
_frame2 = DataFrame(_seriesd, columns=['D', 'C', 'B', 'A'])
_intframe = DataFrame(dict(((k, v.astype(numpy.int64)) for (k, v) in pandas.compat.iteritems(_seriesd))))
_tsframe = DataFrame(_tsd)
_cat_frame = _frame.copy()
cat = ((((['bah'] * 5) + (['bar'] * 5)) + (['baz'] * 5)) + (['foo'] * (len(_cat_frame) - 15)))
_cat_frame.index = pandas.CategoricalIndex(cat, name='E')
_cat_frame['E'] = list(reversed(cat))
tempResult = arange(len(_cat_frame), dtype='int64')
	
===================================================================	
TestPandasContainer.test_frame_mixedtype_orient: 249	
----------------------------	

vals = [[10, 1, 'foo', 0.1, 0.01], [20, 2, 'bar', 0.2, 0.02], [30, 3, 'baz', 0.3, 0.03], [40, 4, 'qux', 0.4, 0.04]]
df = DataFrame(vals, index=list('abcd'), columns=['1st', '2nd', '3rd', '4th', '5th'])
self.assertTrue(df._is_mixed_type)
right = df.copy()
for orient in ['split', 'index', 'columns']:
    inp = df.to_json(orient=orient)
    left = read_json(inp, orient=orient, convert_axes=False)
    assert_frame_equal(left, right)
tempResult = arange(len(df))
	
===================================================================	
TestPandasContainer.test_frame_mixedtype_orient: 253	
----------------------------	

vals = [[10, 1, 'foo', 0.1, 0.01], [20, 2, 'bar', 0.2, 0.02], [30, 3, 'baz', 0.3, 0.03], [40, 4, 'qux', 0.4, 0.04]]
df = DataFrame(vals, index=list('abcd'), columns=['1st', '2nd', '3rd', '4th', '5th'])
self.assertTrue(df._is_mixed_type)
right = df.copy()
for orient in ['split', 'index', 'columns']:
    inp = df.to_json(orient=orient)
    left = read_json(inp, orient=orient, convert_axes=False)
    assert_frame_equal(left, right)
right.index = numpy.arange(len(df))
inp = df.to_json(orient='records')
left = read_json(inp, orient='records', convert_axes=False)
assert_frame_equal(left, right)
tempResult = arange(df.shape[1])
	
===================================================================	
NumpyJSONTests.testFloatArray: 893	
----------------------------	

tempResult = arange(12.5, 185.72, 1.7322, dtype=numpy.float)
	
===================================================================	
NumpyJSONTests.testArrays: 909	
----------------------------	

tempResult = arange(100)
	
===================================================================	
NumpyJSONTests.testArrays: 919	
----------------------------	

arr = numpy.arange(100)
arr = arr.reshape((10, 10))
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
pandas.util.testing.assert_numpy_array_equal(pandas.json.decode(pandas.json.encode(arr), numpy=True), arr)
arr = arr.reshape((5, 5, 4))
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
pandas.util.testing.assert_numpy_array_equal(pandas.json.decode(pandas.json.encode(arr), numpy=True), arr)
arr = arr.reshape((100, 1))
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
pandas.util.testing.assert_numpy_array_equal(pandas.json.decode(pandas.json.encode(arr), numpy=True), arr)
tempResult = arange(96)
	
===================================================================	
NumpyJSONTests.testArrays: 926	
----------------------------	

arr = numpy.arange(100)
arr = arr.reshape((10, 10))
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
pandas.util.testing.assert_numpy_array_equal(pandas.json.decode(pandas.json.encode(arr), numpy=True), arr)
arr = arr.reshape((5, 5, 4))
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
pandas.util.testing.assert_numpy_array_equal(pandas.json.decode(pandas.json.encode(arr), numpy=True), arr)
arr = arr.reshape((100, 1))
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
pandas.util.testing.assert_numpy_array_equal(pandas.json.decode(pandas.json.encode(arr), numpy=True), arr)
arr = numpy.arange(96)
arr = arr.reshape((2, 2, 2, 2, 3, 2))
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
pandas.util.testing.assert_numpy_array_equal(pandas.json.decode(pandas.json.encode(arr), numpy=True), arr)
l = ['a', list(), dict(), dict(), list(), 42, 97.8, ['a', 'b'], {'key': 'val'}]
arr = numpy.array(l)
pandas.util.testing.assert_numpy_array_equal(numpy.array(pandas.json.decode(pandas.json.encode(arr))), arr)
tempResult = arange(100.202, 200.202, 1, dtype=numpy.float32)
	
===================================================================	
NumpyJSONTests.testIntArray: 856	
----------------------------	

tempResult = arange(100, dtype=numpy.int)
	
===================================================================	
_construct_dataframe: 17	
----------------------------	

df = DataFrame(numpy.random.rand(num_rows, 5), columns=list('abcde'))
df['foo'] = 'foo'
df['bar'] = 'bar'
df['baz'] = 'baz'
df['date'] = pandas.date_range('20000101 09:00:00', periods=num_rows, freq='s')
tempResult = arange(num_rows, dtype='int64')
	
===================================================================	
SkipRowsTests.test_skiprows_blank: 30	
----------------------------	

text = '#foo,a,b,c\n#foo,a,b,c\n\n#foo,a,b,c\n#foo,a,b,c\n\n1/1/2000,1.,2.,3.\n1/2/2000,4,5,6\n1/3/2000,7,8,9\n'
data = self.read_csv(StringIO(text), skiprows=6, header=None, index_col=0, parse_dates=True)
tempResult = arange(1.0, 10.0)
	
===================================================================	
SkipRowsTests.test_skiprows_bug: 15	
----------------------------	

text = '#foo,a,b,c\n#foo,a,b,c\n#foo,a,b,c\n#foo,a,b,c\n#foo,a,b,c\n#foo,a,b,c\n1/1/2000,1.,2.,3.\n1/2/2000,4,5,6\n1/3/2000,7,8,9\n'
data = self.read_csv(StringIO(text), skiprows=lrange(6), header=None, index_col=0, parse_dates=True)
data2 = self.read_csv(StringIO(text), skiprows=6, header=None, index_col=0, parse_dates=True)
tempResult = arange(1.0, 10.0)
	
===================================================================	
TestCommon.test_convert_frame: 34	
----------------------------	

df = r['faithful']
converted = pandas.rpy.common.convert_robj(df)
assert numpy.array_equal(converted.columns, ['eruptions', 'waiting'])
tempResult = arange(1, 273)
	
===================================================================	
SparseArray.take: 333	
----------------------------	

'\n        Sparse-compatible version of ndarray.take\n\n        Returns\n        -------\n        taken : ndarray\n        '
pandas.compat.numpy.function.validate_take(tuple(), kwargs)
if axis:
    raise ValueError('axis must be 0, input was {0}'.format(axis))
if is_integer(indices):
    return self[indices]
indices = _ensure_platform_int(indices)
n = len(self)
if (allow_fill and (fill_value is not None)):
    if (indices < (- 1)).any():
        msg = 'When allow_fill=True and fill_value is not None, all indices must be >= -1'
        raise ValueError(msg)
    elif (n <= indices).any():
        msg = 'index is out of bounds for size {0}'
        raise IndexError(msg.format(n))
elif ((indices < (- n)) | (n <= indices)).any():
    msg = 'index is out of bounds for size {0}'
    raise IndexError(msg.format(n))
indices = indices.astype(numpy.int32)
if (not (allow_fill and (fill_value is not None))):
    indices = indices.copy()
    indices[(indices < 0)] += n
locs = self.sp_index.lookup_array(indices)
tempResult = arange(len(locs), dtype=numpy.int32)
	
===================================================================	
make_sparse: 502	
----------------------------	

"\n    Convert ndarray to sparse format\n\n    Parameters\n    ----------\n    arr : ndarray\n    kind : {'block', 'integer'}\n    fill_value : NaN or another value\n\n    Returns\n    -------\n    (sparse_values, index) : (ndarray, SparseIndex)\n    "
arr = _sanitize_values(arr)
if (arr.ndim > 1):
    raise TypeError('expected dimension <= 1 data')
if (fill_value is None):
    fill_value = na_value_for_dtype(arr.dtype)
if isnull(fill_value):
    mask = notnull(arr)
else:
    mask = (arr != fill_value)
length = len(arr)
if (length != mask.size):
    indices = mask.sp_index.indices
else:
    tempResult = arange(length, dtype=numpy.int32)
	
===================================================================	
stack_sparse_frame: 489	
----------------------------	

'\n    Only makes sense when fill_value is NaN\n    '
lengths = [s.sp_index.npoints for (_, s) in pandas.compat.iteritems(frame)]
nobs = sum(lengths)
tempResult = arange(len(frame.columns))
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 318	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 319	
----------------------------	

da = pandas.Series(numpy.arange(4))
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 320	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 321	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 323	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 324	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 326	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=numpy.nan)
self._check_numeric_ops(sa, sb, da, db)
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 327	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=numpy.nan)
self._check_numeric_ops(sa, sb, da, db)
da = pandas.Series(numpy.arange(4))
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 328	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=numpy.nan)
self._check_numeric_ops(sa, sb, da, db)
da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[10, 11, 12, 13])
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 329	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=numpy.nan)
self._check_numeric_ops(sa, sb, da, db)
da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[10, 11, 12, 13])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 331	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=numpy.nan)
self._check_numeric_ops(sa, sb, da, db)
da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[10, 11, 12, 13])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[10, 11, 12, 13], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
tempResult = arange(4)
	
===================================================================	
TestSparseSeriesArithmetic.test_alignment: 332	
----------------------------	

da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[1, 2, 3, 4])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
sb = pandas.SparseSeries(numpy.arange(4), index=[1, 2, 3, 4], dtype=numpy.int64, fill_value=numpy.nan)
self._check_numeric_ops(sa, sb, da, db)
da = pandas.Series(numpy.arange(4))
db = pandas.Series(numpy.arange(4), index=[10, 11, 12, 13])
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=0)
sb = pandas.SparseSeries(numpy.arange(4), index=[10, 11, 12, 13], dtype=numpy.int64, fill_value=0)
self._check_numeric_ops(sa, sb, da, db)
sa = pandas.SparseSeries(numpy.arange(4), dtype=numpy.int64, fill_value=numpy.nan)
tempResult = arange(4)
	
===================================================================	
TestSparseArrayAnalytics.test_numpy_mean: 576	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSparseArrayAnalytics.test_mean: 568	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSparseArrayAnalytics.test_cumsum: 538	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSparseArray.test_constructor_from_too_large_array: 223	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSparseArrayAnalytics.test_numpy_cumsum: 551	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSparseArrayAnalytics.test_numpy_sum: 524	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSparseArrayAnalytics.test_sum: 514	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSparseDataFrame.setUp: 22	
----------------------------	

tempResult = arange(10, dtype=numpy.float64)
	
===================================================================	
TestSparseDataFrame.test_density: 191	
----------------------------	

df = SparseSeries([nan, nan, nan, 0, 1, 2, 3, 4, 5, 6])
self.assertEqual(df.density, 0.7)
tempResult = arange(10)
	
===================================================================	
TestSparseDataFrameAnalytics.setUp: 698	
----------------------------	

tempResult = arange(10, dtype=float)
	
===================================================================	
TestSparseDataFrame.test_as_matrix: 48	
----------------------------	

empty = self.empty.as_matrix()
self.assertEqual(empty.shape, (0, 0))
tempResult = arange(10)
	
===================================================================	
TestSparseDataFrame.test_as_matrix: 51	
----------------------------	

empty = self.empty.as_matrix()
self.assertEqual(empty.shape, (0, 0))
no_cols = SparseDataFrame(index=numpy.arange(10))
mat = no_cols.as_matrix()
self.assertEqual(mat.shape, (10, 0))
tempResult = arange(10)
	
===================================================================	
TestSparseDataFrame.test_constructor: 68	
----------------------------	

for (col, series) in pandas.compat.iteritems(self.frame):
    pandas.util.testing.assertIsInstance(series, SparseSeries)
pandas.util.testing.assertIsInstance(self.iframe['A'].sp_index, IntIndex)
self.assertEqual(self.zframe['A'].fill_value, 0)
pandas.util.testing.assert_numpy_array_equal(pandas.SparseArray([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]), self.zframe['A'].values)
pandas.util.testing.assert_numpy_array_equal(numpy.array([0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]), self.zframe['A'].to_dense().values)
tempResult = arange(10)
	
===================================================================	
TestSparseDataFrame.test_constructor: 68	
----------------------------	

for (col, series) in pandas.compat.iteritems(self.frame):
    pandas.util.testing.assertIsInstance(series, SparseSeries)
pandas.util.testing.assertIsInstance(self.iframe['A'].sp_index, IntIndex)
self.assertEqual(self.zframe['A'].fill_value, 0)
pandas.util.testing.assert_numpy_array_equal(pandas.SparseArray([1.0, 2.0, 3.0, 4.0, 5.0, 6.0]), self.zframe['A'].values)
pandas.util.testing.assert_numpy_array_equal(numpy.array([0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]), self.zframe['A'].to_dense().values)
tempResult = arange(10)
	
===================================================================	
TestBlockIndex.test_make_block_boundary: 315	
----------------------------	

for i in [5, 10, 100, 101]:
    tempResult = arange(0, i, 2, dtype=numpy.int32)
	
===================================================================	
TestBlockIndex.test_make_block_boundary: 316	
----------------------------	

for i in [5, 10, 100, 101]:
    idx = _make_index(i, numpy.arange(0, i, 2, dtype=numpy.int32), kind='block')
    tempResult = arange(0, i, 2, dtype=numpy.int32)
	
===================================================================	
TestSparseOperators._check_case: 392	
----------------------------	

xindex = BlockIndex(TEST_LENGTH, xloc, xlen)
yindex = BlockIndex(TEST_LENGTH, yloc, ylen)
xdindex = xindex.to_int_index()
ydindex = yindex.to_int_index()
tempResult = arange(xindex.npoints)
	
===================================================================	
TestSparseOperators._check_case: 393	
----------------------------	

xindex = BlockIndex(TEST_LENGTH, xloc, xlen)
yindex = BlockIndex(TEST_LENGTH, yloc, ylen)
xdindex = xindex.to_int_index()
ydindex = yindex.to_int_index()
x = ((numpy.arange(xindex.npoints) * 10.0) + 1)
tempResult = arange(yindex.npoints)
	
===================================================================	
TestSparseOperators._check_case: 402	
----------------------------	

xindex = BlockIndex(TEST_LENGTH, xloc, xlen)
yindex = BlockIndex(TEST_LENGTH, yloc, ylen)
xdindex = xindex.to_int_index()
ydindex = yindex.to_int_index()
x = ((numpy.arange(xindex.npoints) * 10.0) + 1)
y = ((numpy.arange(yindex.npoints) * 100.0) + 1)
xfill = 0
yfill = 2
(result_block_vals, rb_index, bfill) = sparse_op(x, xindex, xfill, y, yindex, yfill)
(result_int_vals, ri_index, ifill) = sparse_op(x, xdindex, xfill, y, ydindex, yfill)
self.assertTrue(rb_index.to_int_index().equals(ri_index))
pandas.util.testing.assert_numpy_array_equal(result_block_vals, result_int_vals)
self.assertEqual(bfill, ifill)
xseries = Series(x, xdindex.indices)
tempResult = arange(TEST_LENGTH)
	
===================================================================	
TestSparseOperators._check_case: 404	
----------------------------	

xindex = BlockIndex(TEST_LENGTH, xloc, xlen)
yindex = BlockIndex(TEST_LENGTH, yloc, ylen)
xdindex = xindex.to_int_index()
ydindex = yindex.to_int_index()
x = ((numpy.arange(xindex.npoints) * 10.0) + 1)
y = ((numpy.arange(yindex.npoints) * 100.0) + 1)
xfill = 0
yfill = 2
(result_block_vals, rb_index, bfill) = sparse_op(x, xindex, xfill, y, yindex, yfill)
(result_int_vals, ri_index, ifill) = sparse_op(x, xdindex, xfill, y, ydindex, yfill)
self.assertTrue(rb_index.to_int_index().equals(ri_index))
pandas.util.testing.assert_numpy_array_equal(result_block_vals, result_int_vals)
self.assertEqual(bfill, ifill)
xseries = Series(x, xdindex.indices)
xseries = xseries.reindex(np.arange(TEST_LENGTH)).fillna(xfill)
yseries = Series(y, ydindex.indices)
tempResult = arange(TEST_LENGTH)
	
===================================================================	
_test_data2: 27	
----------------------------	

tempResult = arange(15, dtype=float)
	
===================================================================	
_test_data2: 28	
----------------------------	

arr = numpy.arange(15, dtype=float)
tempResult = arange(15)
	
===================================================================	
TestSparseSeries.test_sparse_reindex: 517	
----------------------------	

length = 10

def _check(values, index1, index2, fill_value):
    first_series = SparseSeries(values, sparse_index=index1, fill_value=fill_value)
    reindexed = first_series.sparse_reindex(index2)
    self.assertIs(reindexed.sp_index, index2)
    int_indices1 = index1.to_int_index().indices
    int_indices2 = index2.to_int_index().indices
    expected = Series(values, index=int_indices1)
    expected = expected.reindex(int_indices2).fillna(fill_value)
    pandas.util.testing.assert_almost_equal(expected.values, reindexed.sp_values)
    expected = expected.reindex(int_indices2).fillna(fill_value)

def _check_with_fill_value(values, first, second, fill_value=nan):
    i_index1 = IntIndex(length, first)
    i_index2 = IntIndex(length, second)
    b_index1 = i_index1.to_block_index()
    b_index2 = i_index2.to_block_index()
    _check(values, i_index1, i_index2, fill_value)
    _check(values, b_index1, b_index2, fill_value)

def _check_all(values, first, second):
    _check_with_fill_value(values, first, second, fill_value=nan)
    _check_with_fill_value(values, first, second, fill_value=0)
index1 = [2, 4, 5, 6, 8, 9]
tempResult = arange(6.0)
	
===================================================================	
TestSparseSeries.test_fill_value_when_combine_const: 613	
----------------------------	

tempResult = arange(6)
	
===================================================================	
_test_data1: 19	
----------------------------	

tempResult = arange(20, dtype=float)
	
===================================================================	
_test_data1: 20	
----------------------------	

arr = numpy.arange(20, dtype=float)
tempResult = arange(20)
	
===================================================================	
TestSparseSeries.test_shift: 619	
----------------------------	

tempResult = arange(6)
	
===================================================================	
Get no callers of function numpy.arange at line 483 col 33.	
===================================================================	
TestSparseSeries.test_homogenize: 598	
----------------------------	


def _check_matches(indices, expected):
    data = {}
    for (i, idx) in enumerate(indices):
        data[i] = SparseSeries(idx.to_int_index().indices, sparse_index=idx, fill_value=numpy.nan)
    homogenized = pandas.sparse.frame.homogenize(data)
    for (k, v) in pandas.compat.iteritems(homogenized):
        assert v.sp_index.equals(expected)
indices1 = [BlockIndex(10, [2], [7]), BlockIndex(10, [1, 6], [3, 4]), BlockIndex(10, [0], [10])]
expected1 = BlockIndex(10, [2, 6], [2, 3])
_check_matches(indices1, expected1)
indices2 = [BlockIndex(10, [2], [7]), BlockIndex(10, [2], [7])]
expected2 = indices2[0]
_check_matches(indices2, expected2)
tempResult = arange(7)
	
===================================================================	
TestSparseSeries.test_constructor_scalar: 215	
----------------------------	

data = 5
tempResult = arange(100)
	
===================================================================	
TestSparseSeries.test_constructor_scalar: 216	
----------------------------	

data = 5
sp = SparseSeries(data, numpy.arange(100))
tempResult = arange(200)
	
===================================================================	
TestSparseSeries.test_constructor_scalar: 220	
----------------------------	

data = 5
sp = SparseSeries(data, numpy.arange(100))
sp = sp.reindex(numpy.arange(200))
self.assertTrue((sp.ix[:99] == data).all())
self.assertTrue(isnull(sp.ix[100:]).all())
data = numpy.nan
tempResult = arange(100)
	
===================================================================	
TestSparseSeriesScipyInteraction.test_to_coo_nlevels_less_than_two: 771	
----------------------------	

ss = self.sparse_series[0]
tempResult = arange(len(ss.index))
	
===================================================================	
_split_quantile: 90	
----------------------------	

arr = numpy.asarray(arr)
mask = numpy.isfinite(arr)
order = arr[mask].argsort()
n = len(arr)
tempResult = arange(n)
	
===================================================================	
bucket: 83	
----------------------------	

'\n    Produce DataFrame representing quantiles of a Series\n\n    Parameters\n    ----------\n    series : Series\n    k : int\n        number of quantiles\n    by : Series or same-length array\n        bucket by value\n\n    Returns\n    -------\n    DataFrame\n    '
if (by is None):
    by = series
else:
    by = by.reindex(series.index)
split = _split_quantile(by, k)
mat = (numpy.empty((len(series), k), dtype=float) * numpy.NaN)
for (i, v) in enumerate(split):
    mat[:, i][v] = series.take(v)
tempResult = arange(k)
	
===================================================================	
bucketcat: 95	
----------------------------	

"\n    Produce DataFrame representing quantiles of a Series\n\n    Parameters\n    ----------\n    series : Series\n    cat : Series or same-length array\n        bucket by category; mutually exclusive with 'by'\n\n    Returns\n    -------\n    DataFrame\n    "
if (not isinstance(series, Series)):
    tempResult = arange(len(series))
	
===================================================================	
_bucket_labels: 178	
----------------------------	

arr = numpy.asarray(series)
mask = numpy.isfinite(arr)
order = arr[mask].argsort()
n = len(series)
tempResult = arange(n)
	
===================================================================	
MovingOLS._beta_matrix: 892	
----------------------------	

if (lag < 0):
    raise AssertionError("'lag' must be greater than or equal to 0, input was {0}".format(lag))
betas = self._beta_raw
tempResult = arange(len(self._y))
	
===================================================================	
MovingOLS._calc_betas: 567	
----------------------------	

N = len(self._index)
K = len(self._x.columns)
betas = numpy.empty((N, K), dtype=float)
betas[:] = numpy.NaN
valid = self._time_has_obs
enough = self._enough_obs
window = self._window
cum_xx = self._cum_xx(x)
cum_xy = self._cum_xy(x, y)
for i in range(N):
    if ((not valid[i]) or (not enough[i])):
        continue
    xx = cum_xx[i]
    xy = cum_xy[i]
    if (self._is_rolling and (i >= window)):
        xx = (xx - cum_xx[(i - window)])
        xy = (xy - cum_xy[(i - window)])
    betas[i] = pandas.stats.math.solve(xx, xy)
mask = (~ np.isnan(betas).any(axis=1))
tempResult = arange(N)
	
===================================================================	
_drop_incomplete_rows: 335	
----------------------------	

mask = np.isfinite(array).all(1)
tempResult = arange(len(array))
	
===================================================================	
TestMath: 21	
----------------------------	

tempResult = arange(20, 40)
	
===================================================================	
TestMath.setUp: 30	
----------------------------	

arr = randn(N)
arr[self._nan_locs] = numpy.NaN
self.arr = arr
self.rng = date_range(datetime(2009, 1, 1), periods=N)
self.series = Series(arr.copy(), index=self.rng)
tempResult = arange(K)
	
===================================================================	
TestOLS.checkDataSet: 104	
----------------------------	

exog = dataset.exog[start:end]
endog = dataset.endog[start:end]
tempResult = arange(exog.shape[0])
	
===================================================================	
TestOLS.checkDataSet: 104	
----------------------------	

exog = dataset.exog[start:end]
endog = dataset.endog[start:end]
tempResult = arange(exog.shape[1])
	
===================================================================	
TestOLS.checkDataSet: 105	
----------------------------	

exog = dataset.exog[start:end]
endog = dataset.endog[start:end]
x = DataFrame(exog, index=numpy.arange(exog.shape[0]), columns=numpy.arange(exog.shape[1]))
tempResult = arange(len(endog))
	
===================================================================	
TestPanelOLS.test_group_agg: 591	
----------------------------	

from pandas.stats.plm import _group_agg
tempResult = arange(10)
	
===================================================================	
TestPanelOLS.test_group_agg: 592	
----------------------------	

from pandas.stats.plm import _group_agg
values = (numpy.ones((10, 2)) * np.arange(10).reshape((10, 1)))
tempResult = arange(5)
	
===================================================================	
GroupVarTestMixin.test_group_var_generic_2d_all_finite: 593	
----------------------------	

prng = RandomState(1234)
out = (np.nan * np.ones((5, 2))).astype(self.dtype)
counts = numpy.zeros(5, dtype='int64')
values = (10 * prng.rand(10, 2).astype(self.dtype))
tempResult = arange(5)
	
===================================================================	
TestFactorize.test_basic: 140	
----------------------------	

(labels, uniques) = pandas.core.algorithms.factorize(['a', 'b', 'b', 'a', 'a', 'c', 'c', 'c'])
self.assert_numpy_array_equal(uniques, numpy.array(['a', 'b', 'c'], dtype=object))
(labels, uniques) = pandas.core.algorithms.factorize(['a', 'b', 'b', 'a', 'a', 'c', 'c', 'c'], sort=True)
exp = numpy.array([0, 1, 1, 0, 0, 2, 2, 2], dtype=numpy.intp)
self.assert_numpy_array_equal(labels, exp)
exp = numpy.array(['a', 'b', 'c'], dtype=object)
self.assert_numpy_array_equal(uniques, exp)
(labels, uniques) = pandas.core.algorithms.factorize(list(reversed(range(5))))
exp = numpy.array([0, 1, 2, 3, 4], dtype=numpy.intp)
self.assert_numpy_array_equal(labels, exp)
exp = numpy.array([4, 3, 2, 1, 0], dtype=numpy.int64)
self.assert_numpy_array_equal(uniques, exp)
(labels, uniques) = pandas.core.algorithms.factorize(list(reversed(range(5))), sort=True)
exp = numpy.array([4, 3, 2, 1, 0], dtype=numpy.intp)
self.assert_numpy_array_equal(labels, exp)
exp = numpy.array([0, 1, 2, 3, 4], dtype=numpy.int64)
self.assert_numpy_array_equal(uniques, exp)
tempResult = arange(5.0)
	
===================================================================	
TestFactorize.test_basic: 145	
----------------------------	

(labels, uniques) = pandas.core.algorithms.factorize(['a', 'b', 'b', 'a', 'a', 'c', 'c', 'c'])
self.assert_numpy_array_equal(uniques, numpy.array(['a', 'b', 'c'], dtype=object))
(labels, uniques) = pandas.core.algorithms.factorize(['a', 'b', 'b', 'a', 'a', 'c', 'c', 'c'], sort=True)
exp = numpy.array([0, 1, 1, 0, 0, 2, 2, 2], dtype=numpy.intp)
self.assert_numpy_array_equal(labels, exp)
exp = numpy.array(['a', 'b', 'c'], dtype=object)
self.assert_numpy_array_equal(uniques, exp)
(labels, uniques) = pandas.core.algorithms.factorize(list(reversed(range(5))))
exp = numpy.array([0, 1, 2, 3, 4], dtype=numpy.intp)
self.assert_numpy_array_equal(labels, exp)
exp = numpy.array([4, 3, 2, 1, 0], dtype=numpy.int64)
self.assert_numpy_array_equal(uniques, exp)
(labels, uniques) = pandas.core.algorithms.factorize(list(reversed(range(5))), sort=True)
exp = numpy.array([4, 3, 2, 1, 0], dtype=numpy.intp)
self.assert_numpy_array_equal(labels, exp)
exp = numpy.array([0, 1, 2, 3, 4], dtype=numpy.int64)
self.assert_numpy_array_equal(uniques, exp)
(labels, uniques) = pandas.core.algorithms.factorize(list(reversed(numpy.arange(5.0))))
exp = numpy.array([0, 1, 2, 3, 4], dtype=numpy.intp)
self.assert_numpy_array_equal(labels, exp)
exp = numpy.array([4.0, 3.0, 2.0, 1.0, 0.0], dtype=numpy.float64)
self.assert_numpy_array_equal(uniques, exp)
tempResult = arange(5.0)
	
===================================================================	
GroupVarTestMixin.test_group_var_generic_2d_some_nan: 606	
----------------------------	

prng = RandomState(1234)
out = (np.nan * np.ones((5, 2))).astype(self.dtype)
counts = numpy.zeros(5, dtype='int64')
values = (10 * prng.rand(10, 2).astype(self.dtype))
values[:, 1] = numpy.nan
tempResult = arange(5)
	
===================================================================	
TestUnique.test_on_index_object: 253	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestUnique.test_on_index_object: 253	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestMatch.test_ints: 31	
----------------------------	

values = numpy.array([0, 2, 1])
to_match = numpy.array([0, 1, 2, 2, 0, 1, 3, 0])
result = pandas.core.algorithms.match(to_match, values)
expected = numpy.array([0, 2, 1, 1, 0, 2, (- 1), 0], dtype=numpy.int64)
self.assert_numpy_array_equal(result, expected)
result = Series(pandas.core.algorithms.match(to_match, values, numpy.nan))
expected = Series(numpy.array([0, 2, 1, 1, 0, 2, numpy.nan, 0]))
pandas.util.testing.assert_series_equal(result, expected)
tempResult = arange(5)
	
===================================================================	
test_ensure_platform_int: 776	
----------------------------	

tempResult = arange(100, dtype=numpy.intp)
	
===================================================================	
GroupVarTestMixin.test_group_var_generic_1d: 569	
----------------------------	

prng = RandomState(1234)
out = (np.nan * np.ones((5, 1))).astype(self.dtype)
counts = numpy.zeros(5, dtype='int64')
values = (10 * prng.rand(15, 1).astype(self.dtype))
tempResult = arange(5)
	
===================================================================	
TestFloat64HashTable.test_lookup_nan: 742	
----------------------------	

from pandas.hashtable import Float64HashTable
xs = numpy.array([2.718, 3.14, numpy.nan, (- 7), 5, 2, 3])
m = Float64HashTable()
m.map_locations(xs)
tempResult = arange(len(xs), dtype=numpy.int64)
	
===================================================================	
TestIndexOps.test_nanops: 314	
----------------------------	

for op in ['max', 'min']:
    for klass in [Index, Series]:
        obj = klass([numpy.nan, 2.0])
        self.assertEqual(getattr(obj, op)(), 2.0)
        obj = klass([numpy.nan])
        self.assertTrue(pandas.isnull(getattr(obj, op)()))
        obj = klass([])
        self.assertTrue(pandas.isnull(getattr(obj, op)()))
        obj = klass([pandas.NaT, datetime(2011, 11, 1)])
        self.assertEqual(getattr(obj, op)(), datetime(2011, 11, 1))
        obj = klass([pandas.NaT, datetime(2011, 11, 1), pandas.NaT])
        self.assertEqual(getattr(obj, op)(), datetime(2011, 11, 1))
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestCategoricalAsBlock.test_categorical_repr: 1469	
----------------------------	

c = pandas.Categorical([1, 2, 3])
exp = '[1, 2, 3]\nCategories (3, int64): [1, 2, 3]'
self.assertEqual(repr(c), exp)
c = pandas.Categorical([1, 2, 3, 1, 2, 3], categories=[1, 2, 3])
exp = '[1, 2, 3, 1, 2, 3]\nCategories (3, int64): [1, 2, 3]'
self.assertEqual(repr(c), exp)
c = pandas.Categorical(([1, 2, 3, 4, 5] * 10))
exp = '[1, 2, 3, 4, 5, ..., 1, 2, 3, 4, 5]\nLength: 50\nCategories (5, int64): [1, 2, 3, 4, 5]'
self.assertEqual(repr(c), exp)
tempResult = arange(20)
	
===================================================================	
TestCategoricalAsBlock.test_categorical_series_repr: 1587	
----------------------------	

s = pandas.Series(pandas.Categorical([1, 2, 3]))
exp = '0    1\n1    2\n2    3\ndtype: category\nCategories (3, int64): [1, 2, 3]'
self.assertEqual(repr(s), exp)
tempResult = arange(10)
	
===================================================================	
TestCategoricalAsBlock.test_categorical_index_repr: 1663	
----------------------------	

idx = pandas.CategoricalIndex(pandas.Categorical([1, 2, 3]))
exp = "CategoricalIndex([1, 2, 3], categories=[1, 2, 3], ordered=False, dtype='category')"
self.assertEqual(repr(idx), exp)
tempResult = arange(10)
	
===================================================================	
TestCategoricalAsBlock.test_categorical_repr_ordered: 1483	
----------------------------	

c = pandas.Categorical([1, 2, 3], ordered=True)
exp = '[1, 2, 3]\nCategories (3, int64): [1 < 2 < 3]'
self.assertEqual(repr(c), exp)
c = pandas.Categorical([1, 2, 3, 1, 2, 3], categories=[1, 2, 3], ordered=True)
exp = '[1, 2, 3, 1, 2, 3]\nCategories (3, int64): [1 < 2 < 3]'
self.assertEqual(repr(c), exp)
c = pandas.Categorical(([1, 2, 3, 4, 5] * 10), ordered=True)
exp = '[1, 2, 3, 4, 5, ..., 1, 2, 3, 4, 5]\nLength: 50\nCategories (5, int64): [1 < 2 < 3 < 4 < 5]'
self.assertEqual(repr(c), exp)
tempResult = arange(20)
	
===================================================================	
TestCategoricalAsBlock.test_categorical_index_repr_ordered: 1671	
----------------------------	

i = pandas.CategoricalIndex(pandas.Categorical([1, 2, 3], ordered=True))
exp = "CategoricalIndex([1, 2, 3], categories=[1, 2, 3], ordered=True, dtype='category')"
self.assertEqual(repr(i), exp)
tempResult = arange(10)
	
===================================================================	
TestCategoricalAsBlock.test_concat_preserve: 2438	
----------------------------	

s = Series(list('abc'), dtype='category')
s2 = Series(list('abd'), dtype='category')
exp = Series(list('abcabd'))
res = pandas.concat([s, s2], ignore_index=True)
pandas.util.testing.assert_series_equal(res, exp)
exp = Series(list('abcabc'), dtype='category')
res = pandas.concat([s, s], ignore_index=True)
pandas.util.testing.assert_series_equal(res, exp)
exp = Series(list('abcabc'), index=[0, 1, 2, 0, 1, 2], dtype='category')
res = pandas.concat([s, s])
pandas.util.testing.assert_series_equal(res, exp)
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestCategoricalAsBlock.test_series_delegations: 1369	
----------------------------	

self.assertRaises(AttributeError, (lambda : Series([1, 2, 3]).cat))
pandas.util.testing.assertRaisesRegexp(AttributeError, "Can only use .cat accessor with a 'category' dtype", (lambda : Series([1, 2, 3]).cat))
self.assertRaises(AttributeError, (lambda : Series(['a', 'b', 'c']).cat))
tempResult = arange(5.0)
	
===================================================================	
TestCategorical.test_constructor_with_datetimelike: 201	
----------------------------	

for dtl in [pandas.date_range('1995-01-01 00:00:00', periods=5, freq='s'), pandas.date_range('1995-01-01 00:00:00', periods=5, freq='s', tz='US/Eastern'), pandas.timedelta_range('1 day', periods=5, freq='s')]:
    s = Series(dtl)
    c = Categorical(s)
    expected = type(dtl)(s)
    expected.freq = None
    pandas.util.testing.assert_index_equal(c.categories, expected)
    tempResult = arange(5, dtype='int8')
	
===================================================================	
TestCategoricalAsBlock.test_slicing: 1981	
----------------------------	

cat = Series(Categorical([1, 2, 3, 4]))
reversed = cat[::(- 1)]
exp = numpy.array([4, 3, 2, 1], dtype=numpy.int64)
self.assert_numpy_array_equal(reversed.__array__(), exp)
tempResult = arange(100)
	
===================================================================	
TestCategoricalAsBlock.test_slicing: 1986	
----------------------------	

cat = Series(Categorical([1, 2, 3, 4]))
reversed = cat[::(- 1)]
exp = numpy.array([4, 3, 2, 1], dtype=numpy.int64)
self.assert_numpy_array_equal(reversed.__array__(), exp)
df = DataFrame({'value': (np.arange(100) + 1).astype('int64')})
df['D'] = pandas.cut(df.value, bins=[0, 25, 50, 75, 100])
expected = Series([11, '(0, 25]'], index=['value', 'D'], name=10)
result = df.iloc[10]
pandas.util.testing.assert_series_equal(result, expected)
tempResult = arange(11, 21)
	
===================================================================	
TestCategoricalAsBlock.test_slicing: 1986	
----------------------------	

cat = Series(Categorical([1, 2, 3, 4]))
reversed = cat[::(- 1)]
exp = numpy.array([4, 3, 2, 1], dtype=numpy.int64)
self.assert_numpy_array_equal(reversed.__array__(), exp)
df = DataFrame({'value': (np.arange(100) + 1).astype('int64')})
df['D'] = pandas.cut(df.value, bins=[0, 25, 50, 75, 100])
expected = Series([11, '(0, 25]'], index=['value', 'D'], name=10)
result = df.iloc[10]
pandas.util.testing.assert_series_equal(result, expected)
tempResult = arange(10, 20)
	
===================================================================	
TestCategoricalAsBlock.test_concat_categorical: 2721	
----------------------------	

tempResult = arange(18, dtype='int64')
	
===================================================================	
TestCategoricalAsBlock.test_concat_categorical: 2722	
----------------------------	

df1 = pandas.DataFrame(np.arange(18, dtype='int64').reshape(6, 3), columns=['a', 'b', 'c'])
tempResult = arange(14, dtype='int64')
	
===================================================================	
TestCategoricalAsBlock.test_categorical_index_preserver: 2446	
----------------------------	

tempResult = arange(6, dtype='int64')
	
===================================================================	
TestCategoricalAsBlock.test_categorical_series_repr_ordered: 1595	
----------------------------	

s = pandas.Series(pandas.Categorical([1, 2, 3], ordered=True))
exp = '0    1\n1    2\n2    3\ndtype: category\nCategories (3, int64): [1 < 2 < 3]'
self.assertEqual(repr(s), exp)
tempResult = arange(10)
	
===================================================================	
Generic.test_numpy_clip: 400	
----------------------------	

lower = 1
upper = 3
tempResult = arange(5)
	
===================================================================	
TestNDFrame.test_equals: 1209	
----------------------------	

s1 = pandas.Series([1, 2, 3], index=[0, 2, 1])
s2 = s1.copy()
self.assertTrue(s1.equals(s2))
s1[1] = 99
self.assertFalse(s1.equals(s2))
s1 = pandas.Series([1, numpy.nan, 3, numpy.nan], index=[0, 2, 1, 3])
s2 = s1.copy()
self.assertTrue(s1.equals(s2))
s2[0] = 9.9
self.assertFalse(s1.equals(s2))
idx = pandas.core.index.MultiIndex.from_tuples([(0, 'a'), (1, 'b'), (2, 'c')])
s1 = Series([1, 2, numpy.nan], index=idx)
s2 = s1.copy()
self.assertTrue(s1.equals(s2))
index = numpy.random.random(10)
df1 = DataFrame(numpy.random.random(10), index=index, columns=['floats'])
df1['text'] = 'the sky is so blue. we could use more chocolate.'.split()
df1['start'] = date_range('2000-1-1', periods=10, freq='T')
df1['end'] = date_range('2000-1-1', periods=10, freq='D')
df1['diff'] = (df1['end'] - df1['start'])
tempResult = arange(10)
	
===================================================================	
TestDataFrame.test_describe_typefiltering_category_bool: 800	
----------------------------	

tempResult = arange(24.0)
	
===================================================================	
Generic.test_clip: 372	
----------------------------	

lower = 1
upper = 3
tempResult = arange(5)
	
===================================================================	
TestDataFrame.test_describe_typefiltering_dupcol: 816	
----------------------------	

tempResult = arange(24)
	
===================================================================	
TestDataFrame.test_describe_typefiltering_dupcol: 816	
----------------------------	

tempResult = arange(24.0)
	
===================================================================	
TestDataFrame.test_to_xarray: 954	
----------------------------	

pandas.util.testing._skip_if_no_xarray()
from xarray import Dataset
tempResult = arange(3, 6)
	
===================================================================	
TestDataFrame.test_to_xarray: 954	
----------------------------	

pandas.util.testing._skip_if_no_xarray()
from xarray import Dataset
tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrame.test_describe_timedelta: 812	
----------------------------	

tempResult = arange(24)
	
===================================================================	
TestDataFrame.test_describe_typefiltering_groupby: 823	
----------------------------	

tempResult = arange(24)
	
===================================================================	
TestDataFrame.test_describe_typefiltering_groupby: 823	
----------------------------	

tempResult = arange(24.0)
	
===================================================================	
TestDataFrame.test_pct_change: 940	
----------------------------	

tempResult = arange(0, 40, 10)
	
===================================================================	
TestDataFrame.test_pct_change: 940	
----------------------------	

tempResult = arange(0, 40, 10)
	
===================================================================	
TestDataFrame.test_pct_change: 940	
----------------------------	

tempResult = arange(0, 40, 10)
	
===================================================================	
Generic.test_get_default: 103	
----------------------------	

d0 = ('a', 'b', 'c', 'd')
tempResult = arange(4, dtype='int64')
	
===================================================================	
TestDataFrame.test_describe_typefiltering: 759	
----------------------------	

tempResult = arange(24, dtype='int64')
	
===================================================================	
TestDataFrame.test_describe_typefiltering: 759	
----------------------------	

tempResult = arange(24.0)
	
===================================================================	
TestGroupBy.test_multi_iter: 1150	
----------------------------	

tempResult = arange(6)
	
===================================================================	
TestGroupBy.test_filter_bad_shapes: 3440	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestGroupBy.test_filter_bad_shapes: 3440	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2427	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
tempResult = arange(1000)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2427	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
tempResult = arange(1000)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2427	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
tempResult = arange(500)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2428	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
B = numpy.concatenate((numpy.arange(1000), numpy.arange(1000), numpy.arange(500)))
tempResult = arange(2500)
	
===================================================================	
TestGroupBy.test_groupby_complex: 1763	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestGroupBy.test_agg_compat: 1122	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestGroupBy.test_grouper_index_types: 244	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestGroupBy.test_multifunc_select_col_integer_cols: 1367	
----------------------------	

df = self.df
tempResult = arange(len(df.columns))
	
===================================================================	
test_decons: 4279	
----------------------------	

from pandas.core.groupby import decons_group_index, get_group_index

def testit(label_list, shape):
    group_index = get_group_index(label_list, shape, sort=True, xnull=True)
    label_list2 = decons_group_index(group_index, shape)
    for (a, b) in zip(label_list, label_list2):
        assert numpy.array_equal(a, b)
shape = (4, 5, 6)
label_list = [numpy.tile([0, 1, 2, 3, 0, 1, 2, 3], 100), numpy.tile([0, 2, 4, 3, 0, 1, 2, 3], 100), numpy.tile([5, 1, 0, 2, 3, 0, 5, 4], 100)]
testit(label_list, shape)
shape = (10000, 10000)
tempResult = arange(10000)
	
===================================================================	
test_decons: 4279	
----------------------------	

from pandas.core.groupby import decons_group_index, get_group_index

def testit(label_list, shape):
    group_index = get_group_index(label_list, shape, sort=True, xnull=True)
    label_list2 = decons_group_index(group_index, shape)
    for (a, b) in zip(label_list, label_list2):
        assert numpy.array_equal(a, b)
shape = (4, 5, 6)
label_list = [numpy.tile([0, 1, 2, 3, 0, 1, 2, 3], 100), numpy.tile([0, 2, 4, 3, 0, 1, 2, 3], 100), numpy.tile([5, 1, 0, 2, 3, 0, 5, 4], 100)]
testit(label_list, shape)
shape = (10000, 10000)
tempResult = arange(10000)
	
===================================================================	
TestBlockPlacement.test_blockplacement_add: 793	
----------------------------	

bpl = BlockPlacement(slice(0, 5))
self.assertEqual(bpl.add(1).as_slice, slice(1, 6, 1))
tempResult = arange(5)
	
===================================================================	
TestBlockPlacement.test_blockplacement_add: 794	
----------------------------	

bpl = BlockPlacement(slice(0, 5))
self.assertEqual(bpl.add(1).as_slice, slice(1, 6, 1))
self.assertEqual(bpl.add(np.arange(5)).as_slice, slice(0, 10, 2))
tempResult = arange(5, 0, (- 1))
	
===================================================================	
create_block: 53	
----------------------------	

'\n    Supported typestr:\n\n        * float, f8, f4, f2\n        * int, i8, i4, i2, i1\n        * uint, u8, u4, u2, u1\n        * complex, c16, c8\n        * bool\n        * object, string, O\n        * datetime, dt, M8[ns], M8[ns, tz]\n        * timedelta, td, m8[ns]\n        * sparse (SparseArray with fill_value=0.0)\n        * sparse_na (SparseArray with fill_value=np.nan)\n        * category, category2\n\n    '
placement = BlockPlacement(placement)
num_items = len(placement)
if (item_shape is None):
    item_shape = (N,)
shape = ((num_items,) + item_shape)
mat = get_numeric_mat(shape)
if (typestr in ('float', 'f8', 'f4', 'f2', 'int', 'i8', 'i4', 'i2', 'i1', 'uint', 'u8', 'u4', 'u2', 'u1')):
    values = (mat.astype(typestr) + num_offset)
elif (typestr in ('complex', 'c16', 'c8')):
    values = (1j * (mat.astype(typestr) + num_offset))
elif (typestr in ('object', 'string', 'O')):
    values = numpy.reshape([('A%d' % i) for i in (mat.ravel() + num_offset)], shape)
elif (typestr in ('b', 'bool')):
    values = numpy.ones(shape, dtype=numpy.bool_)
elif (typestr in ('datetime', 'dt', 'M8[ns]')):
    values = (mat * 1000000000.0).astype('M8[ns]')
elif typestr.startswith('M8[ns'):
    m = re.search('M8\\[ns,\\s*(\\w+\\/?\\w*)\\]', typestr)
    assert (m is not None), 'incompatible typestr -> {0}'.format(typestr)
    tz = m.groups()[0]
    assert (num_items == 1), 'must have only 1 num items for a tz-aware'
    tempResult = arange(N)
	
===================================================================	
get_numeric_mat: 26	
----------------------------	

tempResult = arange(shape[0])
	
===================================================================	
Get no callers of function numpy.arange at line 638 col 56.	
===================================================================	
TestBlockManager.test_insert: 309	
----------------------------	

tempResult = arange(N)
	
===================================================================	
TestBlockManager.test_insert: 311	
----------------------------	

self.mgr.insert(0, 'inserted', numpy.arange(N))
self.assertEqual(self.mgr.items[0], 'inserted')
tempResult = arange(N)
	
===================================================================	
Get no callers of function numpy.arange at line 695 col 81.	
===================================================================	
Get no callers of function numpy.arange at line 696 col 104.	
===================================================================	
Get no callers of function numpy.arange at line 697 col 90.	
===================================================================	
Get no callers of function numpy.arange at line 698 col 81.	
===================================================================	
TestBlockManager.test_consolidate_ordering_issues: 521	
----------------------------	

self.mgr.set('f', randn(N))
self.mgr.set('d', randn(N))
self.mgr.set('b', randn(N))
self.mgr.set('g', randn(N))
self.mgr.set('h', randn(N))
cons = self.mgr.consolidate()
self.assertEqual(cons.nblocks, 4)
cons = self.mgr.consolidate().get_numeric_data()
self.assertEqual(cons.nblocks, 1)
pandas.util.testing.assertIsInstance(cons.blocks[0].mgr_locs, pandas.lib.BlockPlacement)
tempResult = arange(len(cons.items), dtype=numpy.int64)
	
===================================================================	
create_single_mgr: 77	
----------------------------	

if (num_rows is None):
    num_rows = N
tempResult = arange(num_rows)
	
===================================================================	
create_mgr: 94	
----------------------------	

"\n    Construct BlockManager from string description.\n\n    String description syntax looks similar to np.matrix initializer.  It looks\n    like this::\n\n        a,b,c: f8; d,e,f: i8\n\n    Rules are rather simple:\n\n    * see list of supported datatypes in `create_block` method\n    * components are semicolon-separated\n    * each component is `NAME,NAME,NAME: DTYPE_ID`\n    * whitespace around colons & semicolons are removed\n    * components with same DTYPE_ID are combined into single block\n    * to force multiple blocks with same dtype, use '-SUFFIX'::\n\n        'a:f8-1; b:f8-2; c:f8-foobar'\n\n    "
if (item_shape is None):
    item_shape = (N,)
offset = 0
mgr_items = []
block_placements = OrderedDict()
for d in descr.split(';'):
    d = d.strip()
    if (not len(d)):
        continue
    (names, blockstr) = d.partition(':')[::2]
    blockstr = blockstr.strip()
    names = names.strip().split(',')
    mgr_items.extend(names)
    tempResult = arange(len(names))
	
===================================================================	
create_mgr: 107	
----------------------------	

"\n    Construct BlockManager from string description.\n\n    String description syntax looks similar to np.matrix initializer.  It looks\n    like this::\n\n        a,b,c: f8; d,e,f: i8\n\n    Rules are rather simple:\n\n    * see list of supported datatypes in `create_block` method\n    * components are semicolon-separated\n    * each component is `NAME,NAME,NAME: DTYPE_ID`\n    * whitespace around colons & semicolons are removed\n    * components with same DTYPE_ID are combined into single block\n    * to force multiple blocks with same dtype, use '-SUFFIX'::\n\n        'a:f8-1; b:f8-2; c:f8-foobar'\n\n    "
if (item_shape is None):
    item_shape = (N,)
offset = 0
mgr_items = []
block_placements = OrderedDict()
for d in descr.split(';'):
    d = d.strip()
    if (not len(d)):
        continue
    (names, blockstr) = d.partition(':')[::2]
    blockstr = blockstr.strip()
    names = names.strip().split(',')
    mgr_items.extend(names)
    placement = list((numpy.arange(len(names)) + offset))
    try:
        block_placements[blockstr].extend(placement)
    except KeyError:
        block_placements[blockstr] = placement
    offset += len(names)
mgr_items = Index(mgr_items)
blocks = []
num_offset = 0
for (blockstr, placement) in block_placements.items():
    typestr = blockstr.split('-')[0]
    blocks.append(create_block(typestr, placement, item_shape=item_shape, num_offset=num_offset))
    num_offset += len(placement)
tempResult = arange(n)
	
===================================================================	
TestBlockManager.test_get: 290	
----------------------------	

cols = Index(list('abc'))
values = numpy.random.rand(3, 3)
tempResult = arange(3)
	
===================================================================	
TestBlockManager.test_get: 291	
----------------------------	

cols = Index(list('abc'))
values = numpy.random.rand(3, 3)
block = make_block(values=values.copy(), placement=numpy.arange(3))
tempResult = arange(3)
	
===================================================================	
test_left_outer_join_bug: 51	
----------------------------	

left = numpy.array([0, 1, 0, 1, 1, 2, 3, 1, 0, 2, 1, 2, 0, 1, 1, 2, 3, 2, 3, 2, 1, 1, 3, 0, 3, 2, 3, 0, 0, 2, 3, 2, 0, 3, 1, 3, 0, 1, 3, 0, 0, 1, 0, 3, 1, 0, 1, 0, 1, 1, 0, 2, 2, 2, 2, 2, 0, 3, 1, 2, 0, 0, 3, 1, 3, 2, 2, 0, 1, 3, 0, 2, 3, 2, 3, 3, 2, 3, 3, 1, 3, 2, 0, 0, 3, 1, 1, 1, 0, 2, 3, 3, 1, 2, 0, 3, 1, 2, 0, 2], dtype=numpy.int64)
right = numpy.array([3, 1], dtype=numpy.int64)
max_groups = 4
(lidx, ridx) = pandas._join.left_outer_join(left, right, max_groups, sort=False)
tempResult = arange(len(left))
	
===================================================================	
TestIndexer.test_outer_join_indexer: 14	
----------------------------	

typemap = [('int32', pandas._join.outer_join_indexer_int32), ('int64', pandas._join.outer_join_indexer_int64), ('float32', pandas._join.outer_join_indexer_float32), ('float64', pandas._join.outer_join_indexer_float64), ('object', pandas._join.outer_join_indexer_object)]
for (dtype, indexer) in typemap:
    tempResult = arange(3, dtype=dtype)
	
===================================================================	
TestIndexer.test_outer_join_indexer: 15	
----------------------------	

typemap = [('int32', pandas._join.outer_join_indexer_int32), ('int64', pandas._join.outer_join_indexer_int64), ('float32', pandas._join.outer_join_indexer_float32), ('float64', pandas._join.outer_join_indexer_float64), ('object', pandas._join.outer_join_indexer_object)]
for (dtype, indexer) in typemap:
    left = numpy.arange(3, dtype=dtype)
    tempResult = arange(2, 5, dtype=dtype)
	
===================================================================	
TestIndexer.test_outer_join_indexer: 21	
----------------------------	

typemap = [('int32', pandas._join.outer_join_indexer_int32), ('int64', pandas._join.outer_join_indexer_int64), ('float32', pandas._join.outer_join_indexer_float32), ('float64', pandas._join.outer_join_indexer_float64), ('object', pandas._join.outer_join_indexer_object)]
for (dtype, indexer) in typemap:
    left = numpy.arange(3, dtype=dtype)
    right = numpy.arange(2, 5, dtype=dtype)
    empty = numpy.array([], dtype=dtype)
    (result, lindexer, rindexer) = indexer(left, right)
    pandas.util.testing.assertIsInstance(result, numpy.ndarray)
    pandas.util.testing.assertIsInstance(lindexer, numpy.ndarray)
    pandas.util.testing.assertIsInstance(rindexer, numpy.ndarray)
    tempResult = arange(5, dtype=dtype)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_right_edge: 55	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_right_edge: 58	
----------------------------	

target = numpy.arange(100)
for start in [0, 2, 5, 20, 97, 98]:
    for step in [1, 2, 4]:
        tempResult = arange(start, 99, step, dtype=numpy.int64)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_both_edges: 90	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_both_edges: 92	
----------------------------	

target = numpy.arange(10)
for step in [1, 2, 4, 5, 8, 9]:
    tempResult = arange(0, 9, step, dtype=numpy.int64)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_left_edge: 32	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_left_edge: 39	
----------------------------	

target = numpy.arange(100)
indices = numpy.array([], dtype=numpy.int64)
maybe_slice = pandas.lib.maybe_indices_to_slice(indices, len(target))
self.assertTrue(isinstance(maybe_slice, slice))
self.assert_numpy_array_equal(target[indices], target[maybe_slice])
for end in [1, 2, 5, 20, 99]:
    for step in [1, 2, 4]:
        tempResult = arange(0, end, step, dtype=numpy.int64)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_middle: 108	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestIndexing.test_maybe_indices_to_slice_middle: 111	
----------------------------	

target = numpy.arange(100)
for (start, end) in [(2, 10), (5, 25), (65, 97)]:
    for step in [1, 2, 4, 20]:
        tempResult = arange(start, end, step, dtype=numpy.int64)
	
===================================================================	
TestMultiLevel.test_sortlevel_large_cardinality: 513	
----------------------------	

tempResult = arange(4000)
	
===================================================================	
TestMultiLevel.test_sortlevel_large_cardinality: 517	
----------------------------	

index = pandas.core.index.MultiIndex.from_arrays(([numpy.arange(4000)] * 3))
df = DataFrame(numpy.random.randn(4000), index=index, dtype=numpy.int64)
result = df.sortlevel(0)
self.assertTrue((result.index.lexsort_depth == 3))
tempResult = arange(4000)
	
===================================================================	
TestMultiLevel.test_indexing_over_hashtable_size_cutoff: 1430	
----------------------------	

n = 10000
old_cutoff = pandas.index._SIZE_CUTOFF
pandas.index._SIZE_CUTOFF = 20000
tempResult = arange(n)
	
===================================================================	
TestMultiLevel.test_indexing_over_hashtable_size_cutoff: 1430	
----------------------------	

n = 10000
old_cutoff = pandas.index._SIZE_CUTOFF
pandas.index._SIZE_CUTOFF = 20000
tempResult = arange(n)
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1568	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    tempResult = arange(5, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1569	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    tempResult = arange(5, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1569	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    tempResult = arange(5, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1574	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    expected = pandas.DataFrame({'idx1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 1, 2), datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4), datetime.datetime(2011, 1, 5)], 'idx2': numpy.arange(5, dtype='int64'), 'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, columns=['idx1', 'idx2', 'a', 'b'])
    expected['idx1'] = expected['idx1'].apply((lambda d: pandas.Timestamp(d, tz=tz)))
    assert_frame_equal(df.reset_index(), expected)
    idx3 = pandas.date_range('1/1/2012', periods=5, freq='MS', tz='Europe/Paris', name='idx3')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2, idx3])
    tempResult = arange(5, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1575	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    expected = pandas.DataFrame({'idx1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 1, 2), datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4), datetime.datetime(2011, 1, 5)], 'idx2': numpy.arange(5, dtype='int64'), 'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, columns=['idx1', 'idx2', 'a', 'b'])
    expected['idx1'] = expected['idx1'].apply((lambda d: pandas.Timestamp(d, tz=tz)))
    assert_frame_equal(df.reset_index(), expected)
    idx3 = pandas.date_range('1/1/2012', periods=5, freq='MS', tz='Europe/Paris', name='idx3')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2, idx3])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    tempResult = arange(5, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1575	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    expected = pandas.DataFrame({'idx1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 1, 2), datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4), datetime.datetime(2011, 1, 5)], 'idx2': numpy.arange(5, dtype='int64'), 'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, columns=['idx1', 'idx2', 'a', 'b'])
    expected['idx1'] = expected['idx1'].apply((lambda d: pandas.Timestamp(d, tz=tz)))
    assert_frame_equal(df.reset_index(), expected)
    idx3 = pandas.date_range('1/1/2012', periods=5, freq='MS', tz='Europe/Paris', name='idx3')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2, idx3])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    tempResult = arange(5, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1580	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    expected = pandas.DataFrame({'idx1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 1, 2), datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4), datetime.datetime(2011, 1, 5)], 'idx2': numpy.arange(5, dtype='int64'), 'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, columns=['idx1', 'idx2', 'a', 'b'])
    expected['idx1'] = expected['idx1'].apply((lambda d: pandas.Timestamp(d, tz=tz)))
    assert_frame_equal(df.reset_index(), expected)
    idx3 = pandas.date_range('1/1/2012', periods=5, freq='MS', tz='Europe/Paris', name='idx3')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2, idx3])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    expected = pandas.DataFrame({'idx1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 1, 2), datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4), datetime.datetime(2011, 1, 5)], 'idx2': numpy.arange(5, dtype='int64'), 'idx3': [datetime.datetime(2012, 1, 1), datetime.datetime(2012, 2, 1), datetime.datetime(2012, 3, 1), datetime.datetime(2012, 4, 1), datetime.datetime(2012, 5, 1)], 'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, columns=['idx1', 'idx2', 'idx3', 'a', 'b'])
    expected['idx1'] = expected['idx1'].apply((lambda d: pandas.Timestamp(d, tz=tz)))
    expected['idx3'] = expected['idx3'].apply((lambda d: pandas.Timestamp(d, tz='Europe/Paris')))
    assert_frame_equal(df.reset_index(), expected)
    idx = pandas.MultiIndex.from_product([['a', 'b'], pandas.date_range('20130101', periods=3, tz=tz)])
    tempResult = arange(6, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_datetime: 1581	
----------------------------	

for tz in ['UTC', 'Asia/Tokyo', 'US/Eastern']:
    idx1 = pandas.date_range('1/1/2011', periods=5, freq='D', tz=tz, name='idx1')
    idx2 = pandas.Index(range(5), name='idx2', dtype='int64')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    expected = pandas.DataFrame({'idx1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 1, 2), datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4), datetime.datetime(2011, 1, 5)], 'idx2': numpy.arange(5, dtype='int64'), 'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, columns=['idx1', 'idx2', 'a', 'b'])
    expected['idx1'] = expected['idx1'].apply((lambda d: pandas.Timestamp(d, tz=tz)))
    assert_frame_equal(df.reset_index(), expected)
    idx3 = pandas.date_range('1/1/2012', periods=5, freq='MS', tz='Europe/Paris', name='idx3')
    idx = pandas.MultiIndex.from_arrays([idx1, idx2, idx3])
    df = pandas.DataFrame({'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, index=idx)
    expected = pandas.DataFrame({'idx1': [datetime.datetime(2011, 1, 1), datetime.datetime(2011, 1, 2), datetime.datetime(2011, 1, 3), datetime.datetime(2011, 1, 4), datetime.datetime(2011, 1, 5)], 'idx2': numpy.arange(5, dtype='int64'), 'idx3': [datetime.datetime(2012, 1, 1), datetime.datetime(2012, 2, 1), datetime.datetime(2012, 3, 1), datetime.datetime(2012, 4, 1), datetime.datetime(2012, 5, 1)], 'a': numpy.arange(5, dtype='int64'), 'b': ['A', 'B', 'C', 'D', 'E']}, columns=['idx1', 'idx2', 'idx3', 'a', 'b'])
    expected['idx1'] = expected['idx1'].apply((lambda d: pandas.Timestamp(d, tz=tz)))
    expected['idx3'] = expected['idx3'].apply((lambda d: pandas.Timestamp(d, tz='Europe/Paris')))
    assert_frame_equal(df.reset_index(), expected)
    idx = pandas.MultiIndex.from_product([['a', 'b'], pandas.date_range('20130101', periods=3, tz=tz)])
    df = pandas.DataFrame(np.arange(6, dtype='int64').reshape(6, 1), columns=['a'], index=idx)
    tempResult = arange(6, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_period: 1587	
----------------------------	

idx = pandas.MultiIndex.from_product([pandas.period_range('20130101', periods=3, freq='M'), ['a', 'b', 'c']], names=['month', 'feature'])
tempResult = arange(9, dtype='int64')
	
===================================================================	
TestMultiLevel.test_reset_index_period: 1588	
----------------------------	

idx = pandas.MultiIndex.from_product([pandas.period_range('20130101', periods=3, freq='M'), ['a', 'b', 'c']], names=['month', 'feature'])
df = pandas.DataFrame(np.arange(9, dtype='int64').reshape((- 1), 1), index=idx, columns=['a'])
tempResult = arange(9, dtype='int64')
	
===================================================================	
TestMultiLevel.test_sortlevel: 505	
----------------------------	

df = self.frame.copy()
tempResult = arange(len(df))
	
===================================================================	
TestMultiLevel.test_stack: 662	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
tempResult = arange(12)
	
===================================================================	
TestMultiLevel.test_stack: 663	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
df = DataFrame(np.arange(12).reshape(4, 3), index=list('abab'), columns=['1st', '2nd', '3rd'])
tempResult = arange(2)
	
===================================================================	
TestMultiLevel.test_stack: 663	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
df = DataFrame(np.arange(12).reshape(4, 3), index=list('abab'), columns=['1st', '2nd', '3rd'])
tempResult = arange(3)
	
===================================================================	
TestMultiLevel.test_stack: 664	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
df = DataFrame(np.arange(12).reshape(4, 3), index=list('abab'), columns=['1st', '2nd', '3rd'])
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd', '3rd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile(numpy.arange(3), 4)])
tempResult = arange(12)
	
===================================================================	
TestMultiLevel.test_stack: 667	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
df = DataFrame(np.arange(12).reshape(4, 3), index=list('abab'), columns=['1st', '2nd', '3rd'])
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd', '3rd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile(numpy.arange(3), 4)])
(left, right) = (df.stack(), Series(numpy.arange(12), index=mi))
check(left, right)
df.columns = ['1st', '2nd', '1st']
tempResult = arange(2)
	
===================================================================	
TestMultiLevel.test_stack: 668	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
df = DataFrame(np.arange(12).reshape(4, 3), index=list('abab'), columns=['1st', '2nd', '3rd'])
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd', '3rd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile(numpy.arange(3), 4)])
(left, right) = (df.stack(), Series(numpy.arange(12), index=mi))
check(left, right)
df.columns = ['1st', '2nd', '1st']
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile([0, 1, 0], 4)])
tempResult = arange(12)
	
===================================================================	
TestMultiLevel.test_stack: 672	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
df = DataFrame(np.arange(12).reshape(4, 3), index=list('abab'), columns=['1st', '2nd', '3rd'])
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd', '3rd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile(numpy.arange(3), 4)])
(left, right) = (df.stack(), Series(numpy.arange(12), index=mi))
check(left, right)
df.columns = ['1st', '2nd', '1st']
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile([0, 1, 0], 4)])
(left, right) = (df.stack(), Series(numpy.arange(12), index=mi))
check(left, right)
tpls = (('a', 2), ('b', 1), ('a', 1), ('b', 2))
df.index = pandas.core.index.MultiIndex.from_tuples(tpls)
tempResult = arange(2)
	
===================================================================	
TestMultiLevel.test_stack: 673	
----------------------------	

unstacked = self.ymd.unstack()
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unlexsorted = self.ymd.sortlevel(2)
unstacked = unlexsorted.unstack(2)
restacked = unstacked.stack()
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted[::(- 1)]
unstacked = unlexsorted.unstack(1)
restacked = unstacked.stack().swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unlexsorted = unlexsorted.swaplevel(0, 1)
unstacked = unlexsorted.unstack(0).swaplevel(0, 1, axis=1)
restacked = unstacked.stack(0).swaplevel(1, 2)
assert_frame_equal(restacked.sortlevel(0), self.ymd)
unstacked = self.ymd.unstack()
unstacked = unstacked.sort_index(axis=1, ascending=False)
restacked = unstacked.stack()
assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = unstacked.stack(1)
expected = self.ymd.unstack()
assert_frame_equal(result, expected)
result = unstacked.stack(2)
expected = self.ymd.unstack(1)
assert_frame_equal(result, expected)
result = unstacked.stack(0)
expected = self.ymd.stack().unstack(1).unstack(1)
assert_frame_equal(result, expected)
unstacked = self.ymd.unstack(2).ix[:, ::3]
stacked = unstacked.stack().stack()
ymd_stacked = self.ymd.stack()
assert_series_equal(stacked, ymd_stacked.reindex(stacked.index))
result = self.ymd.unstack(0).stack((- 2))
expected = self.ymd.unstack(0).stack(0)

def check(left, right):
    assert_series_equal(left, right)
    self.assertFalse(left.index.is_unique)
    (li, ri) = (left.index, right.index)
    pandas.util.testing.assert_index_equal(li, ri)
df = DataFrame(np.arange(12).reshape(4, 3), index=list('abab'), columns=['1st', '2nd', '3rd'])
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd', '3rd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile(numpy.arange(3), 4)])
(left, right) = (df.stack(), Series(numpy.arange(12), index=mi))
check(left, right)
df.columns = ['1st', '2nd', '1st']
mi = MultiIndex(levels=[['a', 'b'], ['1st', '2nd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.tile([0, 1, 0], 4)])
(left, right) = (df.stack(), Series(numpy.arange(12), index=mi))
check(left, right)
tpls = (('a', 2), ('b', 1), ('a', 1), ('b', 2))
df.index = pandas.core.index.MultiIndex.from_tuples(tpls)
mi = MultiIndex(levels=[['a', 'b'], [1, 2], ['1st', '2nd']], labels=[numpy.tile(np.arange(2).repeat(3), 2), numpy.repeat([1, 0, 1], [3, 6, 3]), numpy.tile([0, 1, 0], 4)])
tempResult = arange(12)
	
===================================================================	
TestMultiLevel.test_indexing_ambiguity_bug_1678: 1413	
----------------------------	

columns = pandas.core.index.MultiIndex.from_tuples([('Ohio', 'Green'), ('Ohio', 'Red'), ('Colorado', 'Green')])
index = pandas.core.index.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
tempResult = arange(12)
	
===================================================================	
TestMultiLevel.test_unstack_bug: 694	
----------------------------	

tempResult = arange(6.0)
	
===================================================================	
TestMultiLevel.test_std_var_pass_ddof: 1068	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestMultiLevel.test_std_var_pass_ddof: 1068	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestMultiLevel.test_unicode_repr_issues: 1369	
----------------------------	

levels = [Index([u('a/σ'), u('b/σ'), u('c/σ')]), Index([0, 1])]
tempResult = arange(3)
	
===================================================================	
TestMultiLevel.test_unicode_repr_issues: 1369	
----------------------------	

levels = [Index([u('a/σ'), u('b/σ'), u('c/σ')]), Index([0, 1])]
tempResult = arange(2)
	
===================================================================	
TestMultiLevel.test_drop_level_nonunique_datetime: 1352	
----------------------------	

idx = pandas.Index([2, 3, 4, 4, 5], name='id')
idxdt = pandas.to_datetime(['201603231400', '201603231500', '201603231600', '201603231600', '201603231700'])
tempResult = arange(10)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1141	
----------------------------	

tempResult = arange(500)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1142	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
tempResult = arange(500)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1143	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
level = numpy.arange(500)
tempResult = arange(2)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1144	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
level = numpy.arange(500)
index = MultiIndex(levels=(([level] * 8) + [[0, 1]]), labels=(([labels] * 8) + [np.arange(2).repeat(500)]))
tempResult = arange(1000)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1149	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
level = numpy.arange(500)
index = MultiIndex(levels=(([level] * 8) + [[0, 1]]), labels=(([labels] * 8) + [np.arange(2).repeat(500)]))
s = Series(numpy.arange(1000), index=index)
result = s.unstack()
self.assertEqual(result.shape, (500, 2))
stacked = result.stack()
assert_series_equal(s, stacked.reindex(s.index))
tempResult = arange(2)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1150	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
level = numpy.arange(500)
index = MultiIndex(levels=(([level] * 8) + [[0, 1]]), labels=(([labels] * 8) + [np.arange(2).repeat(500)]))
s = Series(numpy.arange(1000), index=index)
result = s.unstack()
self.assertEqual(result.shape, (500, 2))
stacked = result.stack()
assert_series_equal(s, stacked.reindex(s.index))
index = MultiIndex(levels=([[0, 1]] + ([level] * 8)), labels=([np.arange(2).repeat(500)] + ([labels] * 8)))
tempResult = arange(1000)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1153	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
level = numpy.arange(500)
index = MultiIndex(levels=(([level] * 8) + [[0, 1]]), labels=(([labels] * 8) + [np.arange(2).repeat(500)]))
s = Series(numpy.arange(1000), index=index)
result = s.unstack()
self.assertEqual(result.shape, (500, 2))
stacked = result.stack()
assert_series_equal(s, stacked.reindex(s.index))
index = MultiIndex(levels=([[0, 1]] + ([level] * 8)), labels=([np.arange(2).repeat(500)] + ([labels] * 8)))
s = Series(numpy.arange(1000), index=index)
result = s.unstack(0)
self.assertEqual(result.shape, (500, 2))
tempResult = arange(2)
	
===================================================================	
TestMultiLevel.test_unstack_group_index_overflow: 1154	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
level = numpy.arange(500)
index = MultiIndex(levels=(([level] * 8) + [[0, 1]]), labels=(([labels] * 8) + [np.arange(2).repeat(500)]))
s = Series(numpy.arange(1000), index=index)
result = s.unstack()
self.assertEqual(result.shape, (500, 2))
stacked = result.stack()
assert_series_equal(s, stacked.reindex(s.index))
index = MultiIndex(levels=([[0, 1]] + ([level] * 8)), labels=([np.arange(2).repeat(500)] + ([labels] * 8)))
s = Series(numpy.arange(1000), index=index)
result = s.unstack(0)
self.assertEqual(result.shape, (500, 2))
index = MultiIndex(levels=((([level] * 4) + [[0, 1]]) + ([level] * 4)), labels=((([labels] * 4) + [np.arange(2).repeat(500)]) + ([labels] * 4)))
tempResult = arange(1000)
	
===================================================================	
SafeForSparse.test_set_axis: 160	
----------------------------	

tempResult = arange(len(self.panel.items))
	
===================================================================	
SafeForSparse.test_set_axis: 161	
----------------------------	

new_items = Index(numpy.arange(len(self.panel.items)))
tempResult = arange(len(self.panel.major_axis))
	
===================================================================	
SafeForSparse.test_set_axis: 162	
----------------------------	

new_items = Index(numpy.arange(len(self.panel.items)))
new_major = Index(numpy.arange(len(self.panel.major_axis)))
tempResult = arange(len(self.panel.minor_axis))
	
===================================================================	
TestPanel.test_reindex_multi: 979	
----------------------------	

result = self.panel.reindex(items=self.panel.items, major=self.panel.major_axis, minor=self.panel.minor_axis, copy=False)
self.assertIs(result.items, self.panel.items)
self.assertIs(result.major_axis, self.panel.major_axis)
self.assertIs(result.minor_axis, self.panel.minor_axis)
result = self.panel.reindex(items=self.panel.items, major=self.panel.major_axis, minor=self.panel.minor_axis, copy=False)
assert_panel_equal(result, self.panel)
df = DataFrame(numpy.random.randn(4, 3))
p = Panel({'Item1': df})
expected = Panel({'Item1': df})
expected['Item2'] = numpy.nan
items = ['Item1', 'Item2']
tempResult = arange(4)
	
===================================================================	
TestPanel.test_reindex_multi: 980	
----------------------------	

result = self.panel.reindex(items=self.panel.items, major=self.panel.major_axis, minor=self.panel.minor_axis, copy=False)
self.assertIs(result.items, self.panel.items)
self.assertIs(result.major_axis, self.panel.major_axis)
self.assertIs(result.minor_axis, self.panel.minor_axis)
result = self.panel.reindex(items=self.panel.items, major=self.panel.major_axis, minor=self.panel.minor_axis, copy=False)
assert_panel_equal(result, self.panel)
df = DataFrame(numpy.random.randn(4, 3))
p = Panel({'Item1': df})
expected = Panel({'Item1': df})
expected['Item2'] = numpy.nan
items = ['Item1', 'Item2']
major_axis = numpy.arange(4)
tempResult = arange(3)
	
===================================================================	
CheckIndexing.test_ix_frame_align: 558	
----------------------------	

p_orig = pandas.util.testing.makePanel()
df = p_orig.ix[0].copy()
assert_frame_equal(p_orig['ItemA'], df)
p = p_orig.copy()
p.ix[0, :, :] = df
assert_panel_equal(p, p_orig)
p = p_orig.copy()
p.ix[0] = df
assert_panel_equal(p, p_orig)
p = p_orig.copy()
p.iloc[0, :, :] = df
assert_panel_equal(p, p_orig)
p = p_orig.copy()
p.iloc[0] = df
assert_panel_equal(p, p_orig)
p = p_orig.copy()
p.loc['ItemA'] = df
assert_panel_equal(p, p_orig)
p = p_orig.copy()
p.loc['ItemA', :, :] = df
assert_panel_equal(p, p_orig)
p = p_orig.copy()
p['ItemA'] = df
assert_panel_equal(p, p_orig)
p = p_orig.copy()
p.ix[0, [0, 1, 3, 5], (- 2):] = df
out = p.ix[0, [0, 1, 3, 5], (- 2):]
assert_frame_equal(out, df.iloc[([0, 1, 3, 5], [2, 3])])
for dtype in ['float64', 'int64']:
    tempResult = arange(40)
	
===================================================================	
TestPanel.test_multiindex_get: 1344	
----------------------------	

ind = pandas.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)], names=['first', 'second'])
tempResult = arange(5)
	
===================================================================	
TestPanel.test_multiindex_get: 1344	
----------------------------	

ind = pandas.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)], names=['first', 'second'])
tempResult = arange(5)
	
===================================================================	
TestPanel.test_apply_slabs: 924	
----------------------------	

result = self.panel.apply((lambda x: (x * 2)), axis=['items', 'major_axis'])
expected = (self.panel * 2).transpose('minor_axis', 'major_axis', 'items')
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['major_axis', 'items'])
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['items', 'minor_axis'])
expected = (self.panel * 2).transpose('major_axis', 'minor_axis', 'items')
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['minor_axis', 'items'])
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['major_axis', 'minor_axis'])
expected = (self.panel * 2)
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['minor_axis', 'major_axis'])
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: x.sum(0)), axis=['items', 'major_axis'])
expected = self.panel.sum(1).T
assert_frame_equal(result, expected)
result = self.panel.apply((lambda x: x.sum(1)), axis=['items', 'major_axis'])
expected = self.panel.sum(0)
assert_frame_equal(result, expected)
f = (lambda x: ((x.T - x.mean(1)) / x.std(1)).T)
with pandas.util.testing.assert_produces_warning(False):
    result = self.panel.apply(f, axis=['items', 'major_axis'])
    expected = Panel(dict([(ax, f(self.panel.loc[:, :, ax])) for ax in self.panel.minor_axis]))
    assert_panel_equal(result, expected)
result = self.panel.apply(f, axis=['major_axis', 'minor_axis'])
expected = Panel(dict([(ax, f(self.panel.loc[ax])) for ax in self.panel.items]))
assert_panel_equal(result, expected)
result = self.panel.apply(f, axis=['minor_axis', 'items'])
expected = Panel(dict([(ax, f(self.panel.loc[:, ax])) for ax in self.panel.major_axis]))
assert_panel_equal(result, expected)
index = pandas.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'), ('two', 'a'), ('two', 'b')])
tempResult = arange(12, dtype='int64')
	
===================================================================	
TestPanel.test_apply_slabs: 925	
----------------------------	

result = self.panel.apply((lambda x: (x * 2)), axis=['items', 'major_axis'])
expected = (self.panel * 2).transpose('minor_axis', 'major_axis', 'items')
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['major_axis', 'items'])
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['items', 'minor_axis'])
expected = (self.panel * 2).transpose('major_axis', 'minor_axis', 'items')
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['minor_axis', 'items'])
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['major_axis', 'minor_axis'])
expected = (self.panel * 2)
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: (x * 2)), axis=['minor_axis', 'major_axis'])
assert_panel_equal(result, expected)
result = self.panel.apply((lambda x: x.sum(0)), axis=['items', 'major_axis'])
expected = self.panel.sum(1).T
assert_frame_equal(result, expected)
result = self.panel.apply((lambda x: x.sum(1)), axis=['items', 'major_axis'])
expected = self.panel.sum(0)
assert_frame_equal(result, expected)
f = (lambda x: ((x.T - x.mean(1)) / x.std(1)).T)
with pandas.util.testing.assert_produces_warning(False):
    result = self.panel.apply(f, axis=['items', 'major_axis'])
    expected = Panel(dict([(ax, f(self.panel.loc[:, :, ax])) for ax in self.panel.minor_axis]))
    assert_panel_equal(result, expected)
result = self.panel.apply(f, axis=['major_axis', 'minor_axis'])
expected = Panel(dict([(ax, f(self.panel.loc[ax])) for ax in self.panel.items]))
assert_panel_equal(result, expected)
result = self.panel.apply(f, axis=['minor_axis', 'items'])
expected = Panel(dict([(ax, f(self.panel.loc[:, ax])) for ax in self.panel.major_axis]))
assert_panel_equal(result, expected)
index = pandas.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'), ('two', 'a'), ('two', 'b')])
dfa = DataFrame(np.array(np.arange(12, dtype='int64')).reshape(4, 3), columns=list('ABC'), index=index)
tempResult = arange(10, 22, dtype='int64')
	
===================================================================	
SafeForSparse.test_raise_when_not_implemented: 281	
----------------------------	

tempResult = arange(((3 * 4) * 5))
	
===================================================================	
TestPanel.test_constructor_dict_mixed: 769	
----------------------------	

data = dict(((k, v.values) for (k, v) in self.panel.iteritems()))
result = Panel(data)
tempResult = arange(len(self.panel.major_axis))
	
===================================================================	
SafeForSparse.test_set_axis: 137	
----------------------------	

tempResult = arange(len(self.panel4d.labels))
	
===================================================================	
SafeForSparse.test_set_axis: 138	
----------------------------	

new_labels = Index(numpy.arange(len(self.panel4d.labels)))
tempResult = arange(len(self.panel4d.major_axis))
	
===================================================================	
SafeForSparse.test_set_axis: 139	
----------------------------	

new_labels = Index(numpy.arange(len(self.panel4d.labels)))
new_major = Index(numpy.arange(len(self.panel4d.major_axis)))
tempResult = arange(len(self.panel4d.minor_axis))
	
===================================================================	
TestPanel4d.test_constructor_dict_mixed: 516	
----------------------------	

with pandas.util.testing.assert_produces_warning(FutureWarning, check_stacklevel=False):
    data = dict(((k, v.values) for (k, v) in self.panel4d.iteritems()))
    result = Panel4D(data)
    tempResult = arange(len(self.panel4d.major_axis))
	
===================================================================	
TestGetDummies.test_basic_drop_first_one_level: 277	
----------------------------	

s_list = list('aaa')
s_series = Series(s_list)
s_series_index = Series(s_list, list('ABC'))
tempResult = arange(3)
	
===================================================================	
TestGetDummies.test_basic_drop_first_NA: 295	
----------------------------	

s_NA = ['a', 'b', numpy.nan]
res = get_dummies(s_NA, sparse=self.sparse, drop_first=True)
exp = DataFrame({'b': {0: 0, 1: 1, 2: 0}}, dtype=numpy.uint8)
assert_frame_equal(res, exp)
res_na = get_dummies(s_NA, dummy_na=True, sparse=self.sparse, drop_first=True)
exp_na = DataFrame({'b': {0: 0, 1: 1, 2: 0}, nan: {0: 0, 1: 0, 2: 1}}, dtype=np.uint8).reindex_axis(['b', nan], 1)
assert_frame_equal(res_na, exp_na)
res_just_na = get_dummies([nan], dummy_na=True, sparse=self.sparse, drop_first=True)
tempResult = arange(1)
	
===================================================================	
Base: 30	
----------------------------	

_multiprocess_can_split_ = True
tempResult = arange(20, 40)
	
===================================================================	
TestGrouperGrouping.setUp: 1715	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestGrouperGrouping.setUp: 1716	
----------------------------	

self.series = Series(numpy.arange(10))
tempResult = arange(40)
	
===================================================================	
TestApi.test_preserve_metadata: 170	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestMoments.test_rolling_apply_out_of_bounds: 655	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestMomentsConsistency._check_binary_ew: 1315	
----------------------------	


def func(A, B, com, **kwargs):
    return getattr(A.ewm(com, **kwargs), name)(B)
tempResult = arange(50)
	
===================================================================	
create_dataframes: 1016	
----------------------------	

tempResult = arange(10)
	
===================================================================	
create_dataframes: 1016	
----------------------------	

tempResult = arange(25)
	
===================================================================	
create_dataframes: 1016	
----------------------------	

tempResult = arange(25)
	
===================================================================	
TestMoments._check_ew_ndarray: 1001	
----------------------------	

result = func(self.arr, com=10)
if preserve_nan:
    assert np.isnan(result[self._nan_locs]).all()
arr = randn(50)
arr[:10] = numpy.NaN
arr[(- 10):] = numpy.NaN
s = Series(arr)
result = func(s, 50, min_periods=2)
self.assertTrue(np.isnan(result.values[:11]).all())
self.assertFalse(np.isnan(result.values[11:]).any())
for min_periods in (0, 1):
    result = func(s, 50, min_periods=min_periods)
    if (func == pandas.stats.moments.ewma):
        self.assertTrue(np.isnan(result.values[:10]).all())
        self.assertFalse(np.isnan(result.values[10:]).any())
    else:
        self.assertTrue(np.isnan(result.values[:11]).all())
        self.assertFalse(np.isnan(result.values[11:]).any())
    result = func(Series([]), 50, min_periods=min_periods)
    pandas.util.testing.assert_series_equal(result, Series([]))
    result = func(Series([1.0]), 50, min_periods=min_periods)
    if (func == pandas.stats.moments.ewma):
        pandas.util.testing.assert_series_equal(result, Series([1.0]))
    else:
        pandas.util.testing.assert_series_equal(result, Series([numpy.NaN]))
tempResult = arange(50)
	
===================================================================	
TestMomentsConsistency.test_rolling_min_max_numeric_types: 1707	
----------------------------	

types_test = [numpy.dtype('f{}'.format(width)) for width in [4, 8]]
types_test.extend([numpy.dtype('{}{}'.format(sign, width)) for width in [1, 2, 4, 8] for sign in 'ui'])
for data_type in types_test:
    tempResult = arange(20, dtype=data_type)
	
===================================================================	
TestMomentsConsistency.test_rolling_min_max_numeric_types: 1709	
----------------------------	

types_test = [numpy.dtype('f{}'.format(width)) for width in [4, 8]]
types_test.extend([numpy.dtype('{}{}'.format(sign, width)) for width in [1, 2, 4, 8] for sign in 'ui'])
for data_type in types_test:
    result = DataFrame(np.arange(20, dtype=data_type)).rolling(window=5).max()
    self.assertEqual(result.dtypes[0], numpy.dtype('f8'))
    tempResult = arange(20, dtype=data_type)
	
===================================================================	
Dtype._create_dtype_data: 361	
----------------------------	

sr1 = Series(range(5), dtype=dtype)
sr2 = Series(range(10, 0, (- 2)), dtype=dtype)
tempResult = arange(10)
	
===================================================================	
Base._create_data: 39	
----------------------------	

arr = randn(N)
arr[self._nan_locs] = numpy.NaN
self.arr = arr
self.rng = bdate_range(datetime(2009, 1, 1), periods=N)
self.series = Series(arr.copy(), index=self.rng)
tempResult = arange(K)
	
===================================================================	
TestRollingTS.test_all2: 2168	
----------------------------	

tempResult = arange(50)
	
===================================================================	
TestDataFrameFormatting.test_to_html_unicode: 543	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1184	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1184	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
tempResult = arange(w)
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1187	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('...' not in df._repr_html_())
(h, w) = ((max_rows + 1), (max_cols + 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1187	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('...' not in df._repr_html_())
(h, w) = ((max_rows + 1), (max_cols + 1))
tempResult = arange(w)
	
===================================================================	
TestDataFrameFormatting.test_to_string: 892	
----------------------------	

from pandas import read_table
import re
biggie = DataFrame({'A': randn(200), 'B': pandas.util.testing.makeStringIndex(200)}, index=lrange(200))
biggie.loc[:20, 'A'] = nan
biggie.loc[:20, 'B'] = nan
s = biggie.to_string()
buf = StringIO()
retval = biggie.to_string(buf=buf)
self.assertIsNone(retval)
self.assertEqual(buf.getvalue(), s)
pandas.util.testing.assertIsInstance(s, pandas.compat.string_types)
result = biggie.to_string(columns=['B', 'A'], col_space=17, float_format='%.5f'.__mod__)
lines = result.split('\n')
header = lines[0].strip().split()
joined = '\n'.join([re.sub('\\s+', ' ', x).strip() for x in lines[1:]])
recons = read_table(StringIO(joined), names=header, header=None, sep=' ')
pandas.util.testing.assert_series_equal(recons['B'], biggie['B'])
self.assertEqual(recons['A'].count(), biggie['A'].count())
self.assertTrue((np.abs((recons['A'].dropna() - biggie['A'].dropna())) < 0.1).all())
result = biggie.to_string(columns=['A'], col_space=17)
header = result.split('\n')[0].strip().split()
expected = ['A']
self.assertEqual(header, expected)
biggie.to_string(columns=['B', 'A'], formatters={'A': (lambda x: ('%.1f' % x))})
biggie.to_string(columns=['B', 'A'], float_format=str)
biggie.to_string(columns=['B', 'A'], col_space=12, float_format=str)
tempResult = arange(200)
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1215	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1215	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
tempResult = arange(w)
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1220	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('&lt;class' not in df._repr_html_())
with option_context('display.large_repr', 'info'):
    assert ('&lt;class' in df._repr_html_())
(h, w) = ((max_rows - 1), (max_cols + 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1220	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('&lt;class' not in df._repr_html_())
with option_context('display.large_repr', 'info'):
    assert ('&lt;class' in df._repr_html_())
(h, w) = ((max_rows - 1), (max_cols + 1))
tempResult = arange(w)
	
===================================================================	
TestSeriesFormatting.test_period: 1714	
----------------------------	

index = pandas.period_range('2013-01', periods=6, freq='M')
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_multiindex: 1169	
----------------------------	

max_rows = get_option('display.max_rows')
max_L1 = (max_rows // 2)
tempResult = arange(max_L1)
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_multiindex: 1174	
----------------------------	

max_rows = get_option('display.max_rows')
max_L1 = (max_rows // 2)
tuples = list(itertools.product(numpy.arange(max_L1), ['foo', 'bar']))
idx = pandas.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = DataFrame(numpy.random.randn((max_L1 * 2), 2), index=idx, columns=['A', 'B'])
reg_repr = df._repr_html_()
assert ('...' not in reg_repr)
tempResult = arange((max_L1 + 1))
	
===================================================================	
TestStyler.test_mi_sparse_column_names: 441	
----------------------------	

tempResult = arange(16)
	
===================================================================	
TestData.all_mixed: 52	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestDataFrameAlterAxes.test_set_columns: 188	
----------------------------	

tempResult = arange(len(self.mixed_frame.columns))
	
===================================================================	
TestDataFrameAlterAxes.test_set_index: 16	
----------------------------	

tempResult = arange(len(self.mixed_frame))
	
===================================================================	
TestDataFrameAlterAxes.test_reset_index: 296	
----------------------------	

stacked = self.frame.stack()[::2]
stacked = DataFrame({'foo': stacked, 'bar': stacked})
names = ['first', 'second']
stacked.index.names = names
deleveled = stacked.reset_index()
for (i, (lev, lab)) in enumerate(zip(stacked.index.levels, stacked.index.labels)):
    values = lev.take(lab)
    name = names[i]
    pandas.util.testing.assert_index_equal(values, Index(deleveled[name]))
stacked.index.names = [None, None]
deleveled2 = stacked.reset_index()
pandas.util.testing.assert_series_equal(deleveled['first'], deleveled2['level_0'], check_names=False)
pandas.util.testing.assert_series_equal(deleveled['second'], deleveled2['level_1'], check_names=False)
rdf = self.frame.reset_index()
exp = pandas.Series(self.frame.index.values, name='index')
self.assert_series_equal(rdf['index'], exp)
df = self.frame.copy()
df['index'] = 'foo'
rdf = df.reset_index()
exp = pandas.Series(self.frame.index.values, name='level_0')
self.assert_series_equal(rdf['level_0'], exp)
self.frame.index.name = 'index'
deleveled = self.frame.reset_index()
self.assert_series_equal(deleveled['index'], pandas.Series(self.frame.index))
tempResult = arange(len(deleveled))
	
===================================================================	
TestDataFrameAlterAxes.test_reset_index_right_dtype: 322	
----------------------------	

tempResult = arange(0.0, 10, (numpy.sqrt(2) / 2))
	
===================================================================	
TestDataFrameAlterAxes.test_reorder_levels: 255	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
tempResult = arange(6)
	
===================================================================	
TestDataFrameAlterAxes.test_reorder_levels: 255	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
tempResult = arange(6)
	
===================================================================	
TestDataFrameAlterAxes.test_reorder_levels: 262	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
df = DataFrame({'A': numpy.arange(6), 'B': numpy.arange(6)}, index=index)
result = df.reorder_levels([0, 1, 2])
assert_frame_equal(df, result)
result = df.reorder_levels(['L0', 'L1', 'L2'])
assert_frame_equal(df, result)
result = df.reorder_levels([1, 2, 0])
e_idx = MultiIndex(levels=[['one', 'two', 'three'], [0, 1], ['bar']], labels=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0]], names=['L1', 'L2', 'L0'])
tempResult = arange(6)
	
===================================================================	
TestDataFrameAlterAxes.test_reorder_levels: 262	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
df = DataFrame({'A': numpy.arange(6), 'B': numpy.arange(6)}, index=index)
result = df.reorder_levels([0, 1, 2])
assert_frame_equal(df, result)
result = df.reorder_levels(['L0', 'L1', 'L2'])
assert_frame_equal(df, result)
result = df.reorder_levels([1, 2, 0])
e_idx = MultiIndex(levels=[['one', 'two', 'three'], [0, 1], ['bar']], labels=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0]], names=['L1', 'L2', 'L0'])
tempResult = arange(6)
	
===================================================================	
TestDataFrameAlterAxes.test_reorder_levels: 266	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
df = DataFrame({'A': numpy.arange(6), 'B': numpy.arange(6)}, index=index)
result = df.reorder_levels([0, 1, 2])
assert_frame_equal(df, result)
result = df.reorder_levels(['L0', 'L1', 'L2'])
assert_frame_equal(df, result)
result = df.reorder_levels([1, 2, 0])
e_idx = MultiIndex(levels=[['one', 'two', 'three'], [0, 1], ['bar']], labels=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0]], names=['L1', 'L2', 'L0'])
expected = DataFrame({'A': numpy.arange(6), 'B': numpy.arange(6)}, index=e_idx)
assert_frame_equal(result, expected)
result = df.reorder_levels([0, 0, 0])
e_idx = MultiIndex(levels=[['bar'], ['bar'], ['bar']], labels=[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], names=['L0', 'L0', 'L0'])
tempResult = arange(6)
	
===================================================================	
TestDataFrameAlterAxes.test_reorder_levels: 266	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
df = DataFrame({'A': numpy.arange(6), 'B': numpy.arange(6)}, index=index)
result = df.reorder_levels([0, 1, 2])
assert_frame_equal(df, result)
result = df.reorder_levels(['L0', 'L1', 'L2'])
assert_frame_equal(df, result)
result = df.reorder_levels([1, 2, 0])
e_idx = MultiIndex(levels=[['one', 'two', 'three'], [0, 1], ['bar']], labels=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0]], names=['L1', 'L2', 'L0'])
expected = DataFrame({'A': numpy.arange(6), 'B': numpy.arange(6)}, index=e_idx)
assert_frame_equal(result, expected)
result = df.reorder_levels([0, 0, 0])
e_idx = MultiIndex(levels=[['bar'], ['bar'], ['bar']], labels=[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], names=['L0', 'L0', 'L0'])
tempResult = arange(6)
	
===================================================================	
TestDataFrameAnalytics.test_corrwith_matches_corrcoef: 159	
----------------------------	

tempResult = arange(10000)
	
===================================================================	
TestDataFrameAnalytics.test_corrwith_matches_corrcoef: 160	
----------------------------	

df1 = DataFrame(numpy.arange(10000), columns=['a'])
tempResult = arange(10000)
	
===================================================================	
TestDataFrameAnalytics.test_mode: 615	
----------------------------	

tempResult = arange(6, dtype='int64')
	
===================================================================	
TestDataFrameAnalytics.test_mode: 630	
----------------------------	

df = pandas.DataFrame({'A': [12, 12, 11, 12, 19, 11], 'B': [10, 10, 10, numpy.nan, 3, 4], 'C': [8, 8, 8, 9, 9, 9], 'D': numpy.arange(6, dtype='int64'), 'E': [8, 8, 1, 1, 3, 3]})
pandas.util.testing.assert_frame_equal(df[['A']].mode(), pandas.DataFrame({'A': [12]}))
expected = pd.Series([], dtype='int64', name='D').to_frame()
pandas.util.testing.assert_frame_equal(df[['D']].mode(), expected)
expected = pd.Series([1, 3, 8], dtype='int64', name='E').to_frame()
pandas.util.testing.assert_frame_equal(df[['E']].mode(), expected)
pandas.util.testing.assert_frame_equal(df[['A', 'B']].mode(), pandas.DataFrame({'A': [12], 'B': [10.0]}))
pandas.util.testing.assert_frame_equal(df.mode(), pandas.DataFrame({'A': [12, numpy.nan, numpy.nan], 'B': [10, numpy.nan, numpy.nan], 'C': [8, 9, numpy.nan], 'D': [numpy.nan, numpy.nan, numpy.nan], 'E': [1, 3, 8]}))
df['C'] = list(reversed(df['C']))
pandas.formats.printing.pprint_thing(df['C'])
pandas.formats.printing.pprint_thing(df['C'].mode())
(a, b) = (df[['A', 'B', 'C']].mode(), pandas.DataFrame({'A': [12, numpy.nan], 'B': [10, numpy.nan], 'C': [8, 9]}))
pandas.formats.printing.pprint_thing(a)
pandas.formats.printing.pprint_thing(b)
pandas.util.testing.assert_frame_equal(a, b)
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestDataFrameAnalytics.test_cumsum_corner: 736	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestDataFrameAnalytics.test_cummax: 308	
----------------------------	

self.tsframe.ix[5:10, 0] = nan
self.tsframe.ix[10:15, 1] = nan
self.tsframe.ix[15:, 2] = nan
cummax = self.tsframe.cummax()
expected = self.tsframe.apply(pandas.Series.cummax)
pandas.util.testing.assert_frame_equal(cummax, expected)
cummax = self.tsframe.cummax(axis=1)
expected = self.tsframe.apply(pandas.Series.cummax, axis=1)
pandas.util.testing.assert_frame_equal(cummax, expected)
tempResult = arange(20)
	
===================================================================	
TestDataFrameAnalytics.test_cummax: 308	
----------------------------	

self.tsframe.ix[5:10, 0] = nan
self.tsframe.ix[10:15, 1] = nan
self.tsframe.ix[15:, 2] = nan
cummax = self.tsframe.cummax()
expected = self.tsframe.apply(pandas.Series.cummax)
pandas.util.testing.assert_frame_equal(cummax, expected)
cummax = self.tsframe.cummax(axis=1)
expected = self.tsframe.apply(pandas.Series.cummax, axis=1)
pandas.util.testing.assert_frame_equal(cummax, expected)
tempResult = arange(20)
	
===================================================================	
TestDataFrameAnalytics.test_cummin: 293	
----------------------------	

self.tsframe.ix[5:10, 0] = nan
self.tsframe.ix[10:15, 1] = nan
self.tsframe.ix[15:, 2] = nan
cummin = self.tsframe.cummin()
expected = self.tsframe.apply(pandas.Series.cummin)
pandas.util.testing.assert_frame_equal(cummin, expected)
cummin = self.tsframe.cummin(axis=1)
expected = self.tsframe.apply(pandas.Series.cummin, axis=1)
pandas.util.testing.assert_frame_equal(cummin, expected)
tempResult = arange(20)
	
===================================================================	
TestDataFrameAnalytics.test_cummin: 293	
----------------------------	

self.tsframe.ix[5:10, 0] = nan
self.tsframe.ix[10:15, 1] = nan
self.tsframe.ix[15:, 2] = nan
cummin = self.tsframe.cummin()
expected = self.tsframe.apply(pandas.Series.cummin)
pandas.util.testing.assert_frame_equal(cummin, expected)
cummin = self.tsframe.cummin(axis=1)
expected = self.tsframe.apply(pandas.Series.cummin, axis=1)
pandas.util.testing.assert_frame_equal(cummin, expected)
tempResult = arange(20)
	
===================================================================	
TestDataFrameAnalytics.test_cumsum: 367	
----------------------------	

self.tsframe.ix[5:10, 0] = nan
self.tsframe.ix[10:15, 1] = nan
self.tsframe.ix[15:, 2] = nan
cumsum = self.tsframe.cumsum()
expected = self.tsframe.apply(pandas.Series.cumsum)
pandas.util.testing.assert_frame_equal(cumsum, expected)
cumsum = self.tsframe.cumsum(axis=1)
expected = self.tsframe.apply(pandas.Series.cumsum, axis=1)
pandas.util.testing.assert_frame_equal(cumsum, expected)
tempResult = arange(20)
	
===================================================================	
TestDataFrameAnalytics.test_cumsum: 367	
----------------------------	

self.tsframe.ix[5:10, 0] = nan
self.tsframe.ix[10:15, 1] = nan
self.tsframe.ix[15:, 2] = nan
cumsum = self.tsframe.cumsum()
expected = self.tsframe.apply(pandas.Series.cumsum)
pandas.util.testing.assert_frame_equal(cumsum, expected)
cumsum = self.tsframe.cumsum(axis=1)
expected = self.tsframe.apply(pandas.Series.cumsum, axis=1)
pandas.util.testing.assert_frame_equal(cumsum, expected)
tempResult = arange(20)
	
===================================================================	
TestDataFrameApply.test_apply_mixed_datetimelike: 35	
----------------------------	

tempResult = arange(3)
	
===================================================================	
TestFrameAsof.setUp: 15	
----------------------------	

self.N = N = 50
rng = date_range('1/1/1990', periods=N, freq='53s')
tempResult = arange(N)
	
===================================================================	
TestFrameAsof.setUp: 15	
----------------------------	

self.N = N = 50
rng = date_range('1/1/1990', periods=N, freq='53s')
tempResult = arange(N)
	
===================================================================	
TestFrameAsof.test_subset: 35	
----------------------------	

N = 10
rng = date_range('1/1/1990', periods=N, freq='53s')
tempResult = arange(N)
	
===================================================================	
TestFrameAsof.test_subset: 35	
----------------------------	

N = 10
rng = date_range('1/1/1990', periods=N, freq='53s')
tempResult = arange(N)
	
===================================================================	
TestDataFrameSelectReindex.test_reindex_name_remains: 164	
----------------------------	

s = Series(numpy.random.rand(10))
tempResult = arange(len(s))
	
===================================================================	
TestDataFrameSelectReindex.test_reindex_name_remains: 165	
----------------------------	

s = Series(numpy.random.rand(10))
df = DataFrame(s, index=numpy.arange(len(s)))
tempResult = arange(10)
	
===================================================================	
TestDataFrameSelectReindex.test_reindex_name_remains: 168	
----------------------------	

s = Series(numpy.random.rand(10))
df = DataFrame(s, index=numpy.arange(len(s)))
i = Series(numpy.arange(10), name='iname')
df = df.reindex(i)
self.assertEqual(df.index.name, 'iname')
tempResult = arange(10)
	
===================================================================	
TestDataFrameSelectReindex.test_reindex_name_remains: 171	
----------------------------	

s = Series(numpy.random.rand(10))
df = DataFrame(s, index=numpy.arange(len(s)))
i = Series(numpy.arange(10), name='iname')
df = df.reindex(i)
self.assertEqual(df.index.name, 'iname')
df = df.reindex(Index(numpy.arange(10), name='tmpname'))
self.assertEqual(df.index.name, 'tmpname')
s = Series(numpy.random.rand(10))
tempResult = arange(len(s))
	
===================================================================	
TestDataFrameSelectReindex.test_reindex_name_remains: 172	
----------------------------	

s = Series(numpy.random.rand(10))
df = DataFrame(s, index=numpy.arange(len(s)))
i = Series(numpy.arange(10), name='iname')
df = df.reindex(i)
self.assertEqual(df.index.name, 'iname')
df = df.reindex(Index(numpy.arange(10), name='tmpname'))
self.assertEqual(df.index.name, 'tmpname')
s = Series(numpy.random.rand(10))
df = DataFrame(s.T, index=numpy.arange(len(s)))
tempResult = arange(10)
	
===================================================================	
TestDataFrameSelectReindex.test_align_multiindex: 378	
----------------------------	

midx = pandas.MultiIndex.from_product([range(2), range(3), range(2)], names=('a', 'b', 'c'))
idx = pandas.Index(range(2), name='b')
tempResult = arange(12, dtype='int64')
	
===================================================================	
TestDataFrameSelectReindex.test_align_multiindex: 379	
----------------------------	

midx = pandas.MultiIndex.from_product([range(2), range(3), range(2)], names=('a', 'b', 'c'))
idx = pandas.Index(range(2), name='b')
df1 = pandas.DataFrame(numpy.arange(12, dtype='int64'), index=midx)
tempResult = arange(2, dtype='int64')
	
===================================================================	
TestDataFrameSelectReindex.test_align_int_fill_bug: 366	
----------------------------	

tempResult = arange((10 * 10), dtype='float64')
	
===================================================================	
TestDataFrameSelectReindex.test_reindex_boolean: 532	
----------------------------	

tempResult = arange(0, 20, 2)
	
===================================================================	
TestDataFrameSelectReindex.test_reindex_boolean: 533	
----------------------------	

frame = DataFrame(numpy.ones((10, 2), dtype=bool), index=numpy.arange(0, 20, 2), columns=[0, 2])
tempResult = arange(10)
	
===================================================================	
TestDataFrameBlockInternals.test_get_numeric_data: 251	
----------------------------	

intname = np.dtype(np.int_).name
floatname = np.dtype(np.float_).name
datetime64name = np.dtype('M8[ns]').name
objectname = np.dtype(np.object_).name
tempResult = arange(10)
	
===================================================================	
TestDataFrameBlockInternals.test_get_numeric_data: 257	
----------------------------	

intname = np.dtype(np.int_).name
floatname = np.dtype(np.float_).name
datetime64name = np.dtype('M8[ns]').name
objectname = np.dtype(np.object_).name
df = DataFrame({'a': 1.0, 'b': 2, 'c': 'foo', 'f': Timestamp('20010102')}, index=numpy.arange(10))
result = df.get_dtype_counts()
expected = Series({'int64': 1, 'float64': 1, datetime64name: 1, objectname: 1})
result.sort_index()
expected.sort_index()
assert_series_equal(result, expected)
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_constructor_error_msgs: 204	
----------------------------	

msg = 'Empty data passed with indices specified.'
with pandas.util.testing.assertRaisesRegexp(ValueError, msg):
    DataFrame(numpy.empty(0), columns=list('abc'))
msg = 'Mixing dicts with non-Series may lead to ambiguous ordering.'
with pandas.util.testing.assertRaisesRegexp(ValueError, msg):
    DataFrame({'A': {'a': 'a', 'b': 'b'}, 'B': ['a', 'b', 'c']})
msg = 'Shape of passed values is \\(3, 4\\), indices imply \\(3, 3\\)'
with pandas.util.testing.assertRaisesRegexp(ValueError, msg):
    tempResult = arange(12)
	
===================================================================	
TestDataFrameConstructors.test_constructor_list_of_series: 640	
----------------------------	

data = [OrderedDict([['a', 1.5], ['b', 3.0], ['c', 4.0]]), OrderedDict([['a', 1.5], ['b', 3.0], ['c', 6.0]])]
sdict = OrderedDict(zip(['x', 'y'], data))
idx = Index(['a', 'b', 'c'])
data2 = [Series([1.5, 3, 4], idx, dtype='O', name='x'), Series([1.5, 3, 6], idx, name='y')]
result = DataFrame(data2)
expected = pandas.DataFrame.from_dict(sdict, orient='index')
pandas.util.testing.assert_frame_equal(result, expected)
data2 = [Series([1.5, 3, 4], idx, dtype='O', name='x'), Series([1.5, 3, 6], idx)]
result = DataFrame(data2)
sdict = OrderedDict(zip(['x', 'Unnamed 0'], data))
expected = pandas.DataFrame.from_dict(sdict, orient='index')
pandas.util.testing.assert_frame_equal(result.sort_index(), expected)
data = [OrderedDict([['a', 1.5], ['b', 3], ['c', 4], ['d', 6]]), OrderedDict([['a', 1.5], ['b', 3], ['d', 6]]), OrderedDict([['a', 1.5], ['d', 6]]), OrderedDict(), OrderedDict([['a', 1.5], ['b', 3], ['c', 4]]), OrderedDict([['b', 3], ['c', 4], ['d', 6]])]
data = [Series(d) for d in data]
result = DataFrame(data)
sdict = OrderedDict(zip(range(len(data)), data))
expected = pandas.DataFrame.from_dict(sdict, orient='index')
pandas.util.testing.assert_frame_equal(result, expected.reindex(result.index))
tempResult = arange(6)
	
===================================================================	
TestDataFrameConstructors.test_constructor_rec: 118	
----------------------------	

rec = self.frame.to_records(index=False)
index = self.frame.index
df = DataFrame(rec)
self.assert_index_equal(df.columns, pandas.Index(rec.dtype.names))
df2 = DataFrame(rec, index=index)
self.assert_index_equal(df2.columns, pandas.Index(rec.dtype.names))
self.assert_index_equal(df2.index, index)
tempResult = arange(len(rec))
	
===================================================================	
TestDataFrameConstructors.check: 985	
----------------------------	

for i in range(len(df.columns)):
    df.iloc[:, i]
tempResult = arange(len(df.columns))
	
===================================================================	
TestDataFrameConstructors.test_from_records_sequencelike: 1161	
----------------------------	

tempResult = arange(6)
	
===================================================================	
TestDataFrameConstructors.test_from_records_sequencelike: 1161	
----------------------------	

tempResult = arange(6)
	
===================================================================	
TestDataFrameConstructors.test_constructor_for_list_with_dtypes: 919	
----------------------------	

intname = np.dtype(np.int_).name
floatname = np.dtype(np.float_).name
datetime64name = np.dtype('M8[ns]').name
objectname = np.dtype(np.object_).name
tempResult = arange(5)
	
===================================================================	
TestDataFrameConstructors.test_constructor_for_list_with_dtypes: 922	
----------------------------	

intname = np.dtype(np.int_).name
floatname = np.dtype(np.float_).name
datetime64name = np.dtype('M8[ns]').name
objectname = np.dtype(np.object_).name
df = DataFrame([numpy.arange(5) for x in range(5)])
result = df.get_dtype_counts()
expected = Series({'int64': 5})
tempResult = arange(5)
	
===================================================================	
TestDataFrameConstructors.test_from_records_dictlike: 1201	
----------------------------	

tempResult = arange(6)
	
===================================================================	
TestDataFrameConstructors.test_from_records_dictlike: 1201	
----------------------------	

tempResult = arange(6)
	
===================================================================	
TestDataFrameConstructors.test_constructor_more: 495	
----------------------------	

arr = randn(10)
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_constructor_more: 501	
----------------------------	

arr = randn(10)
dm = DataFrame(arr, columns=['A'], index=numpy.arange(10))
self.assertEqual(dm.values.ndim, 2)
arr = randn(0)
dm = DataFrame(arr)
self.assertEqual(dm.values.ndim, 2)
self.assertEqual(dm.values.ndim, 2)
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_constructor_more: 505	
----------------------------	

arr = randn(10)
dm = DataFrame(arr, columns=['A'], index=numpy.arange(10))
self.assertEqual(dm.values.ndim, 2)
arr = randn(0)
dm = DataFrame(arr)
self.assertEqual(dm.values.ndim, 2)
self.assertEqual(dm.values.ndim, 2)
dm = DataFrame(columns=['A', 'B'], index=numpy.arange(10))
self.assertEqual(dm.values.shape, (10, 2))
dm = DataFrame(columns=['A', 'B'])
self.assertEqual(dm.values.shape, (0, 2))
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_constructor_more: 514	
----------------------------	

arr = randn(10)
dm = DataFrame(arr, columns=['A'], index=numpy.arange(10))
self.assertEqual(dm.values.ndim, 2)
arr = randn(0)
dm = DataFrame(arr)
self.assertEqual(dm.values.ndim, 2)
self.assertEqual(dm.values.ndim, 2)
dm = DataFrame(columns=['A', 'B'], index=numpy.arange(10))
self.assertEqual(dm.values.shape, (10, 2))
dm = DataFrame(columns=['A', 'B'])
self.assertEqual(dm.values.shape, (0, 2))
dm = DataFrame(index=numpy.arange(10))
self.assertEqual(dm.values.shape, (10, 0))
with pandas.util.testing.assertRaisesRegexp(PandasError, 'constructor not properly called'):
    DataFrame((1, 2, 3))
mat = np.array(['foo', 'bar'], dtype=object).reshape(2, 1)
with pandas.util.testing.assertRaisesRegexp(ValueError, 'cast'):
    DataFrame(mat, index=[0, 1], columns=[0], dtype=float)
dm = DataFrame(DataFrame(self.frame._series))
pandas.util.testing.assert_frame_equal(dm, self.frame)
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_from_records_to_records: 1011	
----------------------------	

arr = numpy.zeros((2,), dtype='i4,f4,a10')
arr[:] = [(1, 2.0, 'Hello'), (2, 3.0, 'World')]
frame = pandas.DataFrame.from_records(arr)
tempResult = arange(len(arr))
	
===================================================================	
TestDataFrameConstructors.test_constructor_scalar_inference: 473	
----------------------------	

data = {'int': 1, 'bool': True, 'float': 3.0, 'complex': 4j, 'object': 'foo'}
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_constructor_with_datetimes: 838	
----------------------------	

intname = np.dtype(np.int_).name
floatname = np.dtype(np.float_).name
datetime64name = np.dtype('M8[ns]').name
objectname = np.dtype(np.object_).name
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_constructor_with_datetimes: 844	
----------------------------	

intname = np.dtype(np.int_).name
floatname = np.dtype(np.float_).name
datetime64name = np.dtype('M8[ns]').name
objectname = np.dtype(np.object_).name
df = DataFrame({'A': 1, 'B': 'foo', 'C': 'bar', 'D': Timestamp('20010101'), 'E': datetime(2001, 1, 2, 0, 0)}, index=numpy.arange(10))
result = df.get_dtype_counts()
expected = Series({'int64': 1, datetime64name: 2, objectname: 2})
result.sort_index()
expected.sort_index()
pandas.util.testing.assert_series_equal(result, expected)
tempResult = arange(10)
	
===================================================================	
TestDataFrameConstructors.test_constructor_with_datetimes: 861	
----------------------------	

intname = np.dtype(np.int_).name
floatname = np.dtype(np.float_).name
datetime64name = np.dtype('M8[ns]').name
objectname = np.dtype(np.object_).name
df = DataFrame({'A': 1, 'B': 'foo', 'C': 'bar', 'D': Timestamp('20010101'), 'E': datetime(2001, 1, 2, 0, 0)}, index=numpy.arange(10))
result = df.get_dtype_counts()
expected = Series({'int64': 1, datetime64name: 2, objectname: 2})
result.sort_index()
expected.sort_index()
pandas.util.testing.assert_series_equal(result, expected)
df = DataFrame({'a': 1.0, 'b': 2, 'c': 'foo', floatname: numpy.array(1.0, dtype=floatname), intname: numpy.array(1, dtype=intname)}, index=numpy.arange(10))
result = df.get_dtype_counts()
expected = {objectname: 1}
if (intname == 'int64'):
    expected['int64'] = 2
else:
    expected['int64'] = 1
    expected[intname] = 1
if (floatname == 'float64'):
    expected['float64'] = 2
else:
    expected['float64'] = 1
    expected[floatname] = 1
result.sort_index()
expected = Series(expected)
expected.sort_index()
pandas.util.testing.assert_series_equal(result, expected)
tempResult = arange(10)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_not_an_attr_but_still_valid_dtype: 96	
----------------------------	

tempResult = arange(3, 6)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_not_an_attr_but_still_valid_dtype: 96	
----------------------------	

tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_include: 68	
----------------------------	

tempResult = arange(3, 6)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_include: 68	
----------------------------	

tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_exclude_include: 83	
----------------------------	

tempResult = arange(3, 6)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_exclude_include: 83	
----------------------------	

tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_bad_arg_raises: 148	
----------------------------	

tempResult = arange(3, 6)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_bad_arg_raises: 148	
----------------------------	

tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_exclude: 77	
----------------------------	

tempResult = arange(3, 6)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_exclude: 77	
----------------------------	

tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_str_raises: 135	
----------------------------	

tempResult = arange(3, 6)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_str_raises: 135	
----------------------------	

tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_bad_datetime64: 121	
----------------------------	

tempResult = arange(3, 6)
	
===================================================================	
TestDataFrameDataTypes.test_select_dtypes_bad_datetime64: 121	
----------------------------	

tempResult = arange(4.0, 7.0, dtype='float64')
	
===================================================================	
TestDataFrameIndexing.test_fancy_index_int_labels_exceptions: 660	
----------------------------	

tempResult = arange(0, 20, 2)
	
===================================================================	
TestDataFrameIndexing.test_fancy_getitem_int_labels: 645	
----------------------------	

tempResult = arange(0, 20, 2)
	
===================================================================	
TestDataFrameIndexing.test_setitem_cast: 371	
----------------------------	

self.frame['D'] = self.frame['D'].astype('i8')
self.assertEqual(self.frame['D'].dtype, numpy.int64)
self.frame['B'] = 0
self.assertEqual(self.frame['B'].dtype, numpy.int64)
tempResult = arange(len(self.frame))
	
===================================================================	
TestDataFrameIndexing.test_setitem_cast: 391	
----------------------------	

self.frame['D'] = self.frame['D'].astype('i8')
self.assertEqual(self.frame['D'].dtype, numpy.int64)
self.frame['B'] = 0
self.assertEqual(self.frame['B'].dtype, numpy.int64)
self.frame['B'] = numpy.arange(len(self.frame))
self.assertTrue(issubclass(self.frame['B'].dtype.type, numpy.integer))
self.frame['foo'] = 'bar'
self.frame['foo'] = 0
self.assertEqual(self.frame['foo'].dtype, numpy.int64)
self.frame['foo'] = 'bar'
self.frame['foo'] = 2.5
self.assertEqual(self.frame['foo'].dtype, numpy.float64)
self.frame['something'] = 0
self.assertEqual(self.frame['something'].dtype, numpy.int64)
self.frame['something'] = 2
self.assertEqual(self.frame['something'].dtype, numpy.int64)
self.frame['something'] = 2.5
self.assertEqual(self.frame['something'].dtype, numpy.float64)
df = DataFrame(numpy.random.rand(30, 3), columns=tuple('ABC'))
df['event'] = numpy.nan
df.loc[(10, 'event')] = 'foo'
result = df.get_dtype_counts().sort_values()
expected = Series({'float64': 3, 'object': 1}).sort_values()
assert_series_equal(result, expected)
tempResult = arange(6, dtype=numpy.int8)
	
===================================================================	
TestDataFrameIndexing.test_getitem_boolean_list: 215	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestDataFrameIndexing.test_fancy_setitem_int_labels: 627	
----------------------------	

tempResult = arange(0, 20, 2)
	
===================================================================	
TestDataFrameIndexing.test_xs_view: 1514	
----------------------------	

tempResult = arange(20.0)
	
===================================================================	
TestDataFrameIndexing.test_head_tail: 1866	
----------------------------	

assert_frame_equal(self.frame.head(), self.frame[:5])
assert_frame_equal(self.frame.tail(), self.frame[(- 5):])
assert_frame_equal(self.frame.head(0), self.frame[0:0])
assert_frame_equal(self.frame.tail(0), self.frame[0:0])
assert_frame_equal(self.frame.head((- 1)), self.frame[:(- 1)])
assert_frame_equal(self.frame.tail((- 1)), self.frame[1:])
assert_frame_equal(self.frame.head(1), self.frame[:1])
assert_frame_equal(self.frame.tail(1), self.frame[(- 1):])
df = self.frame.copy()
tempResult = arange(len(self.frame))
	
===================================================================	
TestDataFrameIndexing.test_setitem_corner: 405	
----------------------------	

tempResult = arange(3)
	
===================================================================	
TestDataFrameIndexing.test_setitem_corner: 426	
----------------------------	

df = DataFrame({'B': [1.0, 2.0, 3.0], 'C': ['a', 'b', 'c']}, index=numpy.arange(3))
del df['B']
df['B'] = [1.0, 2.0, 3.0]
self.assertIn('B', df)
self.assertEqual(len(df.columns), 2)
df['A'] = 'beginning'
df['E'] = 'foo'
df['D'] = 'bar'
df[datetime.datetime.now()] = 'date'
df[datetime.datetime.now()] = 5.0
dm = DataFrame(index=self.frame.index)
dm['A'] = 'foo'
dm['B'] = 'bar'
self.assertEqual(len(dm.columns), 2)
self.assertEqual(dm.values.dtype, numpy.object_)
dm['C'] = 1
self.assertEqual(dm['C'].dtype, numpy.int64)
dm['E'] = 1.0
self.assertEqual(dm['E'].dtype, numpy.float64)
dm['A'] = 'bar'
self.assertEqual('bar', dm['A'][0])
tempResult = arange(3)
	
===================================================================	
TestDataFrameMisc.test_nonzero: 174	
----------------------------	

self.assertTrue(self.empty.empty)
self.assertFalse(self.frame.empty)
self.assertFalse(self.mixed_frame.empty)
tempResult = arange(3)
	
===================================================================	
TestDataFrameMutateColumns.test_insert: 83	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestDataFrameOperators.test_binary_ops_align: 301	
----------------------------	

index = pandas.MultiIndex.from_product([list('abc'), ['one', 'two', 'three'], [1, 2, 3]], names=['first', 'second', 'third'])
tempResult = arange((27 * 3))
	
===================================================================	
TestDataFrameOperators.test_combineAdd: 691	
----------------------------	

with pandas.util.testing.assert_produces_warning(FutureWarning):
    comb = self.frame.combineAdd(self.frame)
assert_frame_equal(comb, (self.frame * 2))
tempResult = arange(5)
	
===================================================================	
TestDataFrameOperators.test_combineAdd: 692	
----------------------------	

with pandas.util.testing.assert_produces_warning(FutureWarning):
    comb = self.frame.combineAdd(self.frame)
assert_frame_equal(comb, (self.frame * 2))
a = DataFrame([[1.0, nan, nan, 2.0, nan]], columns=numpy.arange(5))
tempResult = arange(6)
	
===================================================================	
TestDataFrameOperators.test_combineAdd: 693	
----------------------------	

with pandas.util.testing.assert_produces_warning(FutureWarning):
    comb = self.frame.combineAdd(self.frame)
assert_frame_equal(comb, (self.frame * 2))
a = DataFrame([[1.0, nan, nan, 2.0, nan]], columns=numpy.arange(5))
b = DataFrame([[2.0, 3.0, nan, 2.0, 6.0, nan]], columns=numpy.arange(6))
tempResult = arange(6)
	
===================================================================	
TestDataFrameOperators.test_inplace_ops_alignment: 745	
----------------------------	

columns = list('abcdefg')
tempResult = arange((10 * len(columns)))
	
===================================================================	
TestDataFrameOperators.test_arith_flex_series: 479	
----------------------------	

df = self.simple
row = df.xs('a')
col = df['two']
ops = ['add', 'sub', 'mul', 'mod']
for op in ops:
    f = getattr(df, op)
    op = getattr(operator, op)
    assert_frame_equal(f(row), op(df, row))
    assert_frame_equal(f(col, axis=0), op(df.T, col).T)
assert_frame_equal(df.add(row, axis=None), (df + row))
assert_frame_equal(df.div(row), (df / row))
assert_frame_equal(df.div(col, axis=0), (df.T / col).T)
tempResult = arange((3 * 2))
	
===================================================================	
TestDataFrameOperators.test_arith_flex_series: 483	
----------------------------	

df = self.simple
row = df.xs('a')
col = df['two']
ops = ['add', 'sub', 'mul', 'mod']
for op in ops:
    f = getattr(df, op)
    op = getattr(operator, op)
    assert_frame_equal(f(row), op(df, row))
    assert_frame_equal(f(col, axis=0), op(df.T, col).T)
assert_frame_equal(df.add(row, axis=None), (df + row))
assert_frame_equal(df.div(row), (df / row))
assert_frame_equal(df.div(col, axis=0), (df.T / col).T)
df = DataFrame(np.arange((3 * 2)).reshape((3, 2)), dtype='int64')
expected = DataFrame([[nan, numpy.inf], [1.0, 1.5], [1.0, 1.25]])
result = df.div(df[0], axis='index')
assert_frame_equal(result, expected)
tempResult = arange((3 * 2))
	
===================================================================	
TestDataFrameOperators.test_boolean_comparison: 643	
----------------------------	

tempResult = arange(6)
	
===================================================================	
TestDataFrameOperators.test_boolean_comparison: 677	
----------------------------	

df = DataFrame(np.arange(6).reshape((3, 2)))
b = numpy.array([2, 2])
b_r = numpy.atleast_2d([2, 2])
b_c = b_r.T
l = (2, 2, 2)
tup = tuple(l)
expected = DataFrame([[False, False], [False, True], [True, True]])
result = (df > b)
assert_frame_equal(result, expected)
result = (df.values > b)
assert_numpy_array_equal(result, expected.values)
result = (df > l)
assert_frame_equal(result, expected)
result = (df > tup)
assert_frame_equal(result, expected)
result = (df > b_r)
assert_frame_equal(result, expected)
result = (df.values > b_r)
assert_numpy_array_equal(result, expected.values)
self.assertRaises(ValueError, df.__gt__, b_c)
self.assertRaises(ValueError, df.values.__gt__, b_c)
expected = DataFrame([[False, False], [True, False], [False, False]])
result = (df == b)
assert_frame_equal(result, expected)
result = (df == l)
assert_frame_equal(result, expected)
result = (df == tup)
assert_frame_equal(result, expected)
result = (df == b_r)
assert_frame_equal(result, expected)
result = (df.values == b_r)
assert_numpy_array_equal(result, expected.values)
self.assertRaises(ValueError, (lambda : (df == b_c)))
self.assertFalse(numpy.array_equal(df.values, b_c))
tempResult = arange(6)
	
===================================================================	
TestDataFrameQuantile.test_quantile_nan: 181	
----------------------------	

tempResult = arange(1, 6.0)
	
===================================================================	
TestDataFrameQuantile.test_quantile_nan: 181	
----------------------------	

tempResult = arange(1, 6.0)
	
===================================================================	
TestDataFrameQuantile.test_quantile_nan: 190	
----------------------------	

df = DataFrame({'a': numpy.arange(1, 6.0), 'b': numpy.arange(1, 6.0)})
df.iloc[((- 1), 1)] = numpy.nan
res = df.quantile(0.5)
exp = Series([3.0, 2.5], index=['a', 'b'], name=0.5)
pandas.util.testing.assert_series_equal(res, exp)
res = df.quantile([0.5, 0.75])
exp = DataFrame({'a': [3.0, 4.0], 'b': [2.5, 3.25]}, index=[0.5, 0.75])
pandas.util.testing.assert_frame_equal(res, exp)
res = df.quantile(0.5, axis=1)
tempResult = arange(1.0, 6.0)
	
===================================================================	
TestDataFrameQuantile.test_quantile_nan: 193	
----------------------------	

df = DataFrame({'a': numpy.arange(1, 6.0), 'b': numpy.arange(1, 6.0)})
df.iloc[((- 1), 1)] = numpy.nan
res = df.quantile(0.5)
exp = Series([3.0, 2.5], index=['a', 'b'], name=0.5)
pandas.util.testing.assert_series_equal(res, exp)
res = df.quantile([0.5, 0.75])
exp = DataFrame({'a': [3.0, 4.0], 'b': [2.5, 3.25]}, index=[0.5, 0.75])
pandas.util.testing.assert_frame_equal(res, exp)
res = df.quantile(0.5, axis=1)
exp = Series(numpy.arange(1.0, 6.0), name=0.5)
pandas.util.testing.assert_series_equal(res, exp)
res = df.quantile([0.5, 0.75], axis=1)
tempResult = arange(1.0, 6.0)
	
===================================================================	
TestDataFrameQueryStrings.check_query_lex_compare_strings: 822	
----------------------------	

pandas.util.testing.skip_if_no_ne(engine=engine)
import operator as opr
a = Series(numpy.random.choice(list('abcde'), 20))
tempResult = arange(a.size)
	
===================================================================	
TestDataFrameQueryNumExprPandas.test_date_query_with_non_date: 390	
----------------------------	

(engine, parser) = (self.engine, self.parser)
n = 10
tempResult = arange(n)
	
===================================================================	
TestDataFrameQueryWithMultiIndex.check_query_with_partially_named_multiindex: 233	
----------------------------	

pandas.util.testing.skip_if_no_ne(engine)
a = numpy.random.choice(['red', 'green'], size=10)
tempResult = arange(10)
	
===================================================================	
TestDataFrameReplace.test_replace_str_to_str_chain: 660	
----------------------------	

tempResult = arange(1, 5)
	
===================================================================	
TestDataFrameReplace.test_replace_str_to_str_chain: 662	
----------------------------	

a = numpy.arange(1, 5)
astr = a.astype(str)
tempResult = arange(2, 6)
	
===================================================================	
TestDataFrameReprInfoEtc.test_repr_empty: 20	
----------------------------	

foo = repr(self.empty)
tempResult = arange(1000)
	
===================================================================	
TestDataFrameReprInfoEtc.test_repr_unsortable: 67	
----------------------------	

import warnings
warn_filters = warnings.filters
warnings.filterwarnings('ignore', category=FutureWarning, module='.*format')
tempResult = arange(50)
	
===================================================================	
TestDataFrameReshape.test_unstack_dtypes: 272	
----------------------------	

rows = [[1, 1, 3, 4], [1, 2, 3, 4], [2, 1, 3, 4], [2, 2, 3, 4]]
df = DataFrame(rows, columns=list('ABCD'))
result = df.get_dtype_counts()
expected = Series({'int64': 4})
assert_series_equal(result, expected)
df2 = df.set_index(['A', 'B'])
df3 = df2.unstack('B')
result = df3.get_dtype_counts()
expected = Series({'int64': 4})
assert_series_equal(result, expected)
df2 = df.set_index(['A', 'B'])
df2['C'] = 3.0
df3 = df2.unstack('B')
result = df3.get_dtype_counts()
expected = Series({'int64': 2, 'float64': 2})
assert_series_equal(result, expected)
df2['D'] = 'foo'
df3 = df2.unstack('B')
result = df3.get_dtype_counts()
expected = Series({'float64': 2, 'object': 2})
assert_series_equal(result, expected)
tempResult = arange(5, dtype='f8')
	
===================================================================	
TestDataFrameReshape.test_unstack_dtypes: 272	
----------------------------	

rows = [[1, 1, 3, 4], [1, 2, 3, 4], [2, 1, 3, 4], [2, 2, 3, 4]]
df = DataFrame(rows, columns=list('ABCD'))
result = df.get_dtype_counts()
expected = Series({'int64': 4})
assert_series_equal(result, expected)
df2 = df.set_index(['A', 'B'])
df3 = df2.unstack('B')
result = df3.get_dtype_counts()
expected = Series({'int64': 4})
assert_series_equal(result, expected)
df2 = df.set_index(['A', 'B'])
df2['C'] = 3.0
df3 = df2.unstack('B')
result = df3.get_dtype_counts()
expected = Series({'int64': 2, 'float64': 2})
assert_series_equal(result, expected)
df2['D'] = 'foo'
df3 = df2.unstack('B')
result = df3.get_dtype_counts()
expected = Series({'float64': 2, 'object': 2})
assert_series_equal(result, expected)
tempResult = arange(5, 10, dtype='f8')
	
===================================================================	
TestDataFrameReshape._test_stack_with_multiindex: 388	
----------------------------	

tempResult = arange((3 * len(multiindex)))
	
===================================================================	
TestDataFrameReshape.test_unstack_nan_index: 347	
----------------------------	

cast = (lambda val: '{0:1}'.format(('' if (val != val) else val)))
nan = numpy.nan

def verify(df):
    mk_list = (lambda a: (list(a) if isinstance(a, tuple) else [a]))
    (rows, cols) = df.notnull().values.nonzero()
    for (i, j) in zip(rows, cols):
        left = sorted(df.iloc[(i, j)].split('.'))
        right = (mk_list(df.index[i]) + mk_list(df.columns[j]))
        right = sorted(list(map(cast, right)))
        self.assertEqual(left, right)
df = DataFrame({'jim': ['a', 'b', nan, 'd'], 'joe': ['w', 'x', 'y', 'z'], 'jolie': ['a.w', 'b.x', ' .y', 'd.z']})
left = df.set_index(['jim', 'joe']).unstack()['jolie']
right = df.set_index(['joe', 'jim']).unstack()['jolie'].T
assert_frame_equal(left, right)
for idx in itertools.permutations(df.columns[:2]):
    mi = df.set_index(list(idx))
    for lev in range(2):
        udf = mi.unstack(level=lev)
        self.assertEqual(udf.notnull().values.sum(), len(df))
        verify(udf['jolie'])
df = DataFrame({'1st': ((((((['d'] * 3) + ([nan] * 5)) + (['a'] * 2)) + (['c'] * 3)) + (['e'] * 2)) + (['b'] * 5)), '2nd': (((((((['y'] * 2) + (['w'] * 3)) + ([nan] * 3)) + (['z'] * 4)) + ([nan] * 3)) + (['x'] * 3)) + ([nan] * 2)), '3rd': [67, 39, 53, 72, 57, 80, 31, 18, 11, 30, 59, 50, 62, 59, 76, 52, 14, 53, 60, 51]})
(df['4th'], df['5th']) = (df.apply((lambda r: '.'.join(map(cast, r))), axis=1), df.apply((lambda r: '.'.join(map(cast, r.iloc[::(- 1)]))), axis=1))
for idx in itertools.permutations(['1st', '2nd', '3rd']):
    mi = df.set_index(list(idx))
    for lev in range(3):
        udf = mi.unstack(level=lev)
        self.assertEqual(udf.notnull().values.sum(), (2 * len(df)))
        for col in ['4th', '5th']:
            verify(udf[col])
df = pandas.DataFrame({'A': list('aaaabbbb'), 'B': range(8), 'C': range(8)})
df.iloc[(3, 1)] = numpy.NaN
left = df.set_index(['A', 'B']).unstack(0)
vals = [[3, 0, 1, 2, nan, nan, nan, nan], [nan, nan, nan, nan, 4, 5, 6, 7]]
vals = list(map(list, zip(*vals)))
idx = Index([nan, 0, 1, 2, 4, 5, 6, 7], name='B')
cols = MultiIndex(levels=[['C'], ['a', 'b']], labels=[[0, 0], [0, 1]], names=[None, 'A'])
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
df = DataFrame({'A': list('aaaabbbb'), 'B': (list(range(4)) * 2), 'C': range(8)})
df.iloc[(2, 1)] = numpy.NaN
left = df.set_index(['A', 'B']).unstack(0)
vals = [[2, nan], [0, 4], [1, 5], [nan, 6], [3, 7]]
cols = MultiIndex(levels=[['C'], ['a', 'b']], labels=[[0, 0], [0, 1]], names=[None, 'A'])
idx = Index([nan, 0, 1, 2, 3], name='B')
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
df = pandas.DataFrame({'A': list('aaaabbbb'), 'B': (list(range(4)) * 2), 'C': range(8)})
df.iloc[(3, 1)] = numpy.NaN
left = df.set_index(['A', 'B']).unstack(0)
vals = [[3, nan], [0, 4], [1, 5], [2, 6], [nan, 7]]
cols = MultiIndex(levels=[['C'], ['a', 'b']], labels=[[0, 0], [0, 1]], names=[None, 'A'])
idx = Index([nan, 0, 1, 2, 3], name='B')
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
tempResult = arange(10)
	
===================================================================	
TestDataFrameReshape.test_unstack_nan_index: 365	
----------------------------	

cast = (lambda val: '{0:1}'.format(('' if (val != val) else val)))
nan = numpy.nan

def verify(df):
    mk_list = (lambda a: (list(a) if isinstance(a, tuple) else [a]))
    (rows, cols) = df.notnull().values.nonzero()
    for (i, j) in zip(rows, cols):
        left = sorted(df.iloc[(i, j)].split('.'))
        right = (mk_list(df.index[i]) + mk_list(df.columns[j]))
        right = sorted(list(map(cast, right)))
        self.assertEqual(left, right)
df = DataFrame({'jim': ['a', 'b', nan, 'd'], 'joe': ['w', 'x', 'y', 'z'], 'jolie': ['a.w', 'b.x', ' .y', 'd.z']})
left = df.set_index(['jim', 'joe']).unstack()['jolie']
right = df.set_index(['joe', 'jim']).unstack()['jolie'].T
assert_frame_equal(left, right)
for idx in itertools.permutations(df.columns[:2]):
    mi = df.set_index(list(idx))
    for lev in range(2):
        udf = mi.unstack(level=lev)
        self.assertEqual(udf.notnull().values.sum(), len(df))
        verify(udf['jolie'])
df = DataFrame({'1st': ((((((['d'] * 3) + ([nan] * 5)) + (['a'] * 2)) + (['c'] * 3)) + (['e'] * 2)) + (['b'] * 5)), '2nd': (((((((['y'] * 2) + (['w'] * 3)) + ([nan] * 3)) + (['z'] * 4)) + ([nan] * 3)) + (['x'] * 3)) + ([nan] * 2)), '3rd': [67, 39, 53, 72, 57, 80, 31, 18, 11, 30, 59, 50, 62, 59, 76, 52, 14, 53, 60, 51]})
(df['4th'], df['5th']) = (df.apply((lambda r: '.'.join(map(cast, r))), axis=1), df.apply((lambda r: '.'.join(map(cast, r.iloc[::(- 1)]))), axis=1))
for idx in itertools.permutations(['1st', '2nd', '3rd']):
    mi = df.set_index(list(idx))
    for lev in range(3):
        udf = mi.unstack(level=lev)
        self.assertEqual(udf.notnull().values.sum(), (2 * len(df)))
        for col in ['4th', '5th']:
            verify(udf[col])
df = pandas.DataFrame({'A': list('aaaabbbb'), 'B': range(8), 'C': range(8)})
df.iloc[(3, 1)] = numpy.NaN
left = df.set_index(['A', 'B']).unstack(0)
vals = [[3, 0, 1, 2, nan, nan, nan, nan], [nan, nan, nan, nan, 4, 5, 6, 7]]
vals = list(map(list, zip(*vals)))
idx = Index([nan, 0, 1, 2, 4, 5, 6, 7], name='B')
cols = MultiIndex(levels=[['C'], ['a', 'b']], labels=[[0, 0], [0, 1]], names=[None, 'A'])
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
df = DataFrame({'A': list('aaaabbbb'), 'B': (list(range(4)) * 2), 'C': range(8)})
df.iloc[(2, 1)] = numpy.NaN
left = df.set_index(['A', 'B']).unstack(0)
vals = [[2, nan], [0, 4], [1, 5], [nan, 6], [3, 7]]
cols = MultiIndex(levels=[['C'], ['a', 'b']], labels=[[0, 0], [0, 1]], names=[None, 'A'])
idx = Index([nan, 0, 1, 2, 3], name='B')
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
df = pandas.DataFrame({'A': list('aaaabbbb'), 'B': (list(range(4)) * 2), 'C': range(8)})
df.iloc[(3, 1)] = numpy.NaN
left = df.set_index(['A', 'B']).unstack(0)
vals = [[3, nan], [0, 4], [1, 5], [2, 6], [nan, 7]]
cols = MultiIndex(levels=[['C'], ['a', 'b']], labels=[[0, 0], [0, 1]], names=[None, 'A'])
idx = Index([nan, 0, 1, 2, 3], name='B')
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
df = pandas.DataFrame({'A': list('aaaaabbbbb'), 'C': numpy.arange(10), 'B': (date_range('2012-01-01', periods=5).tolist() * 2)})
df.iloc[(3, 1)] = numpy.NaN
left = df.set_index(['A', 'B']).unstack()
vals = numpy.array([[3, 0, 1, 2, nan, 4], [nan, 5, 6, 7, 8, 9]])
idx = Index(['a', 'b'], name='A')
cols = MultiIndex(levels=[['C'], date_range('2012-01-01', periods=5)], labels=[[0, 0, 0, 0, 0, 0], [(- 1), 0, 1, 2, 3, 4]], names=[None, 'B'])
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
vals = [['Hg', nan, nan, 680585148], ['U', 0.0, nan, 680585148], ['Pb', 7.07e-06, nan, 680585148], ['Sn', 2.3614e-05, 0.0133, 680607017], ['Ag', 0.0, 0.0133, 680607017], ['Hg', (- 0.00015), 0.0133, 680607017]]
df = DataFrame(vals, columns=['agent', 'change', 'dosage', 's_id'], index=[17263, 17264, 17265, 17266, 17267, 17268])
left = df.copy().set_index(['s_id', 'dosage', 'agent']).unstack()
vals = [[nan, nan, 7.07e-06, nan, 0.0], [0.0, (- 0.00015), nan, 2.3614e-05, nan]]
idx = MultiIndex(levels=[[680585148, 680607017], [0.0133]], labels=[[0, 1], [(- 1), 0]], names=['s_id', 'dosage'])
cols = MultiIndex(levels=[['change'], ['Ag', 'Hg', 'Pb', 'Sn', 'U']], labels=[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4]], names=[None, 'agent'])
right = DataFrame(vals, columns=cols, index=idx)
assert_frame_equal(left, right)
left = df.ix[17264:].copy().set_index(['s_id', 'dosage', 'agent'])
assert_frame_equal(left.unstack(), right)
tempResult = arange(6)
	
===================================================================	
Get no callers of function numpy.arange at line 409 col 23.	
===================================================================	
TestDataFrameSorting.test_sort_index_different_sortorder: 223	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestDataFrameSorting.test_sort_index_different_sortorder: 224	
----------------------------	

A = np.arange(20).repeat(5)
tempResult = arange(5)
	
===================================================================	
TestDataFrameSorting.test_sort_index_categorical_index: 112	
----------------------------	

tempResult = arange(6, dtype='int64')
	
===================================================================	
TestDataFrameSorting.test_sort: 46	
----------------------------	

tempResult = arange(16)
	
===================================================================	
TestDataFrameSorting.test_sort_index: 14	
----------------------------	

tempResult = arange(16)
	
===================================================================	
TestDataFrameSorting.test_sort_index_multicolumn: 175	
----------------------------	

import random
tempResult = arange(5)
	
===================================================================	
TestDataFrameSorting.test_sort_index_multicolumn: 176	
----------------------------	

import random
A = np.arange(5).repeat(20)
tempResult = arange(5)
	
===================================================================	
TestDataFrameToCSV.test_to_csv_multiindex: 313	
----------------------------	

frame = self.frame
old_index = frame.index
tempResult = arange((len(old_index) * 2))
	
===================================================================	
TestDataFrameToCSV.test_to_csv_multiindex: 326	
----------------------------	

frame = self.frame
old_index = frame.index
arrays = np.arange((len(old_index) * 2)).reshape(2, (- 1))
new_index = pandas.MultiIndex.from_arrays(arrays, names=['first', 'second'])
frame.index = new_index
with ensure_clean('__tmp_to_csv_multiindex__') as path:
    frame.to_csv(path, header=False)
    frame.to_csv(path, columns=['A', 'B'])
    frame.to_csv(path)
    df = pandas.DataFrame.from_csv(path, index_col=[0, 1], parse_dates=False)
    assert_frame_equal(frame, df, check_names=False)
    self.assertEqual(frame.index.names, df.index.names)
    self.frame.index = old_index
    tsframe = self.tsframe
    old_index = tsframe.index
    tempResult = arange(len(old_index))
	
===================================================================	
Base.test_repeat: 319	
----------------------------	

rep = 2
i = self.create_index()
expected = pandas.Index(i.values.repeat(rep), name=i.name)
pandas.util.testing.assert_index_equal(i.repeat(rep), expected)
i = self.create_index()
tempResult = arange(len(i))
	
===================================================================	
Base.test_reindex_base: 75	
----------------------------	

idx = self.create_index()
tempResult = arange(idx.size, dtype=numpy.intp)
	
===================================================================	
TestIndex.test_get_indexer_invalid: 705	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestIndex.test_get_indexer_nearest_decreasing: 731	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestIndex.test_get_indexer_nearest: 712	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestIndex.test_constructor_ndarray_like: 133	
----------------------------	


class ArrayLike(object):

    def __init__(self, array):
        self.array = array

    def __array__(self, dtype=None):
        return self.array
tempResult = arange(5)
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 706	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 707	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 717	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
didx = self._holder((numpy.arange(5, dtype='int64') ** 2))
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx / 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 718	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
didx = self._holder((numpy.arange(5, dtype='int64') ** 2))
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx / 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, self._holder((numpy.arange(5, dtype='int64') * 5)))
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 720	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
didx = self._holder((numpy.arange(5, dtype='int64') ** 2))
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx / 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, self._holder((numpy.arange(5, dtype='int64') * 5)))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 722	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
didx = self._holder((numpy.arange(5, dtype='int64') ** 2))
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx / 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, self._holder((numpy.arange(5, dtype='int64') * 5)))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series(numpy.arange(5, dtype='int64')))
pandas.util.testing.assert_index_equal(result, didx)
tempResult = arange(5, dtype='float64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 723	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
didx = self._holder((numpy.arange(5, dtype='int64') ** 2))
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx / 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, self._holder((numpy.arange(5, dtype='int64') * 5)))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series(numpy.arange(5, dtype='int64')))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series((numpy.arange(5, dtype='float64') + 0.1)))
tempResult = arange(5, dtype='float64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 723	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
didx = self._holder((numpy.arange(5, dtype='int64') ** 2))
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx / 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, self._holder((numpy.arange(5, dtype='int64') * 5)))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series(numpy.arange(5, dtype='int64')))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series((numpy.arange(5, dtype='float64') + 0.1)))
tempResult = arange(5, dtype='float64')
	
===================================================================	
TestTimedeltaIndex.test_numeric_compat: 725	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
didx = self._holder((numpy.arange(5, dtype='int64') ** 2))
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx / 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, self._holder((numpy.arange(5, dtype='int64') * 5)))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series(numpy.arange(5, dtype='int64')))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series((numpy.arange(5, dtype='float64') + 0.1)))
pandas.util.testing.assert_index_equal(result, self._holder((numpy.arange(5, dtype='float64') * (numpy.arange(5, dtype='float64') + 0.1))))
self.assertRaises(TypeError, (lambda : (idx * idx)))
tempResult = arange(3)
	
===================================================================	
TestDatetimeIndex.test_time_loc: 360	
----------------------------	

from datetime import time
from pandas.index import _SIZE_CUTOFF
ns = (_SIZE_CUTOFF + numpy.array([(- 100), 100], dtype=numpy.int64))
key = time(15, 11, 30)
start = (((key.hour * 3600) + (key.minute * 60)) + key.second)
step = (24 * 3600)
for n in ns:
    idx = pandas.date_range('2014-11-26', periods=n, freq='S')
    ts = pandas.Series(numpy.random.randn(n), index=idx)
    tempResult = arange(start, n, step)
	
===================================================================	
TestMultiIndex.check: 1376	
----------------------------	

tempResult = arange(500)
	
===================================================================	
TestMultiIndex.check: 1377	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
tempResult = arange(500)
	
===================================================================	
TestMultiIndex.check: 1385	
----------------------------	

labels = numpy.tile(numpy.arange(500), 2)
level = numpy.arange(500)
if with_nulls:
    labels[500] = (- 1)
    labels = list((labels.copy() for i in range(nlevels)))
    for i in range(nlevels):
        labels[i][((500 + i) - (nlevels // 2))] = (- 1)
    labels += [np.array([(- 1), 1]).repeat(500)]
else:
    tempResult = arange(2)
	
===================================================================	
TestMultiIndex.test_join_multi: 1319	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestMultiIndex.test_join_multi: 1319	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestMultiIndex.test_join_multi: 1322	
----------------------------	

midx = pandas.MultiIndex.from_product([numpy.arange(4), numpy.arange(4)], names=['a', 'b'])
idx = pandas.Index([1, 2, 5], name='b')
(jidx, lidx, ridx) = midx.join(idx, how='inner', return_indexers=True)
tempResult = arange(4)
	
===================================================================	
TestMultiIndex.test_duplicates: 1402	
----------------------------	

self.assertFalse(self.index.has_duplicates)
self.assertTrue(self.index.append(self.index).has_duplicates)
index = MultiIndex(levels=[[0, 1], [0, 1, 2]], labels=[[0, 0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 0, 1, 2]])
self.assertTrue(index.has_duplicates)
t = [(u('x'), u('out'), u('z'), 5, u('y'), u('in'), u('z'), 169), (u('x'), u('out'), u('z'), 7, u('y'), u('in'), u('z'), 119), (u('x'), u('out'), u('z'), 9, u('y'), u('in'), u('z'), 135), (u('x'), u('out'), u('z'), 13, u('y'), u('in'), u('z'), 145), (u('x'), u('out'), u('z'), 14, u('y'), u('in'), u('z'), 158), (u('x'), u('out'), u('z'), 16, u('y'), u('in'), u('z'), 122), (u('x'), u('out'), u('z'), 17, u('y'), u('in'), u('z'), 160), (u('x'), u('out'), u('z'), 18, u('y'), u('in'), u('z'), 180), (u('x'), u('out'), u('z'), 20, u('y'), u('in'), u('z'), 143), (u('x'), u('out'), u('z'), 21, u('y'), u('in'), u('z'), 128), (u('x'), u('out'), u('z'), 22, u('y'), u('in'), u('z'), 129), (u('x'), u('out'), u('z'), 25, u('y'), u('in'), u('z'), 111), (u('x'), u('out'), u('z'), 28, u('y'), u('in'), u('z'), 114), (u('x'), u('out'), u('z'), 29, u('y'), u('in'), u('z'), 121), (u('x'), u('out'), u('z'), 31, u('y'), u('in'), u('z'), 126), (u('x'), u('out'), u('z'), 32, u('y'), u('in'), u('z'), 155), (u('x'), u('out'), u('z'), 33, u('y'), u('in'), u('z'), 123), (u('x'), u('out'), u('z'), 12, u('y'), u('in'), u('z'), 144)]
index = pandas.MultiIndex.from_tuples(t)
self.assertFalse(index.has_duplicates)

def check(nlevels, with_nulls):
    labels = numpy.tile(numpy.arange(500), 2)
    level = numpy.arange(500)
    if with_nulls:
        labels[500] = (- 1)
        labels = list((labels.copy() for i in range(nlevels)))
        for i in range(nlevels):
            labels[i][((500 + i) - (nlevels // 2))] = (- 1)
        labels += [np.array([(- 1), 1]).repeat(500)]
    else:
        labels = (([labels] * nlevels) + [np.arange(2).repeat(500)])
    levels = (([level] * nlevels) + [[0, 1]])
    index = MultiIndex(levels=levels, labels=labels)
    self.assertFalse(index.has_duplicates)
    if with_nulls:
        f = (lambda a: numpy.insert(a, 1000, a[0]))
        labels = list(map(f, labels))
        index = MultiIndex(levels=levels, labels=labels)
    else:
        values = index.values.tolist()
        index = pandas.MultiIndex.from_tuples((values + [values[0]]))
    self.assertTrue(index.has_duplicates)
check(4, False)
check(4, True)
check(8, False)
check(8, True)
(n, k) = (200, 5000)
tempResult = arange(n)
	
===================================================================	
TestMultiIndex.test_duplicates: 1402	
----------------------------	

self.assertFalse(self.index.has_duplicates)
self.assertTrue(self.index.append(self.index).has_duplicates)
index = MultiIndex(levels=[[0, 1], [0, 1, 2]], labels=[[0, 0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 0, 1, 2]])
self.assertTrue(index.has_duplicates)
t = [(u('x'), u('out'), u('z'), 5, u('y'), u('in'), u('z'), 169), (u('x'), u('out'), u('z'), 7, u('y'), u('in'), u('z'), 119), (u('x'), u('out'), u('z'), 9, u('y'), u('in'), u('z'), 135), (u('x'), u('out'), u('z'), 13, u('y'), u('in'), u('z'), 145), (u('x'), u('out'), u('z'), 14, u('y'), u('in'), u('z'), 158), (u('x'), u('out'), u('z'), 16, u('y'), u('in'), u('z'), 122), (u('x'), u('out'), u('z'), 17, u('y'), u('in'), u('z'), 160), (u('x'), u('out'), u('z'), 18, u('y'), u('in'), u('z'), 180), (u('x'), u('out'), u('z'), 20, u('y'), u('in'), u('z'), 143), (u('x'), u('out'), u('z'), 21, u('y'), u('in'), u('z'), 128), (u('x'), u('out'), u('z'), 22, u('y'), u('in'), u('z'), 129), (u('x'), u('out'), u('z'), 25, u('y'), u('in'), u('z'), 111), (u('x'), u('out'), u('z'), 28, u('y'), u('in'), u('z'), 114), (u('x'), u('out'), u('z'), 29, u('y'), u('in'), u('z'), 121), (u('x'), u('out'), u('z'), 31, u('y'), u('in'), u('z'), 126), (u('x'), u('out'), u('z'), 32, u('y'), u('in'), u('z'), 155), (u('x'), u('out'), u('z'), 33, u('y'), u('in'), u('z'), 123), (u('x'), u('out'), u('z'), 12, u('y'), u('in'), u('z'), 144)]
index = pandas.MultiIndex.from_tuples(t)
self.assertFalse(index.has_duplicates)

def check(nlevels, with_nulls):
    labels = numpy.tile(numpy.arange(500), 2)
    level = numpy.arange(500)
    if with_nulls:
        labels[500] = (- 1)
        labels = list((labels.copy() for i in range(nlevels)))
        for i in range(nlevels):
            labels[i][((500 + i) - (nlevels // 2))] = (- 1)
        labels += [np.array([(- 1), 1]).repeat(500)]
    else:
        labels = (([labels] * nlevels) + [np.arange(2).repeat(500)])
    levels = (([level] * nlevels) + [[0, 1]])
    index = MultiIndex(levels=levels, labels=labels)
    self.assertFalse(index.has_duplicates)
    if with_nulls:
        f = (lambda a: numpy.insert(a, 1000, a[0]))
        labels = list(map(f, labels))
        index = MultiIndex(levels=levels, labels=labels)
    else:
        values = index.values.tolist()
        index = pandas.MultiIndex.from_tuples((values + [values[0]]))
    self.assertTrue(index.has_duplicates)
check(4, False)
check(4, True)
check(8, False)
check(8, True)
(n, k) = (200, 5000)
tempResult = arange(n)
	
===================================================================	
TestMultiIndex.test_consistency: 847	
----------------------------	

major_axis = lrange(70000)
minor_axis = lrange(10)
tempResult = arange(70000)
	
===================================================================	
TestMultiIndex.test_legacy_pickle: 689	
----------------------------	

if PY3:
    raise nose.SkipTest('testing for legacy pickles not support on py3')
path = pandas.util.testing.get_data_path('multiindex_v1.pickle')
obj = pandas.read_pickle(path)
obj2 = pandas.MultiIndex.from_tuples(obj.values)
self.assertTrue(obj.equals(obj2))
res = obj.get_indexer(obj)
tempResult = arange(len(obj), dtype=numpy.intp)
	
===================================================================	
TestMultiIndex.test_rangeindex_fallback_coercion_bug: 1676	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestMultiIndex.test_rangeindex_fallback_coercion_bug: 1677	
----------------------------	

foo = pandas.DataFrame(np.arange(100).reshape((10, 10)))
tempResult = arange(100)
	
===================================================================	
TestMultiIndex.test_rangeindex_fallback_coercion_bug: 1681	
----------------------------	

foo = pandas.DataFrame(np.arange(100).reshape((10, 10)))
bar = pandas.DataFrame(np.arange(100).reshape((10, 10)))
df = pandas.concat({'foo': foo.stack(), 'bar': bar.stack()}, axis=1)
df.index.names = ['fizz', 'buzz']
str(df)
tempResult = arange(100)
	
===================================================================	
TestMultiIndex.test_rangeindex_fallback_coercion_bug: 1681	
----------------------------	

foo = pandas.DataFrame(np.arange(100).reshape((10, 10)))
bar = pandas.DataFrame(np.arange(100).reshape((10, 10)))
df = pandas.concat({'foo': foo.stack(), 'bar': bar.stack()}, axis=1)
df.index.names = ['fizz', 'buzz']
str(df)
tempResult = arange(100)
	
===================================================================	
TestMultiIndex.test_rangeindex_fallback_coercion_bug: 1684	
----------------------------	

foo = pandas.DataFrame(np.arange(100).reshape((10, 10)))
bar = pandas.DataFrame(np.arange(100).reshape((10, 10)))
df = pandas.concat({'foo': foo.stack(), 'bar': bar.stack()}, axis=1)
df.index.names = ['fizz', 'buzz']
str(df)
expected = pandas.DataFrame({'bar': numpy.arange(100), 'foo': numpy.arange(100)}, index=pandas.MultiIndex.from_product([range(10), range(10)], names=['fizz', 'buzz']))
pandas.util.testing.assert_frame_equal(df, expected, check_like=True)
result = df.index.get_level_values('fizz')
tempResult = arange(10)
	
===================================================================	
TestMultiIndex.test_rangeindex_fallback_coercion_bug: 1687	
----------------------------	

foo = pandas.DataFrame(np.arange(100).reshape((10, 10)))
bar = pandas.DataFrame(np.arange(100).reshape((10, 10)))
df = pandas.concat({'foo': foo.stack(), 'bar': bar.stack()}, axis=1)
df.index.names = ['fizz', 'buzz']
str(df)
expected = pandas.DataFrame({'bar': numpy.arange(100), 'foo': numpy.arange(100)}, index=pandas.MultiIndex.from_product([range(10), range(10)], names=['fizz', 'buzz']))
pandas.util.testing.assert_frame_equal(df, expected, check_like=True)
result = df.index.get_level_values('fizz')
expected = pd.Int64Index(np.arange(10), name='fizz').repeat(10)
pandas.util.testing.assert_index_equal(result, expected)
result = df.index.get_level_values('buzz')
tempResult = arange(10)
	
===================================================================	
TestMultiIndex.test_isin: 1536	
----------------------------	

values = [('foo', 2), ('bar', 3), ('quux', 4)]
tempResult = arange(4)
	
===================================================================	
TestMultiIndex.test_isin_level_kwarg: 1551	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestMultiIndex.test_legacy_v2_unpickle: 703	
----------------------------	

path = pandas.util.testing.get_data_path('mindex_073.pickle')
obj = pandas.read_pickle(path)
obj2 = pandas.MultiIndex.from_tuples(obj.values)
self.assertTrue(obj.equals(obj2))
res = obj.get_indexer(obj)
tempResult = arange(len(obj), dtype=numpy.intp)
	
===================================================================	
TestInt64Index.test_repr_summary: 668	
----------------------------	

with pandas.core.config.option_context('display.max_seq_items', 10):
    tempResult = arange(1000)
	
===================================================================	
TestFloat64Index.setUp: 121	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestFloat64Index.test_constructor_explicit: 179	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestInt64Index.test_get_indexer_pad: 442	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestInt64Index.create_index: 332	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
TestFloat64Index.create_index: 125	
----------------------------	

tempResult = arange(5, dtype='float64')
	
===================================================================	
Numeric.test_ufunc_compat: 93	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
Numeric.test_ufunc_compat: 95	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
result = numpy.sin(idx)
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestFloat64Index.test_constructor_coerce: 175	
----------------------------	

self.check_coerce(self.mixed, Index([1.5, 2, 3, 4, 5]))
tempResult = arange(5)
	
===================================================================	
TestFloat64Index.test_constructor_coerce: 176	
----------------------------	

self.check_coerce(self.mixed, Index([1.5, 2, 3, 4, 5]))
self.check_coerce(self.float, Index((numpy.arange(5) * 2.5)))
tempResult = arange(5)
	
===================================================================	
Numeric.test_explicit_conversions: 76	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
Numeric.test_explicit_conversions: 77	
----------------------------	

idx = self._holder(numpy.arange(5, dtype='int64'))
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestInt64Index.test_get_indexer_backfill: 448	
----------------------------	

tempResult = arange(10)
	
===================================================================	
Numeric.test_numeric_compat: 46	
----------------------------	

idx = self.create_index()
didx = (idx * idx)
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
if (not isinstance(idx, RangeIndex)):
    result = (idx * idx)
    pandas.util.testing.assert_index_equal(result, (idx ** 2))
result = (idx / 1)
expected = idx
if PY3:
    expected = expected.astype('float64')
pandas.util.testing.assert_index_equal(result, expected)
result = (idx / 2)
if PY3:
    expected = expected.astype('float64')
expected = Index((idx.values / 2))
pandas.util.testing.assert_index_equal(result, expected)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, (idx * 5))
tempResult = arange(5, dtype='int64')
	
===================================================================	
Numeric.test_numeric_compat: 48	
----------------------------	

idx = self.create_index()
didx = (idx * idx)
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
if (not isinstance(idx, RangeIndex)):
    result = (idx * idx)
    pandas.util.testing.assert_index_equal(result, (idx ** 2))
result = (idx / 1)
expected = idx
if PY3:
    expected = expected.astype('float64')
pandas.util.testing.assert_index_equal(result, expected)
result = (idx / 2)
if PY3:
    expected = expected.astype('float64')
expected = Index((idx.values / 2))
pandas.util.testing.assert_index_equal(result, expected)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, (idx * 5))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
tempResult = arange(5, dtype='int64')
	
===================================================================	
Numeric.test_numeric_compat: 50	
----------------------------	

idx = self.create_index()
didx = (idx * idx)
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
if (not isinstance(idx, RangeIndex)):
    result = (idx * idx)
    pandas.util.testing.assert_index_equal(result, (idx ** 2))
result = (idx / 1)
expected = idx
if PY3:
    expected = expected.astype('float64')
pandas.util.testing.assert_index_equal(result, expected)
result = (idx / 2)
if PY3:
    expected = expected.astype('float64')
expected = Index((idx.values / 2))
pandas.util.testing.assert_index_equal(result, expected)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, (idx * 5))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series(numpy.arange(5, dtype='int64')))
pandas.util.testing.assert_index_equal(result, didx)
tempResult = arange(5, dtype='float64')
	
===================================================================	
Numeric.test_numeric_compat: 51	
----------------------------	

idx = self.create_index()
didx = (idx * idx)
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
if (not isinstance(idx, RangeIndex)):
    result = (idx * idx)
    pandas.util.testing.assert_index_equal(result, (idx ** 2))
result = (idx / 1)
expected = idx
if PY3:
    expected = expected.astype('float64')
pandas.util.testing.assert_index_equal(result, expected)
result = (idx / 2)
if PY3:
    expected = expected.astype('float64')
expected = Index((idx.values / 2))
pandas.util.testing.assert_index_equal(result, expected)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, (idx * 5))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series(numpy.arange(5, dtype='int64')))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series((numpy.arange(5, dtype='float64') + 0.1)))
tempResult = arange(5, dtype='float64')
	
===================================================================	
Numeric.test_numeric_compat: 51	
----------------------------	

idx = self.create_index()
didx = (idx * idx)
result = (idx * 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (1 * idx)
pandas.util.testing.assert_index_equal(result, idx)
if (not isinstance(idx, RangeIndex)):
    result = (idx * idx)
    pandas.util.testing.assert_index_equal(result, (idx ** 2))
result = (idx / 1)
expected = idx
if PY3:
    expected = expected.astype('float64')
pandas.util.testing.assert_index_equal(result, expected)
result = (idx / 2)
if PY3:
    expected = expected.astype('float64')
expected = Index((idx.values / 2))
pandas.util.testing.assert_index_equal(result, expected)
result = (idx // 1)
pandas.util.testing.assert_index_equal(result, idx)
result = (idx * numpy.array(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, (idx * 5))
result = (idx * numpy.arange(5, dtype='int64'))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series(numpy.arange(5, dtype='int64')))
pandas.util.testing.assert_index_equal(result, didx)
result = (idx * Series((numpy.arange(5, dtype='float64') + 0.1)))
tempResult = arange(5, dtype='float64')
	
===================================================================	
TestInt64Index.setUp: 328	
----------------------------	

tempResult = arange(0, 20, 2)
	
===================================================================	
TestInt64Index.test_get_indexer: 436	
----------------------------	

tempResult = arange(10)
	
===================================================================	
Numeric.test_index_groupby: 100	
----------------------------	

int_idx = Index(range(6))
tempResult = arange(0, 0.6, 0.1)
	
===================================================================	
TestRangeIndex.test_join_inner: 324	
----------------------------	

tempResult = arange(25, 14, (- 1))
	
===================================================================	
TestRangeIndex.test_join_right: 361	
----------------------------	

tempResult = arange(25, 14, (- 1))
	
===================================================================	
TestRangeIndex.test_intersection: 415	
----------------------------	

tempResult = arange(1, 6)
	
===================================================================	
TestRangeIndex.test_explicit_conversions: 535	
----------------------------	

idx = RangeIndex(5)
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestRangeIndex.test_len_specialised: 637	
----------------------------	

tempResult = arange(1, 6, 1)
	
===================================================================	
TestRangeIndex.test_len_specialised: 638	
----------------------------	

for step in numpy.arange(1, 6, 1):
    tempResult = arange(0, 5, step)
	
===================================================================	
TestRangeIndex.test_len_specialised: 643	
----------------------------	

for step in numpy.arange(1, 6, 1):
    arr = numpy.arange(0, 5, step)
    i = RangeIndex(0, 5, step)
    self.assertEqual(len(i), len(arr))
    i = RangeIndex(5, 0, step)
    self.assertEqual(len(i), 0)
tempResult = arange((- 6), (- 1), 1)
	
===================================================================	
TestRangeIndex.test_len_specialised: 644	
----------------------------	

for step in numpy.arange(1, 6, 1):
    arr = numpy.arange(0, 5, step)
    i = RangeIndex(0, 5, step)
    self.assertEqual(len(i), len(arr))
    i = RangeIndex(5, 0, step)
    self.assertEqual(len(i), 0)
for step in numpy.arange((- 6), (- 1), 1):
    tempResult = arange(5, 0, step)
	
===================================================================	
TestRangeIndex.test_join_left: 345	
----------------------------	

tempResult = arange(25, 14, (- 1))
	
===================================================================	
TestRangeIndex.test_join_left: 353	
----------------------------	

other = Int64Index(numpy.arange(25, 14, (- 1)))
(res, lidx, ridx) = self.index.join(other, how='left', return_indexers=True)
eres = self.index
eridx = numpy.array([(- 1), (- 1), (- 1), (- 1), (- 1), (- 1), (- 1), (- 1), 9, 7], dtype=numpy.intp)
self.assertIsInstance(res, RangeIndex)
self.assert_index_equal(res, eres)
self.assertIsNone(lidx)
self.assert_numpy_array_equal(ridx, eridx)
tempResult = arange(25, 14, (- 1))
	
===================================================================	
TestRangeIndex.test_constructor: 56	
----------------------------	

index = RangeIndex(5)
tempResult = arange(5, dtype=numpy.int64)
	
===================================================================	
TestRangeIndex.test_constructor: 64	
----------------------------	

index = RangeIndex(5)
expected = numpy.arange(5, dtype=numpy.int64)
self.assertIsInstance(index, RangeIndex)
self.assertEqual(index._start, 0)
self.assertEqual(index._stop, 5)
self.assertEqual(index._step, 1)
self.assertEqual(index.name, None)
pandas.util.testing.assert_index_equal(Index(expected), index)
index = RangeIndex(1, 5)
tempResult = arange(1, 5, dtype=numpy.int64)
	
===================================================================	
TestRangeIndex.test_constructor: 69	
----------------------------	

index = RangeIndex(5)
expected = numpy.arange(5, dtype=numpy.int64)
self.assertIsInstance(index, RangeIndex)
self.assertEqual(index._start, 0)
self.assertEqual(index._stop, 5)
self.assertEqual(index._step, 1)
self.assertEqual(index.name, None)
pandas.util.testing.assert_index_equal(Index(expected), index)
index = RangeIndex(1, 5)
expected = numpy.arange(1, 5, dtype=numpy.int64)
self.assertIsInstance(index, RangeIndex)
self.assertEqual(index._start, 1)
pandas.util.testing.assert_index_equal(Index(expected), index)
index = RangeIndex(1, 5, 2)
tempResult = arange(1, 5, 2, dtype=numpy.int64)
	
===================================================================	
TestRangeIndex.test_constructor: 89	
----------------------------	

index = RangeIndex(5)
expected = numpy.arange(5, dtype=numpy.int64)
self.assertIsInstance(index, RangeIndex)
self.assertEqual(index._start, 0)
self.assertEqual(index._stop, 5)
self.assertEqual(index._step, 1)
self.assertEqual(index.name, None)
pandas.util.testing.assert_index_equal(Index(expected), index)
index = RangeIndex(1, 5)
expected = numpy.arange(1, 5, dtype=numpy.int64)
self.assertIsInstance(index, RangeIndex)
self.assertEqual(index._start, 1)
pandas.util.testing.assert_index_equal(Index(expected), index)
index = RangeIndex(1, 5, 2)
expected = numpy.arange(1, 5, 2, dtype=numpy.int64)
self.assertIsInstance(index, RangeIndex)
self.assertEqual(index._step, 2)
pandas.util.testing.assert_index_equal(Index(expected), index)
msg = 'RangeIndex\\(\\.\\.\\.\\) must be called with integers'
with pandas.util.testing.assertRaisesRegexp(TypeError, msg):
    RangeIndex()
for index in [RangeIndex(0), RangeIndex(start=0), RangeIndex(stop=0), RangeIndex(0, 0)]:
    expected = numpy.empty(0, dtype=numpy.int64)
    self.assertIsInstance(index, RangeIndex)
    self.assertEqual(index._start, 0)
    self.assertEqual(index._stop, 0)
    self.assertEqual(index._step, 1)
    pandas.util.testing.assert_index_equal(Index(expected), index)
with pandas.util.testing.assertRaisesRegexp(TypeError, msg):
    RangeIndex(name='Foo')
for index in [RangeIndex(0, name='Foo'), RangeIndex(start=0, name='Foo'), RangeIndex(stop=0, name='Foo'), RangeIndex(0, 0, name='Foo')]:
    self.assertIsInstance(index, RangeIndex)
    self.assertEqual(index.name, 'Foo')
self.assertRaises(TypeError, (lambda : Index(0, 1000)))
tempResult = arange(0, 10)
	
===================================================================	
TestRangeIndex.test_join_outer: 301	
----------------------------	

tempResult = arange(25, 14, (- 1))
	
===================================================================	
TestRangeIndex.test_ufunc_compat: 561	
----------------------------	

idx = RangeIndex(5)
result = numpy.sin(idx)
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestCategoricalIndex.setUp: 11	
----------------------------	

tempResult = arange(6, dtype='int64')
	
===================================================================	
TestCategoricalIndex.setUp: 12	
----------------------------	

self.df = DataFrame({'A': np.arange(6, dtype='int64'), 'B': Series(list('aabbca')).astype('category', categories=list('cab'))}).set_index('B')
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestCategoricalIndex.setUp: 13	
----------------------------	

self.df = DataFrame({'A': np.arange(6, dtype='int64'), 'B': Series(list('aabbca')).astype('category', categories=list('cab'))}).set_index('B')
self.df2 = DataFrame({'A': np.arange(6, dtype='int64'), 'B': Series(list('aabbca')).astype('category', categories=list('cabe'))}).set_index('B')
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestCategoricalIndex.setUp: 14	
----------------------------	

self.df = DataFrame({'A': np.arange(6, dtype='int64'), 'B': Series(list('aabbca')).astype('category', categories=list('cab'))}).set_index('B')
self.df2 = DataFrame({'A': np.arange(6, dtype='int64'), 'B': Series(list('aabbca')).astype('category', categories=list('cabe'))}).set_index('B')
self.df3 = DataFrame({'A': np.arange(6, dtype='int64'), 'B': Series([1, 1, 2, 1, 3, 2]).astype('category', categories=[3, 2, 1], ordered=True)}).set_index('B')
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestFloatIndexers.test_slice_float: 277	
----------------------------	

tempResult = arange(5.0)
	
===================================================================	
TestFloatIndexers.test_scalar_error: 22	
----------------------------	

for index in [pandas.util.testing.makeStringIndex, pandas.util.testing.makeUnicodeIndex, pandas.util.testing.makeCategoricalIndex, pandas.util.testing.makeDateIndex, pandas.util.testing.makeTimedeltaIndex, pandas.util.testing.makePeriodIndex, pandas.util.testing.makeIntIndex, pandas.util.testing.makeRangeIndex]:
    i = index(5)
    tempResult = arange(len(i))
	
===================================================================	
TestFloatIndexers.test_scalar_float: 126	
----------------------------	

tempResult = arange(5.0)
	
===================================================================	
TestFloatIndexers.test_scalar_float: 127	
----------------------------	

index = Index(numpy.arange(5.0))
tempResult = arange(len(index))
	
===================================================================	
TestFloatIndexers.test_floating_misc: 301	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestFloatIndexers.test_floating_misc: 301	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestFloatIndexers.test_scalar_non_numeric: 35	
----------------------------	

for index in [pandas.util.testing.makeStringIndex, pandas.util.testing.makeUnicodeIndex, pandas.util.testing.makeCategoricalIndex, pandas.util.testing.makeDateIndex, pandas.util.testing.makeTimedeltaIndex, pandas.util.testing.makePeriodIndex]:
    i = index(5)
    tempResult = arange(len(i))
	
===================================================================	
TestFloatIndexers.test_scalar_non_numeric: 70	
----------------------------	

for index in [pandas.util.testing.makeStringIndex, pandas.util.testing.makeUnicodeIndex, pandas.util.testing.makeCategoricalIndex, pandas.util.testing.makeDateIndex, pandas.util.testing.makeTimedeltaIndex, pandas.util.testing.makePeriodIndex]:
    i = index(5)
    for s in [Series(numpy.arange(len(i)), index=i), DataFrame(numpy.random.randn(len(i), len(i)), index=i, columns=i)]:
        for (idxr, getitem) in [((lambda x: x.ix), False), ((lambda x: x.iloc), False), ((lambda x: x), True)]:

            def f():
                idxr(s)[3.0]
            if (getitem and isinstance(s, DataFrame)):
                error = KeyError
            else:
                error = TypeError
            self.assertRaises(error, f)

        def f():
            s.loc[3.0]
        if (s.index.inferred_type in ['string', 'unicode', 'mixed']):
            error = KeyError
        else:
            error = TypeError
        self.assertRaises(error, f)
        self.assertFalse((3.0 in s))

        def f():
            s.iloc[3.0] = 0
        self.assertRaises(TypeError, f)
        if (s.index.inferred_type in ['categorical']):
            pass
        elif (s.index.inferred_type in ['datetime64', 'timedelta64', 'period']):
            pass
        else:
            s2 = s.copy()
            s2.loc[3.0] = 10
            self.assertTrue(s2.index.is_object())
            for idxr in [(lambda x: x.ix), (lambda x: x)]:
                s2 = s.copy()
                idxr(s2)[3.0] = 0
                self.assertTrue(s2.index.is_object())
    tempResult = arange(len(i))
	
===================================================================	
TestFloatIndexers.test_scalar_integer: 103	
----------------------------	

for index in [pandas.util.testing.makeIntIndex, pandas.util.testing.makeRangeIndex]:
    i = index(5)
    tempResult = arange(len(i))
	
===================================================================	
TestIndexing.test_getitem_multiindex: 1610	
----------------------------	

index = MultiIndex(levels=[['D', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
arr = numpy.random.randn(len(index), 1)
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['D']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['A']
self.assertRaises(KeyError, f)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
index = MultiIndex(levels=[['A', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['A']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
idx = pandas.MultiIndex.from_product([['A', 'B', 'C'], ['foo', 'bar', 'baz']], names=['one', 'two'])
tempResult = arange(9, dtype='int64')
	
===================================================================	
TestIndexing.test_getitem_multiindex: 1612	
----------------------------	

index = MultiIndex(levels=[['D', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
arr = numpy.random.randn(len(index), 1)
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['D']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['A']
self.assertRaises(KeyError, f)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
index = MultiIndex(levels=[['A', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['A']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
idx = pandas.MultiIndex.from_product([['A', 'B', 'C'], ['foo', 'bar', 'baz']], names=['one', 'two'])
s = pd.Series(np.arange(9, dtype='int64'), index=idx).sortlevel()
exp_idx = pandas.MultiIndex.from_product([['A'], ['foo', 'bar', 'baz']], names=['one', 'two'])
tempResult = arange(3, dtype='int64')
	
===================================================================	
TestIndexing.test_setitem_cache_updating: 2646	
----------------------------	

cont = ['one', 'two', 'three', 'four', 'five', 'six', 'seven']
for do_ref in [False, False]:
    tempResult = arange(7)
	
===================================================================	
TestIndexing.test_iloc_setitem_list_of_lists: 1120	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
TestIndexing.test_iloc_setitem_list_of_lists: 1120	
----------------------------	

tempResult = arange(5, 10, dtype='int64')
	
===================================================================	
TestIndexing.test_iloc_setitem_list_of_lists: 1124	
----------------------------	

df = DataFrame(dict(A=numpy.arange(5, dtype='int64'), B=numpy.arange(5, 10, dtype='int64')))
df.iloc[2:4] = [[10, 11], [12, 13]]
expected = DataFrame(dict(A=[0, 1, 10, 12, 4], B=[5, 6, 11, 13, 9]))
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(5, 10, dtype='int64')
	
===================================================================	
TestIndexing.test_iloc_getitem_panel_multiindex: 1040	
----------------------------	

multi_index = pandas.MultiIndex.from_tuples([('ONE', 'one'), ('TWO', 'two'), ('THREE', 'three')], names=['UPPER', 'lower'])
simple_index = [x[0] for x in multi_index]
wd1 = Panel(items=['First', 'Second'], major_axis=['a', 'b', 'c', 'd'], minor_axis=multi_index)
wd2 = Panel(items=['First', 'Second'], major_axis=['a', 'b', 'c', 'd'], minor_axis=simple_index)
expected1 = wd1['First'].iloc[([True, True, True, False], [0, 2])]
result1 = wd1.iloc[(0, [True, True, True, False], [0, 2])]
pandas.util.testing.assert_frame_equal(result1, expected1)
expected2 = wd2['First'].iloc[([True, True, True, False], [0, 2])]
result2 = wd2.iloc[(0, [True, True, True, False], [0, 2])]
pandas.util.testing.assert_frame_equal(result2, expected2)
expected1 = DataFrame(index=['a'], columns=multi_index, dtype='float64')
result1 = wd1.iloc[(0, [0], [0, 1, 2])]
pandas.util.testing.assert_frame_equal(result1, expected1)
expected2 = DataFrame(index=['a'], columns=simple_index, dtype='float64')
result2 = wd2.iloc[(0, [0], [0, 1, 2])]
pandas.util.testing.assert_frame_equal(result2, expected2)
mi = pandas.core.api.MultiIndex.from_tuples([(0, 'x'), (1, 'y'), (2, 'z')])
tempResult = arange(((3 * 3) * 3), dtype='int64')
	
===================================================================	
TestIndexing.test_loc_multiindex_indexer_none: 1174	
----------------------------	

attributes = [('Attribute' + str(i)) for i in range(1)]
attribute_values = [('Value' + str(i)) for i in range(5)]
index = pandas.core.api.MultiIndex.from_product([attributes, attribute_values])
df = ((0.1 * numpy.random.randn(10, (1 * 5))) + 0.5)
df = DataFrame(df, columns=index)
result = df[attributes]
pandas.util.testing.assert_frame_equal(result, df)
tempResult = arange(12)
	
===================================================================	
TestIndexing.test_per_axis_per_level_setitem: 1473	
----------------------------	

idx = pandas.IndexSlice
index = pandas.core.api.MultiIndex.from_tuples([('A', 1), ('A', 2), ('A', 3), ('B', 1)], names=['one', 'two'])
columns = pandas.core.api.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'), ('b', 'foo'), ('b', 'bah')], names=['lvl0', 'lvl1'])
tempResult = arange(16, dtype='int64')
	
===================================================================	
TestIndexing.test_non_unique_loc_memory_error: 2117	
----------------------------	

columns = list('ABCDEFG')

def gen_test(l, l2):
    return pandas.concat([DataFrame(randn(l, len(columns)), index=lrange(l), columns=columns), DataFrame(numpy.ones((l2, len(columns))), index=([0] * l2), columns=columns)])

def gen_expected(df, mask):
    l = len(mask)
    return pandas.concat([df.take([0], convert=False), DataFrame(numpy.ones((l, len(columns))), index=([0] * l), columns=columns), df.take(mask[1:], convert=False)])
df = gen_test(900, 100)
self.assertFalse(df.index.is_unique)
tempResult = arange(100)
	
===================================================================	
TestIndexing.test_non_unique_loc_memory_error: 2123	
----------------------------	

columns = list('ABCDEFG')

def gen_test(l, l2):
    return pandas.concat([DataFrame(randn(l, len(columns)), index=lrange(l), columns=columns), DataFrame(numpy.ones((l2, len(columns))), index=([0] * l2), columns=columns)])

def gen_expected(df, mask):
    l = len(mask)
    return pandas.concat([df.take([0], convert=False), DataFrame(numpy.ones((l, len(columns))), index=([0] * l), columns=columns), df.take(mask[1:], convert=False)])
df = gen_test(900, 100)
self.assertFalse(df.index.is_unique)
mask = numpy.arange(100)
result = df.loc[mask]
expected = gen_expected(df, mask)
pandas.util.testing.assert_frame_equal(result, expected)
df = gen_test(900000, 100000)
self.assertFalse(df.index.is_unique)
tempResult = arange(100000)
	
===================================================================	
TestIndexing.test_multiindex_assignment: 1872	
----------------------------	

df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df['d'] = numpy.nan
arr = numpy.array([0.0, 1.0])
df.ix[(4, 'd')] = arr
pandas.util.testing.assert_series_equal(df.ix[(4, 'd')], Series(arr, index=[8, 10], name='d'))
df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df.ix[(4, 'c')] = arr
exp = Series(arr, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)
df.ix[(4, 'c')] = 10
exp = Series(10, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)

def f():
    df.ix[(4, 'c')] = [0, 1, 2, 3]
self.assertRaises(ValueError, f)

def f():
    df.ix[(4, 'c')] = [0]
self.assertRaises(ValueError, f)
NUM_ROWS = 100
NUM_COLS = 10
tempResult = arange(NUM_COLS)
	
===================================================================	
TestIndexing.test_multiindex_assignment: 1878	
----------------------------	

df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df['d'] = numpy.nan
arr = numpy.array([0.0, 1.0])
df.ix[(4, 'd')] = arr
pandas.util.testing.assert_series_equal(df.ix[(4, 'd')], Series(arr, index=[8, 10], name='d'))
df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df.ix[(4, 'c')] = arr
exp = Series(arr, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)
df.ix[(4, 'c')] = 10
exp = Series(10, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)

def f():
    df.ix[(4, 'c')] = [0, 1, 2, 3]
self.assertRaises(ValueError, f)

def f():
    df.ix[(4, 'c')] = [0]
self.assertRaises(ValueError, f)
NUM_ROWS = 100
NUM_COLS = 10
col_names = [('A' + num) for num in map(str, np.arange(NUM_COLS).tolist())]
index_cols = col_names[:5]
df = DataFrame(numpy.random.randint(5, size=(NUM_ROWS, NUM_COLS)), dtype=numpy.int64, columns=col_names)
df = df.set_index(index_cols).sort_index()
grp = df.groupby(level=index_cols[:4])
df['new_col'] = numpy.nan
tempResult = arange(5)
	
===================================================================	
TestIndexing.test_multiindex_assignment: 1883	
----------------------------	

df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df['d'] = numpy.nan
arr = numpy.array([0.0, 1.0])
df.ix[(4, 'd')] = arr
pandas.util.testing.assert_series_equal(df.ix[(4, 'd')], Series(arr, index=[8, 10], name='d'))
df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df.ix[(4, 'c')] = arr
exp = Series(arr, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)
df.ix[(4, 'c')] = 10
exp = Series(10, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)

def f():
    df.ix[(4, 'c')] = [0, 1, 2, 3]
self.assertRaises(ValueError, f)

def f():
    df.ix[(4, 'c')] = [0]
self.assertRaises(ValueError, f)
NUM_ROWS = 100
NUM_COLS = 10
col_names = [('A' + num) for num in map(str, np.arange(NUM_COLS).tolist())]
index_cols = col_names[:5]
df = DataFrame(numpy.random.randint(5, size=(NUM_ROWS, NUM_COLS)), dtype=numpy.int64, columns=col_names)
df = df.set_index(index_cols).sort_index()
grp = df.groupby(level=index_cols[:4])
df['new_col'] = numpy.nan
f_index = numpy.arange(5)

def f(name, df2):
    return Series(np.arange(df2.shape[0]), name=df2.index.values[0]).reindex(f_index)
for (name, df2) in grp:
    tempResult = arange(df2.shape[0])
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2702	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
tempResult = arange(4)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2707	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(2, 4)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2718	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})
self.assertIsNone(df.is_copy)

def f():
    df['A'][0] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df['A'][1] = numpy.nan
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
self.assertIsNone(df['A'].is_copy)
tempResult = arange(2, 4)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2811	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})
self.assertIsNone(df.is_copy)

def f():
    df['A'][0] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df['A'][1] = numpy.nan
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
self.assertIsNone(df['A'].is_copy)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})

def f():
    df.loc[0]['A'] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': Series(range(7), dtype='int64')})
self.assertIsNone(df.is_copy)
expected = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': [42, 42, 2, 3, 4, 42, 6]})

def f():
    indexer = df.a.str.startswith('o')
    df[indexer]['c'] = 42
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
expected = DataFrame({'A': [111, 'bbb', 'ccc'], 'B': [1, 2, 3]})
df = DataFrame({'A': ['aaa', 'bbb', 'ccc'], 'B': [1, 2, 3]})

def f():
    df['A'][0] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df.loc[0]['A'] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df.loc[(0, 'A')] = 111
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': [1, 2]})
self.assertIsNone(df.is_copy)
with pandas.util.testing.ensure_clean('__tmp__pickle') as path:
    df.to_pickle(path)
    df2 = pandas.read_pickle(path)
    df2['B'] = df2['A']
    df2['B'] = df2['A']
from string import ascii_letters as letters

def random_text(nobs=100):
    df = []
    for i in range(nobs):
        idx = numpy.random.randint(len(letters), size=2)
        idx.sort()
        df.append([letters[idx[0]:idx[1]]])
    return DataFrame(df, columns=['letters'])
df = random_text(100000)
x = df.iloc[[0, 1, 2]]
self.assertIsNotNone(x.is_copy)
x = df.iloc[[0, 1, 2, 4]]
self.assertIsNotNone(x.is_copy)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer].copy()
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df.loc[:, 'letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df.ix[(indexer, 'letters')] = df.ix[(indexer, 'letters')].apply(str.lower)
df = DataFrame({'a': [1]}).dropna()
self.assertIsNone(df.is_copy)
df['a'] += 1
a = [12, 23]
b = [123, None]
c = [1234, 2345]
d = [12345, 23456]
tuples = [('eyes', 'left'), ('eyes', 'right'), ('ears', 'left'), ('ears', 'right')]
events = {('eyes', 'left'): a, ('eyes', 'right'): b, ('ears', 'left'): c, ('ears', 'right'): d}
multiind = pandas.core.api.MultiIndex.from_tuples(tuples, names=['part', 'side'])
zed = DataFrame(events, index=['a', 'b'], columns=multiind)

def f():
    zed['eyes']['right'].fillna(value=555, inplace=True)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame(numpy.random.randn(10, 4))
s = df.iloc[:, 0].sort_values()
pandas.util.testing.assert_series_equal(s, df.iloc[:, 0].sort_values())
pandas.util.testing.assert_series_equal(s, df[0].sort_values())
df = DataFrame({'column1': ['a', 'a', 'a'], 'column2': [4, 8, 9]})
str(df)
df['column1'] = (df['column1'] + 'b')
str(df)
df = df[(df['column2'] != 8)]
str(df)
df['column1'] = (df['column1'] + 'c')
str(df)
tempResult = arange(0, 9)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2817	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})
self.assertIsNone(df.is_copy)

def f():
    df['A'][0] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df['A'][1] = numpy.nan
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
self.assertIsNone(df['A'].is_copy)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})

def f():
    df.loc[0]['A'] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': Series(range(7), dtype='int64')})
self.assertIsNone(df.is_copy)
expected = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': [42, 42, 2, 3, 4, 42, 6]})

def f():
    indexer = df.a.str.startswith('o')
    df[indexer]['c'] = 42
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
expected = DataFrame({'A': [111, 'bbb', 'ccc'], 'B': [1, 2, 3]})
df = DataFrame({'A': ['aaa', 'bbb', 'ccc'], 'B': [1, 2, 3]})

def f():
    df['A'][0] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df.loc[0]['A'] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df.loc[(0, 'A')] = 111
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': [1, 2]})
self.assertIsNone(df.is_copy)
with pandas.util.testing.ensure_clean('__tmp__pickle') as path:
    df.to_pickle(path)
    df2 = pandas.read_pickle(path)
    df2['B'] = df2['A']
    df2['B'] = df2['A']
from string import ascii_letters as letters

def random_text(nobs=100):
    df = []
    for i in range(nobs):
        idx = numpy.random.randint(len(letters), size=2)
        idx.sort()
        df.append([letters[idx[0]:idx[1]]])
    return DataFrame(df, columns=['letters'])
df = random_text(100000)
x = df.iloc[[0, 1, 2]]
self.assertIsNotNone(x.is_copy)
x = df.iloc[[0, 1, 2, 4]]
self.assertIsNotNone(x.is_copy)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer].copy()
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df.loc[:, 'letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df.ix[(indexer, 'letters')] = df.ix[(indexer, 'letters')].apply(str.lower)
df = DataFrame({'a': [1]}).dropna()
self.assertIsNone(df.is_copy)
df['a'] += 1
a = [12, 23]
b = [123, None]
c = [1234, 2345]
d = [12345, 23456]
tuples = [('eyes', 'left'), ('eyes', 'right'), ('ears', 'left'), ('ears', 'right')]
events = {('eyes', 'left'): a, ('eyes', 'right'): b, ('ears', 'left'): c, ('ears', 'right'): d}
multiind = pandas.core.api.MultiIndex.from_tuples(tuples, names=['part', 'side'])
zed = DataFrame(events, index=['a', 'b'], columns=multiind)

def f():
    zed['eyes']['right'].fillna(value=555, inplace=True)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame(numpy.random.randn(10, 4))
s = df.iloc[:, 0].sort_values()
pandas.util.testing.assert_series_equal(s, df.iloc[:, 0].sort_values())
pandas.util.testing.assert_series_equal(s, df[0].sort_values())
df = DataFrame({'column1': ['a', 'a', 'a'], 'column2': [4, 8, 9]})
str(df)
df['column1'] = (df['column1'] + 'b')
str(df)
df = df[(df['column2'] != 8)]
str(df)
df['column1'] = (df['column1'] + 'c')
str(df)
df = DataFrame(numpy.arange(0, 9), columns=['count'])
df['group'] = 'b'

def f():
    df.iloc[0:5]['group'] = 'a'
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestIndexing.test_index_type_coercion: 2900	
----------------------------	

for s in [Series(range(5)), Series(range(5), index=range(1, 6))]:
    self.assertTrue(s.index.is_integer())
    for indexer in [(lambda x: x.ix), (lambda x: x.loc), (lambda x: x)]:
        s2 = s.copy()
        indexer(s2)[0.1] = 0
        self.assertTrue(s2.index.is_floating())
        self.assertTrue((indexer(s2)[0.1] == 0))
        s2 = s.copy()
        indexer(s2)[0.0] = 0
        exp = s.index
        if (0 not in s):
            exp = Index((s.index.tolist() + [0]))
        pandas.util.testing.assert_index_equal(s2.index, exp)
        s2 = s.copy()
        indexer(s2)['0'] = 0
        self.assertTrue(s2.index.is_object())
tempResult = arange(5.0)
	
===================================================================	
TestIndexing.test_partial_setting: 2197	
----------------------------	

s_orig = Series([1, 2, 3])
s = s_orig.copy()
s[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()

def f():
    s.iloc[3] = 5.0
self.assertRaises(IndexError, f)

def f():
    s.iat[3] = 5.0
self.assertRaises(IndexError, f)
tempResult = arange(6)
	
===================================================================	
TestIndexing.test_partial_setting: 2238	
----------------------------	

s_orig = Series([1, 2, 3])
s = s_orig.copy()
s[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()

def f():
    s.iloc[3] = 5.0
self.assertRaises(IndexError, f)

def f():
    s.iat[3] = 5.0
self.assertRaises(IndexError, f)
df_orig = DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'], dtype='int64')
df = df_orig.copy()

def f():
    df.iloc[(4, 2)] = 5.0
self.assertRaises(IndexError, f)

def f():
    df.iat[(4, 2)] = 5.0
self.assertRaises(IndexError, f)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.iloc[1] = df.iloc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.loc[1] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4, 4], 'B': [1, 3, 5, 5]}))
df = df_orig.copy()
df.loc[3] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': [0, 2, 4]}))
df = df_orig.copy()
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': Series([0, 2, 4])}))
df = df_orig.copy()
df['B'] = df['B'].astype(numpy.float64)
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(16)
	
===================================================================	
TestIndexing.test_partial_setting: 2239	
----------------------------	

s_orig = Series([1, 2, 3])
s = s_orig.copy()
s[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()

def f():
    s.iloc[3] = 5.0
self.assertRaises(IndexError, f)

def f():
    s.iat[3] = 5.0
self.assertRaises(IndexError, f)
df_orig = DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'], dtype='int64')
df = df_orig.copy()

def f():
    df.iloc[(4, 2)] = 5.0
self.assertRaises(IndexError, f)

def f():
    df.iat[(4, 2)] = 5.0
self.assertRaises(IndexError, f)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.iloc[1] = df.iloc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.loc[1] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4, 4], 'B': [1, 3, 5, 5]}))
df = df_orig.copy()
df.loc[3] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': [0, 2, 4]}))
df = df_orig.copy()
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': Series([0, 2, 4])}))
df = df_orig.copy()
df['B'] = df['B'].astype(numpy.float64)
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
p_orig = Panel(np.arange(16).reshape(2, 4, 2), items=['Item1', 'Item2'], major_axis=pandas.date_range('2001/1/12', periods=4), minor_axis=['A', 'B'], dtype='float64')
tempResult = arange(16)
	
===================================================================	
TestIndexing.test_setitem_iloc: 1662	
----------------------------	

tempResult = arange(9)
	
===================================================================	
TestIndexing.test_loc_setitem_dups: 635	
----------------------------	

tempResult = arange(5, dtype='float64')
	
===================================================================	
TestIndexing.test_loc_setitem_dups: 635	
----------------------------	

tempResult = arange(5, dtype='float64')
	
===================================================================	
TestIndexing.test_slice_with_zero_step_raises: 3013	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestIndexing.test_ix_get_set_consistency: 1943	
----------------------------	

tempResult = arange(16)
	
===================================================================	
TestIndexing.test_loc_setitem_frame: 888	
----------------------------	

df = self.frame_labels
result = df.iloc[(0, 0)]
df.loc[('a', 'A')] = 1
result = df.loc[('a', 'A')]
self.assertEqual(result, 1)
result = df.iloc[(0, 0)]
self.assertEqual(result, 1)
df.loc[:, 'B':'D'] = 0
expected = df.loc[:, 'B':'D']
result = df.ix[:, 1:]
pandas.util.testing.assert_frame_equal(result, expected)
df = DataFrame(index=[3, 5, 4], columns=['A'])
df.loc[([4, 3, 5], 'A')] = numpy.array([1, 2, 3], dtype='int64')
expected = DataFrame(dict(A=Series([1, 2, 3], index=[4, 3, 5]))).reindex(index=[3, 5, 4])
pandas.util.testing.assert_frame_equal(df, expected)
keys1 = [('@' + str(i)) for i in range(5)]
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestIndexing.test_loc_setitem_frame: 890	
----------------------------	

df = self.frame_labels
result = df.iloc[(0, 0)]
df.loc[('a', 'A')] = 1
result = df.loc[('a', 'A')]
self.assertEqual(result, 1)
result = df.iloc[(0, 0)]
self.assertEqual(result, 1)
df.loc[:, 'B':'D'] = 0
expected = df.loc[:, 'B':'D']
result = df.ix[:, 1:]
pandas.util.testing.assert_frame_equal(result, expected)
df = DataFrame(index=[3, 5, 4], columns=['A'])
df.loc[([4, 3, 5], 'A')] = numpy.array([1, 2, 3], dtype='int64')
expected = DataFrame(dict(A=Series([1, 2, 3], index=[4, 3, 5]))).reindex(index=[3, 5, 4])
pandas.util.testing.assert_frame_equal(df, expected)
keys1 = [('@' + str(i)) for i in range(5)]
val1 = numpy.arange(5, dtype='int64')
keys2 = [('@' + str(i)) for i in range(4)]
tempResult = arange(4, dtype='int64')
	
===================================================================	
TestIndexing.test_setitem_dtype_upcast: 1646	
----------------------------	

df = DataFrame([{'a': 1}, {'a': 3, 'b': 2}])
df['c'] = numpy.nan
self.assertEqual(df['c'].dtype, numpy.float64)
df.ix[(0, 'c')] = 'foo'
expected = DataFrame([{'a': 1, 'c': 'foo'}, {'a': 3, 'b': 2, 'c': numpy.nan}])
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestIndexing.test_setitem_dtype_upcast: 1654	
----------------------------	

df = DataFrame([{'a': 1}, {'a': 3, 'b': 2}])
df['c'] = numpy.nan
self.assertEqual(df['c'].dtype, numpy.float64)
df.ix[(0, 'c')] = 'foo'
expected = DataFrame([{'a': 1, 'c': 'foo'}, {'a': 3, 'b': 2, 'c': numpy.nan}])
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame(np.arange(6, dtype='int64').reshape(2, 3), index=list('ab'), columns=['foo', 'bar', 'baz'])
for val in [3.14, 'wxyz']:
    left = df.copy()
    left.loc[('a', 'bar')] = val
    right = DataFrame([[0, val, 2], [3, 4, 5]], index=list('ab'), columns=['foo', 'bar', 'baz'])
    pandas.util.testing.assert_frame_equal(left, right)
    self.assertTrue(is_integer_dtype(left['foo']))
    self.assertTrue(is_integer_dtype(left['baz']))
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestIndexing.test_multiindex_label_slicing_with_negative_step: 2991	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestIndexing.test_multiindex_label_slicing_with_negative_step: 2991	
----------------------------	

tempResult = arange(4)
	
===================================================================	
Get no callers of function numpy.arange at line 2962 col 13.	
===================================================================	
TestIndexing.test_multiindex_setitem: 1563	
----------------------------	

tempResult = arange(0, 6, 1)
	
===================================================================	
TestIndexing.test_iloc_mask: 1995	
----------------------------	

df = DataFrame(lrange(5), list('ABCDE'), columns=['a'])
mask = ((df.a % 2) == 0)
self.assertRaises(ValueError, df.iloc.__getitem__, tuple([mask]))
mask.index = lrange(len(mask))
self.assertRaises(NotImplementedError, df.iloc.__getitem__, tuple([mask]))
result = df.iloc[numpy.array(([True] * len(mask)), dtype=bool)]
pandas.util.testing.assert_frame_equal(result, df)
tempResult = arange(4)
	
===================================================================	
TestIndexing.test_str_label_slicing_with_negative_step: 2982	
----------------------------	

SLC = pandas.IndexSlice

def assert_slices_equivalent(l_slc, i_slc):
    pandas.util.testing.assert_series_equal(s.loc[l_slc], s.iloc[i_slc])
    if (not idx.is_integer):
        pandas.util.testing.assert_series_equal(s[l_slc], s.iloc[i_slc])
        pandas.util.testing.assert_series_equal(s.ix[l_slc], s.iloc[i_slc])
tempResult = arange(20)
	
===================================================================	
TestIndexing.test_str_label_slicing_with_negative_step: 2984	
----------------------------	

SLC = pandas.IndexSlice

def assert_slices_equivalent(l_slc, i_slc):
    pandas.util.testing.assert_series_equal(s.loc[l_slc], s.iloc[i_slc])
    if (not idx.is_integer):
        pandas.util.testing.assert_series_equal(s[l_slc], s.iloc[i_slc])
        pandas.util.testing.assert_series_equal(s.ix[l_slc], s.iloc[i_slc])
for idx in [_mklbl('A', 20), (numpy.arange(20) + 100), numpy.linspace(100, 150, 20)]:
    idx = Index(idx)
    tempResult = arange(20)
	
===================================================================	
TestIndexing.f11111111111111111111111: 1881	
----------------------------	

tempResult = arange(df2.shape[0])
	
===================================================================	
TestIndexing.test_loc_multiindex_incomplete: 1180	
----------------------------	

tempResult = arange(15, dtype='int64')
	
===================================================================	
TestIndexing.test_loc_multiindex_incomplete: 1191	
----------------------------	

s = pandas.Series(numpy.arange(15, dtype='int64'), pandas.core.api.MultiIndex.from_product([range(5), ['a', 'b', 'c']]))
expected = s.loc[:, 'a':'c']
result = s.loc[0:4, 'a':'c']
pandas.util.testing.assert_series_equal(result, expected)
pandas.util.testing.assert_series_equal(result, expected)
result = s.loc[:4, 'a':'c']
pandas.util.testing.assert_series_equal(result, expected)
pandas.util.testing.assert_series_equal(result, expected)
result = s.loc[0:, 'a':'c']
pandas.util.testing.assert_series_equal(result, expected)
pandas.util.testing.assert_series_equal(result, expected)
tempResult = arange(15, dtype='int64')
	
===================================================================	
TestIndexing.test_per_axis_per_level_getitem: 1267	
----------------------------	

ix = pandas.core.api.MultiIndex.from_product([_mklbl('A', 5), _mklbl('B', 7), _mklbl('C', 4), _mklbl('D', 2)])
tempResult = arange(len(ix.get_values()))
	
===================================================================	
TestIndexing.test_per_axis_per_level_getitem: 1276	
----------------------------	

ix = pandas.core.api.MultiIndex.from_product([_mklbl('A', 5), _mklbl('B', 7), _mklbl('C', 4), _mklbl('D', 2)])
df = DataFrame(numpy.arange(len(ix.get_values())), index=ix)
result = df.loc[(slice('A1', 'A3'), slice(None), ['C1', 'C3']), :]
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C3')))]]
pandas.util.testing.assert_frame_equal(result, expected)
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C2') or (c == 'C3')))]]
result = df.loc[(slice('A1', 'A3'), slice(None), slice('C1', 'C3')), :]
pandas.util.testing.assert_frame_equal(result, expected)
index = pandas.core.api.MultiIndex.from_tuples([('A', 1), ('A', 2), ('A', 3), ('B', 1)], names=['one', 'two'])
columns = pandas.core.api.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'), ('b', 'foo'), ('b', 'bah')], names=['lvl0', 'lvl1'])
tempResult = arange(16, dtype='int64')
	
===================================================================	
TestIndexing.test_per_axis_per_level_getitem: 1302	
----------------------------	

ix = pandas.core.api.MultiIndex.from_product([_mklbl('A', 5), _mklbl('B', 7), _mklbl('C', 4), _mklbl('D', 2)])
df = DataFrame(numpy.arange(len(ix.get_values())), index=ix)
result = df.loc[(slice('A1', 'A3'), slice(None), ['C1', 'C3']), :]
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C3')))]]
pandas.util.testing.assert_frame_equal(result, expected)
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C2') or (c == 'C3')))]]
result = df.loc[(slice('A1', 'A3'), slice(None), slice('C1', 'C3')), :]
pandas.util.testing.assert_frame_equal(result, expected)
index = pandas.core.api.MultiIndex.from_tuples([('A', 1), ('A', 2), ('A', 3), ('B', 1)], names=['one', 'two'])
columns = pandas.core.api.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'), ('b', 'foo'), ('b', 'bah')], names=['lvl0', 'lvl1'])
df = DataFrame(np.arange(16, dtype='int64').reshape(4, 4), index=index, columns=columns)
df = df.sortlevel(axis=0).sortlevel(axis=1)
result = df.loc[(slice(None), slice(None)), :]
pandas.util.testing.assert_frame_equal(result, df)
result = df.loc[((slice(None), slice(None)), (slice(None), slice(None)))]
pandas.util.testing.assert_frame_equal(result, df)
result = df.loc[:, (slice(None), slice(None))]
pandas.util.testing.assert_frame_equal(result, df)
result = df.loc[(slice(None), [1]), :]
expected = df.iloc[[0, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[(slice(None), 1), :]
expected = df.iloc[[0, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[:, (slice(None), ['foo'])]
expected = df.iloc[:, [1, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[((slice(None), 1), (slice(None), ['foo']))]
expected = df.iloc[([0, 3], [1, 3])]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[('A', 'a')]
expected = DataFrame(dict(bar=[1, 5, 9], foo=[0, 4, 8]), index=Index([1, 2, 3], name='two'), columns=Index(['bar', 'foo'], name='lvl1'))
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[(slice(None), [1, 2]), :]
expected = df.iloc[[0, 1, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
tempResult = arange(len(ix.get_values()))
	
===================================================================	
TestIndexing.test_iloc_getitem_panel: 971	
----------------------------	

tempResult = arange(((4 * 3) * 2))
	
===================================================================	
TestIndexing.f11111111: 1092	
----------------------------	

tempResult = arange(1, 4)
	
===================================================================	
TestIndexing.test_setitem_multiindex: 545	
----------------------------	

for index_fn in ('ix', 'loc'):

    def check(target, indexers, value, compare_fn, expected=None):
        fn = getattr(target, index_fn)
        fn.__setitem__(indexers, value)
        result = fn.__getitem__(indexers)
        if (expected is None):
            expected = value
        compare_fn(result, expected)
    tempResult = arange(0, 100)
	
===================================================================	
TestIndexing.test_setitem_multiindex: 545	
----------------------------	

for index_fn in ('ix', 'loc'):

    def check(target, indexers, value, compare_fn, expected=None):
        fn = getattr(target, index_fn)
        fn.__setitem__(indexers, value)
        result = fn.__getitem__(indexers)
        if (expected is None):
            expected = value
        compare_fn(result, expected)
    tempResult = arange(0, 80)
	
===================================================================	
TestIndexing.test_setitem_multiindex: 555	
----------------------------	

for index_fn in ('ix', 'loc'):

    def check(target, indexers, value, compare_fn, expected=None):
        fn = getattr(target, index_fn)
        fn.__setitem__(indexers, value)
        result = fn.__getitem__(indexers)
        if (expected is None):
            expected = value
        compare_fn(result, expected)
    index = pandas.MultiIndex.from_product([numpy.arange(0, 100), numpy.arange(0, 80)], names=['time', 'firm'])
    (t, n) = (0, 2)
    df = DataFrame(numpy.nan, columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=0, compare_fn=self.assertEqual)
    df = DataFrame((- 999), columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=1, compare_fn=self.assertEqual)
    df = DataFrame(columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=2, compare_fn=self.assertEqual)
    df = DataFrame((- 999), columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=numpy.array(3), compare_fn=self.assertEqual, expected=3)
    tempResult = arange(25)
	
===================================================================	
TestPlotBase.setUp: 63	
----------------------------	

import matplotlib as mpl
matplotlib.rcdefaults()
self.mpl_le_1_2_1 = pandas.tools.plotting._mpl_le_1_2_1()
self.mpl_ge_1_3_1 = pandas.tools.plotting._mpl_ge_1_3_1()
self.mpl_ge_1_4_0 = pandas.tools.plotting._mpl_ge_1_4_0()
self.mpl_ge_1_5_0 = pandas.tools.plotting._mpl_ge_1_5_0()
self.mpl_ge_2_0_0 = pandas.tools.plotting._mpl_ge_2_0_0()
if self.mpl_ge_1_4_0:
    self.bp_n_objects = 7
else:
    self.bp_n_objects = 8
if self.mpl_ge_1_5_0:
    self.polycollection_factor = 2
else:
    self.polycollection_factor = 1
if self.mpl_ge_2_0_0:
    self.default_figsize = (6.4, 4.8)
else:
    self.default_figsize = (8.0, 6.0)
self.default_tick_position = ('left' if self.mpl_ge_2_0_0 else 'default')
from pandas import read_csv
path = os.path.join(os.path.dirname(curpath()), 'data', 'iris.csv')
self.iris = read_csv(path)
n = 100
with pandas.util.testing.RNGContext(42):
    gender = numpy.random.choice(['Male', 'Female'], size=n)
    classroom = numpy.random.choice(['A', 'B', 'C'], size=n)
    self.hist_df = DataFrame({'gender': gender, 'classroom': classroom, 'height': numpy.random.normal(66, 4, size=n), 'weight': numpy.random.normal(161, 32, size=n), 'category': numpy.random.randint(4, size=n)})
self.tdf = pandas.util.testing.makeTimeDataFrame()
tempResult = arange(20)
	
===================================================================	
TestTSPlot.test_nonzero_base: 268	
----------------------------	

idx = (date_range('2012-12-20', periods=24, freq='H') + timedelta(minutes=30))
tempResult = arange(24)
	
===================================================================	
TestTSPlot.test_mpl_nopandas: 967	
----------------------------	

import matplotlib.pyplot as plt
dates = [date(2008, 12, 31), date(2009, 1, 31)]
tempResult = arange(10.0, 11.0, 0.5)
	
===================================================================	
TestTSPlot.test_mpl_nopandas: 968	
----------------------------	

import matplotlib.pyplot as plt
dates = [date(2008, 12, 31), date(2009, 1, 31)]
values1 = numpy.arange(10.0, 11.0, 0.5)
tempResult = arange(11.0, 12.0, 0.5)
	
===================================================================	
TestDataFramePlots.setUp: 28	
----------------------------	

pandas.tests.plotting.common.TestPlotBase.setUp(self)
import matplotlib as mpl
matplotlib.rcdefaults()
self.tdf = pandas.util.testing.makeTimeDataFrame()
tempResult = arange(20)
	
===================================================================	
TestDataFramePlots.test_errorbar_timeseries: 1578	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestDataFramePlots.test_errorbar_timeseries: 1578	
----------------------------	

tempResult = arange(12, 0, (- 1))
	
===================================================================	
TestDataFramePlots.test_errorbar_with_partial_columns: 1568	
----------------------------	

df = DataFrame(numpy.random.randn(10, 3))
df_err = DataFrame(numpy.random.randn(10, 2), columns=[0, 2])
kinds = ['line', 'bar']
for kind in kinds:
    ax = _check_plot_works(df.plot, yerr=df_err, kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=2)
ix = date_range('1/1/2000', periods=10, freq='M')
df.set_index(ix, inplace=True)
df_err.set_index(ix, inplace=True)
ax = _check_plot_works(df.plot, yerr=df_err, kind='line')
self._check_has_errorbars(ax, xerr=0, yerr=2)
tempResult = arange(12)
	
===================================================================	
TestDataFramePlots.test_errorbar_with_partial_columns: 1568	
----------------------------	

df = DataFrame(numpy.random.randn(10, 3))
df_err = DataFrame(numpy.random.randn(10, 2), columns=[0, 2])
kinds = ['line', 'bar']
for kind in kinds:
    ax = _check_plot_works(df.plot, yerr=df_err, kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=2)
ix = date_range('1/1/2000', periods=10, freq='M')
df.set_index(ix, inplace=True)
df_err.set_index(ix, inplace=True)
ax = _check_plot_works(df.plot, yerr=df_err, kind='line')
self._check_has_errorbars(ax, xerr=0, yerr=2)
tempResult = arange(12, 0, (- 1))
	
===================================================================	
TestDataFramePlots.test_boxplot: 812	
----------------------------	

df = self.hist_df
series = df['height']
numeric_cols = df._get_numeric_data().columns
labels = [pprint_thing(c) for c in numeric_cols]
ax = _check_plot_works(df.plot.box)
self._check_text_labels(ax.get_xticklabels(), labels)
tempResult = arange(1, (len(numeric_cols) + 1))
	
===================================================================	
TestDataFramePlots.test_errorbar_plot: 1499	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestDataFramePlots.test_errorbar_plot: 1499	
----------------------------	

tempResult = arange(12, 0, (- 1))
	
===================================================================	
TestDataFramePlots.test_logscales: 135	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestDataFramePlots.test_logscales: 135	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestDataFramePlots._check_bar_alignment: 727	
----------------------------	

axes = df.plot(kind=kind, stacked=stacked, subplots=subplots, align=align, width=width, position=position, grid=True)
axes = self._flatten_visible(axes)
for ax in axes:
    if (kind == 'bar'):
        axis = ax.xaxis
        (ax_min, ax_max) = ax.get_xlim()
        min_edge = min([p.get_x() for p in ax.patches])
        max_edge = max([(p.get_x() + p.get_width()) for p in ax.patches])
    elif (kind == 'barh'):
        axis = ax.yaxis
        (ax_min, ax_max) = ax.get_ylim()
        min_edge = min([p.get_y() for p in ax.patches])
        max_edge = max([(p.get_y() + p.get_height()) for p in ax.patches])
    else:
        raise ValueError
    self.assertAlmostEqual(ax_min, (min_edge - 0.25))
    self.assertAlmostEqual(ax_max, (max_edge + 0.25))
    p = ax.patches[0]
    if ((kind == 'bar') and ((stacked is True) or (subplots is True))):
        edge = p.get_x()
        center = (edge + (p.get_width() * position))
    elif ((kind == 'bar') and (stacked is False)):
        center = (p.get_x() + ((p.get_width() * len(df.columns)) * position))
        edge = p.get_x()
    elif ((kind == 'barh') and ((stacked is True) or (subplots is True))):
        center = (p.get_y() + (p.get_height() * position))
        edge = p.get_y()
    elif ((kind == 'barh') and (stacked is False)):
        center = (p.get_y() + ((p.get_height() * len(df.columns)) * position))
        edge = p.get_y()
    else:
        raise ValueError
    tempResult = arange(len(df))
	
===================================================================	
TestDataFramePlots.test_unsorted_index: 179	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestDataFramePlots.test_unsorted_index: 179	
----------------------------	

tempResult = arange(99, (- 1), (- 1))
	
===================================================================	
TestDataFramePlots.test_unsorted_index: 186	
----------------------------	

df = DataFrame({'y': numpy.arange(100)}, index=numpy.arange(99, (- 1), (- 1)), dtype=numpy.int64)
ax = df.plot()
l = ax.get_lines()[0]
rs = l.get_xydata()
rs = Series(rs[:, 1], rs[:, 0], dtype=numpy.int64, name='y')
pandas.util.testing.assert_series_equal(rs, df.y, check_index_type=False)
pandas.util.testing.close()
tempResult = arange(99, (- 1), (- 1))
	
===================================================================	
TestDataFramePlots.test_errorbar_asymmetrical: 1602	
----------------------------	

numpy.random.seed(0)
err = numpy.random.rand(3, 2, 5)
tempResult = arange(15)
	
===================================================================	
TestDataFrameGroupByPlots.test_hist_single_row: 33	
----------------------------	

tempResult = arange(80, (100 + 2), 1)
	
===================================================================	
TestSeriesPlots.test_errorbar_plot: 533	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSeriesPlots.test_errorbar_plot: 551	
----------------------------	

s = Series(numpy.arange(10), name='x')
s_err = numpy.random.randn(10)
d_err = DataFrame(randn(10, 2), index=s.index, columns=['x', 'y'])
kinds = ['line', 'bar']
for kind in kinds:
    ax = _check_plot_works(s.plot, yerr=Series(s_err), kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, yerr=s_err, kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, yerr=s_err.tolist(), kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, yerr=d_err, kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, xerr=0.2, yerr=0.2, kind=kind)
    self._check_has_errorbars(ax, xerr=1, yerr=1)
ax = _check_plot_works(s.plot, xerr=s_err)
self._check_has_errorbars(ax, xerr=1, yerr=0)
ix = date_range('1/1/2000', '1/1/2001', freq='M')
tempResult = arange(12)
	
===================================================================	
TestSeriesPlots.test_errorbar_plot: 559	
----------------------------	

s = Series(numpy.arange(10), name='x')
s_err = numpy.random.randn(10)
d_err = DataFrame(randn(10, 2), index=s.index, columns=['x', 'y'])
kinds = ['line', 'bar']
for kind in kinds:
    ax = _check_plot_works(s.plot, yerr=Series(s_err), kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, yerr=s_err, kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, yerr=s_err.tolist(), kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, yerr=d_err, kind=kind)
    self._check_has_errorbars(ax, xerr=0, yerr=1)
    ax = _check_plot_works(s.plot, xerr=0.2, yerr=0.2, kind=kind)
    self._check_has_errorbars(ax, xerr=1, yerr=1)
ax = _check_plot_works(s.plot, xerr=s_err)
self._check_has_errorbars(ax, xerr=1, yerr=0)
ix = date_range('1/1/2000', '1/1/2001', freq='M')
ts = Series(numpy.arange(12), index=ix, name='x')
ts_err = Series(numpy.random.randn(12), index=ix)
td_err = DataFrame(randn(12, 2), index=ix, columns=['x', 'y'])
ax = _check_plot_works(ts.plot, yerr=ts_err)
self._check_has_errorbars(ax, xerr=0, yerr=1)
ax = _check_plot_works(ts.plot, yerr=td_err)
self._check_has_errorbars(ax, xerr=0, yerr=1)
with pandas.util.testing.assertRaises(ValueError):
    tempResult = arange(11)
	
===================================================================	
TestSeriesPlots.test_time_series_plot_color_kwargs: 611	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestSeriesPlots.test_series_plot_color_kwargs: 607	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestSeriesPlots.test_xticklabels: 628	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSeriesPlots.test_time_series_plot_color_with_empty_kwargs: 621	
----------------------------	

import matplotlib as mpl
if self.mpl_ge_1_5_0:
    def_colors = self._maybe_unpack_cycler(matplotlib.rcParams)
else:
    def_colors = matplotlib.rcParams['axes.color_cycle']
index = date_range('1/1/2000', periods=12)
tempResult = arange(1, 13)
	
===================================================================	
TestSeriesAlterAxes.test_setindex: 19	
----------------------------	

series = self.series.copy()
self.assertRaises(TypeError, setattr, series, 'index', None)
series = self.series.copy()
tempResult = arange((len(series) - 1))
	
===================================================================	
TestSeriesAlterAxes.test_setindex: 21	
----------------------------	

series = self.series.copy()
self.assertRaises(TypeError, setattr, series, 'index', None)
series = self.series.copy()
self.assertRaises(Exception, setattr, series, 'index', numpy.arange((len(series) - 1)))
series = self.series.copy()
tempResult = arange(len(series))
	
===================================================================	
TestSeriesAlterAxes.test_reorder_levels: 121	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
tempResult = arange(6)
	
===================================================================	
TestSeriesAlterAxes.test_reorder_levels: 128	
----------------------------	

index = MultiIndex(levels=[['bar'], ['one', 'two', 'three'], [0, 1]], labels=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]], names=['L0', 'L1', 'L2'])
s = Series(numpy.arange(6), index=index)
result = s.reorder_levels([0, 1, 2])
assert_series_equal(s, result)
result = s.reorder_levels(['L0', 'L1', 'L2'])
assert_series_equal(s, result)
result = s.reorder_levels([1, 2, 0])
e_idx = MultiIndex(levels=[['one', 'two', 'three'], [0, 1], ['bar']], labels=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0]], names=['L1', 'L2', 'L0'])
tempResult = arange(6)
	
===================================================================	
TestSeriesAlterAxes.test_rename: 31	
----------------------------	

renamer = (lambda x: x.strftime('%Y%m%d'))
renamed = self.ts.rename(renamer)
self.assertEqual(renamed.index[0], renamer(self.ts.index[0]))
rename_dict = dict(zip(self.ts.index, renamed.index))
renamed2 = self.ts.rename(rename_dict)
assert_series_equal(renamed, renamed2)
tempResult = arange(4)
	
===================================================================	
TestSeriesAlterAxes.test_rename: 34	
----------------------------	

renamer = (lambda x: x.strftime('%Y%m%d'))
renamed = self.ts.rename(renamer)
self.assertEqual(renamed.index[0], renamer(self.ts.index[0]))
rename_dict = dict(zip(self.ts.index, renamed.index))
renamed2 = self.ts.rename(rename_dict)
assert_series_equal(renamed, renamed2)
s = Series(numpy.arange(4), index=['a', 'b', 'c', 'd'], dtype='int64')
renamed = s.rename({'b': 'foo', 'd': 'bar'})
self.assert_index_equal(renamed.index, Index(['a', 'foo', 'c', 'bar']))
tempResult = arange(4)
	
===================================================================	
TestSeriesAnalytics.test_count: 522	
----------------------------	

self.assertEqual(self.ts.count(), len(self.ts))
self.ts[::2] = numpy.NaN
self.assertEqual(self.ts.count(), np.isfinite(self.ts).sum())
mi = pandas.core.index.MultiIndex.from_arrays([list('aabbcc'), [1, 2, 2, nan, 1, 2]])
tempResult = arange(len(mi))
	
===================================================================	
TestSeriesAnalytics.test_overflow: 40	
----------------------------	

for dtype in ['int32', 'int64']:
    tempResult = arange(5000000, dtype=dtype)
	
===================================================================	
TestSeriesAnalytics.test_overflow: 55	
----------------------------	

for dtype in ['int32', 'int64']:
    v = numpy.arange(5000000, dtype=dtype)
    s = Series(v)
    result = s.sum(skipna=False)
    self.assertEqual(int(result), v.sum(dtype='int64'))
    result = s.min(skipna=False)
    self.assertEqual(int(result), 0)
    result = s.max(skipna=False)
    self.assertEqual(int(result), v[(- 1)])
    result = s.sum()
    self.assertEqual(int(result), v.sum(dtype='int64'))
    result = s.min()
    self.assertEqual(int(result), 0)
    result = s.max()
    self.assertEqual(int(result), v[(- 1)])
for dtype in ['float32', 'float64']:
    tempResult = arange(5000000, dtype=dtype)
	
===================================================================	
TestSeriesAnalytics.test_rank: 640	
----------------------------	

pandas.util.testing._skip_if_no_scipy()
from scipy.stats import rankdata
self.ts[::2] = numpy.nan
self.ts[:10][::3] = 4.0
ranks = self.ts.rank()
oranks = self.ts.astype('O').rank()
assert_series_equal(ranks, oranks)
mask = numpy.isnan(self.ts)
filled = self.ts.fillna(numpy.inf)
exp = Series(rankdata(filled), index=filled.index, name='ts')
exp[mask] = numpy.nan
pandas.util.testing.assert_series_equal(ranks, exp)
tempResult = arange(5)
	
===================================================================	
TestSeriesAnalytics.test_rank: 644	
----------------------------	

pandas.util.testing._skip_if_no_scipy()
from scipy.stats import rankdata
self.ts[::2] = numpy.nan
self.ts[:10][::3] = 4.0
ranks = self.ts.rank()
oranks = self.ts.astype('O').rank()
assert_series_equal(ranks, oranks)
mask = numpy.isnan(self.ts)
filled = self.ts.fillna(numpy.inf)
exp = Series(rankdata(filled), index=filled.index, name='ts')
exp[mask] = numpy.nan
pandas.util.testing.assert_series_equal(ranks, exp)
iseries = Series(np.arange(5).repeat(2))
iranks = iseries.rank()
exp = iseries.astype(float).rank()
assert_series_equal(iranks, exp)
tempResult = arange(5)
	
===================================================================	
TestSeriesAnalytics.test_rank: 657	
----------------------------	

pandas.util.testing._skip_if_no_scipy()
from scipy.stats import rankdata
self.ts[::2] = numpy.nan
self.ts[:10][::3] = 4.0
ranks = self.ts.rank()
oranks = self.ts.astype('O').rank()
assert_series_equal(ranks, oranks)
mask = numpy.isnan(self.ts)
filled = self.ts.fillna(numpy.inf)
exp = Series(rankdata(filled), index=filled.index, name='ts')
exp[mask] = numpy.nan
pandas.util.testing.assert_series_equal(ranks, exp)
iseries = Series(np.arange(5).repeat(2))
iranks = iseries.rank()
exp = iseries.astype(float).rank()
assert_series_equal(iranks, exp)
iseries = (Series(numpy.arange(5)) + 1.0)
exp = (iseries / 5.0)
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = Series(numpy.repeat(1, 100))
exp = Series(numpy.repeat(0.505, 100))
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries[1] = numpy.nan
exp = Series(numpy.repeat((50.0 / 99.0), 100))
exp[1] = numpy.nan
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
tempResult = arange(5)
	
===================================================================	
TestSeriesAnalytics.test_rank: 666	
----------------------------	

pandas.util.testing._skip_if_no_scipy()
from scipy.stats import rankdata
self.ts[::2] = numpy.nan
self.ts[:10][::3] = 4.0
ranks = self.ts.rank()
oranks = self.ts.astype('O').rank()
assert_series_equal(ranks, oranks)
mask = numpy.isnan(self.ts)
filled = self.ts.fillna(numpy.inf)
exp = Series(rankdata(filled), index=filled.index, name='ts')
exp[mask] = numpy.nan
pandas.util.testing.assert_series_equal(ranks, exp)
iseries = Series(np.arange(5).repeat(2))
iranks = iseries.rank()
exp = iseries.astype(float).rank()
assert_series_equal(iranks, exp)
iseries = (Series(numpy.arange(5)) + 1.0)
exp = (iseries / 5.0)
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = Series(numpy.repeat(1, 100))
exp = Series(numpy.repeat(0.505, 100))
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries[1] = numpy.nan
exp = Series(numpy.repeat((50.0 / 99.0), 100))
exp[1] = numpy.nan
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = (Series(numpy.arange(5)) + 1.0)
iseries[4] = numpy.nan
exp = (iseries / 4.0)
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = Series(numpy.repeat(numpy.nan, 100))
exp = iseries.copy()
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
tempResult = arange(5)
	
===================================================================	
TestSeriesAnalytics.test_rank: 672	
----------------------------	

pandas.util.testing._skip_if_no_scipy()
from scipy.stats import rankdata
self.ts[::2] = numpy.nan
self.ts[:10][::3] = 4.0
ranks = self.ts.rank()
oranks = self.ts.astype('O').rank()
assert_series_equal(ranks, oranks)
mask = numpy.isnan(self.ts)
filled = self.ts.fillna(numpy.inf)
exp = Series(rankdata(filled), index=filled.index, name='ts')
exp[mask] = numpy.nan
pandas.util.testing.assert_series_equal(ranks, exp)
iseries = Series(np.arange(5).repeat(2))
iranks = iseries.rank()
exp = iseries.astype(float).rank()
assert_series_equal(iranks, exp)
iseries = (Series(numpy.arange(5)) + 1.0)
exp = (iseries / 5.0)
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = Series(numpy.repeat(1, 100))
exp = Series(numpy.repeat(0.505, 100))
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries[1] = numpy.nan
exp = Series(numpy.repeat((50.0 / 99.0), 100))
exp[1] = numpy.nan
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = (Series(numpy.arange(5)) + 1.0)
iseries[4] = numpy.nan
exp = (iseries / 4.0)
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = Series(numpy.repeat(numpy.nan, 100))
exp = iseries.copy()
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
iseries = (Series(numpy.arange(5)) + 1)
iseries[4] = numpy.nan
exp = (iseries / 4.0)
iranks = iseries.rank(pct=True)
assert_series_equal(iranks, exp)
rng = date_range('1/1/1990', periods=5)
tempResult = arange(5)
	
===================================================================	
TestSeriesAnalytics.test_is_unique: 980	
----------------------------	

s = Series(numpy.random.randint(0, 10, size=1000))
self.assertFalse(s.is_unique)
tempResult = arange(1000)
	
===================================================================	
TestSeriesAnalytics.test_numpy_repeat: 936	
----------------------------	

tempResult = arange(3)
	
===================================================================	
TestSeriesAnalytics.test_unstack: 1141	
----------------------------	

from numpy import nan
index = MultiIndex(levels=[['bar', 'foo'], ['one', 'three', 'two']], labels=[[1, 1, 0, 0], [0, 1, 0, 2]])
tempResult = arange(4.0)
	
===================================================================	
TestSeriesAnalytics.test_npdiff: 291	
----------------------------	

raise nose.SkipTest('skipping due to Series no longer being an ndarray')
tempResult = arange(5)
	
===================================================================	
TestSeriesAnalytics.test_is_monotonic: 986	
----------------------------	

s = Series(numpy.random.randint(0, 10, size=1000))
self.assertFalse(s.is_monotonic)
tempResult = arange(1000)
	
===================================================================	
TestSeriesAnalytics.test_is_monotonic: 989	
----------------------------	

s = Series(numpy.random.randint(0, 10, size=1000))
self.assertFalse(s.is_monotonic)
s = Series(numpy.arange(1000))
self.assertTrue(s.is_monotonic)
self.assertTrue(s.is_monotonic_increasing)
tempResult = arange(1000, 0, (- 1))
	
===================================================================	
TestSeriesAsof.test_with_nan: 50	
----------------------------	

rng = date_range('1/1/2000', '1/2/2000', freq='4h')
tempResult = arange(len(rng))
	
===================================================================	
TestSeriesAsof.test_scalar: 34	
----------------------------	

N = 30
rng = date_range('1/1/1990', periods=N, freq='53s')
tempResult = arange(N)
	
===================================================================	
TestSeriesConstructors.test_constructor_default_index: 178	
----------------------------	

s = Series([0, 1, 2])
tempResult = arange(3)
	
===================================================================	
TestSeriesConstructors.test_constructor: 51	
----------------------------	

with pandas.util.testing.assert_produces_warning(FutureWarning):
    self.assertTrue(self.ts.is_time_series)
self.assertTrue(self.ts.index.is_all_dates)
derived = Series(self.ts)
with pandas.util.testing.assert_produces_warning(FutureWarning):
    self.assertTrue(derived.is_time_series)
self.assertTrue(derived.index.is_all_dates)
self.assertTrue(pandas.util.testing.equalContents(derived.index, self.ts.index))
self.assertEqual(id(self.ts.index), id(derived.index))
mixed = Series(['hello', numpy.NaN], index=[0, 1])
self.assertEqual(mixed.dtype, numpy.object_)
self.assertIs(mixed[1], numpy.NaN)
with pandas.util.testing.assert_produces_warning(FutureWarning):
    self.assertFalse(self.empty.is_time_series)
self.assertFalse(self.empty.index.is_all_dates)
with pandas.util.testing.assert_produces_warning(FutureWarning):
    self.assertFalse(Series({}).is_time_series)
self.assertFalse(Series({}).index.is_all_dates)
tempResult = arange(3)
	
===================================================================	
TestSeriesDatetimeValues.test_dt_accessor_api: 228	
----------------------------	

from pandas.tseries.common import CombinedDatetimelikeProperties, DatetimeProperties
self.assertIs(pandas.Series.dt, CombinedDatetimelikeProperties)
s = Series(date_range('2000-01-01', periods=3))
self.assertIsInstance(s.dt, DatetimeProperties)
tempResult = arange(5)
	
===================================================================	
TestSeriesDtypes.test_astype_cast_object_int: 45	
----------------------------	

arr = Series(['car', 'house', 'tree', '1'])
self.assertRaises(ValueError, arr.astype, int)
self.assertRaises(ValueError, arr.astype, numpy.int64)
self.assertRaises(ValueError, arr.astype, numpy.int8)
arr = Series(['1', '2', '3', '4'], dtype=object)
result = arr.astype(int)
tempResult = arange(1, 5)
	
===================================================================	
TestSeriesIndexing.test_mask_broadcast: 848	
----------------------------	

for size in range(2, 6):
    for selection in [numpy.resize([True, False, False, False, False], size), numpy.resize([True, False], size), numpy.resize([False], size)]:
        for item in [2.0, numpy.nan, np.finfo(np.float).max, np.finfo(np.float).min]:
            for arr in [numpy.array([item]), [item], (item,)]:
                tempResult = arange(size, dtype=float)
	
===================================================================	
TestSeriesIndexing.test_setitem_na: 951	
----------------------------	

expected = Series([numpy.nan, 3, numpy.nan, 5, numpy.nan, 7, numpy.nan, 9, numpy.nan])
s = Series([2, 3, 4, 5, 6, 7, 8, 9, 10])
s[::2] = numpy.nan
assert_series_equal(s, expected)
expected = Series([numpy.nan, 1, numpy.nan, 0])
s = Series([True, True, False, False])
s[::2] = numpy.nan
assert_series_equal(s, expected)
expected = Series([numpy.nan, numpy.nan, numpy.nan, numpy.nan, numpy.nan, 5, 6, 7, 8, 9])
tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_align_multiindex: 1152	
----------------------------	

midx = pandas.MultiIndex.from_product([range(2), range(3), range(2)], names=('a', 'b', 'c'))
idx = pandas.Index(range(2), name='b')
tempResult = arange(12, dtype='int64')
	
===================================================================	
TestSeriesIndexing.test_align_multiindex: 1153	
----------------------------	

midx = pandas.MultiIndex.from_product([range(2), range(3), range(2)], names=('a', 'b', 'c'))
idx = pandas.Index(range(2), name='b')
s1 = pandas.Series(numpy.arange(12, dtype='int64'), index=midx)
tempResult = arange(2, dtype='int64')
	
===================================================================	
TestSeriesIndexing.test_reindex_nearest: 1241	
----------------------------	

tempResult = arange(10, dtype='int64')
	
===================================================================	
TestSeriesIndexing.test_slice_floats2: 352	
----------------------------	

tempResult = arange(10, 20, dtype=float)
	
===================================================================	
TestSeriesIndexing.test_slice_floats2: 355	
----------------------------	

s = Series(numpy.random.rand(10), index=numpy.arange(10, 20, dtype=float))
self.assertEqual(len(s.ix[12.0:]), 8)
self.assertEqual(len(s.ix[12.5:]), 7)
tempResult = arange(10, 20, dtype=float)
	
===================================================================	
TestSeriesIndexing.test_slice_float64: 362	
----------------------------	

tempResult = arange(10.0, 50.0, 2)
	
===================================================================	
TestSeriesIndexing.test_where: 645	
----------------------------	

s = Series(numpy.random.randn(5))
cond = (s > 0)
rs = s.where(cond).dropna()
rs2 = s[cond]
assert_series_equal(rs, rs2)
rs = s.where(cond, (- s))
assert_series_equal(rs, s.abs())
rs = s.where(cond)
assert (s.shape == rs.shape)
assert (rs is not s)
cond = Series([True, False, False, True, False], index=s.index)
s2 = (- s.abs())
expected = s2[cond].reindex(s2.index[:3]).reindex(s2.index)
rs = s2.where(cond[:3])
assert_series_equal(rs, expected)
expected = s2.abs()
expected.ix[0] = s2[0]
rs = s2.where(cond[:3], (- s2))
assert_series_equal(rs, expected)
self.assertRaises(ValueError, s.where, 1)
self.assertRaises(ValueError, s.where, cond[:3].values, (- s))
s = Series([1, 2])
s[[True, False]] = [0, 1]
expected = Series([0, 2])
assert_series_equal(s, expected)
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [0, 2, 3])
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [])
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64]:
    tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_where: 652	
----------------------------	

s = Series(numpy.random.randn(5))
cond = (s > 0)
rs = s.where(cond).dropna()
rs2 = s[cond]
assert_series_equal(rs, rs2)
rs = s.where(cond, (- s))
assert_series_equal(rs, s.abs())
rs = s.where(cond)
assert (s.shape == rs.shape)
assert (rs is not s)
cond = Series([True, False, False, True, False], index=s.index)
s2 = (- s.abs())
expected = s2[cond].reindex(s2.index[:3]).reindex(s2.index)
rs = s2.where(cond[:3])
assert_series_equal(rs, expected)
expected = s2.abs()
expected.ix[0] = s2[0]
rs = s2.where(cond[:3], (- s2))
assert_series_equal(rs, expected)
self.assertRaises(ValueError, s.where, 1)
self.assertRaises(ValueError, s.where, cond[:3].values, (- s))
s = Series([1, 2])
s[[True, False]] = [0, 1]
expected = Series([0, 2])
assert_series_equal(s, expected)
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [0, 2, 3])
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [])
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    s[mask] = lrange(2, 7)
    expected = Series((lrange(2, 7) + lrange(5, 10)), dtype=dtype)
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
for dtype in [numpy.int64, numpy.float64]:
    tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_where: 659	
----------------------------	

s = Series(numpy.random.randn(5))
cond = (s > 0)
rs = s.where(cond).dropna()
rs2 = s[cond]
assert_series_equal(rs, rs2)
rs = s.where(cond, (- s))
assert_series_equal(rs, s.abs())
rs = s.where(cond)
assert (s.shape == rs.shape)
assert (rs is not s)
cond = Series([True, False, False, True, False], index=s.index)
s2 = (- s.abs())
expected = s2[cond].reindex(s2.index[:3]).reindex(s2.index)
rs = s2.where(cond[:3])
assert_series_equal(rs, expected)
expected = s2.abs()
expected.ix[0] = s2[0]
rs = s2.where(cond[:3], (- s2))
assert_series_equal(rs, expected)
self.assertRaises(ValueError, s.where, 1)
self.assertRaises(ValueError, s.where, cond[:3].values, (- s))
s = Series([1, 2])
s[[True, False]] = [0, 1]
expected = Series([0, 2])
assert_series_equal(s, expected)
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [0, 2, 3])
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [])
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    s[mask] = lrange(2, 7)
    expected = Series((lrange(2, 7) + lrange(5, 10)), dtype=dtype)
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
for dtype in [numpy.int64, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    s[mask] = values
    expected = Series((values + lrange(5, 10)), dtype='float64')
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_where: 666	
----------------------------	

s = Series(numpy.random.randn(5))
cond = (s > 0)
rs = s.where(cond).dropna()
rs2 = s[cond]
assert_series_equal(rs, rs2)
rs = s.where(cond, (- s))
assert_series_equal(rs, s.abs())
rs = s.where(cond)
assert (s.shape == rs.shape)
assert (rs is not s)
cond = Series([True, False, False, True, False], index=s.index)
s2 = (- s.abs())
expected = s2[cond].reindex(s2.index[:3]).reindex(s2.index)
rs = s2.where(cond[:3])
assert_series_equal(rs, expected)
expected = s2.abs()
expected.ix[0] = s2[0]
rs = s2.where(cond[:3], (- s2))
assert_series_equal(rs, expected)
self.assertRaises(ValueError, s.where, 1)
self.assertRaises(ValueError, s.where, cond[:3].values, (- s))
s = Series([1, 2])
s[[True, False]] = [0, 1]
expected = Series([0, 2])
assert_series_equal(s, expected)
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [0, 2, 3])
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [])
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    s[mask] = lrange(2, 7)
    expected = Series((lrange(2, 7) + lrange(5, 10)), dtype=dtype)
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
for dtype in [numpy.int64, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    s[mask] = values
    expected = Series((values + lrange(5, 10)), dtype='float64')
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
s = Series(numpy.arange(10), dtype='int64')
mask = (s > 5)
values = [2.5, 3.5, 4.5, 5.5]
s[mask] = values
expected = Series((lrange(6) + values), dtype='float64')
assert_series_equal(s, expected)
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.float16, numpy.float32]:
    tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_where: 670	
----------------------------	

s = Series(numpy.random.randn(5))
cond = (s > 0)
rs = s.where(cond).dropna()
rs2 = s[cond]
assert_series_equal(rs, rs2)
rs = s.where(cond, (- s))
assert_series_equal(rs, s.abs())
rs = s.where(cond)
assert (s.shape == rs.shape)
assert (rs is not s)
cond = Series([True, False, False, True, False], index=s.index)
s2 = (- s.abs())
expected = s2[cond].reindex(s2.index[:3]).reindex(s2.index)
rs = s2.where(cond[:3])
assert_series_equal(rs, expected)
expected = s2.abs()
expected.ix[0] = s2[0]
rs = s2.where(cond[:3], (- s2))
assert_series_equal(rs, expected)
self.assertRaises(ValueError, s.where, 1)
self.assertRaises(ValueError, s.where, cond[:3].values, (- s))
s = Series([1, 2])
s[[True, False]] = [0, 1]
expected = Series([0, 2])
assert_series_equal(s, expected)
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [0, 2, 3])
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [])
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    s[mask] = lrange(2, 7)
    expected = Series((lrange(2, 7) + lrange(5, 10)), dtype=dtype)
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
for dtype in [numpy.int64, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    s[mask] = values
    expected = Series((values + lrange(5, 10)), dtype='float64')
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
s = Series(numpy.arange(10), dtype='int64')
mask = (s > 5)
values = [2.5, 3.5, 4.5, 5.5]
s[mask] = values
expected = Series((lrange(6) + values), dtype='float64')
assert_series_equal(s, expected)
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.float16, numpy.float32]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    self.assertRaises(Exception, s.__setitem__, tuple(mask), values)
tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_where: 676	
----------------------------	

s = Series(numpy.random.randn(5))
cond = (s > 0)
rs = s.where(cond).dropna()
rs2 = s[cond]
assert_series_equal(rs, rs2)
rs = s.where(cond, (- s))
assert_series_equal(rs, s.abs())
rs = s.where(cond)
assert (s.shape == rs.shape)
assert (rs is not s)
cond = Series([True, False, False, True, False], index=s.index)
s2 = (- s.abs())
expected = s2[cond].reindex(s2.index[:3]).reindex(s2.index)
rs = s2.where(cond[:3])
assert_series_equal(rs, expected)
expected = s2.abs()
expected.ix[0] = s2[0]
rs = s2.where(cond[:3], (- s2))
assert_series_equal(rs, expected)
self.assertRaises(ValueError, s.where, 1)
self.assertRaises(ValueError, s.where, cond[:3].values, (- s))
s = Series([1, 2])
s[[True, False]] = [0, 1]
expected = Series([0, 2])
assert_series_equal(s, expected)
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [0, 2, 3])
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [])
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    s[mask] = lrange(2, 7)
    expected = Series((lrange(2, 7) + lrange(5, 10)), dtype=dtype)
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
for dtype in [numpy.int64, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    s[mask] = values
    expected = Series((values + lrange(5, 10)), dtype='float64')
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
s = Series(numpy.arange(10), dtype='int64')
mask = (s > 5)
values = [2.5, 3.5, 4.5, 5.5]
s[mask] = values
expected = Series((lrange(6) + values), dtype='float64')
assert_series_equal(s, expected)
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.float16, numpy.float32]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    self.assertRaises(Exception, s.__setitem__, tuple(mask), values)
s = Series(numpy.arange(10), dtype='int64')
mask = (s < 5)
s[mask] = lrange(2, 7)
expected = Series((lrange(2, 7) + lrange(5, 10)), dtype='int64')
assert_series_equal(s, expected)
self.assertEqual(s.dtype, expected.dtype)
tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_where: 681	
----------------------------	

s = Series(numpy.random.randn(5))
cond = (s > 0)
rs = s.where(cond).dropna()
rs2 = s[cond]
assert_series_equal(rs, rs2)
rs = s.where(cond, (- s))
assert_series_equal(rs, s.abs())
rs = s.where(cond)
assert (s.shape == rs.shape)
assert (rs is not s)
cond = Series([True, False, False, True, False], index=s.index)
s2 = (- s.abs())
expected = s2[cond].reindex(s2.index[:3]).reindex(s2.index)
rs = s2.where(cond[:3])
assert_series_equal(rs, expected)
expected = s2.abs()
expected.ix[0] = s2[0]
rs = s2.where(cond[:3], (- s2))
assert_series_equal(rs, expected)
self.assertRaises(ValueError, s.where, 1)
self.assertRaises(ValueError, s.where, cond[:3].values, (- s))
s = Series([1, 2])
s[[True, False]] = [0, 1]
expected = Series([0, 2])
assert_series_equal(s, expected)
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [0, 2, 3])
self.assertRaises(ValueError, s.__setitem__, tuple([[[True, False]]]), [])
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.int64, numpy.float16, numpy.float32, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    s[mask] = lrange(2, 7)
    expected = Series((lrange(2, 7) + lrange(5, 10)), dtype=dtype)
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
for dtype in [numpy.int64, numpy.float64]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    s[mask] = values
    expected = Series((values + lrange(5, 10)), dtype='float64')
    assert_series_equal(s, expected)
    self.assertEqual(s.dtype, expected.dtype)
s = Series(numpy.arange(10), dtype='int64')
mask = (s > 5)
values = [2.5, 3.5, 4.5, 5.5]
s[mask] = values
expected = Series((lrange(6) + values), dtype='float64')
assert_series_equal(s, expected)
for dtype in [numpy.int8, numpy.int16, numpy.int32, numpy.float16, numpy.float32]:
    s = Series(numpy.arange(10), dtype=dtype)
    mask = (s < 5)
    values = [2.5, 3.5, 4.5, 5.5, 6.5]
    self.assertRaises(Exception, s.__setitem__, tuple(mask), values)
s = Series(numpy.arange(10), dtype='int64')
mask = (s < 5)
s[mask] = lrange(2, 7)
expected = Series((lrange(2, 7) + lrange(5, 10)), dtype='int64')
assert_series_equal(s, expected)
self.assertEqual(s.dtype, expected.dtype)
s = Series(numpy.arange(10), dtype='int64')
mask = (s > 5)
s[mask] = ([0] * 4)
expected = Series(([0, 1, 2, 3, 4, 5] + ([0] * 4)), dtype='int64')
assert_series_equal(s, expected)
tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_where_broadcast: 751	
----------------------------	

for size in range(2, 6):
    for selection in [numpy.resize([True, False, False, False, False], size), numpy.resize([True, False], size), numpy.resize([False], size)]:
        for item in [2.0, numpy.nan, np.finfo(np.float).max, np.finfo(np.float).min]:
            for arr in [numpy.array([item]), [item], (item,)]:
                tempResult = arange(size, dtype=float)
	
===================================================================	
TestSeriesIndexing.test_type_promote_putmask: 1338	
----------------------------	

tempResult = arange(100, 0, (- 1))
	
===================================================================	
TestSeriesIndexing.test_reindex_pad: 1213	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_reindex_pad: 1218	
----------------------------	

s = Series(numpy.arange(10), dtype='int64')
s2 = s[::2]
reindexed = s2.reindex(s.index, method='pad')
reindexed2 = s2.reindex(s.index, method='ffill')
assert_series_equal(reindexed, reindexed2)
tempResult = arange(10)
	
===================================================================	
TestSeriesIndexing.test_pop: 84	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
TestSeriesMisc.test_copy: 182	
----------------------------	

for deep in [None, False, True]:
    tempResult = arange(10)
	
===================================================================	
TestSeriesInterpolateData.test_interpolate: 349	
----------------------------	

tempResult = arange(len(self.ts), dtype=float)
	
===================================================================	
TestSeriesInterpolateData.test_spline_interpolation: 607	
----------------------------	

pandas.util.testing._skip_if_no_scipy()
tempResult = arange(10)
	
===================================================================	
TestSeriesInterpolateData.test_spline_error: 615	
----------------------------	

pandas.util.testing._skip_if_no_scipy()
tempResult = arange(10)
	
===================================================================	
TestSeriesOperators.test_operators_combine: 1139	
----------------------------	


def _check_fill(meth, op, a, b, fill_value=0):
    exp_index = a.index.union(b.index)
    a = a.reindex(exp_index)
    b = b.reindex(exp_index)
    amask = isnull(a)
    bmask = isnull(b)
    exp_values = []
    for i in range(len(exp_index)):
        with numpy.errstate(all='ignore'):
            if amask[i]:
                if bmask[i]:
                    exp_values.append(nan)
                    continue
                exp_values.append(op(fill_value, b[i]))
            elif bmask[i]:
                if amask[i]:
                    exp_values.append(nan)
                    continue
                exp_values.append(op(a[i], fill_value))
            else:
                exp_values.append(op(a[i], b[i]))
    result = meth(a, b, fill_value=fill_value)
    expected = Series(exp_values, exp_index)
    assert_series_equal(result, expected)
tempResult = arange(5)
	
===================================================================	
TestSeriesOperators.test_operators_combine: 1140	
----------------------------	


def _check_fill(meth, op, a, b, fill_value=0):
    exp_index = a.index.union(b.index)
    a = a.reindex(exp_index)
    b = b.reindex(exp_index)
    amask = isnull(a)
    bmask = isnull(b)
    exp_values = []
    for i in range(len(exp_index)):
        with numpy.errstate(all='ignore'):
            if amask[i]:
                if bmask[i]:
                    exp_values.append(nan)
                    continue
                exp_values.append(op(fill_value, b[i]))
            elif bmask[i]:
                if amask[i]:
                    exp_values.append(nan)
                    continue
                exp_values.append(op(a[i], fill_value))
            else:
                exp_values.append(op(a[i], b[i]))
    result = meth(a, b, fill_value=fill_value)
    expected = Series(exp_values, exp_index)
    assert_series_equal(result, expected)
a = Series([nan, 1.0, 2.0, 3.0, nan], index=numpy.arange(5))
tempResult = arange(6)
	
===================================================================	
TestSeriesOperators.test_operators_reverse_object: 961	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSeriesReplace.test_replace_mixed_types: 88	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestSeriesRepr.test_repr: 41	
----------------------------	

str(self.ts)
str(self.series)
str(self.series.astype(int))
str(self.objSeries)
tempResult = arange(1000)
	
===================================================================	
TestSeriesRepr.test_repr: 42	
----------------------------	

str(self.ts)
str(self.series)
str(self.series.astype(int))
str(self.objSeries)
str(Series(pandas.util.testing.randn(1000), index=numpy.arange(1000)))
tempResult = arange(1000, 0, step=(- 1))
	
===================================================================	
TestSeriesRepr.test_repr: 52	
----------------------------	

str(self.ts)
str(self.series)
str(self.series.astype(int))
str(self.objSeries)
str(Series(pandas.util.testing.randn(1000), index=numpy.arange(1000)))
str(Series(pandas.util.testing.randn(1000), index=numpy.arange(1000, 0, step=(- 1))))
str(self.empty)
self.series[5:7] = numpy.NaN
str(self.series)
ots = self.ts.astype('O')
ots[::2] = None
repr(ots)
for name in ['', 1, 1.2, 'foo', u('αβγ'), 'loooooooooooooooooooooooooooooooooooooooooooooooooooong', ('foo', 'bar', 'baz'), (1, 2), ('foo', 1, 2.3), (u('α'), u('β'), u('γ')), (u('α'), 'bar')]:
    self.series.name = name
    repr(self.series)
tempResult = arange(1000)
	
===================================================================	
TestSeriesTimeSeries.test_shift: 49	
----------------------------	

shifted = self.ts.shift(1)
unshifted = shifted.shift((- 1))
pandas.util.testing.assert_index_equal(shifted.index, self.ts.index)
pandas.util.testing.assert_index_equal(unshifted.index, self.ts.index)
pandas.util.testing.assert_numpy_array_equal(unshifted.valid().values, self.ts.values[:(- 1)])
offset = BDay()
shifted = self.ts.shift(1, freq=offset)
unshifted = shifted.shift((- 1), freq=offset)
assert_series_equal(unshifted, self.ts)
unshifted = self.ts.shift(0, freq=offset)
assert_series_equal(unshifted, self.ts)
shifted = self.ts.shift(1, freq='B')
unshifted = shifted.shift((- 1), freq='B')
assert_series_equal(unshifted, self.ts)
unshifted = self.ts.shift(0)
assert_series_equal(unshifted, self.ts)
ps = pandas.util.testing.makePeriodSeries()
shifted = ps.shift(1)
unshifted = shifted.shift((- 1))
pandas.util.testing.assert_index_equal(shifted.index, ps.index)
pandas.util.testing.assert_index_equal(unshifted.index, ps.index)
pandas.util.testing.assert_numpy_array_equal(unshifted.valid().values, ps.values[:(- 1)])
shifted2 = ps.shift(1, 'B')
shifted3 = ps.shift(1, BDay())
assert_series_equal(shifted2, shifted3)
assert_series_equal(ps, shifted2.shift((- 1), 'B'))
self.assertRaises(ValueError, ps.shift, freq='D')
shifted4 = ps.shift(1, freq='B')
assert_series_equal(shifted2, shifted4)
shifted5 = ps.shift(1, freq=BDay())
assert_series_equal(shifted5, shifted4)
index = date_range('2000-01-01', periods=5)
for dtype in ['int32', 'int64']:
    tempResult = arange(5, dtype=dtype)
	
===================================================================	
TestABCClasses: 13	
----------------------------	

tuples = [[1, 2, 2], ['red', 'blue', 'red']]
multi_index = pandas.MultiIndex.from_arrays(tuples, names=('number', 'color'))
datetime_index = pandas.to_datetime(['2000/1/1', '2010/1/1'])
tempResult = arange(5)
	
===================================================================	
test_ensure_int32: 606	
----------------------------	

tempResult = arange(10, dtype=numpy.int32)
	
===================================================================	
test_ensure_int32: 609	
----------------------------	

values = numpy.arange(10, dtype=numpy.int32)
result = _ensure_int32(values)
assert (result.dtype == numpy.int32)
tempResult = arange(10, dtype=numpy.int64)
	
===================================================================	
test_ensure_categorical: 614	
----------------------------	

tempResult = arange(10, dtype=numpy.int32)
	
===================================================================	
_MergeOperation._get_join_info: 259	
----------------------------	

left_ax = self.left._data.axes[self.axis]
right_ax = self.right._data.axes[self.axis]
if (self.left_index and self.right_index and (self.how != 'asof')):
    (join_index, left_indexer, right_indexer) = left_ax.join(right_ax, how=self.how, return_indexers=True)
elif (self.right_index and (self.how == 'left')):
    (join_index, left_indexer, right_indexer) = _left_join_on_index(left_ax, right_ax, self.left_join_keys, sort=self.sort)
elif (self.left_index and (self.how == 'right')):
    (join_index, right_indexer, left_indexer) = _left_join_on_index(right_ax, left_ax, self.right_join_keys, sort=self.sort)
else:
    (left_indexer, right_indexer) = self._get_join_indexers()
    if self.right_index:
        if (len(self.left) > 0):
            join_index = self.left.index.take(left_indexer)
        else:
            join_index = self.right.index.take(right_indexer)
            left_indexer = numpy.array(([(- 1)] * len(join_index)))
    elif self.left_index:
        if (len(self.right) > 0):
            join_index = self.right.index.take(right_indexer)
        else:
            join_index = self.left.index.take(left_indexer)
            right_indexer = numpy.array(([(- 1)] * len(join_index)))
    else:
        tempResult = arange(len(left_indexer))
	
===================================================================	
_make_concat_multiindex: 941	
----------------------------	

if (((levels is None) and isinstance(keys[0], tuple)) or ((levels is not None) and (len(levels) > 1))):
    zipped = lzip(*keys)
    if (names is None):
        names = ([None] * len(zipped))
    if (levels is None):
        (_, levels) = _factorize_from_iterables(zipped)
    else:
        levels = [_ensure_index(x) for x in levels]
else:
    zipped = [keys]
    if (names is None):
        names = [None]
    if (levels is None):
        levels = [_ensure_index(keys)]
    else:
        levels = [_ensure_index(x) for x in levels]
if (not _all_indexes_same(indexes)):
    label_list = []
    for (hlevel, level) in zip(zipped, levels):
        to_concat = []
        for (key, index) in zip(hlevel, indexes):
            try:
                i = level.get_loc(key)
            except KeyError:
                raise ValueError(('Key %s not in level %s' % (str(key), str(level))))
            to_concat.append(numpy.repeat(i, len(index)))
        label_list.append(numpy.concatenate(to_concat))
    concat_index = _concat_indexes(indexes)
    if isinstance(concat_index, MultiIndex):
        levels.extend(concat_index.levels)
        label_list.extend(concat_index.labels)
    else:
        (codes, categories) = _factorize_from_iterable(concat_index)
        levels.append(categories)
        label_list.append(codes)
    if (len(names) == len(levels)):
        names = list(names)
    else:
        if (not (len(set([idx.nlevels for idx in indexes])) == 1)):
            raise AssertionError('Cannot concat indices that do not have the same number of levels')
        names = (names + _get_consensus_names(indexes))
    return MultiIndex(levels=levels, labels=label_list, names=names, verify_integrity=False)
new_index = indexes[0]
n = len(new_index)
kpieces = len(indexes)
new_names = list(names)
new_levels = list(levels)
new_labels = []
for (hlevel, level) in zip(zipped, levels):
    hlevel = _ensure_index(hlevel)
    mapped = level.get_indexer(hlevel)
    mask = (mapped == (- 1))
    if mask.any():
        raise ValueError(('Values not found in passed level: %s' % str(hlevel[mask])))
    new_labels.append(numpy.repeat(mapped, n))
if isinstance(new_index, MultiIndex):
    new_levels.extend(new_index.levels)
    new_labels.extend([numpy.tile(lab, kpieces) for lab in new_index.labels])
else:
    new_levels.append(new_index)
    tempResult = arange(n)
	
===================================================================	
f: 307	
----------------------------	

x1 = amplitudes[0]
result = (x1 / sqrt(2.0))
coeffs = numpy.delete(numpy.copy(amplitudes), 0)
coeffs.resize(int(((coeffs.size + 1) / 2)), 2)
tempResult = arange(0, coeffs.shape[0])
	
===================================================================	
autocorrelation_plot: 456	
----------------------------	

'Autocorrelation plot for time series.\n\n    Parameters:\n    -----------\n    series: Time series\n    ax: Matplotlib axis object, optional\n    kwds : keywords\n        Options to pass to matplotlib plotting method\n\n    Returns:\n    -----------\n    ax: Matplotlib axis object\n    '
import matplotlib.pyplot as plt
n = len(series)
data = numpy.asarray(series)
if (ax is None):
    ax = matplotlib.pyplot.gca(xlim=(1, n), ylim=((- 1.0), 1.0))
mean = numpy.mean(data)
c0 = (numpy.sum(((data - mean) ** 2)) / float(n))

def r(h):
    return ((((data[:(n - h)] - mean) * (data[h:] - mean)).sum() / float(n)) / c0)
tempResult = arange(n)
	
===================================================================	
BarPlot.__init__: 1262	
----------------------------	

self.bar_width = kwargs.pop('width', 0.5)
pos = kwargs.pop('position', 0.5)
kwargs.setdefault('align', 'center')
tempResult = arange(len(data))
	
===================================================================	
TestConcatenate.test_concat_multiindex_rangeindex: 1343	
----------------------------	

df = DataFrame(numpy.random.randn(9, 2))
tempResult = arange(3)
	
===================================================================	
TestConcatenate.test_concat_multiindex_rangeindex: 1343	
----------------------------	

df = DataFrame(numpy.random.randn(9, 2))
tempResult = arange(3)
	
===================================================================	
TestConcatenate.test_concat_series: 867	
----------------------------	

ts = pandas.util.testing.makeTimeSeries()
ts.name = 'foo'
pieces = [ts[:5], ts[5:15], ts[15:]]
result = concat(pieces)
pandas.util.testing.assert_series_equal(result, ts)
self.assertEqual(result.name, ts.name)
result = concat(pieces, keys=[0, 1, 2])
expected = ts.copy()
ts.index = DatetimeIndex(numpy.array(ts.index.values, dtype='M8[ns]'))
tempResult = arange(len(ts))
	
===================================================================	
TestConcatenate.test_concat_timedelta64_block: 918	
----------------------------	

from pandas import to_timedelta
tempResult = arange(10)
	
===================================================================	
TestConcatenate.test_concat_mixed_objs: 743	
----------------------------	

index = date_range('01-Jan-2013', periods=10, freq='H')
tempResult = arange(10, dtype='int64')
	
===================================================================	
TestJoin.test_join_many_non_unique_index: 338	
----------------------------	

df1 = DataFrame({'a': [1, 1], 'b': [1, 1], 'c': [10, 20]})
df2 = DataFrame({'a': [1, 1], 'b': [1, 2], 'd': [100, 200]})
df3 = DataFrame({'a': [1, 1], 'b': [1, 2], 'e': [1000, 2000]})
idf1 = df1.set_index(['a', 'b'])
idf2 = df2.set_index(['a', 'b'])
idf3 = df3.set_index(['a', 'b'])
result = idf1.join([idf2, idf3], how='outer')
df_partially_merged = merge(df1, df2, on=['a', 'b'], how='outer')
expected = merge(df_partially_merged, df3, on=['a', 'b'], how='outer')
result = result.reset_index()
expected = expected[result.columns]
expected['a'] = expected.a.astype('int64')
expected['b'] = expected.b.astype('int64')
assert_frame_equal(result, expected)
df1 = DataFrame({'a': [1, 1, 1], 'b': [1, 1, 1], 'c': [10, 20, 30]})
df2 = DataFrame({'a': [1, 1, 1], 'b': [1, 1, 2], 'd': [100, 200, 300]})
df3 = DataFrame({'a': [1, 1, 1], 'b': [1, 1, 2], 'e': [1000, 2000, 3000]})
idf1 = df1.set_index(['a', 'b'])
idf2 = df2.set_index(['a', 'b'])
idf3 = df3.set_index(['a', 'b'])
result = idf1.join([idf2, idf3], how='inner')
df_partially_merged = merge(df1, df2, on=['a', 'b'], how='inner')
expected = merge(df_partially_merged, df3, on=['a', 'b'], how='inner')
result = result.reset_index()
assert_frame_equal(result, expected.ix[:, result.columns])
df = DataFrame({'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C': numpy.random.randn(8), 'D': numpy.random.randn(8)})
tempResult = arange(8)
	
===================================================================	
TestJoin.test_join_many_non_unique_index: 338	
----------------------------	

df1 = DataFrame({'a': [1, 1], 'b': [1, 1], 'c': [10, 20]})
df2 = DataFrame({'a': [1, 1], 'b': [1, 2], 'd': [100, 200]})
df3 = DataFrame({'a': [1, 1], 'b': [1, 2], 'e': [1000, 2000]})
idf1 = df1.set_index(['a', 'b'])
idf2 = df2.set_index(['a', 'b'])
idf3 = df3.set_index(['a', 'b'])
result = idf1.join([idf2, idf3], how='outer')
df_partially_merged = merge(df1, df2, on=['a', 'b'], how='outer')
expected = merge(df_partially_merged, df3, on=['a', 'b'], how='outer')
result = result.reset_index()
expected = expected[result.columns]
expected['a'] = expected.a.astype('int64')
expected['b'] = expected.b.astype('int64')
assert_frame_equal(result, expected)
df1 = DataFrame({'a': [1, 1, 1], 'b': [1, 1, 1], 'c': [10, 20, 30]})
df2 = DataFrame({'a': [1, 1, 1], 'b': [1, 1, 2], 'd': [100, 200, 300]})
df3 = DataFrame({'a': [1, 1, 1], 'b': [1, 1, 2], 'e': [1000, 2000, 3000]})
idf1 = df1.set_index(['a', 'b'])
idf2 = df2.set_index(['a', 'b'])
idf3 = df3.set_index(['a', 'b'])
result = idf1.join([idf2, idf3], how='inner')
df_partially_merged = merge(df1, df2, on=['a', 'b'], how='inner')
expected = merge(df_partially_merged, df3, on=['a', 'b'], how='inner')
result = result.reset_index()
assert_frame_equal(result, expected.ix[:, result.columns])
df = DataFrame({'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C': numpy.random.randn(8), 'D': numpy.random.randn(8)})
tempResult = arange(8)
	
===================================================================	
TestJoin.test_join_index_mixed: 205	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestJoin.test_join_index_mixed: 208	
----------------------------	

df1 = DataFrame({'A': 1.0, 'B': 2, 'C': 'foo', 'D': True}, index=numpy.arange(10), columns=['A', 'B', 'C', 'D'])
self.assertEqual(df1['B'].dtype, numpy.int64)
self.assertEqual(df1['D'].dtype, numpy.bool_)
tempResult = arange(0, 10, 2)
	
===================================================================	
TestJoin.test_join_index_mixed: 215	
----------------------------	

df1 = DataFrame({'A': 1.0, 'B': 2, 'C': 'foo', 'D': True}, index=numpy.arange(10), columns=['A', 'B', 'C', 'D'])
self.assertEqual(df1['B'].dtype, numpy.int64)
self.assertEqual(df1['D'].dtype, numpy.bool_)
df2 = DataFrame({'A': 1.0, 'B': 2, 'C': 'foo', 'D': True}, index=numpy.arange(0, 10, 2), columns=['A', 'B', 'C', 'D'])
joined = df1.join(df2, lsuffix='_one', rsuffix='_two')
expected_columns = ['A_one', 'B_one', 'C_one', 'D_one', 'A_two', 'B_two', 'C_two', 'D_two']
df1.columns = expected_columns[:4]
df2.columns = expected_columns[4:]
expected = _join_by_hand(df1, df2)
assert_frame_equal(joined, expected)
tempResult = arange(10)
	
===================================================================	
TestJoin.test_join_index_mixed: 218	
----------------------------	

df1 = DataFrame({'A': 1.0, 'B': 2, 'C': 'foo', 'D': True}, index=numpy.arange(10), columns=['A', 'B', 'C', 'D'])
self.assertEqual(df1['B'].dtype, numpy.int64)
self.assertEqual(df1['D'].dtype, numpy.bool_)
df2 = DataFrame({'A': 1.0, 'B': 2, 'C': 'foo', 'D': True}, index=numpy.arange(0, 10, 2), columns=['A', 'B', 'C', 'D'])
joined = df1.join(df2, lsuffix='_one', rsuffix='_two')
expected_columns = ['A_one', 'B_one', 'C_one', 'D_one', 'A_two', 'B_two', 'C_two', 'D_two']
df1.columns = expected_columns[:4]
df2.columns = expected_columns[4:]
expected = _join_by_hand(df1, df2)
assert_frame_equal(joined, expected)
df1 = DataFrame(index=numpy.arange(10))
df1['bool'] = True
df1['string'] = 'foo'
tempResult = arange(5, 15)
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 616	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
tempResult = arange(len(right))
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 625	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
right.index = numpy.arange(len(right))
right['right'] *= (- 1)
out = merge(left, right, how='outer')
self.assertEqual(len(out), len(left))
assert_series_equal(out['left'], (- out['right']), check_names=False)
result = out.iloc[:, :(- 2)].sum(axis=1)
assert_series_equal(out['left'], result, check_names=False)
self.assertTrue((result.name is None))
out.sort_values(out.columns.tolist(), inplace=True)
tempResult = arange(len(out))
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 644	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
right.index = numpy.arange(len(right))
right['right'] *= (- 1)
out = merge(left, right, how='outer')
self.assertEqual(len(out), len(left))
assert_series_equal(out['left'], (- out['right']), check_names=False)
result = out.iloc[:, :(- 2)].sum(axis=1)
assert_series_equal(out['left'], result, check_names=False)
self.assertTrue((result.name is None))
out.sort_values(out.columns.tolist(), inplace=True)
out.index = numpy.arange(len(out))
for how in ['left', 'right', 'outer', 'inner']:
    assert_frame_equal(out, merge(left, right, how=how, sort=True))
out = merge(left, right, how='left', sort=False)
assert_frame_equal(left, out[left.columns.tolist()])
out = merge(right, left, how='left', sort=False)
assert_frame_equal(right, out[right.columns.tolist()])
n = (1 << 11)
left = DataFrame(np.random.randint(low, high, (n, 7)).astype('int64'), columns=list('ABCDEFG'))
shape = left.apply(Series.nunique).values
self.assertTrue(_int64_overflow_possible(shape))
left = concat([left, left], ignore_index=True)
right = DataFrame(np.random.randint(low, high, ((n // 2), 7)).astype('int64'), columns=list('ABCDEFG'))
i = numpy.random.choice(len(left), n)
right = concat([right, right, left.iloc[i]], ignore_index=True)
left['left'] = numpy.random.randn(len(left))
right['right'] = numpy.random.randn(len(right))
i = numpy.random.permutation(len(left))
left = left.iloc[i].copy()
tempResult = arange(len(left))
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 647	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
right.index = numpy.arange(len(right))
right['right'] *= (- 1)
out = merge(left, right, how='outer')
self.assertEqual(len(out), len(left))
assert_series_equal(out['left'], (- out['right']), check_names=False)
result = out.iloc[:, :(- 2)].sum(axis=1)
assert_series_equal(out['left'], result, check_names=False)
self.assertTrue((result.name is None))
out.sort_values(out.columns.tolist(), inplace=True)
out.index = numpy.arange(len(out))
for how in ['left', 'right', 'outer', 'inner']:
    assert_frame_equal(out, merge(left, right, how=how, sort=True))
out = merge(left, right, how='left', sort=False)
assert_frame_equal(left, out[left.columns.tolist()])
out = merge(right, left, how='left', sort=False)
assert_frame_equal(right, out[right.columns.tolist()])
n = (1 << 11)
left = DataFrame(np.random.randint(low, high, (n, 7)).astype('int64'), columns=list('ABCDEFG'))
shape = left.apply(Series.nunique).values
self.assertTrue(_int64_overflow_possible(shape))
left = concat([left, left], ignore_index=True)
right = DataFrame(np.random.randint(low, high, ((n // 2), 7)).astype('int64'), columns=list('ABCDEFG'))
i = numpy.random.choice(len(left), n)
right = concat([right, right, left.iloc[i]], ignore_index=True)
left['left'] = numpy.random.randn(len(left))
right['right'] = numpy.random.randn(len(right))
i = numpy.random.permutation(len(left))
left = left.iloc[i].copy()
left.index = numpy.arange(len(left))
i = numpy.random.permutation(len(right))
right = right.iloc[i].copy()
tempResult = arange(len(right))
	
===================================================================	
TestMergeMulti.run_asserts: 474	
----------------------------	

for sort in [False, True]:
    res = left.join(right, on=icols, how='left', sort=sort)
    self.assertTrue((len(left) < (len(res) + 1)))
    self.assertFalse(res['4th'].isnull().any())
    self.assertFalse(res['5th'].isnull().any())
    pandas.util.testing.assert_series_equal(res['4th'], (- res['5th']), check_names=False)
    result = bind_cols(res.iloc[:, :(- 2)])
    pandas.util.testing.assert_series_equal(res['4th'], result, check_names=False)
    self.assertTrue((result.name is None))
    if sort:
        pandas.util.testing.assert_frame_equal(res, res.sort_values(icols, kind='mergesort'))
    out = merge(left, right.reset_index(), on=icols, sort=sort, how='left')
    tempResult = arange(len(res))
	
===================================================================	
TestAsOfMerge.test_tolerance_tz: 217	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestAsOfMerge.test_tolerance_tz: 220	
----------------------------	

left = pandas.DataFrame({'date': pandas.DatetimeIndex(start=pandas.to_datetime('2016-01-02'), freq='D', periods=5, tz=pytz.timezone('UTC')), 'value1': numpy.arange(5)})
right = pandas.DataFrame({'date': pandas.DatetimeIndex(start=pandas.to_datetime('2016-01-01'), freq='D', periods=5, tz=pytz.timezone('UTC')), 'value2': list('ABCDE')})
result = pandas.merge_asof(left, right, on='date', tolerance=pandas.Timedelta('1 day'))
tempResult = arange(5)
	
===================================================================	
TestPivotTable.test_pivot_with_tz: 137	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_with_tz: 137	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_index_with_nan: 126	
----------------------------	

nan = numpy.nan
df = DataFrame({'a': ['R1', 'R2', nan, 'R4'], 'b': ['C1', 'C2', 'C3', 'C4'], 'c': [10, 15, 17, 20]})
result = df.pivot('a', 'b', 'c')
expected = DataFrame([[nan, nan, 17, nan], [10, nan, nan, nan], [nan, 15, nan, nan], [nan, nan, nan, 20]], index=Index([nan, 'R1', 'R2', 'R4'], name='a'), columns=Index(['C1', 'C2', 'C3', 'C4'], name='b'))
pandas.util.testing.assert_frame_equal(result, expected)
pandas.util.testing.assert_frame_equal(df.pivot('b', 'a', 'c'), expected.T)
tempResult = arange(6)
	
===================================================================	
TestPivotTable.test_pivot_dtaccessor: 342	
----------------------------	

dates1 = ['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00']
dates2 = ['2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00']
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_datetime_tz: 321	
----------------------------	

dates1 = ['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00']
dates2 = ['2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00']
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestPivotTable.test_categorical_margins: 380	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestPivotTable.test_categorical_margins: 380	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestPivotTable.test_categorical_margins: 380	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestPivotTable.test_pivot_periods: 151	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_periods: 151	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestCut.test_series_retbins: 206	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestCut.test_qcut_include_lowest: 155	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestCut.test_label_precision: 87	
----------------------------	

tempResult = arange(0, 0.73, 0.01)
	
===================================================================	
TestCut.test_na_handling: 93	
----------------------------	

tempResult = arange(0, 0.75, 0.01)
	
===================================================================	
TestCut.test_label_formatting: 168	
----------------------------	

self.assertEqual(pandas.tools.tile._trim_zeros('1.000'), '1')
tempResult = arange(11.0)
	
===================================================================	
TestCut.test_label_formatting: 169	
----------------------------	

self.assertEqual(pandas.tools.tile._trim_zeros('1.000'), '1')
result = cut(numpy.arange(11.0), 2)
tempResult = arange(11.0)
	
===================================================================	
TestCut.test_labels: 73	
----------------------------	

tempResult = arange(0, 1.01, 0.1)
	
===================================================================	
TestCut.test_inf_handling: 104	
----------------------------	

tempResult = arange(6)
	
===================================================================	
_monthly_finder: 536	
----------------------------	

periodsperyear = 12
vmin_orig = vmin
(vmin, vmax) = (int(vmin), int(vmax))
span = ((vmax - vmin) + 1)
info = numpy.zeros(span, dtype=[('val', int), ('maj', bool), ('min', bool), ('fmt', '|S8')])
tempResult = arange(vmin, (vmax + 1))
	
===================================================================	
_quarterly_finder: 587	
----------------------------	

periodsperyear = 4
vmin_orig = vmin
(vmin, vmax) = (int(vmin), int(vmax))
span = ((vmax - vmin) + 1)
info = numpy.zeros(span, dtype=[('val', int), ('maj', bool), ('min', bool), ('fmt', '|S8')])
tempResult = arange(vmin, (vmax + 1))
	
===================================================================	
_annual_finder: 622	
----------------------------	

(vmin, vmax) = (int(vmin), int((vmax + 1)))
span = ((vmax - vmin) + 1)
info = numpy.zeros(span, dtype=[('val', int), ('maj', bool), ('min', bool), ('fmt', '|S8')])
tempResult = arange(vmin, (vmax + 1))
	
===================================================================	
DatetimeIndex._local_timestamps: 343	
----------------------------	

utc = _utc()
if self.is_monotonic:
    return pandas.tslib.tz_convert(self.asi8, utc, self.tz)
else:
    values = self.asi8
    indexer = values.argsort()
    result = pandas.tslib.tz_convert(values.take(indexer), utc, self.tz)
    n = len(indexer)
    reverse = numpy.empty(n, dtype=numpy.int_)
    tempResult = arange(n)
	
===================================================================	
_generate_regular_range: 1167	
----------------------------	

if isinstance(offset, Tick):
    stride = offset.nanos
    if (periods is None):
        b = Timestamp(start).value
        e = (((b + (((Timestamp(end).value - b) // stride) * stride)) + (stride // 2)) + 1)
        tz = start.tz
    elif (start is not None):
        b = Timestamp(start).value
        e = (b + (numpy.int64(periods) * stride))
        tz = start.tz
    elif (end is not None):
        e = (Timestamp(end).value + stride)
        b = (e - (numpy.int64(periods) * stride))
        tz = end.tz
    else:
        raise ValueError("at least 'start' or 'end' should be specified if a 'period' is given.")
    tempResult = arange(b, e, stride, dtype=numpy.int64)
	
===================================================================	
PeriodIndex.asof_locs: 290	
----------------------------	

'\n        where : array of timestamps\n        mask : array of booleans where data is not NA\n\n        '
where_idx = where
if isinstance(where_idx, DatetimeIndex):
    where_idx = PeriodIndex(where_idx.values, freq=self.freq)
locs = self._values[mask].searchsorted(where_idx._values, side='right')
locs = numpy.where((locs > 0), (locs - 1), 0)
tempResult = arange(len(self))
	
===================================================================	
_get_ordinal_range: 698	
----------------------------	

if (pandas.core.common._count_not_none(start, end, periods) < 2):
    raise ValueError('Must specify 2 of start, end, periods')
if (freq is not None):
    (_, mult) = _gfc(freq)
if (start is not None):
    start = Period(start, freq)
if (end is not None):
    end = Period(end, freq)
is_start_per = isinstance(start, Period)
is_end_per = isinstance(end, Period)
if (is_start_per and is_end_per and (start.freq != end.freq)):
    raise ValueError('Start and end must have same freq')
if ((start is pandas.tslib.NaT) or (end is pandas.tslib.NaT)):
    raise ValueError('Start and end must not be NaT')
if (freq is None):
    if is_start_per:
        freq = start.freq
    elif is_end_per:
        freq = end.freq
    else:
        raise ValueError('Could not infer freq from start/end')
if (periods is not None):
    periods = (periods * mult)
    if (start is None):
        tempResult = arange(((end.ordinal - periods) + mult), (end.ordinal + 1), mult, dtype=numpy.int64)
	
===================================================================	
_get_ordinal_range: 700	
----------------------------	

if (pandas.core.common._count_not_none(start, end, periods) < 2):
    raise ValueError('Must specify 2 of start, end, periods')
if (freq is not None):
    (_, mult) = _gfc(freq)
if (start is not None):
    start = Period(start, freq)
if (end is not None):
    end = Period(end, freq)
is_start_per = isinstance(start, Period)
is_end_per = isinstance(end, Period)
if (is_start_per and is_end_per and (start.freq != end.freq)):
    raise ValueError('Start and end must have same freq')
if ((start is pandas.tslib.NaT) or (end is pandas.tslib.NaT)):
    raise ValueError('Start and end must not be NaT')
if (freq is None):
    if is_start_per:
        freq = start.freq
    elif is_end_per:
        freq = end.freq
    else:
        raise ValueError('Could not infer freq from start/end')
if (periods is not None):
    periods = (periods * mult)
    if (start is None):
        data = numpy.arange(((end.ordinal - periods) + mult), (end.ordinal + 1), mult, dtype=numpy.int64)
    else:
        tempResult = arange(start.ordinal, (start.ordinal + periods), mult, dtype=numpy.int64)
	
===================================================================	
_get_ordinal_range: 702	
----------------------------	

if (pandas.core.common._count_not_none(start, end, periods) < 2):
    raise ValueError('Must specify 2 of start, end, periods')
if (freq is not None):
    (_, mult) = _gfc(freq)
if (start is not None):
    start = Period(start, freq)
if (end is not None):
    end = Period(end, freq)
is_start_per = isinstance(start, Period)
is_end_per = isinstance(end, Period)
if (is_start_per and is_end_per and (start.freq != end.freq)):
    raise ValueError('Start and end must have same freq')
if ((start is pandas.tslib.NaT) or (end is pandas.tslib.NaT)):
    raise ValueError('Start and end must not be NaT')
if (freq is None):
    if is_start_per:
        freq = start.freq
    elif is_end_per:
        freq = end.freq
    else:
        raise ValueError('Could not infer freq from start/end')
if (periods is not None):
    periods = (periods * mult)
    if (start is None):
        data = numpy.arange(((end.ordinal - periods) + mult), (end.ordinal + 1), mult, dtype=numpy.int64)
    else:
        data = numpy.arange(start.ordinal, (start.ordinal + periods), mult, dtype=numpy.int64)
else:
    tempResult = arange(start.ordinal, (end.ordinal + 1), mult, dtype=numpy.int64)
	
===================================================================	
PeriodIndexResampler._downsample: 459	
----------------------------	

'\n        Downsample the cython defined function\n\n        Parameters\n        ----------\n        how : string / cython mapped function\n        **kwargs : kw args passed to how function\n        '
if (self.kind == 'timestamp'):
    return super(PeriodIndexResampler, self)._downsample(how, **kwargs)
how = (self._is_cython_func(how) or how)
ax = self.ax
new_index = self._get_new_index()
memb = ax.asfreq(self.freq, how=self.convention)
if is_subperiod(ax.freq, self.freq):
    if (len(new_index) == 0):
        bins = []
    else:
        i8 = memb.asi8
        tempResult = arange(i8[0], (i8[(- 1)] + 1))
	
===================================================================	
_generate_regular_range: 609	
----------------------------	

stride = offset.nanos
if (periods is None):
    b = Timedelta(start).value
    e = Timedelta(end).value
    e += (stride - (e % stride))
elif (start is not None):
    b = Timedelta(start).value
    e = (b + (periods * stride))
elif (end is not None):
    e = (Timedelta(end).value + stride)
    b = (e - (periods * stride))
else:
    raise ValueError("at least 'start' or 'end' should be specified if a 'period' is given.")
tempResult = arange(b, e, stride, dtype=numpy.int64)
	
===================================================================	
TestDatetimeIndexOps.test_drop_duplicates: 468	
----------------------------	

base = pandas.date_range('2011-01-01', '2011-01-31', freq='D', name='idx')
idx = base.append(base[:5])
res = idx.drop_duplicates()
pandas.util.testing.assert_index_equal(res, base)
res = Series(idx).drop_duplicates()
pandas.util.testing.assert_series_equal(res, Series(base))
res = idx.drop_duplicates(keep='last')
exp = base[5:].append(base[:5])
pandas.util.testing.assert_index_equal(res, exp)
res = Series(idx).drop_duplicates(keep='last')
tempResult = arange(5, 36)
	
===================================================================	
TestDatetimeIndexOps.test_drop_duplicates: 472	
----------------------------	

base = pandas.date_range('2011-01-01', '2011-01-31', freq='D', name='idx')
idx = base.append(base[:5])
res = idx.drop_duplicates()
pandas.util.testing.assert_index_equal(res, base)
res = Series(idx).drop_duplicates()
pandas.util.testing.assert_series_equal(res, Series(base))
res = idx.drop_duplicates(keep='last')
exp = base[5:].append(base[:5])
pandas.util.testing.assert_index_equal(res, exp)
res = Series(idx).drop_duplicates(keep='last')
pandas.util.testing.assert_series_equal(res, Series(exp, index=numpy.arange(5, 36)))
res = idx.drop_duplicates(keep=False)
pandas.util.testing.assert_index_equal(res, base[5:])
res = Series(idx).drop_duplicates(keep=False)
tempResult = arange(5, 31)
	
===================================================================	
TestTimedeltaIndexOps.test_ops_compat: 753	
----------------------------	

offsets = [pandas.offsets.Hour(2), timedelta(hours=2), numpy.timedelta64(2, 'h'), Timedelta(hours=2)]
rng = timedelta_range('1 days', '10 days', name='foo')
for offset in offsets:
    self.assertRaises(TypeError, (lambda : (rng * offset)))
tempResult = arange(10)
	
===================================================================	
TestTimedeltaIndexOps.test_drop_duplicates: 1034	
----------------------------	

base = pandas.timedelta_range('1 day', '31 day', freq='D', name='idx')
idx = base.append(base[:5])
res = idx.drop_duplicates()
pandas.util.testing.assert_index_equal(res, base)
res = Series(idx).drop_duplicates()
pandas.util.testing.assert_series_equal(res, Series(base))
res = idx.drop_duplicates(keep='last')
exp = base[5:].append(base[:5])
pandas.util.testing.assert_index_equal(res, exp)
res = Series(idx).drop_duplicates(keep='last')
tempResult = arange(5, 36)
	
===================================================================	
TestTimedeltaIndexOps.test_drop_duplicates: 1038	
----------------------------	

base = pandas.timedelta_range('1 day', '31 day', freq='D', name='idx')
idx = base.append(base[:5])
res = idx.drop_duplicates()
pandas.util.testing.assert_index_equal(res, base)
res = Series(idx).drop_duplicates()
pandas.util.testing.assert_series_equal(res, Series(base))
res = idx.drop_duplicates(keep='last')
exp = base[5:].append(base[:5])
pandas.util.testing.assert_index_equal(res, exp)
res = Series(idx).drop_duplicates(keep='last')
pandas.util.testing.assert_series_equal(res, Series(exp, index=numpy.arange(5, 36)))
res = idx.drop_duplicates(keep=False)
pandas.util.testing.assert_index_equal(res, base[5:])
res = Series(idx).drop_duplicates(keep=False)
tempResult = arange(5, 31)
	
===================================================================	
TestPeriodIndexOps.test_drop_duplicates: 1546	
----------------------------	

base = pandas.period_range('2011-01-01', '2011-01-31', freq='D', name='idx')
idx = base.append(base[:5])
res = idx.drop_duplicates()
pandas.util.testing.assert_index_equal(res, base)
res = Series(idx).drop_duplicates()
pandas.util.testing.assert_series_equal(res, Series(base))
res = idx.drop_duplicates(keep='last')
exp = base[5:].append(base[:5])
pandas.util.testing.assert_index_equal(res, exp)
res = Series(idx).drop_duplicates(keep='last')
tempResult = arange(5, 36)
	
===================================================================	
TestPeriodIndexOps.test_drop_duplicates: 1550	
----------------------------	

base = pandas.period_range('2011-01-01', '2011-01-31', freq='D', name='idx')
idx = base.append(base[:5])
res = idx.drop_duplicates()
pandas.util.testing.assert_index_equal(res, base)
res = Series(idx).drop_duplicates()
pandas.util.testing.assert_series_equal(res, Series(base))
res = idx.drop_duplicates(keep='last')
exp = base[5:].append(base[:5])
pandas.util.testing.assert_index_equal(res, exp)
res = Series(idx).drop_duplicates(keep='last')
pandas.util.testing.assert_series_equal(res, Series(exp, index=numpy.arange(5, 36)))
res = idx.drop_duplicates(keep=False)
pandas.util.testing.assert_index_equal(res, base[5:])
res = Series(idx).drop_duplicates(keep=False)
tempResult = arange(5, 31)
	
===================================================================	
_check: 69	
----------------------------	

obj = numpy.array(numpy.random.randn(20), dtype=dtype)
bins = numpy.array([6, 12, 20])
out = numpy.zeros((3, 4), dtype)
counts = numpy.zeros(len(out), dtype=numpy.int64)
tempResult = arange(3)
	
===================================================================	
TestReducer.test_int_index: 95	
----------------------------	

from pandas.core.series import Series
arr = numpy.random.randn(100, 4)
tempResult = arange(4)
	
===================================================================	
TestReducer.test_int_index: 98	
----------------------------	

from pandas.core.series import Series
arr = numpy.random.randn(100, 4)
result = pandas.lib.reduce(arr, numpy.sum, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
tempResult = arange(100)
	
===================================================================	
TestReducer.test_int_index: 101	
----------------------------	

from pandas.core.series import Series
arr = numpy.random.randn(100, 4)
result = pandas.lib.reduce(arr, numpy.sum, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
result = pandas.lib.reduce(arr, numpy.sum, axis=1, labels=Index(numpy.arange(100)))
expected = arr.sum(1)
assert_almost_equal(result, expected)
tempResult = arange(100)
	
===================================================================	
TestReducer.test_int_index: 102	
----------------------------	

from pandas.core.series import Series
arr = numpy.random.randn(100, 4)
result = pandas.lib.reduce(arr, numpy.sum, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
result = pandas.lib.reduce(arr, numpy.sum, axis=1, labels=Index(numpy.arange(100)))
expected = arr.sum(1)
assert_almost_equal(result, expected)
dummy = Series(0.0, index=numpy.arange(100))
tempResult = arange(4)
	
===================================================================	
TestReducer.test_int_index: 105	
----------------------------	

from pandas.core.series import Series
arr = numpy.random.randn(100, 4)
result = pandas.lib.reduce(arr, numpy.sum, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
result = pandas.lib.reduce(arr, numpy.sum, axis=1, labels=Index(numpy.arange(100)))
expected = arr.sum(1)
assert_almost_equal(result, expected)
dummy = Series(0.0, index=numpy.arange(100))
result = pandas.lib.reduce(arr, numpy.sum, dummy=dummy, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
tempResult = arange(4)
	
===================================================================	
TestReducer.test_int_index: 106	
----------------------------	

from pandas.core.series import Series
arr = numpy.random.randn(100, 4)
result = pandas.lib.reduce(arr, numpy.sum, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
result = pandas.lib.reduce(arr, numpy.sum, axis=1, labels=Index(numpy.arange(100)))
expected = arr.sum(1)
assert_almost_equal(result, expected)
dummy = Series(0.0, index=numpy.arange(100))
result = pandas.lib.reduce(arr, numpy.sum, dummy=dummy, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
dummy = Series(0.0, index=numpy.arange(4))
tempResult = arange(100)
	
===================================================================	
TestReducer.test_int_index: 109	
----------------------------	

from pandas.core.series import Series
arr = numpy.random.randn(100, 4)
result = pandas.lib.reduce(arr, numpy.sum, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
result = pandas.lib.reduce(arr, numpy.sum, axis=1, labels=Index(numpy.arange(100)))
expected = arr.sum(1)
assert_almost_equal(result, expected)
dummy = Series(0.0, index=numpy.arange(100))
result = pandas.lib.reduce(arr, numpy.sum, dummy=dummy, labels=Index(numpy.arange(4)))
expected = arr.sum(0)
assert_almost_equal(result, expected)
dummy = Series(0.0, index=numpy.arange(4))
result = pandas.lib.reduce(arr, numpy.sum, axis=1, dummy=dummy, labels=Index(numpy.arange(100)))
expected = arr.sum(1)
assert_almost_equal(result, expected)
tempResult = arange(100)
	
===================================================================	
TestFrequencyInference.test_series: 507	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestFrequencyInference.test_series: 507	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestPeriodIndex.test_slice_with_zero_step_raises: 1653	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestPeriodRepresentation._check_freq: 3304	
----------------------------	

rng = PeriodIndex(start=base_date, periods=10, freq=freq)
tempResult = arange(10, dtype=numpy.int64)
	
===================================================================	
TestPeriodIndex.test_constructor_arrays_negative_year: 1296	
----------------------------	

tempResult = arange(1960, 2000, dtype=numpy.int64)
	
===================================================================	
TestPeriodIndex.test_to_timestamp_quarterly_bug: 1726	
----------------------------	

tempResult = arange(1960, 2000)
	
===================================================================	
TestPeriodIndex.test_constructor_field_arrays: 1267	
----------------------------	

tempResult = arange(1990, 2010)
	
===================================================================	
TestPeriodIndex.test_constructor_field_arrays: 1268	
----------------------------	

years = np.arange(1990, 2010).repeat(4)[2:(- 2)]
tempResult = arange(1, 5)
	
===================================================================	
TestPeriodIndex.test_slice_with_negative_step: 1635	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestPeriodIndex.test_constructor_datetime64arr: 1343	
----------------------------	

tempResult = arange(100000, (100000 + 10000), 100, dtype=numpy.int64)
	
===================================================================	
TestDatetimeIndex.test_resample_consistency: 1116	
----------------------------	

i30 = pandas.date_range('2002-02-02', periods=4, freq='30T')
tempResult = arange(4.0)
	
===================================================================	
TestDatetimeIndex.test_resample_timegrouper: 1133	
----------------------------	

dates1 = [datetime(2014, 10, 1), datetime(2014, 9, 3), datetime(2014, 11, 5), datetime(2014, 9, 5), datetime(2014, 10, 8), datetime(2014, 7, 15)]
dates2 = ((((dates1[:2] + [pandas.NaT]) + dates1[2:4]) + [pandas.NaT]) + dates1[4:])
dates3 = (([pandas.NaT] + dates1) + [pandas.NaT])
for dates in [dates1, dates2, dates3]:
    tempResult = arange(len(dates))
	
===================================================================	
TestDatetimeIndex.test_resample_timegrouper: 1140	
----------------------------	

dates1 = [datetime(2014, 10, 1), datetime(2014, 9, 3), datetime(2014, 11, 5), datetime(2014, 9, 5), datetime(2014, 10, 8), datetime(2014, 7, 15)]
dates2 = ((((dates1[:2] + [pandas.NaT]) + dates1[2:4]) + [pandas.NaT]) + dates1[4:])
dates3 = (([pandas.NaT] + dates1) + [pandas.NaT])
for dates in [dates1, dates2, dates3]:
    df = DataFrame(dict(A=dates, B=numpy.arange(len(dates))))
    result = df.set_index('A').resample('M').count()
    exp_idx = pandas.DatetimeIndex(['2014-07-31', '2014-08-31', '2014-09-30', '2014-10-31', '2014-11-30'], freq='M', name='A')
    expected = DataFrame({'B': [1, 0, 2, 2, 1]}, index=exp_idx)
    assert_frame_equal(result, expected)
    result = df.groupby(pd.Grouper(freq='M', key='A')).count()
    assert_frame_equal(result, expected)
    tempResult = arange(len(dates))
	
===================================================================	
TestDatetimeIndex.test_resample_timegrouper: 1140	
----------------------------	

dates1 = [datetime(2014, 10, 1), datetime(2014, 9, 3), datetime(2014, 11, 5), datetime(2014, 9, 5), datetime(2014, 10, 8), datetime(2014, 7, 15)]
dates2 = ((((dates1[:2] + [pandas.NaT]) + dates1[2:4]) + [pandas.NaT]) + dates1[4:])
dates3 = (([pandas.NaT] + dates1) + [pandas.NaT])
for dates in [dates1, dates2, dates3]:
    df = DataFrame(dict(A=dates, B=numpy.arange(len(dates))))
    result = df.set_index('A').resample('M').count()
    exp_idx = pandas.DatetimeIndex(['2014-07-31', '2014-08-31', '2014-09-30', '2014-10-31', '2014-11-30'], freq='M', name='A')
    expected = DataFrame({'B': [1, 0, 2, 2, 1]}, index=exp_idx)
    assert_frame_equal(result, expected)
    result = df.groupby(pd.Grouper(freq='M', key='A')).count()
    assert_frame_equal(result, expected)
    tempResult = arange(len(dates))
	
===================================================================	
TestPeriodIndex.test_asfreq_downsample: 1259	
----------------------------	

s = self.create_series()
tempResult = arange(0, len(s.index), 2)
	
===================================================================	
TestPeriodIndex.test_asfreq_downsample: 1265	
----------------------------	

s = self.create_series()
expected = s.reindex(s.index.take(numpy.arange(0, len(s.index), 2)))
expected.index = expected.index.to_timestamp()
expected.index.freq = to_offset('2D')
result = s.resample('2D').asfreq()
assert_series_equal(result, expected)
frame = s.to_frame('value')
tempResult = arange(0, len(frame.index), 2)
	
===================================================================	
TestPeriodIndex.test_quarterly_resampling: 1584	
----------------------------	

rng = period_range('2000Q1', periods=10, freq='Q-DEC')
tempResult = arange(10)
	
===================================================================	
TestResampleAPI.test_fillna: 216	
----------------------------	

rng = pandas.date_range('1/1/2012', periods=10, freq='2S')
tempResult = arange(len(rng), dtype='int64')
	
===================================================================	
TestResampleAPI.setUp: 44	
----------------------------	

dti = DatetimeIndex(start=datetime(2005, 1, 1), end=datetime(2005, 1, 10), freq='Min')
self.series = Series(numpy.random.rand(len(dti)), dti)
tempResult = arange(len(dti))
	
===================================================================	
Base.test_asfreq_downsample: 393	
----------------------------	

s = self.create_series()
result = s.resample('2D').asfreq()
tempResult = arange(0, len(s.index), 2)
	
===================================================================	
Base.test_asfreq_downsample: 398	
----------------------------	

s = self.create_series()
result = s.resample('2D').asfreq()
expected = s.reindex(s.index.take(numpy.arange(0, len(s.index), 2)))
expected.index.freq = to_offset('2D')
assert_series_equal(result, expected)
frame = s.to_frame('value')
result = frame.resample('2D').asfreq()
tempResult = arange(0, len(frame.index), 2)
	
===================================================================	
TestTimedeltaIndex.create_series: 1649	
----------------------------	

i = timedelta_range('1 day', '10 day', freq='D')
tempResult = arange(len(i))
	
===================================================================	
TestDatetimeIndex.test_resample_upsampling_picked_but_not_correct: 676	
----------------------------	

dates = date_range('01-Jan-2014', '05-Jan-2014', freq='D')
series = Series(1, index=dates)
result = series.resample('D').mean()
self.assertEqual(result.index[0], dates[0])
import datetime
tempResult = arange(1.0, 6)
	
===================================================================	
TestDatetimeIndex.test_resample_upsampling_picked_but_not_correct: 677	
----------------------------	

dates = date_range('01-Jan-2014', '05-Jan-2014', freq='D')
series = Series(1, index=dates)
result = series.resample('D').mean()
self.assertEqual(result.index[0], dates[0])
import datetime
s = Series(numpy.arange(1.0, 6), index=[datetime.datetime(1975, 1, i, 12, 0) for i in range(1, 6)])
tempResult = arange(1.0, 6)
	
===================================================================	
TestDatetimeIndex.test_resample_ohlc_result: 785	
----------------------------	

index = pandas.date_range('1-1-2000', '2-15-2000', freq='h')
index = index.union(pandas.date_range('4-15-2000', '5-15-2000', freq='h'))
s = Series(range(len(index)), index=index)
a = s.loc[:'4-15-2000'].resample('30T').ohlc()
self.assertIsInstance(a, DataFrame)
b = s.loc[:'4-14-2000'].resample('30T').ohlc()
self.assertIsInstance(b, DataFrame)
rng = date_range('2013-12-30', '2014-01-07')
index = rng.drop([Timestamp('2014-01-01'), Timestamp('2013-12-31'), Timestamp('2014-01-04'), Timestamp('2014-01-05')])
tempResult = arange(len(index))
	
===================================================================	
TestSlicing.test_slice_with_zero_step_raises: 1237	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestTimedeltas.testit: 524	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestTimedeltas.testit: 525	
----------------------------	

result = to_timedelta(numpy.arange(5), unit=unit)
tempResult = arange(5)
	
===================================================================	
TestTimedeltas.test_timedelta_range: 360	
----------------------------	

tempResult = arange(5)
	
===================================================================	
TestTimedeltas.test_timedelta_range: 363	
----------------------------	

expected = to_timedelta(numpy.arange(5), unit='D')
result = timedelta_range('0 days', periods=5, freq='D')
pandas.util.testing.assert_index_equal(result, expected)
tempResult = arange(11)
	
===================================================================	
TestTimedeltas.test_timedelta_range: 366	
----------------------------	

expected = to_timedelta(numpy.arange(5), unit='D')
result = timedelta_range('0 days', periods=5, freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = to_timedelta(numpy.arange(11), unit='D')
result = timedelta_range('0 days', '10 days', freq='D')
pandas.util.testing.assert_index_equal(result, expected)
tempResult = arange(5)
	
===================================================================	
TestTimedeltas.test_timedelta_range: 372	
----------------------------	

expected = to_timedelta(numpy.arange(5), unit='D')
result = timedelta_range('0 days', periods=5, freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = to_timedelta(numpy.arange(11), unit='D')
result = timedelta_range('0 days', '10 days', freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = ((to_timedelta(numpy.arange(5), unit='D') + Second(2)) + Day())
result = timedelta_range('1 days, 00:00:02', '5 days, 00:00:02', freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = (to_timedelta([1, 3, 5, 7, 9], unit='D') + Second(2))
result = timedelta_range('1 days, 00:00:02', periods=5, freq='2D')
pandas.util.testing.assert_index_equal(result, expected)
tempResult = arange(50)
	
===================================================================	
TestTimedeltas.test_timedelta_range: 375	
----------------------------	

expected = to_timedelta(numpy.arange(5), unit='D')
result = timedelta_range('0 days', periods=5, freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = to_timedelta(numpy.arange(11), unit='D')
result = timedelta_range('0 days', '10 days', freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = ((to_timedelta(numpy.arange(5), unit='D') + Second(2)) + Day())
result = timedelta_range('1 days, 00:00:02', '5 days, 00:00:02', freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = (to_timedelta([1, 3, 5, 7, 9], unit='D') + Second(2))
result = timedelta_range('1 days, 00:00:02', periods=5, freq='2D')
pandas.util.testing.assert_index_equal(result, expected)
expected = (to_timedelta(numpy.arange(50), unit='T') * 30)
result = timedelta_range('0 days', freq='30T', periods=50)
pandas.util.testing.assert_index_equal(result, expected)
tempResult = arange(10)
	
===================================================================	
TestTimedeltas.test_timedelta_range: 376	
----------------------------	

expected = to_timedelta(numpy.arange(5), unit='D')
result = timedelta_range('0 days', periods=5, freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = to_timedelta(numpy.arange(11), unit='D')
result = timedelta_range('0 days', '10 days', freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = ((to_timedelta(numpy.arange(5), unit='D') + Second(2)) + Day())
result = timedelta_range('1 days, 00:00:02', '5 days, 00:00:02', freq='D')
pandas.util.testing.assert_index_equal(result, expected)
expected = (to_timedelta([1, 3, 5, 7, 9], unit='D') + Second(2))
result = timedelta_range('1 days, 00:00:02', periods=5, freq='2D')
pandas.util.testing.assert_index_equal(result, expected)
expected = (to_timedelta(numpy.arange(50), unit='T') * 30)
result = timedelta_range('0 days', freq='30T', periods=50)
pandas.util.testing.assert_index_equal(result, expected)
arr = np.arange(10).reshape(2, 5)
tempResult = arange(10)
	
===================================================================	
TestSlicing.test_partial_slice: 1192	
----------------------------	

rng = timedelta_range('1 day 10:11:12', freq='h', periods=500)
tempResult = arange(len(rng))
	
===================================================================	
TestTimedeltaIndex.test_union: 991	
----------------------------	

i1 = timedelta_range('1day', periods=5)
i2 = timedelta_range('3day', periods=5)
result = i1.union(i2)
expected = timedelta_range('1day', periods=7)
self.assert_index_equal(result, expected)
tempResult = arange(0, 20, 2)
	
===================================================================	
TestSlicing.test_partial_slice_high_reso: 1208	
----------------------------	

rng = timedelta_range('1 day 10:11:12', freq='us', periods=2000)
tempResult = arange(len(rng))
	
===================================================================	
TestSlicing.test_slice_with_negative_step: 1219	
----------------------------	

tempResult = arange(20)
	
===================================================================	
Get no callers of function numpy.arange at line 512 col 29.	
===================================================================	
Get no callers of function numpy.arange at line 515 col 30.	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_col_other_units: 537	
----------------------------	

n = 100
units = ['h', 'm', 's', 'ms', 'D', 'M', 'Y']
ns_dtype = numpy.dtype('M8[ns]')
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    tempResult = arange(n, dtype=numpy.int64)
	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_col_other_units: 538	
----------------------------	

n = 100
units = ['h', 'm', 's', 'ms', 'D', 'M', 'Y']
ns_dtype = numpy.dtype('M8[ns]')
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    vals = np.arange(n, dtype=np.int64).view(dtype)
    tempResult = arange(n)
	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_col_other_units: 538	
----------------------------	

n = 100
units = ['h', 'm', 's', 'ms', 'D', 'M', 'Y']
ns_dtype = numpy.dtype('M8[ns]')
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    vals = np.arange(n, dtype=np.int64).view(dtype)
    tempResult = arange(n)
	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_col_other_units: 543	
----------------------------	

n = 100
units = ['h', 'm', 's', 'ms', 'D', 'M', 'Y']
ns_dtype = numpy.dtype('M8[ns]')
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    vals = np.arange(n, dtype=np.int64).view(dtype)
    df = DataFrame({'ints': numpy.arange(n)}, index=numpy.arange(n))
    df[unit] = vals
    ex_vals = to_datetime(vals.astype('O')).values
    self.assertEqual(df[unit].dtype, ns_dtype)
    self.assertTrue((df[unit].values == ex_vals).all())
tempResult = arange(n)
	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_col_other_units: 543	
----------------------------	

n = 100
units = ['h', 'm', 's', 'ms', 'D', 'M', 'Y']
ns_dtype = numpy.dtype('M8[ns]')
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    vals = np.arange(n, dtype=np.int64).view(dtype)
    df = DataFrame({'ints': numpy.arange(n)}, index=numpy.arange(n))
    df[unit] = vals
    ex_vals = to_datetime(vals.astype('O')).values
    self.assertEqual(df[unit].dtype, ns_dtype)
    self.assertTrue((df[unit].values == ex_vals).all())
tempResult = arange(n)
	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_col_other_units: 544	
----------------------------	

n = 100
units = ['h', 'm', 's', 'ms', 'D', 'M', 'Y']
ns_dtype = numpy.dtype('M8[ns]')
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    vals = np.arange(n, dtype=np.int64).view(dtype)
    df = DataFrame({'ints': numpy.arange(n)}, index=numpy.arange(n))
    df[unit] = vals
    ex_vals = to_datetime(vals.astype('O')).values
    self.assertEqual(df[unit].dtype, ns_dtype)
    self.assertTrue((df[unit].values == ex_vals).all())
df = DataFrame({'ints': numpy.arange(n)}, index=numpy.arange(n))
tempResult = arange(n, dtype=numpy.int64)
	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_col_other_units: 547	
----------------------------	

n = 100
units = ['h', 'm', 's', 'ms', 'D', 'M', 'Y']
ns_dtype = numpy.dtype('M8[ns]')
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    vals = np.arange(n, dtype=np.int64).view(dtype)
    df = DataFrame({'ints': numpy.arange(n)}, index=numpy.arange(n))
    df[unit] = vals
    ex_vals = to_datetime(vals.astype('O')).values
    self.assertEqual(df[unit].dtype, ns_dtype)
    self.assertTrue((df[unit].values == ex_vals).all())
df = DataFrame({'ints': numpy.arange(n)}, index=numpy.arange(n))
df['dates'] = np.arange(n, dtype=np.int64).view(ns_dtype)
for unit in units:
    dtype = numpy.dtype(('M8[%s]' % unit))
    tempResult = arange(n, dtype=numpy.int64)
	
===================================================================	
TestSlicing.test_slice_month: 3332	
----------------------------	

dti = DatetimeIndex(freq='D', start=datetime(2005, 1, 1), periods=500)
tempResult = arange(len(dti))
	
===================================================================	
TestTimeSeries.test_frame_add_datetime64_column: 523	
----------------------------	

rng = date_range('1/1/2000 00:00:00', '1/1/2000 1:59:50', freq='10s')
tempResult = arange(len(rng))
	
===================================================================	
TestDatetimeIndex.test_ns_index: 2551	
----------------------------	

nsamples = 400
ns = int((1000000000.0 / 24414))
dtstart = numpy.datetime64('2012-09-20T00:00:00')
tempResult = arange(nsamples)
	
===================================================================	
TestDatetime64.test_fancy_setitem: 2763	
----------------------------	

dti = DatetimeIndex(freq='WOM-1FRI', start=datetime(2005, 1, 1), end=datetime(2010, 1, 1))
tempResult = arange(len(dti))
	
===================================================================	
TestDatetimeIndex.test_union: 2498	
----------------------------	

tempResult = arange(0, 20, 2)
	
===================================================================	
TestDatetimeIndex.test_union: 2499	
----------------------------	

i1 = Int64Index(numpy.arange(0, 20, 2))
tempResult = arange(10, 30, 2)
	
===================================================================	
TestDatetimeIndex.test_union: 2501	
----------------------------	

i1 = Int64Index(numpy.arange(0, 20, 2))
i2 = Int64Index(numpy.arange(10, 30, 2))
result = i1.union(i2)
tempResult = arange(0, 30, 2)
	
===================================================================	
TestSlicing.test_partial_slice_doesnt_require_monotonicity: 3546	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestTimeSeries.test_to_datetime_unit: 571	
----------------------------	

epoch = 1370745748
s = Series([(epoch + t) for t in range(20)])
result = to_datetime(s, unit='s')
expected = Series([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)])
assert_series_equal(result, expected)
s = Series([(epoch + t) for t in range(20)]).astype(float)
result = to_datetime(s, unit='s')
expected = Series([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)])
assert_series_equal(result, expected)
s = Series(([(epoch + t) for t in range(20)] + [iNaT]))
result = to_datetime(s, unit='s')
expected = Series(([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)] + [NaT]))
assert_series_equal(result, expected)
s = Series(([(epoch + t) for t in range(20)] + [iNaT])).astype(float)
result = to_datetime(s, unit='s')
expected = Series(([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)] + [NaT]))
assert_series_equal(result, expected)
tempResult = arange(0, 2, 0.25)
	
===================================================================	
TestTimeSeries.test_to_datetime_unit: 573	
----------------------------	

epoch = 1370745748
s = Series([(epoch + t) for t in range(20)])
result = to_datetime(s, unit='s')
expected = Series([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)])
assert_series_equal(result, expected)
s = Series([(epoch + t) for t in range(20)]).astype(float)
result = to_datetime(s, unit='s')
expected = Series([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)])
assert_series_equal(result, expected)
s = Series(([(epoch + t) for t in range(20)] + [iNaT]))
result = to_datetime(s, unit='s')
expected = Series(([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)] + [NaT]))
assert_series_equal(result, expected)
s = Series(([(epoch + t) for t in range(20)] + [iNaT])).astype(float)
result = to_datetime(s, unit='s')
expected = Series(([(Timestamp('2013-06-09 02:42:28') + timedelta(seconds=t)) for t in range(20)] + [NaT]))
assert_series_equal(result, expected)
s = Series(([(epoch + t) for t in np.arange(0, 2, 0.25)] + [iNaT])).astype(float)
result = to_datetime(s, unit='s')
tempResult = arange(0, 2, 0.25)
	
===================================================================	
TestSlicing.test_partial_slice_second_precision: 3383	
----------------------------	

rng = DatetimeIndex(start=datetime(2005, 1, 1, 0, 0, 59, microsecond=999990), periods=20, freq='US')
tempResult = arange(20)
	
===================================================================	
TestSlicing.test_slice_quarter: 3325	
----------------------------	

dti = DatetimeIndex(freq='D', start=datetime(2000, 6, 1), periods=500)
tempResult = arange(len(dti))
	
===================================================================	
TestTimeSeries.test_sparse_frame_fillna_limit: 492	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestDatetime64.test_slicing_datetimes: 2892	
----------------------------	

tempResult = arange(4.0, dtype='float64')
	
===================================================================	
TestDatetime64.test_slicing_datetimes: 2904	
----------------------------	

df = DataFrame(numpy.arange(4.0, dtype='float64'), index=[datetime(2001, 1, i, 10, 0) for i in [1, 2, 3, 4]])
result = df.ix[datetime(2001, 1, 1, 10):]
assert_frame_equal(result, df)
result = df.ix[:datetime(2001, 1, 4, 10)]
assert_frame_equal(result, df)
result = df.ix[datetime(2001, 1, 1, 10):datetime(2001, 1, 4, 10)]
assert_frame_equal(result, df)
result = df.ix[datetime(2001, 1, 1, 11):]
expected = df.iloc[1:]
assert_frame_equal(result, expected)
result = df.ix['20010101 11':]
assert_frame_equal(result, expected)
tempResult = arange(5.0, dtype='float64')
	
===================================================================	
TestTimeSeries.test_frame_fillna_limit: 418	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSlicing.test_partial_slice_daily: 3356	
----------------------------	

rng = DatetimeIndex(freq='H', start=datetime(2005, 1, 31), periods=500)
tempResult = arange(len(rng))
	
===================================================================	
TestDatetime64.test_fancy_getitem: 2752	
----------------------------	

dti = DatetimeIndex(freq='WOM-1FRI', start=datetime(2005, 1, 1), end=datetime(2010, 1, 1))
tempResult = arange(len(dti))
	
===================================================================	
TestTimeSeries.test_index_cast_datetime64_other_units: 599	
----------------------------	

tempResult = arange(0, 100, 10, dtype=numpy.int64)
	
===================================================================	
TestTimeSeries.test_series_fillna_limit: 392	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestDatetimeIndex.test_union_with_DatetimeIndex: 2505	
----------------------------	

tempResult = arange(0, 20, 2)
	
===================================================================	
TestDatetime64.test_nanosecond_field: 2742	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestDatetime64.test_nanosecond_field: 2743	
----------------------------	

dti = DatetimeIndex(numpy.arange(10))
tempResult = arange(10, dtype=numpy.int32)
	
===================================================================	
TestTimeSeries.test_groupby_count_dateparseerror: 1458	
----------------------------	

dr = date_range(start='1/1/2012', freq='5min', periods=10)
tempResult = arange(10)
	
===================================================================	
TestTimeSeries.test_groupby_count_dateparseerror: 1461	
----------------------------	

dr = date_range(start='1/1/2012', freq='5min', periods=10)
s = Series(numpy.arange(10), index=[dr, lrange(10)])
grouped = s.groupby((lambda x: ((x[1] % 2) == 0)))
result = grouped.count()
tempResult = arange(10)
	
===================================================================	
TestTimeSeries.test_sparse_series_fillna_limit: 440	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestTimeSeriesDuplicates.test_indexing_unordered: 162	
----------------------------	

rng = date_range(start='2011-01-01', end='2011-01-15')
ts = Series(randn(len(rng)), index=rng)
ts2 = concat([ts[0:4], ts[(- 4):], ts[4:(- 4)]])
for t in ts.index:
    s = str(t)
    expected = ts[t]
    result = ts2[t]
    self.assertTrue((expected == result))

def compare(slobj):
    result = ts2[slobj].copy()
    result = result.sort_index()
    expected = ts[slobj]
    assert_series_equal(result, expected)
compare(slice('2011-01-01', '2011-01-15'))
compare(slice('2010-12-30', '2011-01-15'))
compare(slice('2011-01-01', '2011-01-16'))
compare(slice('2011-01-01', '2011-01-6'))
compare(slice('2011-01-06', '2011-01-8'))
compare(slice('2011-01-06', '2011-01-12'))
result = ts2['2011'].sort_index()
expected = ts['2011']
assert_series_equal(result, expected)
rng = date_range(datetime(2005, 1, 1), periods=20, freq='M')
tempResult = arange(len(rng))
	
===================================================================	
TestTimeSeries.test_frame_pad_backfill_limit: 406	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestDatetimeIndex.test_slice_with_negative_step: 2621	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestTimeSeries.test_sparse_series_pad_backfill_limit: 458	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestDatetimeIndex.test_factorize_dst: 2612	
----------------------------	

idx = pandas.date_range('2016-11-06', freq='H', periods=12, tz='US/Eastern')
for obj in [idx, pandas.Series(idx)]:
    (arr, res) = obj.factorize()
    tempResult = arange(12, dtype=numpy.intp)
	
===================================================================	
TestDatetimeIndex.test_factorize_dst: 2617	
----------------------------	

idx = pandas.date_range('2016-11-06', freq='H', periods=12, tz='US/Eastern')
for obj in [idx, pandas.Series(idx)]:
    (arr, res) = obj.factorize()
    self.assert_numpy_array_equal(arr, numpy.arange(12, dtype=numpy.intp))
    pandas.util.testing.assert_index_equal(res, idx)
idx = pandas.date_range('2016-06-13', freq='H', periods=12, tz='US/Eastern')
for obj in [idx, pandas.Series(idx)]:
    (arr, res) = obj.factorize()
    tempResult = arange(12, dtype=numpy.intp)
	
===================================================================	
TestDatetimeIndex.test_factorize_tz: 2602	
----------------------------	

for tz in [None, 'UTC', 'US/Eastern', 'Asia/Tokyo']:
    base = pandas.date_range('2016-11-05', freq='H', periods=100, tz=tz)
    idx = base.repeat(5)
    tempResult = arange(100, dtype=numpy.intp)
	
===================================================================	
TestTimeSeries.test_constructor_int64_nocopy: 1475	
----------------------------	

tempResult = arange(1000, dtype=numpy.int64)
	
===================================================================	
TestTimeSeries.test_constructor_int64_nocopy: 1479	
----------------------------	

arr = numpy.arange(1000, dtype=numpy.int64)
index = DatetimeIndex(arr)
arr[50:100] = (- 1)
self.assertTrue((index.asi8[50:100] == (- 1)).all())
tempResult = arange(1000, dtype=numpy.int64)
	
===================================================================	
TestSlicing.test_partial_slice_hourly: 3363	
----------------------------	

rng = DatetimeIndex(freq='T', start=datetime(2005, 1, 1, 20, 0, 0), periods=500)
tempResult = arange(len(rng))
	
===================================================================	
TestTimeSeries.test_sparse_frame_pad_backfill_limit: 475	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestSlicing.test_partial_slice: 3340	
----------------------------	

rng = DatetimeIndex(freq='D', start=datetime(2005, 1, 1), periods=500)
tempResult = arange(len(rng))
	
===================================================================	
TestSlicing.test_partial_slice_minutely: 3373	
----------------------------	

rng = DatetimeIndex(freq='S', start=datetime(2005, 1, 1, 23, 59, 0), periods=500)
tempResult = arange(len(rng))
	
===================================================================	
TestTimeSeries.test_series_pad_backfill_limit: 380	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestDatetimeIndex.test_slice_with_zero_step_raises: 2639	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestSlicing.test_slice_year: 3310	
----------------------------	

dti = DatetimeIndex(freq='B', start=datetime(2005, 1, 1), periods=500)
tempResult = arange(len(dti))
	
===================================================================	
LegacySupport.test_unpickle_legacy_frame: 45	
----------------------------	

dtindex = DatetimeIndex(start='1/3/2005', end='1/14/2005', freq=BDay(1))
unpickled = self.frame
self.assertEqual(type(unpickled.index), DatetimeIndex)
self.assertEqual(len(unpickled), 10)
tempResult = arange(5)
	
===================================================================	
TestTimeZoneSupportPytz.test_field_access_localize: 249	
----------------------------	

strdates = ['1/1/2012', '3/1/2012', '4/1/2012']
rng = DatetimeIndex(strdates, tz=self.tzstr('US/Eastern'))
self.assertTrue((rng.hour == 0).all())
dr = date_range('2011-10-02 00:00', freq='h', periods=10, tz=self.tzstr('America/Atikokan'))
tempResult = arange(10, dtype=numpy.int32)
	
===================================================================	
getArangeMat: 776	
----------------------------	

tempResult = arange((N * K))
	
===================================================================	
TestGroupBy.test_multi_iter: 1150	
----------------------------	

tempResult = arange(6)
	
===================================================================	
TestGroupBy.test_filter_bad_shapes: 3440	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestGroupBy.test_filter_bad_shapes: 3440	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2427	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
tempResult = arange(1000)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2427	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
tempResult = arange(1000)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2427	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
tempResult = arange(500)
	
===================================================================	
TestGroupBy.test_int64_overflow: 2428	
----------------------------	

from pandas.core.groupby import _int64_overflow_possible
B = numpy.concatenate((numpy.arange(1000), numpy.arange(1000), numpy.arange(500)))
tempResult = arange(2500)
	
===================================================================	
TestGroupBy.test_groupby_complex: 1763	
----------------------------	

tempResult = arange(4)
	
===================================================================	
TestGroupBy.test_agg_compat: 1122	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestGroupBy.test_grouper_index_types: 244	
----------------------------	

tempResult = arange(10)
	
===================================================================	
TestGroupBy.test_multifunc_select_col_integer_cols: 1367	
----------------------------	

df = self.df
tempResult = arange(len(df.columns))
	
===================================================================	
test_decons: 4279	
----------------------------	

from pandas.core.groupby import decons_group_index, get_group_index

def testit(label_list, shape):
    group_index = get_group_index(label_list, shape, sort=True, xnull=True)
    label_list2 = decons_group_index(group_index, shape)
    for (a, b) in zip(label_list, label_list2):
        assert numpy.array_equal(a, b)
shape = (4, 5, 6)
label_list = [numpy.tile([0, 1, 2, 3, 0, 1, 2, 3], 100), numpy.tile([0, 2, 4, 3, 0, 1, 2, 3], 100), numpy.tile([5, 1, 0, 2, 3, 0, 5, 4], 100)]
testit(label_list, shape)
shape = (10000, 10000)
tempResult = arange(10000)
	
===================================================================	
test_decons: 4279	
----------------------------	

from pandas.core.groupby import decons_group_index, get_group_index

def testit(label_list, shape):
    group_index = get_group_index(label_list, shape, sort=True, xnull=True)
    label_list2 = decons_group_index(group_index, shape)
    for (a, b) in zip(label_list, label_list2):
        assert numpy.array_equal(a, b)
shape = (4, 5, 6)
label_list = [numpy.tile([0, 1, 2, 3, 0, 1, 2, 3], 100), numpy.tile([0, 2, 4, 3, 0, 1, 2, 3], 100), numpy.tile([5, 1, 0, 2, 3, 0, 5, 4], 100)]
testit(label_list, shape)
shape = (10000, 10000)
tempResult = arange(10000)
	
===================================================================	
TestDataFrameFormatting.test_to_html_unicode: 543	
----------------------------	

tempResult = arange(10.0)
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1184	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1184	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
tempResult = arange(w)
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1187	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('...' not in df._repr_html_())
(h, w) = ((max_rows + 1), (max_cols + 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_and_wide: 1187	
----------------------------	

max_cols = get_option('display.max_columns')
max_rows = get_option('display.max_rows')
(h, w) = ((max_rows - 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('...' not in df._repr_html_())
(h, w) = ((max_rows + 1), (max_cols + 1))
tempResult = arange(w)
	
===================================================================	
TestDataFrameFormatting.test_to_string: 892	
----------------------------	

from pandas import read_table
import re
biggie = DataFrame({'A': randn(200), 'B': pandas.util.testing.makeStringIndex(200)}, index=lrange(200))
biggie.loc[:20, 'A'] = nan
biggie.loc[:20, 'B'] = nan
s = biggie.to_string()
buf = StringIO()
retval = biggie.to_string(buf=buf)
self.assertIsNone(retval)
self.assertEqual(buf.getvalue(), s)
pandas.util.testing.assertIsInstance(s, pandas.compat.string_types)
result = biggie.to_string(columns=['B', 'A'], col_space=17, float_format='%.5f'.__mod__)
lines = result.split('\n')
header = lines[0].strip().split()
joined = '\n'.join([re.sub('\\s+', ' ', x).strip() for x in lines[1:]])
recons = read_table(StringIO(joined), names=header, header=None, sep=' ')
pandas.util.testing.assert_series_equal(recons['B'], biggie['B'])
self.assertEqual(recons['A'].count(), biggie['A'].count())
self.assertTrue((np.abs((recons['A'].dropna() - biggie['A'].dropna())) < 0.1).all())
result = biggie.to_string(columns=['A'], col_space=17)
header = result.split('\n')[0].strip().split()
expected = ['A']
self.assertEqual(header, expected)
biggie.to_string(columns=['B', 'A'], formatters={'A': (lambda x: ('%.1f' % x))})
biggie.to_string(columns=['B', 'A'], float_format=str)
biggie.to_string(columns=['B', 'A'], col_space=12, float_format=str)
tempResult = arange(200)
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1215	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1215	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
tempResult = arange(w)
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1220	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('&lt;class' not in df._repr_html_())
with option_context('display.large_repr', 'info'):
    assert ('&lt;class' in df._repr_html_())
(h, w) = ((max_rows - 1), (max_cols + 1))
tempResult = arange(1, (1 + h))
	
===================================================================	
TestDataFrameFormatting.test_info_repr_html: 1220	
----------------------------	

max_rows = get_option('display.max_rows')
max_cols = get_option('display.max_columns')
(h, w) = ((max_rows + 1), (max_cols - 1))
df = DataFrame(dict(((k, numpy.arange(1, (1 + h))) for k in numpy.arange(w))))
assert ('&lt;class' not in df._repr_html_())
with option_context('display.large_repr', 'info'):
    assert ('&lt;class' in df._repr_html_())
(h, w) = ((max_rows - 1), (max_cols + 1))
tempResult = arange(w)
	
===================================================================	
TestSeriesFormatting.test_period: 1714	
----------------------------	

index = pandas.period_range('2013-01', periods=6, freq='M')
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_multiindex: 1169	
----------------------------	

max_rows = get_option('display.max_rows')
max_L1 = (max_rows // 2)
tempResult = arange(max_L1)
	
===================================================================	
TestDataFrameFormatting.test_repr_html_long_multiindex: 1174	
----------------------------	

max_rows = get_option('display.max_rows')
max_L1 = (max_rows // 2)
tuples = list(itertools.product(numpy.arange(max_L1), ['foo', 'bar']))
idx = pandas.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = DataFrame(numpy.random.randn((max_L1 * 2), 2), index=idx, columns=['A', 'B'])
reg_repr = df._repr_html_()
assert ('...' not in reg_repr)
tempResult = arange((max_L1 + 1))
	
===================================================================	
TestIndexing.test_getitem_multiindex: 1610	
----------------------------	

index = MultiIndex(levels=[['D', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
arr = numpy.random.randn(len(index), 1)
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['D']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['A']
self.assertRaises(KeyError, f)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
index = MultiIndex(levels=[['A', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['A']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
idx = pandas.MultiIndex.from_product([['A', 'B', 'C'], ['foo', 'bar', 'baz']], names=['one', 'two'])
tempResult = arange(9, dtype='int64')
	
===================================================================	
TestIndexing.test_getitem_multiindex: 1612	
----------------------------	

index = MultiIndex(levels=[['D', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
arr = numpy.random.randn(len(index), 1)
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['D']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['A']
self.assertRaises(KeyError, f)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
index = MultiIndex(levels=[['A', 'B', 'C'], [0, 26, 27, 37, 57, 67, 75, 82]], labels=[[0, 0, 0, 1, 2, 2, 2, 2, 2, 2], [1, 3, 4, 6, 0, 2, 2, 3, 5, 7]], names=['tag', 'day'])
df = DataFrame(arr, index=index, columns=['val'])
result = df.val['A']
expected = Series(arr.ravel()[0:3], name='val', index=Index([26, 37, 57], name='day'))
pandas.util.testing.assert_series_equal(result, expected)

def f():
    df.val['X']
self.assertRaises(KeyError, f)
idx = pandas.MultiIndex.from_product([['A', 'B', 'C'], ['foo', 'bar', 'baz']], names=['one', 'two'])
s = pd.Series(np.arange(9, dtype='int64'), index=idx).sortlevel()
exp_idx = pandas.MultiIndex.from_product([['A'], ['foo', 'bar', 'baz']], names=['one', 'two'])
tempResult = arange(3, dtype='int64')
	
===================================================================	
TestIndexing.test_setitem_cache_updating: 2646	
----------------------------	

cont = ['one', 'two', 'three', 'four', 'five', 'six', 'seven']
for do_ref in [False, False]:
    tempResult = arange(7)
	
===================================================================	
TestIndexing.test_iloc_setitem_list_of_lists: 1120	
----------------------------	

tempResult = arange(5, dtype='int64')
	
===================================================================	
TestIndexing.test_iloc_setitem_list_of_lists: 1120	
----------------------------	

tempResult = arange(5, 10, dtype='int64')
	
===================================================================	
TestIndexing.test_iloc_setitem_list_of_lists: 1124	
----------------------------	

df = DataFrame(dict(A=numpy.arange(5, dtype='int64'), B=numpy.arange(5, 10, dtype='int64')))
df.iloc[2:4] = [[10, 11], [12, 13]]
expected = DataFrame(dict(A=[0, 1, 10, 12, 4], B=[5, 6, 11, 13, 9]))
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(5, 10, dtype='int64')
	
===================================================================	
TestIndexing.test_iloc_getitem_panel_multiindex: 1040	
----------------------------	

multi_index = pandas.MultiIndex.from_tuples([('ONE', 'one'), ('TWO', 'two'), ('THREE', 'three')], names=['UPPER', 'lower'])
simple_index = [x[0] for x in multi_index]
wd1 = Panel(items=['First', 'Second'], major_axis=['a', 'b', 'c', 'd'], minor_axis=multi_index)
wd2 = Panel(items=['First', 'Second'], major_axis=['a', 'b', 'c', 'd'], minor_axis=simple_index)
expected1 = wd1['First'].iloc[([True, True, True, False], [0, 2])]
result1 = wd1.iloc[(0, [True, True, True, False], [0, 2])]
pandas.util.testing.assert_frame_equal(result1, expected1)
expected2 = wd2['First'].iloc[([True, True, True, False], [0, 2])]
result2 = wd2.iloc[(0, [True, True, True, False], [0, 2])]
pandas.util.testing.assert_frame_equal(result2, expected2)
expected1 = DataFrame(index=['a'], columns=multi_index, dtype='float64')
result1 = wd1.iloc[(0, [0], [0, 1, 2])]
pandas.util.testing.assert_frame_equal(result1, expected1)
expected2 = DataFrame(index=['a'], columns=simple_index, dtype='float64')
result2 = wd2.iloc[(0, [0], [0, 1, 2])]
pandas.util.testing.assert_frame_equal(result2, expected2)
mi = pandas.core.api.MultiIndex.from_tuples([(0, 'x'), (1, 'y'), (2, 'z')])
tempResult = arange(((3 * 3) * 3), dtype='int64')
	
===================================================================	
TestIndexing.test_loc_multiindex_indexer_none: 1174	
----------------------------	

attributes = [('Attribute' + str(i)) for i in range(1)]
attribute_values = [('Value' + str(i)) for i in range(5)]
index = pandas.core.api.MultiIndex.from_product([attributes, attribute_values])
df = ((0.1 * numpy.random.randn(10, (1 * 5))) + 0.5)
df = DataFrame(df, columns=index)
result = df[attributes]
pandas.util.testing.assert_frame_equal(result, df)
tempResult = arange(12)
	
===================================================================	
TestIndexing.test_per_axis_per_level_setitem: 1473	
----------------------------	

idx = pandas.IndexSlice
index = pandas.core.api.MultiIndex.from_tuples([('A', 1), ('A', 2), ('A', 3), ('B', 1)], names=['one', 'two'])
columns = pandas.core.api.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'), ('b', 'foo'), ('b', 'bah')], names=['lvl0', 'lvl1'])
tempResult = arange(16, dtype='int64')
	
===================================================================	
TestIndexing.test_non_unique_loc_memory_error: 2117	
----------------------------	

columns = list('ABCDEFG')

def gen_test(l, l2):
    return pandas.concat([DataFrame(randn(l, len(columns)), index=lrange(l), columns=columns), DataFrame(numpy.ones((l2, len(columns))), index=([0] * l2), columns=columns)])

def gen_expected(df, mask):
    l = len(mask)
    return pandas.concat([df.take([0], convert=False), DataFrame(numpy.ones((l, len(columns))), index=([0] * l), columns=columns), df.take(mask[1:], convert=False)])
df = gen_test(900, 100)
self.assertFalse(df.index.is_unique)
tempResult = arange(100)
	
===================================================================	
TestIndexing.test_non_unique_loc_memory_error: 2123	
----------------------------	

columns = list('ABCDEFG')

def gen_test(l, l2):
    return pandas.concat([DataFrame(randn(l, len(columns)), index=lrange(l), columns=columns), DataFrame(numpy.ones((l2, len(columns))), index=([0] * l2), columns=columns)])

def gen_expected(df, mask):
    l = len(mask)
    return pandas.concat([df.take([0], convert=False), DataFrame(numpy.ones((l, len(columns))), index=([0] * l), columns=columns), df.take(mask[1:], convert=False)])
df = gen_test(900, 100)
self.assertFalse(df.index.is_unique)
mask = numpy.arange(100)
result = df.loc[mask]
expected = gen_expected(df, mask)
pandas.util.testing.assert_frame_equal(result, expected)
df = gen_test(900000, 100000)
self.assertFalse(df.index.is_unique)
tempResult = arange(100000)
	
===================================================================	
TestIndexing.test_multiindex_assignment: 1872	
----------------------------	

df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df['d'] = numpy.nan
arr = numpy.array([0.0, 1.0])
df.ix[(4, 'd')] = arr
pandas.util.testing.assert_series_equal(df.ix[(4, 'd')], Series(arr, index=[8, 10], name='d'))
df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df.ix[(4, 'c')] = arr
exp = Series(arr, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)
df.ix[(4, 'c')] = 10
exp = Series(10, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)

def f():
    df.ix[(4, 'c')] = [0, 1, 2, 3]
self.assertRaises(ValueError, f)

def f():
    df.ix[(4, 'c')] = [0]
self.assertRaises(ValueError, f)
NUM_ROWS = 100
NUM_COLS = 10
tempResult = arange(NUM_COLS)
	
===================================================================	
TestIndexing.test_multiindex_assignment: 1878	
----------------------------	

df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df['d'] = numpy.nan
arr = numpy.array([0.0, 1.0])
df.ix[(4, 'd')] = arr
pandas.util.testing.assert_series_equal(df.ix[(4, 'd')], Series(arr, index=[8, 10], name='d'))
df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df.ix[(4, 'c')] = arr
exp = Series(arr, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)
df.ix[(4, 'c')] = 10
exp = Series(10, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)

def f():
    df.ix[(4, 'c')] = [0, 1, 2, 3]
self.assertRaises(ValueError, f)

def f():
    df.ix[(4, 'c')] = [0]
self.assertRaises(ValueError, f)
NUM_ROWS = 100
NUM_COLS = 10
col_names = [('A' + num) for num in map(str, np.arange(NUM_COLS).tolist())]
index_cols = col_names[:5]
df = DataFrame(numpy.random.randint(5, size=(NUM_ROWS, NUM_COLS)), dtype=numpy.int64, columns=col_names)
df = df.set_index(index_cols).sort_index()
grp = df.groupby(level=index_cols[:4])
df['new_col'] = numpy.nan
tempResult = arange(5)
	
===================================================================	
TestIndexing.test_multiindex_assignment: 1883	
----------------------------	

df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df['d'] = numpy.nan
arr = numpy.array([0.0, 1.0])
df.ix[(4, 'd')] = arr
pandas.util.testing.assert_series_equal(df.ix[(4, 'd')], Series(arr, index=[8, 10], name='d'))
df = DataFrame(np.random.randint(5, 10, size=9).reshape(3, 3), columns=list('abc'), index=[[4, 4, 8], [8, 10, 12]])
df.ix[(4, 'c')] = arr
exp = Series(arr, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)
df.ix[(4, 'c')] = 10
exp = Series(10, index=[8, 10], name='c', dtype='float64')
pandas.util.testing.assert_series_equal(df.ix[(4, 'c')], exp)

def f():
    df.ix[(4, 'c')] = [0, 1, 2, 3]
self.assertRaises(ValueError, f)

def f():
    df.ix[(4, 'c')] = [0]
self.assertRaises(ValueError, f)
NUM_ROWS = 100
NUM_COLS = 10
col_names = [('A' + num) for num in map(str, np.arange(NUM_COLS).tolist())]
index_cols = col_names[:5]
df = DataFrame(numpy.random.randint(5, size=(NUM_ROWS, NUM_COLS)), dtype=numpy.int64, columns=col_names)
df = df.set_index(index_cols).sort_index()
grp = df.groupby(level=index_cols[:4])
df['new_col'] = numpy.nan
f_index = numpy.arange(5)

def f(name, df2):
    return Series(np.arange(df2.shape[0]), name=df2.index.values[0]).reindex(f_index)
for (name, df2) in grp:
    tempResult = arange(df2.shape[0])
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2702	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
tempResult = arange(4)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2707	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(2, 4)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2718	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})
self.assertIsNone(df.is_copy)

def f():
    df['A'][0] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df['A'][1] = numpy.nan
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
self.assertIsNone(df['A'].is_copy)
tempResult = arange(2, 4)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2811	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})
self.assertIsNone(df.is_copy)

def f():
    df['A'][0] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df['A'][1] = numpy.nan
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
self.assertIsNone(df['A'].is_copy)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})

def f():
    df.loc[0]['A'] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': Series(range(7), dtype='int64')})
self.assertIsNone(df.is_copy)
expected = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': [42, 42, 2, 3, 4, 42, 6]})

def f():
    indexer = df.a.str.startswith('o')
    df[indexer]['c'] = 42
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
expected = DataFrame({'A': [111, 'bbb', 'ccc'], 'B': [1, 2, 3]})
df = DataFrame({'A': ['aaa', 'bbb', 'ccc'], 'B': [1, 2, 3]})

def f():
    df['A'][0] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df.loc[0]['A'] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df.loc[(0, 'A')] = 111
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': [1, 2]})
self.assertIsNone(df.is_copy)
with pandas.util.testing.ensure_clean('__tmp__pickle') as path:
    df.to_pickle(path)
    df2 = pandas.read_pickle(path)
    df2['B'] = df2['A']
    df2['B'] = df2['A']
from string import ascii_letters as letters

def random_text(nobs=100):
    df = []
    for i in range(nobs):
        idx = numpy.random.randint(len(letters), size=2)
        idx.sort()
        df.append([letters[idx[0]:idx[1]]])
    return DataFrame(df, columns=['letters'])
df = random_text(100000)
x = df.iloc[[0, 1, 2]]
self.assertIsNotNone(x.is_copy)
x = df.iloc[[0, 1, 2, 4]]
self.assertIsNotNone(x.is_copy)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer].copy()
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df.loc[:, 'letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df.ix[(indexer, 'letters')] = df.ix[(indexer, 'letters')].apply(str.lower)
df = DataFrame({'a': [1]}).dropna()
self.assertIsNone(df.is_copy)
df['a'] += 1
a = [12, 23]
b = [123, None]
c = [1234, 2345]
d = [12345, 23456]
tuples = [('eyes', 'left'), ('eyes', 'right'), ('ears', 'left'), ('ears', 'right')]
events = {('eyes', 'left'): a, ('eyes', 'right'): b, ('ears', 'left'): c, ('ears', 'right'): d}
multiind = pandas.core.api.MultiIndex.from_tuples(tuples, names=['part', 'side'])
zed = DataFrame(events, index=['a', 'b'], columns=multiind)

def f():
    zed['eyes']['right'].fillna(value=555, inplace=True)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame(numpy.random.randn(10, 4))
s = df.iloc[:, 0].sort_values()
pandas.util.testing.assert_series_equal(s, df.iloc[:, 0].sort_values())
pandas.util.testing.assert_series_equal(s, df[0].sort_values())
df = DataFrame({'column1': ['a', 'a', 'a'], 'column2': [4, 8, 9]})
str(df)
df['column1'] = (df['column1'] + 'b')
str(df)
df = df[(df['column2'] != 8)]
str(df)
df['column1'] = (df['column1'] + 'c')
str(df)
tempResult = arange(0, 9)
	
===================================================================	
TestIndexing.test_detect_chained_assignment: 2817	
----------------------------	

pandas.set_option('chained_assignment', 'raise')
expected = DataFrame([[(- 5), 1], [(- 6), 3]], columns=list('AB'))
df = DataFrame(np.arange(4).reshape(2, 2), columns=list('AB'), dtype='int64')
self.assertIsNone(df.is_copy)
df['A'][0] = (- 5)
df['A'][1] = (- 6)
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})
self.assertIsNone(df.is_copy)

def f():
    df['A'][0] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df['A'][1] = numpy.nan
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
self.assertIsNone(df['A'].is_copy)
df = DataFrame({'A': Series(range(2), dtype='int64'), 'B': numpy.array(numpy.arange(2, 4), dtype=numpy.float64)})

def f():
    df.loc[0]['A'] = (- 5)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': Series(range(7), dtype='int64')})
self.assertIsNone(df.is_copy)
expected = DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'], 'c': [42, 42, 2, 3, 4, 42, 6]})

def f():
    indexer = df.a.str.startswith('o')
    df[indexer]['c'] = 42
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
expected = DataFrame({'A': [111, 'bbb', 'ccc'], 'B': [1, 2, 3]})
df = DataFrame({'A': ['aaa', 'bbb', 'ccc'], 'B': [1, 2, 3]})

def f():
    df['A'][0] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)

def f():
    df.loc[0]['A'] = 111
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df.loc[(0, 'A')] = 111
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame({'A': [1, 2]})
self.assertIsNone(df.is_copy)
with pandas.util.testing.ensure_clean('__tmp__pickle') as path:
    df.to_pickle(path)
    df2 = pandas.read_pickle(path)
    df2['B'] = df2['A']
    df2['B'] = df2['A']
from string import ascii_letters as letters

def random_text(nobs=100):
    df = []
    for i in range(nobs):
        idx = numpy.random.randint(len(letters), size=2)
        idx.sort()
        df.append([letters[idx[0]:idx[1]]])
    return DataFrame(df, columns=['letters'])
df = random_text(100000)
x = df.iloc[[0, 1, 2]]
self.assertIsNotNone(x.is_copy)
x = df.iloc[[0, 1, 2, 4]]
self.assertIsNotNone(x.is_copy)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer].copy()
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df = df.ix[indexer]
self.assertIsNotNone(df.is_copy)
df.loc[:, 'letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df['letters'] = df['letters'].apply(str.lower)
self.assertIsNone(df.is_copy)
df = random_text(100000)
indexer = df.letters.apply((lambda x: (len(x) > 10)))
df.ix[(indexer, 'letters')] = df.ix[(indexer, 'letters')].apply(str.lower)
df = DataFrame({'a': [1]}).dropna()
self.assertIsNone(df.is_copy)
df['a'] += 1
a = [12, 23]
b = [123, None]
c = [1234, 2345]
d = [12345, 23456]
tuples = [('eyes', 'left'), ('eyes', 'right'), ('ears', 'left'), ('ears', 'right')]
events = {('eyes', 'left'): a, ('eyes', 'right'): b, ('ears', 'left'): c, ('ears', 'right'): d}
multiind = pandas.core.api.MultiIndex.from_tuples(tuples, names=['part', 'side'])
zed = DataFrame(events, index=['a', 'b'], columns=multiind)

def f():
    zed['eyes']['right'].fillna(value=555, inplace=True)
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
df = DataFrame(numpy.random.randn(10, 4))
s = df.iloc[:, 0].sort_values()
pandas.util.testing.assert_series_equal(s, df.iloc[:, 0].sort_values())
pandas.util.testing.assert_series_equal(s, df[0].sort_values())
df = DataFrame({'column1': ['a', 'a', 'a'], 'column2': [4, 8, 9]})
str(df)
df['column1'] = (df['column1'] + 'b')
str(df)
df = df[(df['column2'] != 8)]
str(df)
df['column1'] = (df['column1'] + 'c')
str(df)
df = DataFrame(numpy.arange(0, 9), columns=['count'])
df['group'] = 'b'

def f():
    df.iloc[0:5]['group'] = 'a'
self.assertRaises(pandas.core.common.SettingWithCopyError, f)
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestIndexing.test_index_type_coercion: 2900	
----------------------------	

for s in [Series(range(5)), Series(range(5), index=range(1, 6))]:
    self.assertTrue(s.index.is_integer())
    for indexer in [(lambda x: x.ix), (lambda x: x.loc), (lambda x: x)]:
        s2 = s.copy()
        indexer(s2)[0.1] = 0
        self.assertTrue(s2.index.is_floating())
        self.assertTrue((indexer(s2)[0.1] == 0))
        s2 = s.copy()
        indexer(s2)[0.0] = 0
        exp = s.index
        if (0 not in s):
            exp = Index((s.index.tolist() + [0]))
        pandas.util.testing.assert_index_equal(s2.index, exp)
        s2 = s.copy()
        indexer(s2)['0'] = 0
        self.assertTrue(s2.index.is_object())
tempResult = arange(5.0)
	
===================================================================	
TestIndexing.test_partial_setting: 2197	
----------------------------	

s_orig = Series([1, 2, 3])
s = s_orig.copy()
s[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()

def f():
    s.iloc[3] = 5.0
self.assertRaises(IndexError, f)

def f():
    s.iat[3] = 5.0
self.assertRaises(IndexError, f)
tempResult = arange(6)
	
===================================================================	
TestIndexing.test_partial_setting: 2238	
----------------------------	

s_orig = Series([1, 2, 3])
s = s_orig.copy()
s[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()

def f():
    s.iloc[3] = 5.0
self.assertRaises(IndexError, f)

def f():
    s.iat[3] = 5.0
self.assertRaises(IndexError, f)
df_orig = DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'], dtype='int64')
df = df_orig.copy()

def f():
    df.iloc[(4, 2)] = 5.0
self.assertRaises(IndexError, f)

def f():
    df.iat[(4, 2)] = 5.0
self.assertRaises(IndexError, f)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.iloc[1] = df.iloc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.loc[1] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4, 4], 'B': [1, 3, 5, 5]}))
df = df_orig.copy()
df.loc[3] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': [0, 2, 4]}))
df = df_orig.copy()
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': Series([0, 2, 4])}))
df = df_orig.copy()
df['B'] = df['B'].astype(numpy.float64)
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(16)
	
===================================================================	
TestIndexing.test_partial_setting: 2239	
----------------------------	

s_orig = Series([1, 2, 3])
s = s_orig.copy()
s[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5
expected = Series([1, 2, 3, 5], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()
s.loc[5] = 5.0
expected = Series([1, 2, 3, 5.0], index=[0, 1, 2, 5])
pandas.util.testing.assert_series_equal(s, expected)
s = s_orig.copy()

def f():
    s.iloc[3] = 5.0
self.assertRaises(IndexError, f)

def f():
    s.iat[3] = 5.0
self.assertRaises(IndexError, f)
df_orig = DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'], dtype='int64')
df = df_orig.copy()

def f():
    df.iloc[(4, 2)] = 5.0
self.assertRaises(IndexError, f)

def f():
    df.iat[(4, 2)] = 5.0
self.assertRaises(IndexError, f)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.iloc[1] = df.iloc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 4, 4], 'B': [1, 5, 5]}))
df = df_orig.copy()
df.loc[1] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4, 4], 'B': [1, 3, 5, 5]}))
df = df_orig.copy()
df.loc[3] = df.loc[2]
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': [0, 2, 4]}))
df = df_orig.copy()
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = DataFrame(dict({'A': [0, 2, 4], 'B': Series([0, 2, 4])}))
df = df_orig.copy()
df['B'] = df['B'].astype(numpy.float64)
df.ix[:, 'B'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
expected = df_orig.copy()
expected['C'] = df['A']
df = df_orig.copy()
df.ix[:, 'C'] = df.ix[:, 'A']
pandas.util.testing.assert_frame_equal(df, expected)
p_orig = Panel(np.arange(16).reshape(2, 4, 2), items=['Item1', 'Item2'], major_axis=pandas.date_range('2001/1/12', periods=4), minor_axis=['A', 'B'], dtype='float64')
tempResult = arange(16)
	
===================================================================	
TestIndexing.test_setitem_iloc: 1662	
----------------------------	

tempResult = arange(9)
	
===================================================================	
TestIndexing.test_loc_setitem_dups: 635	
----------------------------	

tempResult = arange(5, dtype='float64')
	
===================================================================	
TestIndexing.test_loc_setitem_dups: 635	
----------------------------	

tempResult = arange(5, dtype='float64')
	
===================================================================	
TestIndexing.test_slice_with_zero_step_raises: 3013	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestIndexing.test_ix_get_set_consistency: 1943	
----------------------------	

tempResult = arange(16)
	
===================================================================	
TestIndexing.test_loc_setitem_frame: 888	
----------------------------	

df = self.frame_labels
result = df.iloc[(0, 0)]
df.loc[('a', 'A')] = 1
result = df.loc[('a', 'A')]
self.assertEqual(result, 1)
result = df.iloc[(0, 0)]
self.assertEqual(result, 1)
df.loc[:, 'B':'D'] = 0
expected = df.loc[:, 'B':'D']
result = df.ix[:, 1:]
pandas.util.testing.assert_frame_equal(result, expected)
df = DataFrame(index=[3, 5, 4], columns=['A'])
df.loc[([4, 3, 5], 'A')] = numpy.array([1, 2, 3], dtype='int64')
expected = DataFrame(dict(A=Series([1, 2, 3], index=[4, 3, 5]))).reindex(index=[3, 5, 4])
pandas.util.testing.assert_frame_equal(df, expected)
keys1 = [('@' + str(i)) for i in range(5)]
tempResult = arange(5, dtype='int64')
	
===================================================================	
TestIndexing.test_loc_setitem_frame: 890	
----------------------------	

df = self.frame_labels
result = df.iloc[(0, 0)]
df.loc[('a', 'A')] = 1
result = df.loc[('a', 'A')]
self.assertEqual(result, 1)
result = df.iloc[(0, 0)]
self.assertEqual(result, 1)
df.loc[:, 'B':'D'] = 0
expected = df.loc[:, 'B':'D']
result = df.ix[:, 1:]
pandas.util.testing.assert_frame_equal(result, expected)
df = DataFrame(index=[3, 5, 4], columns=['A'])
df.loc[([4, 3, 5], 'A')] = numpy.array([1, 2, 3], dtype='int64')
expected = DataFrame(dict(A=Series([1, 2, 3], index=[4, 3, 5]))).reindex(index=[3, 5, 4])
pandas.util.testing.assert_frame_equal(df, expected)
keys1 = [('@' + str(i)) for i in range(5)]
val1 = numpy.arange(5, dtype='int64')
keys2 = [('@' + str(i)) for i in range(4)]
tempResult = arange(4, dtype='int64')
	
===================================================================	
TestIndexing.test_setitem_dtype_upcast: 1646	
----------------------------	

df = DataFrame([{'a': 1}, {'a': 3, 'b': 2}])
df['c'] = numpy.nan
self.assertEqual(df['c'].dtype, numpy.float64)
df.ix[(0, 'c')] = 'foo'
expected = DataFrame([{'a': 1, 'c': 'foo'}, {'a': 3, 'b': 2, 'c': numpy.nan}])
pandas.util.testing.assert_frame_equal(df, expected)
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestIndexing.test_setitem_dtype_upcast: 1654	
----------------------------	

df = DataFrame([{'a': 1}, {'a': 3, 'b': 2}])
df['c'] = numpy.nan
self.assertEqual(df['c'].dtype, numpy.float64)
df.ix[(0, 'c')] = 'foo'
expected = DataFrame([{'a': 1, 'c': 'foo'}, {'a': 3, 'b': 2, 'c': numpy.nan}])
pandas.util.testing.assert_frame_equal(df, expected)
df = DataFrame(np.arange(6, dtype='int64').reshape(2, 3), index=list('ab'), columns=['foo', 'bar', 'baz'])
for val in [3.14, 'wxyz']:
    left = df.copy()
    left.loc[('a', 'bar')] = val
    right = DataFrame([[0, val, 2], [3, 4, 5]], index=list('ab'), columns=['foo', 'bar', 'baz'])
    pandas.util.testing.assert_frame_equal(left, right)
    self.assertTrue(is_integer_dtype(left['foo']))
    self.assertTrue(is_integer_dtype(left['baz']))
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestIndexing.test_multiindex_label_slicing_with_negative_step: 2991	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestIndexing.test_multiindex_label_slicing_with_negative_step: 2991	
----------------------------	

tempResult = arange(4)
	
===================================================================	
Get no callers of function numpy.arange at line 2962 col 13.	
===================================================================	
TestIndexing.test_multiindex_setitem: 1563	
----------------------------	

tempResult = arange(0, 6, 1)
	
===================================================================	
TestIndexing.test_iloc_mask: 1995	
----------------------------	

df = DataFrame(lrange(5), list('ABCDE'), columns=['a'])
mask = ((df.a % 2) == 0)
self.assertRaises(ValueError, df.iloc.__getitem__, tuple([mask]))
mask.index = lrange(len(mask))
self.assertRaises(NotImplementedError, df.iloc.__getitem__, tuple([mask]))
result = df.iloc[numpy.array(([True] * len(mask)), dtype=bool)]
pandas.util.testing.assert_frame_equal(result, df)
tempResult = arange(4)
	
===================================================================	
TestIndexing.test_str_label_slicing_with_negative_step: 2982	
----------------------------	

SLC = pandas.IndexSlice

def assert_slices_equivalent(l_slc, i_slc):
    pandas.util.testing.assert_series_equal(s.loc[l_slc], s.iloc[i_slc])
    if (not idx.is_integer):
        pandas.util.testing.assert_series_equal(s[l_slc], s.iloc[i_slc])
        pandas.util.testing.assert_series_equal(s.ix[l_slc], s.iloc[i_slc])
tempResult = arange(20)
	
===================================================================	
TestIndexing.test_str_label_slicing_with_negative_step: 2984	
----------------------------	

SLC = pandas.IndexSlice

def assert_slices_equivalent(l_slc, i_slc):
    pandas.util.testing.assert_series_equal(s.loc[l_slc], s.iloc[i_slc])
    if (not idx.is_integer):
        pandas.util.testing.assert_series_equal(s[l_slc], s.iloc[i_slc])
        pandas.util.testing.assert_series_equal(s.ix[l_slc], s.iloc[i_slc])
for idx in [_mklbl('A', 20), (numpy.arange(20) + 100), numpy.linspace(100, 150, 20)]:
    idx = Index(idx)
    tempResult = arange(20)
	
===================================================================	
TestIndexing.f11111111111111111111111: 1881	
----------------------------	

tempResult = arange(df2.shape[0])
	
===================================================================	
TestIndexing.test_loc_multiindex_incomplete: 1180	
----------------------------	

tempResult = arange(15, dtype='int64')
	
===================================================================	
TestIndexing.test_loc_multiindex_incomplete: 1191	
----------------------------	

s = pandas.Series(numpy.arange(15, dtype='int64'), pandas.core.api.MultiIndex.from_product([range(5), ['a', 'b', 'c']]))
expected = s.loc[:, 'a':'c']
result = s.loc[0:4, 'a':'c']
pandas.util.testing.assert_series_equal(result, expected)
pandas.util.testing.assert_series_equal(result, expected)
result = s.loc[:4, 'a':'c']
pandas.util.testing.assert_series_equal(result, expected)
pandas.util.testing.assert_series_equal(result, expected)
result = s.loc[0:, 'a':'c']
pandas.util.testing.assert_series_equal(result, expected)
pandas.util.testing.assert_series_equal(result, expected)
tempResult = arange(15, dtype='int64')
	
===================================================================	
TestIndexing.test_per_axis_per_level_getitem: 1267	
----------------------------	

ix = pandas.core.api.MultiIndex.from_product([_mklbl('A', 5), _mklbl('B', 7), _mklbl('C', 4), _mklbl('D', 2)])
tempResult = arange(len(ix.get_values()))
	
===================================================================	
TestIndexing.test_per_axis_per_level_getitem: 1276	
----------------------------	

ix = pandas.core.api.MultiIndex.from_product([_mklbl('A', 5), _mklbl('B', 7), _mklbl('C', 4), _mklbl('D', 2)])
df = DataFrame(numpy.arange(len(ix.get_values())), index=ix)
result = df.loc[(slice('A1', 'A3'), slice(None), ['C1', 'C3']), :]
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C3')))]]
pandas.util.testing.assert_frame_equal(result, expected)
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C2') or (c == 'C3')))]]
result = df.loc[(slice('A1', 'A3'), slice(None), slice('C1', 'C3')), :]
pandas.util.testing.assert_frame_equal(result, expected)
index = pandas.core.api.MultiIndex.from_tuples([('A', 1), ('A', 2), ('A', 3), ('B', 1)], names=['one', 'two'])
columns = pandas.core.api.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'), ('b', 'foo'), ('b', 'bah')], names=['lvl0', 'lvl1'])
tempResult = arange(16, dtype='int64')
	
===================================================================	
TestIndexing.test_per_axis_per_level_getitem: 1302	
----------------------------	

ix = pandas.core.api.MultiIndex.from_product([_mklbl('A', 5), _mklbl('B', 7), _mklbl('C', 4), _mklbl('D', 2)])
df = DataFrame(numpy.arange(len(ix.get_values())), index=ix)
result = df.loc[(slice('A1', 'A3'), slice(None), ['C1', 'C3']), :]
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C3')))]]
pandas.util.testing.assert_frame_equal(result, expected)
expected = df.loc[[tuple([a, b, c, d]) for (a, b, c, d) in df.index.values if (((a == 'A1') or (a == 'A2') or (a == 'A3')) and ((c == 'C1') or (c == 'C2') or (c == 'C3')))]]
result = df.loc[(slice('A1', 'A3'), slice(None), slice('C1', 'C3')), :]
pandas.util.testing.assert_frame_equal(result, expected)
index = pandas.core.api.MultiIndex.from_tuples([('A', 1), ('A', 2), ('A', 3), ('B', 1)], names=['one', 'two'])
columns = pandas.core.api.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'), ('b', 'foo'), ('b', 'bah')], names=['lvl0', 'lvl1'])
df = DataFrame(np.arange(16, dtype='int64').reshape(4, 4), index=index, columns=columns)
df = df.sortlevel(axis=0).sortlevel(axis=1)
result = df.loc[(slice(None), slice(None)), :]
pandas.util.testing.assert_frame_equal(result, df)
result = df.loc[((slice(None), slice(None)), (slice(None), slice(None)))]
pandas.util.testing.assert_frame_equal(result, df)
result = df.loc[:, (slice(None), slice(None))]
pandas.util.testing.assert_frame_equal(result, df)
result = df.loc[(slice(None), [1]), :]
expected = df.iloc[[0, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[(slice(None), 1), :]
expected = df.iloc[[0, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[:, (slice(None), ['foo'])]
expected = df.iloc[:, [1, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[((slice(None), 1), (slice(None), ['foo']))]
expected = df.iloc[([0, 3], [1, 3])]
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[('A', 'a')]
expected = DataFrame(dict(bar=[1, 5, 9], foo=[0, 4, 8]), index=Index([1, 2, 3], name='two'), columns=Index(['bar', 'foo'], name='lvl1'))
pandas.util.testing.assert_frame_equal(result, expected)
result = df.loc[(slice(None), [1, 2]), :]
expected = df.iloc[[0, 1, 3]]
pandas.util.testing.assert_frame_equal(result, expected)
tempResult = arange(len(ix.get_values()))
	
===================================================================	
TestIndexing.test_iloc_getitem_panel: 971	
----------------------------	

tempResult = arange(((4 * 3) * 2))
	
===================================================================	
TestIndexing.f11111111: 1092	
----------------------------	

tempResult = arange(1, 4)
	
===================================================================	
TestIndexing.test_setitem_multiindex: 545	
----------------------------	

for index_fn in ('ix', 'loc'):

    def check(target, indexers, value, compare_fn, expected=None):
        fn = getattr(target, index_fn)
        fn.__setitem__(indexers, value)
        result = fn.__getitem__(indexers)
        if (expected is None):
            expected = value
        compare_fn(result, expected)
    tempResult = arange(0, 100)
	
===================================================================	
TestIndexing.test_setitem_multiindex: 545	
----------------------------	

for index_fn in ('ix', 'loc'):

    def check(target, indexers, value, compare_fn, expected=None):
        fn = getattr(target, index_fn)
        fn.__setitem__(indexers, value)
        result = fn.__getitem__(indexers)
        if (expected is None):
            expected = value
        compare_fn(result, expected)
    tempResult = arange(0, 80)
	
===================================================================	
TestIndexing.test_setitem_multiindex: 555	
----------------------------	

for index_fn in ('ix', 'loc'):

    def check(target, indexers, value, compare_fn, expected=None):
        fn = getattr(target, index_fn)
        fn.__setitem__(indexers, value)
        result = fn.__getitem__(indexers)
        if (expected is None):
            expected = value
        compare_fn(result, expected)
    index = pandas.MultiIndex.from_product([numpy.arange(0, 100), numpy.arange(0, 80)], names=['time', 'firm'])
    (t, n) = (0, 2)
    df = DataFrame(numpy.nan, columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=0, compare_fn=self.assertEqual)
    df = DataFrame((- 999), columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=1, compare_fn=self.assertEqual)
    df = DataFrame(columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=2, compare_fn=self.assertEqual)
    df = DataFrame((- 999), columns=['A', 'w', 'l', 'a', 'x', 'X', 'd', 'profit'], index=index)
    check(target=df, indexers=((t, n), 'X'), value=numpy.array(3), compare_fn=self.assertEqual, expected=3)
    tempResult = arange(25)
	
===================================================================	
TestSeriesDtypes.test_astype_cast_object_int: 45	
----------------------------	

arr = Series(['car', 'house', 'tree', '1'])
self.assertRaises(ValueError, arr.astype, int)
self.assertRaises(ValueError, arr.astype, numpy.int64)
self.assertRaises(ValueError, arr.astype, numpy.int8)
arr = Series(['1', '2', '3', '4'], dtype=object)
result = arr.astype(int)
tempResult = arange(1, 5)
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 616	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
tempResult = arange(len(right))
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 625	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
right.index = numpy.arange(len(right))
right['right'] *= (- 1)
out = merge(left, right, how='outer')
self.assertEqual(len(out), len(left))
assert_series_equal(out['left'], (- out['right']), check_names=False)
result = out.iloc[:, :(- 2)].sum(axis=1)
assert_series_equal(out['left'], result, check_names=False)
self.assertTrue((result.name is None))
out.sort_values(out.columns.tolist(), inplace=True)
tempResult = arange(len(out))
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 644	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
right.index = numpy.arange(len(right))
right['right'] *= (- 1)
out = merge(left, right, how='outer')
self.assertEqual(len(out), len(left))
assert_series_equal(out['left'], (- out['right']), check_names=False)
result = out.iloc[:, :(- 2)].sum(axis=1)
assert_series_equal(out['left'], result, check_names=False)
self.assertTrue((result.name is None))
out.sort_values(out.columns.tolist(), inplace=True)
out.index = numpy.arange(len(out))
for how in ['left', 'right', 'outer', 'inner']:
    assert_frame_equal(out, merge(left, right, how=how, sort=True))
out = merge(left, right, how='left', sort=False)
assert_frame_equal(left, out[left.columns.tolist()])
out = merge(right, left, how='left', sort=False)
assert_frame_equal(right, out[right.columns.tolist()])
n = (1 << 11)
left = DataFrame(np.random.randint(low, high, (n, 7)).astype('int64'), columns=list('ABCDEFG'))
shape = left.apply(Series.nunique).values
self.assertTrue(_int64_overflow_possible(shape))
left = concat([left, left], ignore_index=True)
right = DataFrame(np.random.randint(low, high, ((n // 2), 7)).astype('int64'), columns=list('ABCDEFG'))
i = numpy.random.choice(len(left), n)
right = concat([right, right, left.iloc[i]], ignore_index=True)
left['left'] = numpy.random.randn(len(left))
right['right'] = numpy.random.randn(len(right))
i = numpy.random.permutation(len(left))
left = left.iloc[i].copy()
tempResult = arange(len(left))
	
===================================================================	
TestMergeMulti.test_int64_overflow_issues: 647	
----------------------------	

from itertools import product
from collections import defaultdict
from pandas.core.groupby import _int64_overflow_possible
df1 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G1']))
df2 = DataFrame(numpy.random.randn(1000, 7), columns=(list('ABCDEF') + ['G2']))
result = merge(df1, df2, how='outer')
self.assertTrue((len(result) == 2000))
(low, high, n) = (((- 1) << 10), (1 << 10), (1 << 20))
left = DataFrame(numpy.random.randint(low, high, (n, 7)), columns=list('ABCDEFG'))
left['left'] = left.sum(axis=1)
i = numpy.random.permutation(len(left))
right = left.iloc[i].copy()
right.columns = (right.columns[:(- 1)].tolist() + ['right'])
right.index = numpy.arange(len(right))
right['right'] *= (- 1)
out = merge(left, right, how='outer')
self.assertEqual(len(out), len(left))
assert_series_equal(out['left'], (- out['right']), check_names=False)
result = out.iloc[:, :(- 2)].sum(axis=1)
assert_series_equal(out['left'], result, check_names=False)
self.assertTrue((result.name is None))
out.sort_values(out.columns.tolist(), inplace=True)
out.index = numpy.arange(len(out))
for how in ['left', 'right', 'outer', 'inner']:
    assert_frame_equal(out, merge(left, right, how=how, sort=True))
out = merge(left, right, how='left', sort=False)
assert_frame_equal(left, out[left.columns.tolist()])
out = merge(right, left, how='left', sort=False)
assert_frame_equal(right, out[right.columns.tolist()])
n = (1 << 11)
left = DataFrame(np.random.randint(low, high, (n, 7)).astype('int64'), columns=list('ABCDEFG'))
shape = left.apply(Series.nunique).values
self.assertTrue(_int64_overflow_possible(shape))
left = concat([left, left], ignore_index=True)
right = DataFrame(np.random.randint(low, high, ((n // 2), 7)).astype('int64'), columns=list('ABCDEFG'))
i = numpy.random.choice(len(left), n)
right = concat([right, right, left.iloc[i]], ignore_index=True)
left['left'] = numpy.random.randn(len(left))
right['right'] = numpy.random.randn(len(right))
i = numpy.random.permutation(len(left))
left = left.iloc[i].copy()
left.index = numpy.arange(len(left))
i = numpy.random.permutation(len(right))
right = right.iloc[i].copy()
tempResult = arange(len(right))
	
===================================================================	
TestMergeMulti.run_asserts: 474	
----------------------------	

for sort in [False, True]:
    res = left.join(right, on=icols, how='left', sort=sort)
    self.assertTrue((len(left) < (len(res) + 1)))
    self.assertFalse(res['4th'].isnull().any())
    self.assertFalse(res['5th'].isnull().any())
    pandas.util.testing.assert_series_equal(res['4th'], (- res['5th']), check_names=False)
    result = bind_cols(res.iloc[:, :(- 2)])
    pandas.util.testing.assert_series_equal(res['4th'], result, check_names=False)
    self.assertTrue((result.name is None))
    if sort:
        pandas.util.testing.assert_frame_equal(res, res.sort_values(icols, kind='mergesort'))
    out = merge(left, right.reset_index(), on=icols, sort=sort, how='left')
    tempResult = arange(len(res))
	
===================================================================	
TestPivotTable.test_pivot_with_tz: 137	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_with_tz: 137	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_index_with_nan: 126	
----------------------------	

nan = numpy.nan
df = DataFrame({'a': ['R1', 'R2', nan, 'R4'], 'b': ['C1', 'C2', 'C3', 'C4'], 'c': [10, 15, 17, 20]})
result = df.pivot('a', 'b', 'c')
expected = DataFrame([[nan, nan, 17, nan], [10, nan, nan, nan], [nan, 15, nan, nan], [nan, nan, nan, 20]], index=Index([nan, 'R1', 'R2', 'R4'], name='a'), columns=Index(['C1', 'C2', 'C3', 'C4'], name='b'))
pandas.util.testing.assert_frame_equal(result, expected)
pandas.util.testing.assert_frame_equal(df.pivot('b', 'a', 'c'), expected.T)
tempResult = arange(6)
	
===================================================================	
TestPivotTable.test_pivot_dtaccessor: 342	
----------------------------	

dates1 = ['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00']
dates2 = ['2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00']
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_datetime_tz: 321	
----------------------------	

dates1 = ['2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00', '2011-07-19 07:00:00', '2011-07-19 08:00:00', '2011-07-19 09:00:00']
dates2 = ['2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-01-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00', '2013-02-01 15:00:00']
tempResult = arange(6, dtype='int64')
	
===================================================================	
TestPivotTable.test_categorical_margins: 380	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestPivotTable.test_categorical_margins: 380	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestPivotTable.test_categorical_margins: 380	
----------------------------	

tempResult = arange(8)
	
===================================================================	
TestPivotTable.test_pivot_periods: 151	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestPivotTable.test_pivot_periods: 151	
----------------------------	

tempResult = arange(4, dtype='int64')
	
===================================================================	
TestDatetimeIndex.test_resample_consistency: 1116	
----------------------------	

i30 = pandas.date_range('2002-02-02', periods=4, freq='30T')
tempResult = arange(4.0)
	
===================================================================	
TestDatetimeIndex.test_resample_timegrouper: 1133	
----------------------------	

dates1 = [datetime(2014, 10, 1), datetime(2014, 9, 3), datetime(2014, 11, 5), datetime(2014, 9, 5), datetime(2014, 10, 8), datetime(2014, 7, 15)]
dates2 = ((((dates1[:2] + [pandas.NaT]) + dates1[2:4]) + [pandas.NaT]) + dates1[4:])
dates3 = (([pandas.NaT] + dates1) + [pandas.NaT])
for dates in [dates1, dates2, dates3]:
    tempResult = arange(len(dates))
	
===================================================================	
TestDatetimeIndex.test_resample_timegrouper: 1140	
----------------------------	

dates1 = [datetime(2014, 10, 1), datetime(2014, 9, 3), datetime(2014, 11, 5), datetime(2014, 9, 5), datetime(2014, 10, 8), datetime(2014, 7, 15)]
dates2 = ((((dates1[:2] + [pandas.NaT]) + dates1[2:4]) + [pandas.NaT]) + dates1[4:])
dates3 = (([pandas.NaT] + dates1) + [pandas.NaT])
for dates in [dates1, dates2, dates3]:
    df = DataFrame(dict(A=dates, B=numpy.arange(len(dates))))
    result = df.set_index('A').resample('M').count()
    exp_idx = pandas.DatetimeIndex(['2014-07-31', '2014-08-31', '2014-09-30', '2014-10-31', '2014-11-30'], freq='M', name='A')
    expected = DataFrame({'B': [1, 0, 2, 2, 1]}, index=exp_idx)
    assert_frame_equal(result, expected)
    result = df.groupby(pd.Grouper(freq='M', key='A')).count()
    assert_frame_equal(result, expected)
    tempResult = arange(len(dates))
	
===================================================================	
TestDatetimeIndex.test_resample_timegrouper: 1140	
----------------------------	

dates1 = [datetime(2014, 10, 1), datetime(2014, 9, 3), datetime(2014, 11, 5), datetime(2014, 9, 5), datetime(2014, 10, 8), datetime(2014, 7, 15)]
dates2 = ((((dates1[:2] + [pandas.NaT]) + dates1[2:4]) + [pandas.NaT]) + dates1[4:])
dates3 = (([pandas.NaT] + dates1) + [pandas.NaT])
for dates in [dates1, dates2, dates3]:
    df = DataFrame(dict(A=dates, B=numpy.arange(len(dates))))
    result = df.set_index('A').resample('M').count()
    exp_idx = pandas.DatetimeIndex(['2014-07-31', '2014-08-31', '2014-09-30', '2014-10-31', '2014-11-30'], freq='M', name='A')
    expected = DataFrame({'B': [1, 0, 2, 2, 1]}, index=exp_idx)
    assert_frame_equal(result, expected)
    result = df.groupby(pd.Grouper(freq='M', key='A')).count()
    assert_frame_equal(result, expected)
    tempResult = arange(len(dates))
	
===================================================================	
TestPeriodIndex.test_asfreq_downsample: 1259	
----------------------------	

s = self.create_series()
tempResult = arange(0, len(s.index), 2)
	
===================================================================	
TestPeriodIndex.test_asfreq_downsample: 1265	
----------------------------	

s = self.create_series()
expected = s.reindex(s.index.take(numpy.arange(0, len(s.index), 2)))
expected.index = expected.index.to_timestamp()
expected.index.freq = to_offset('2D')
result = s.resample('2D').asfreq()
assert_series_equal(result, expected)
frame = s.to_frame('value')
tempResult = arange(0, len(frame.index), 2)
	
===================================================================	
TestPeriodIndex.test_quarterly_resampling: 1584	
----------------------------	

rng = period_range('2000Q1', periods=10, freq='Q-DEC')
tempResult = arange(10)
	
===================================================================	
TestResampleAPI.test_fillna: 216	
----------------------------	

rng = pandas.date_range('1/1/2012', periods=10, freq='2S')
tempResult = arange(len(rng), dtype='int64')
	
===================================================================	
TestResampleAPI.setUp: 44	
----------------------------	

dti = DatetimeIndex(start=datetime(2005, 1, 1), end=datetime(2005, 1, 10), freq='Min')
self.series = Series(numpy.random.rand(len(dti)), dti)
tempResult = arange(len(dti))
	
===================================================================	
Base.test_asfreq_downsample: 393	
----------------------------	

s = self.create_series()
result = s.resample('2D').asfreq()
tempResult = arange(0, len(s.index), 2)
	
===================================================================	
Base.test_asfreq_downsample: 398	
----------------------------	

s = self.create_series()
result = s.resample('2D').asfreq()
expected = s.reindex(s.index.take(numpy.arange(0, len(s.index), 2)))
expected.index.freq = to_offset('2D')
assert_series_equal(result, expected)
frame = s.to_frame('value')
result = frame.resample('2D').asfreq()
tempResult = arange(0, len(frame.index), 2)
	
===================================================================	
TestTimedeltaIndex.create_series: 1649	
----------------------------	

i = timedelta_range('1 day', '10 day', freq='D')
tempResult = arange(len(i))
	
===================================================================	
TestDatetimeIndex.test_resample_upsampling_picked_but_not_correct: 676	
----------------------------	

dates = date_range('01-Jan-2014', '05-Jan-2014', freq='D')
series = Series(1, index=dates)
result = series.resample('D').mean()
self.assertEqual(result.index[0], dates[0])
import datetime
tempResult = arange(1.0, 6)
	
===================================================================	
TestDatetimeIndex.test_resample_upsampling_picked_but_not_correct: 677	
----------------------------	

dates = date_range('01-Jan-2014', '05-Jan-2014', freq='D')
series = Series(1, index=dates)
result = series.resample('D').mean()
self.assertEqual(result.index[0], dates[0])
import datetime
s = Series(numpy.arange(1.0, 6), index=[datetime.datetime(1975, 1, i, 12, 0) for i in range(1, 6)])
tempResult = arange(1.0, 6)
	
===================================================================	
TestDatetimeIndex.test_resample_ohlc_result: 785	
----------------------------	

index = pandas.date_range('1-1-2000', '2-15-2000', freq='h')
index = index.union(pandas.date_range('4-15-2000', '5-15-2000', freq='h'))
s = Series(range(len(index)), index=index)
a = s.loc[:'4-15-2000'].resample('30T').ohlc()
self.assertIsInstance(a, DataFrame)
b = s.loc[:'4-14-2000'].resample('30T').ohlc()
self.assertIsInstance(b, DataFrame)
rng = date_range('2013-12-30', '2014-01-07')
index = rng.drop([Timestamp('2014-01-01'), Timestamp('2013-12-31'), Timestamp('2014-01-04'), Timestamp('2014-01-05')])
tempResult = arange(len(index))
	
***************************************************	
dask_dask-0.7.0: 51	
===================================================================	
insert: 972	
----------------------------	

if (not ((- arr.ndim) <= axis < arr.ndim)):
    raise IndexError(('axis %r is out of bounds for an array of dimension %s' % (axis, arr.ndim)))
if (axis < 0):
    axis += arr.ndim
if isinstance(obj, slice):
    tempResult = arange(*obj.indices(arr.shape[axis]))
	
===================================================================	
arange: 45	
----------------------------	

"\n    Return evenly spaced values from `start` to `stop` with step size `step`.\n\n    The values are half-open [start, stop), so including start and excluding\n    stop. This is basically the same as python's range function but for dask\n    arrays.\n\n    When using a non-integer step, such as 0.1, the results will often not be\n    consistent. It is better to use linspace for these cases.\n\n    Parameters\n    ----------\n    start : int, optional\n        The starting value of the sequence. The default is 0.\n    stop : int\n        The end of the interval, this value is excluded from the interval.\n    step : int, optional\n        The spacing between the values. The default is 1 when not specified.\n        The last value of the sequence.\n    chunks :  int\n        The number of samples on each block. Note that the last block will have\n        fewer samples if `num % chunks != 0`.\n    num : int, optional\n        Number of samples to in the returned dask array, including the\n        endpoints.\n\n    Returns\n    -------\n    samples : dask array\n\n    "
if (len(args) == 1):
    start = 0
    stop = args[0]
    step = 1
elif (len(args) == 2):
    start = args[0]
    stop = args[1]
    step = 1
elif (len(args) == 3):
    (start, stop, step) = args
else:
    raise TypeError('\n        arange takes 3 positional arguments: arange([start], stop, [step])\n        ')
if ('chunks' not in kwargs):
    raise ValueError('Must supply a chunks= keyword argument')
chunks = kwargs['chunks']
dtype = kwargs.get('dtype', None)
if (dtype is None):
    tempResult = arange(0, 1, step)
	
===================================================================	
test_keepdims_wrapper_no_axis: 16	
----------------------------	


def summer(a, axis=None):
    return a.sum(axis=axis)
summer_wrapped = keepdims_wrapper(summer)
assert (summer_wrapped != summer)
assert (summer_wrapped == keepdims_wrapper(summer_wrapped))
tempResult = arange(24)
	
===================================================================	
Get no callers of function numpy.arange at line 37 col 8.	
===================================================================	
Get no callers of function numpy.arange at line 58 col 8.	
===================================================================	
test_arange_float_step: 72	
----------------------------	

darr = dask.array.arange(2.0, 13.0, 0.3, chunks=4)
tempResult = arange(2.0, 13.0, 0.3)
	
===================================================================	
test_arange_float_step: 75	
----------------------------	

darr = dask.array.arange(2.0, 13.0, 0.3, chunks=4)
nparr = numpy.arange(2.0, 13.0, 0.3)
eq(darr, nparr)
darr = dask.array.arange(7.7, 1.5, (- 0.8), chunks=3)
tempResult = arange(7.7, 1.5, (- 0.8))
	
===================================================================	
test_arange_cast_float_int_step: 66	
----------------------------	

darr = dask.array.arange(3.3, (- 9.1), (- 0.25), chunks=3, dtype='i8')
tempResult = arange(3.3, (- 9.1), (- 0.25), dtype='i8')
	
===================================================================	
test_arange_working_float_step: 60	
----------------------------	

'Sometimes floating point step arguments work, but this could be platform\n    dependent.\n    '
darr = dask.array.arange(3.3, (- 9.1), (- 0.25), chunks=3)
tempResult = arange(3.3, (- 9.1), (- 0.25))
	
===================================================================	
test_arange: 34	
----------------------------	

darr = dask.array.arange(77, chunks=13)
tempResult = arange(77)
	
===================================================================	
test_arange: 37	
----------------------------	

darr = dask.array.arange(77, chunks=13)
nparr = numpy.arange(77)
eq(darr, nparr)
darr = dask.array.arange(2, 13, chunks=5)
tempResult = arange(2, 13)
	
===================================================================	
test_arange: 40	
----------------------------	

darr = dask.array.arange(77, chunks=13)
nparr = numpy.arange(77)
eq(darr, nparr)
darr = dask.array.arange(2, 13, chunks=5)
nparr = numpy.arange(2, 13)
eq(darr, nparr)
darr = dask.array.arange(4, 21, 9, chunks=13)
tempResult = arange(4, 21, 9)
	
===================================================================	
test_arange: 43	
----------------------------	

darr = dask.array.arange(77, chunks=13)
nparr = numpy.arange(77)
eq(darr, nparr)
darr = dask.array.arange(2, 13, chunks=5)
nparr = numpy.arange(2, 13)
eq(darr, nparr)
darr = dask.array.arange(4, 21, 9, chunks=13)
nparr = numpy.arange(4, 21, 9)
eq(darr, nparr)
darr = dask.array.arange(53, 5, (- 3), chunks=5)
tempResult = arange(53, 5, (- 3))
	
===================================================================	
test_arange: 46	
----------------------------	

darr = dask.array.arange(77, chunks=13)
nparr = numpy.arange(77)
eq(darr, nparr)
darr = dask.array.arange(2, 13, chunks=5)
nparr = numpy.arange(2, 13)
eq(darr, nparr)
darr = dask.array.arange(4, 21, 9, chunks=13)
nparr = numpy.arange(4, 21, 9)
eq(darr, nparr)
darr = dask.array.arange(53, 5, (- 3), chunks=5)
nparr = numpy.arange(53, 5, (- 3))
eq(darr, nparr)
darr = dask.array.arange(77, chunks=13, dtype=float)
tempResult = arange(77, dtype=float)
	
===================================================================	
test_arange: 49	
----------------------------	

darr = dask.array.arange(77, chunks=13)
nparr = numpy.arange(77)
eq(darr, nparr)
darr = dask.array.arange(2, 13, chunks=5)
nparr = numpy.arange(2, 13)
eq(darr, nparr)
darr = dask.array.arange(4, 21, 9, chunks=13)
nparr = numpy.arange(4, 21, 9)
eq(darr, nparr)
darr = dask.array.arange(53, 5, (- 3), chunks=5)
nparr = numpy.arange(53, 5, (- 3))
eq(darr, nparr)
darr = dask.array.arange(77, chunks=13, dtype=float)
nparr = numpy.arange(77, dtype=float)
eq(darr, nparr)
darr = dask.array.arange(2, 13, chunks=5, dtype=int)
tempResult = arange(2, 13, dtype=int)
	
===================================================================	
module: 44	
----------------------------	

import numpy as np
import numpy.fft as npfft
import dask
from dask.array.core import Array
from dask.utils import raises
import dask.array as da
from dask.array.fft import fft, ifft, rfft, irfft, hfft, ihfft

def mismatch_err(mismatch_type, got, expected):
    return ('%s mismatch, got %s, expected %s' % (mismatch_type, got, expected))

def eq(a, b):
    assert (a.shape == b.shape), mismatch_err('shape', a.shape, b.shape)
    if isinstance(a, Array):
        adt = a._dtype
        a = a.compute(get=dask.get)
    else:
        adt = getattr(a, 'dtype', None)
    if isinstance(b, Array):
        bdt = b._dtype
        b = b.compute(get=dask.get)
    else:
        bdt = getattr(b, 'dtype', None)
    assert (str(adt) == str(bdt)), mismatch_err('dtype', str(adt), str(bdt))
    try:
        return numpy.allclose(a, b)
    except TypeError:
        pass
    c = (a == b)
    if isinstance(c, numpy.ndarray):
        return c.all()
    else:
        return c

def same_keys(a, b):

    def key(k):
        if isinstance(k, str):
            return (k, (- 1), (- 1), (- 1))
        else:
            return k
    return (sorted(a.dask, key=key) == sorted(b.dask, key=key))
tempResult = arange(100)
	
===================================================================	
test_some_0_depth: 136	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_depth_equals_boundary_length: 165	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_map_overlap: 109	
----------------------------	

x = dask.array.arange(10, chunks=5)
y = x.map_overlap((lambda x: (x + len(x))), depth=2)
tempResult = arange(10)
	
===================================================================	
test_constant: 80	
----------------------------	

tempResult = arange(64)
	
===================================================================	
test_0_depth: 119	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_boundaries: 89	
----------------------------	

tempResult = arange(64)
	
===================================================================	
test_constant_boundaries: 159	
----------------------------	

tempResult = arange((1 * 9))
	
===================================================================	
test_one_chunk_along_axis: 153	
----------------------------	

tempResult = arange((2 * 9))
	
===================================================================	
test_depth_greater_than_boundary_length: 183	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_nearest_ghost: 112	
----------------------------	

tempResult = arange(144)
	
===================================================================	
test_ghost: 96	
----------------------------	

tempResult = arange(64)
	
===================================================================	
test_reflect: 60	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_bad_depth_raises: 200	
----------------------------	

tempResult = arange(144)
	
===================================================================	
test_periodic: 51	
----------------------------	

tempResult = arange(64)
	
===================================================================	
test_ghost_internal: 36	
----------------------------	

tempResult = arange(64)
	
===================================================================	
test_nearest: 70	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_rechunk_with_integer: 132	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_moment: 172	
----------------------------	


def moment(x, n, axis=None):
    return (((x - x.mean(axis=axis, keepdims=True)) ** n).sum(axis=axis) / np.ones_like(x).sum(axis=axis))
x = (np.array(([1.0, 2.0, 3.0] * 10)).reshape((3, 10)) + 100000000.0)
a = dask.array.from_array(x, chunks=5)
assert eq(a.moment(2), moment(x, 2))
assert eq(a.moment(3), moment(x, 3))
assert eq(a.moment(4), moment(x, 4))
tempResult = arange(1, 122)
	
===================================================================	
test_reductions_2D_float: 108	
----------------------------	

tempResult = arange(1, 122)
	
===================================================================	
test_reductions_2D_int: 135	
----------------------------	

tempResult = arange(1, 122)
	
===================================================================	
test_reductions_1D_int: 70	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_reductions_1D_float: 45	
----------------------------	

tempResult = arange(5)
	
===================================================================	
test_slicing_with_negative_step_flops_keys: 212	
----------------------------	

x = dask.array.arange(10, chunks=5)
y = x[:1:(- 1)]
assert ((x.name, 1) in y.dask[(y.name, 0)])
assert ((x.name, 0) in y.dask[(y.name, 1)])
tempResult = arange(10)
	
===================================================================	
test_slicing_exhaustively: 201	
----------------------------	

x = numpy.random.rand(6, 7, 8)
a = dask.array.from_array(x, chunks=(3, 3, 3))
I = ReturnItem()
indexers = [0, (- 2), I[:], I[:5], [0, 1], [0, 1, 2], [4, 2], I[::(- 1)], None, I[:0], []]
for i in indexers:
    assert eq(x[i], a[i]), i
    for j in indexers:
        assert eq(x[i][:, j], a[i][:, j]), (i, j)
        assert eq(x[:, i][j], a[:, i][j]), (i, j)
        for k in indexers:
            assert eq(x[(..., i)][:, j][k], a[(..., i)][:, j][k]), (i, j, k)
tempResult = arange(5)
	
===================================================================	
test_slicing_exhaustively: 201	
----------------------------	

x = numpy.random.rand(6, 7, 8)
a = dask.array.from_array(x, chunks=(3, 3, 3))
I = ReturnItem()
indexers = [0, (- 2), I[:], I[:5], [0, 1], [0, 1, 2], [4, 2], I[::(- 1)], None, I[:0], []]
for i in indexers:
    assert eq(x[i], a[i]), i
    for j in indexers:
        assert eq(x[i][:, j], a[i][:, j]), (i, j)
        assert eq(x[:, i][j], a[:, i][j]), (i, j)
        for k in indexers:
            assert eq(x[(..., i)][:, j][k], a[(..., i)][:, j][k]), (i, j, k)
tempResult = arange(6)
	
===================================================================	
test_slicing_consistent_names: 234	
----------------------------	

tempResult = arange(100)
	
===================================================================	
partition: 117	
----------------------------	

' Partition a dataframe along a grouper, store partitions to partd '
tempResult = arange(len(df))
	
===================================================================	
test_gh580: 1034	
----------------------------	

tempResult = arange(10, dtype=float)
	
===================================================================	
test_from_array: 137	
----------------------------	

tempResult = arange((10 * 3))
	
===================================================================	
test_dask_client_from_ipclient: 55	
----------------------------	

with ipcluster() as c:
    dask_client = dask_client_from_ipclient(c)
    tempResult = arange(100)
	
===================================================================	
test_compute_array_dataframe: 67	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_compute_array_bag: 81	
----------------------------	

x = da.arange(5, chunks=2)
b = db.from_sequence([1, 2, 3])
assert raises(ValueError, (lambda : compute(x, b)))
(xx, bb) = compute(x, b, get=dask.async.get_sync)
tempResult = arange(5)
	
===================================================================	
test_compute_array: 46	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_array_bag_imperative: 154	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_array_imperative: 137	
----------------------------	

tempResult = arange(100)
	
***************************************************	
nengo_nengo-2.0.0: 34	
===================================================================	
ObjView.__init__: 69	
----------------------------	
self.obj = obj
if isinstance(key, int):
    if (key == (- 1)):
        key = slice(key, None)
    else:
        key = slice(key, (key + 1))
self.slice = key
try:
    tempResult = arange(self.obj.size_in)	
===================================================================	
ObjView.__init__: 73	
----------------------------	
self.obj = obj
if isinstance(key, int):
    if (key == (- 1)):
        key = slice(key, None)
    else:
        key = slice(key, (key + 1))
self.slice = key
try:
    self.size_in = np.arange(self.obj.size_in)[self.slice].size
except IndexError:
    self.size_in = None
try:
    tempResult = arange(self.obj.size_out)	
===================================================================	
Simulator.trange: 85	
----------------------------	

'Create a range of times matching probe data.\n\n        Note that the range does not start at 0 as one might expect, but at\n        the first timestep (i.e., dt).\n\n        Parameters\n        ----------\n        dt : float (optional)\n            The sampling period of the probe to create a range for. If empty,\n            will use the default probe sampling period.\n        '
dt = (self.dt if (dt is None) else dt)
n_steps = int((self.n_steps * (self.dt / dt)))
tempResult = arange(1, (n_steps + 1))
	
===================================================================	
remove_imag_rows: 52	
----------------------------	

"Throw away imaginary row we don't need (since they're zero)"
tempResult = arange(tr.shape[0])
	
===================================================================	
dft_half: 60	
----------------------------	

tempResult = arange(n)
	
===================================================================	
dft_half: 61	
----------------------------	

x = numpy.arange(n)
tempResult = arange(((n // 2) + 1))
	
===================================================================	
test_sine_waves: 12	
----------------------------	

radius = 2
dim = 5
product = nengo.networks.Product(200, dim, radius, net=nengo.Network(seed=seed))
tempResult = arange(1, (dim + 1))
	
===================================================================	
test_sine_waves: 13	
----------------------------	

radius = 2
dim = 5
product = nengo.networks.Product(200, dim, radius, net=nengo.Network(seed=seed))
func_A = (lambda t: (numpy.sqrt(radius) * numpy.sin((((numpy.arange(1, (dim + 1)) * 2) * numpy.pi) * t))))
tempResult = arange(dim, 0, (- 1))
	
===================================================================	
convolution: 23	
----------------------------	

'Implement an action_objects.Convolution.\n\n    Parameters\n    ----------\n    module : spa.Module\n        The module that will own this convolution\n    target_name : string\n        The name of the object to send the convolution result to\n    effect : action_objects.Convolution\n        The details of the convolution to implement\n    n_neurons_cconv : int\n        Number of neurons in each product population\n    synapse : float (or nengo.Synapse)\n        The synapse to use for connections into and out of the convolution\n\n    Returns the created nengo.networks.CircularConvolution.\n    '
source1 = effect.source1
source2 = effect.source2
(target, target_vocab) = module.spa.get_module_input(target_name)
(s1_output, s1_vocab) = module.spa.get_module_output(source1.name)
(s2_output, s2_vocab) = module.spa.get_module_output(source2.name)
with module:
    cconv = nengo.networks.CircularConvolution(n_neurons_cconv, s1_vocab.dimensions, invert_a=False, invert_b=False, net=nengo.Network(label=('cconv_%s' % str(effect))))
with module.spa:
    t = s1_vocab.parse(str(effect.transform)).get_convolution_matrix()
    if (target_vocab is not s1_vocab):
        t = numpy.dot(s1_vocab.transform_to(target_vocab), t)
    nengo.Connection(cconv.output, target, transform=t, synapse=synapse)
    t1 = s1_vocab.parse(source1.transform.symbol).get_convolution_matrix()
    if source1.inverted:
        D = s1_vocab.dimensions
        tempResult = arange(D)
	
===================================================================	
convolution: 28	
----------------------------	

'Implement an action_objects.Convolution.\n\n    Parameters\n    ----------\n    module : spa.Module\n        The module that will own this convolution\n    target_name : string\n        The name of the object to send the convolution result to\n    effect : action_objects.Convolution\n        The details of the convolution to implement\n    n_neurons_cconv : int\n        Number of neurons in each product population\n    synapse : float (or nengo.Synapse)\n        The synapse to use for connections into and out of the convolution\n\n    Returns the created nengo.networks.CircularConvolution.\n    '
source1 = effect.source1
source2 = effect.source2
(target, target_vocab) = module.spa.get_module_input(target_name)
(s1_output, s1_vocab) = module.spa.get_module_output(source1.name)
(s2_output, s2_vocab) = module.spa.get_module_output(source2.name)
with module:
    cconv = nengo.networks.CircularConvolution(n_neurons_cconv, s1_vocab.dimensions, invert_a=False, invert_b=False, net=nengo.Network(label=('cconv_%s' % str(effect))))
with module.spa:
    t = s1_vocab.parse(str(effect.transform)).get_convolution_matrix()
    if (target_vocab is not s1_vocab):
        t = numpy.dot(s1_vocab.transform_to(target_vocab), t)
    nengo.Connection(cconv.output, target, transform=t, synapse=synapse)
    t1 = s1_vocab.parse(source1.transform.symbol).get_convolution_matrix()
    if source1.inverted:
        D = s1_vocab.dimensions
        t1 = numpy.dot(t1, numpy.eye(D)[(- numpy.arange(D))])
    nengo.Connection(s1_output, cconv.A, transform=t1, synapse=synapse)
    t2 = s2_vocab.parse(source2.transform.symbol).get_convolution_matrix()
    if source2.inverted:
        D = s2_vocab.dimensions
        tempResult = arange(D)
	
===================================================================	
Cortical.add_route_effect: 59	
----------------------------	

'Connect a module output to a module input\n\n        Parameters\n        ----------\n        target_name : string\n            The name of the module input to affect\n        source_name : string\n            The name of the module output to read from.  If this output uses\n            a different Vocabulary than the target, a linear transform\n            will be applied to convert from one to the other.\n        transform : string\n            A semantic pointer to convolve with the source value before\n            sending it into the target.  This transform takes\n            place in the source Vocabulary.\n        '
(target, target_vocab) = self.spa.get_module_input(target_name)
(source, source_vocab) = self.spa.get_module_output(source_name)
t = source_vocab.parse(transform).get_convolution_matrix()
if inverted:
    D = source_vocab.dimensions
    tempResult = arange(D)
	
===================================================================	
SemanticPointer.__invert__: 124	
----------------------------	

'Return a reorganized vector that acts as an inverse for convolution.\n\n        This reorganization turns circular convolution into circular\n        correlation, meaning that A*B*~B is approximately A.\n\n        For the vector [1,2,3,4,5], the inverse is [1,5,4,3,2].\n        '
tempResult = arange(len(self))
	
===================================================================	
Thalamus.add_route_effect: 93	
----------------------------	

'Set an action to send source to target with the given transform\n\n        Parameters\n        ----------\n        index : int\n            The action number that will cause this effect\n        target_name : string\n            The name of the module input to affect\n        source_name : string\n            The name of the module output to read from.  If this output uses\n            a different Vocabulary than the target, a linear transform\n            will be applied to convert from one to the other.\n        transform : string\n            A semantic point to convolve with the source value before\n            sending it into the target.  This transform takes\n            place in the source Vocabulary.\n        inverted : bool\n            Whether to perform inverse convolution on the source.\n        '
with self:
    gate = self.get_gate(index)
    (target, target_vocab) = self.spa.get_module_input(target_name)
    (source, source_vocab) = self.spa.get_module_output(source_name)
    target_module = self.spa.get_module(target_name)
    dim = target_vocab.dimensions
    subdim = self.subdim_channel
    if isinstance(target_module, nengo.spa.Buffer):
        subdim = target_module.state.dimensions_per_ensemble
    elif (dim < subdim):
        subdim = dim
    elif ((dim % subdim) != 0):
        subdim = 1
    channel = nengo.networks.EnsembleArray((self.neurons_channel_dim * subdim), (dim // subdim), ens_dimensions=subdim, radius=numpy.sqrt((float(subdim) / dim)), label=('channel_%d_%s' % (index, target_name)))
    inhibit = ([[(- self.route_inhibit)]] * (self.neurons_channel_dim * subdim))
    for e in channel.ensembles:
        nengo.Connection(gate, e.neurons, transform=inhibit, synapse=self.synapse_inhibit)
with self.spa:
    t = source_vocab.parse(transform).get_convolution_matrix()
    if inverted:
        D = source_vocab.dimensions
        tempResult = arange(D)
	
===================================================================	
test_lif_builtin: 22	
----------------------------	

'Test that the dynamic model approximately matches the rates.'
dt = 0.001
t_final = 1.0
N = 10
lif = nengo.LIF()
(gain, bias) = lif.gain_bias(rng.uniform(80, 100, size=N), rng.uniform((- 1), 1, size=N))
tempResult = arange((- 2), 2, 0.1)
	
===================================================================	
test_len: 186	
----------------------------	

with nengo.Network():
    n1 = nengo.Node(None, size_in=1)
    n3 = nengo.Node([1, 2, 3])
    tempResult = arange(4)
	
===================================================================	
test_set_output: 216	
----------------------------	

counter = []

def accumulate(t):
    counter.append(t)
    return t

def noreturn(t):
    pass
with nengo.Network() as model:
    with warns(UserWarning):
        passthrough = nengo.Node(None, size_in=20, size_out=30)
    assert (passthrough.output is None)
    assert (passthrough.size_out == 20)
    with pytest.raises(TypeError):
        nengo.Node(numpy.ones(1), size_in=1)
    with pytest.raises(ValueError):
        nengo.Node(numpy.ones(3), size_out=2)
    with pytest.raises(ValueError):
        nengo.Node(numpy.ones((2, 2)))
    scalar = nengo.Node(2)
    assert (scalar.output.shape == (1,))
    assert (str(scalar.output.dtype) == 'float64')
    tempResult = arange(3)
	
===================================================================	
test_multirun: 20	
----------------------------	

'Test probing the time on multiple runs'
rtol = 0.0001
model = nengo.Network(label='Multi-run')
sim = Simulator(model)
t_stops = (sim.dt * rng.randint(low=100, high=2000, size=10))
t_sum = 0
for ti in t_stops:
    sim.run(ti)
    sim_t = sim.trange()
    tempResult = arange(1, (len(sim_t) + 1))
	
===================================================================	
test_whitenoise_high: 103	
----------------------------	

rms = 0.5
d = 500
t = 1000
dt = 0.001
process = WhiteNoise((dt * t), high, rms=rms)
values = nengo.processes.sample(t, process, dt=dt, d=d, rng=rng)
(freq, val_psd) = psd(values)
tempResult = arange(1, (t + 1))
	
===================================================================	
test_brownnoise: 35	
----------------------------	

d = 5000
t = 500
dt = 0.001
samples = nengo.processes.sample(t, BrownNoise(), dt=dt, d=d, rng=rng)
tempResult = arange(1, (t + 1))
	
===================================================================	
test_brownnoise: 36	
----------------------------	

d = 5000
t = 500
dt = 0.001
samples = nengo.processes.sample(t, BrownNoise(), dt=dt, d=d, rng=rng)
trange = (numpy.arange(1, (t + 1)) * dt)
tempResult = arange(t)
	
===================================================================	
test_whitenoise_dt: 125	
----------------------------	

rms = 0.5
high = 10
d = 500
t = 100
dt = 0.01
process = WhiteNoise((dt * t), high, rms=rms)
values = nengo.processes.sample(t, process, dt=dt, d=d, rng=rng)
(freq, val_psd) = psd(values, dt=dt)
tempResult = arange(1, (t + 1))
	
===================================================================	
test_whitenoise_rms: 82	
----------------------------	

d = 500
t = 100
dt = 0.001
process = WhiteNoise((dt * t), rms=rms)
values = nengo.processes.sample(t, process, dt=dt, d=d, rng=rng)
(freq, val_psd) = psd(values)
tempResult = arange(1, (t + 1))
	
===================================================================	
test_gaussian_whitenoise: 62	
----------------------------	

d = 500
t = 100
dt = 0.001
process = StochasticProcess(Gaussian(0.0, rms))
values = nengo.processes.sample(t, process, dt=dt, d=d, rng=rng)
(freq, val_psd) = psd(values)
tempResult = arange(1, (t + 1))
	
===================================================================	
test_trange_with_probes: 41	
----------------------------	

dt = 0.001
m = nengo.Network()
tempResult = arange(1, 21)
	
===================================================================	
test_lti_lowpass: 89	
----------------------------	

dt = 0.001
tend = 3.0
tempResult = arange((tend / dt))
	
===================================================================	
test_filt: 60	
----------------------------	

dt = 0.001
tend = 3.0
tempResult = arange((tend / dt))
	
===================================================================	
test_filt: 64	
----------------------------	

dt = 0.001
tend = 3.0
t = (dt * numpy.arange((tend / dt)))
nt = len(t)
tau = (0.1 / dt)
u = rng.normal(size=nt)
tempResult = arange(0, (30 * tau))
	
===================================================================	
test_filtfilt: 75	
----------------------------	

dt = 0.001
tend = 3.0
tempResult = arange((tend / dt))
	
===================================================================	
full_transform: 23	
----------------------------	

'Compute the full transform for a connection.\n\n    Parameters\n    ----------\n    conn : Connection\n        The connection for which to compute the full transform.\n    slice_pre : boolean, optional (True)\n        Whether to compute the pre slice as part of the transform.\n    slice_post : boolean, optional (True)\n        Whether to compute the post slice as part of the transform.\n    allow_scalars : boolean, optional (True)\n        If true (default), will not make scalars into full transforms when\n        not using slicing, since these work fine in the reference builder.\n        If false, these scalars will be turned into scaled identity matrices.\n    '
transform = conn.transform
pre_slice = (conn.pre_slice if slice_pre else slice(None))
post_slice = (conn.post_slice if slice_post else slice(None))
if ((pre_slice == slice(None)) and (post_slice == slice(None))):
    if (transform.ndim == 2):
        return numpy.array(transform)
    elif ((transform.size == 1) and allow_scalars):
        return numpy.array(transform)
func_size = conn.function_info.size
size_in = ((conn.pre_obj.size_out if (func_size is None) else func_size) if slice_pre else conn.size_mid)
size_out = (conn.post_obj.size_in if slice_post else conn.size_out)
new_transform = numpy.zeros((size_out, size_in))
if (transform.ndim < 2):
    tempResult = arange(size_out)
	
===================================================================	
full_transform: 23	
----------------------------	

'Compute the full transform for a connection.\n\n    Parameters\n    ----------\n    conn : Connection\n        The connection for which to compute the full transform.\n    slice_pre : boolean, optional (True)\n        Whether to compute the pre slice as part of the transform.\n    slice_post : boolean, optional (True)\n        Whether to compute the post slice as part of the transform.\n    allow_scalars : boolean, optional (True)\n        If true (default), will not make scalars into full transforms when\n        not using slicing, since these work fine in the reference builder.\n        If false, these scalars will be turned into scaled identity matrices.\n    '
transform = conn.transform
pre_slice = (conn.pre_slice if slice_pre else slice(None))
post_slice = (conn.post_slice if slice_post else slice(None))
if ((pre_slice == slice(None)) and (post_slice == slice(None))):
    if (transform.ndim == 2):
        return numpy.array(transform)
    elif ((transform.size == 1) and allow_scalars):
        return numpy.array(transform)
func_size = conn.function_info.size
size_in = ((conn.pre_obj.size_out if (func_size is None) else func_size) if slice_pre else conn.size_mid)
size_out = (conn.post_obj.size_in if slice_post else conn.size_out)
new_transform = numpy.zeros((size_out, size_in))
if (transform.ndim < 2):
    tempResult = arange(size_in)
	
===================================================================	
sorted_neurons: 57	
----------------------------	

"Sort neurons in an ensemble by encoder and intercept.\n\n    Parameters\n    ----------\n    ensemble: nengo.Ensemble\n        The population of neurons to be sorted.\n        The ensemble must have its encoders specified.\n\n    iterations: int\n        The number of times to iterate during the sort.\n\n    seed: float\n        A random number seed.\n\n    Returns\n    -------\n    indices: ndarray\n        An array with sorted indices into the neurons in the ensemble\n\n    Examples\n    --------\n\n    You can use this to generate an array of sorted indices for plotting. This\n    can be done after collecting the data. E.g.\n\n    >>> indices = sorted_neurons(simulator, 'My neurons')\n    >>> plt.figure()\n    >>> rasterplot(sim.data['My neurons.spikes'][:,indices])\n\n    Algorithm\n    ---------\n\n    The algorithm is for each encoder in the initial set, randomly\n    pick another encoder and check to see if swapping those two\n    encoders would reduce the average difference between the\n    encoders and their neighbours.  Difference is measured as the\n    dot product.  Each encoder has four neighbours (N, S, E, W),\n    except for the ones on the edges which have fewer (no wrapping).\n    This algorithm is repeated `iterations` times, so a total of\n    `iterations*N` swaps are considered.\n    "
encoders = numpy.array(sim.data[ensemble].encoders)
encoders /= numpy.norm(encoders, axis=1, keepdims=True)
N = encoders.shape[0]
tempResult = arange(N)
	
===================================================================	
lowpass_filter: 41	
----------------------------	

nt = x.shape[(- 1)]
if (kind == 'expon'):
    tempResult = arange(0, (5 * tau))
	
===================================================================	
lowpass_filter: 46	
----------------------------	

nt = x.shape[(- 1)]
if (kind == 'expon'):
    t = numpy.arange(0, (5 * tau))
    kern = (numpy.exp(((- t) / tau)) / tau)
    delay = tau
elif (kind == 'gauss'):
    std = (tau / 2.0)
    tempResult = arange(((- 4) * std), (4 * std))
	
===================================================================	
lowpass_filter: 51	
----------------------------	

nt = x.shape[(- 1)]
if (kind == 'expon'):
    t = numpy.arange(0, (5 * tau))
    kern = (numpy.exp(((- t) / tau)) / tau)
    delay = tau
elif (kind == 'gauss'):
    std = (tau / 2.0)
    t = numpy.arange(((- 4) * std), (4 * std))
    kern = (numpy.exp(((- 0.5) * ((t / std) ** 2))) / numpy.sqrt(((2 * numpy.pi) * (std ** 2))))
    delay = (4 * std)
elif (kind == 'alpha'):
    alpha = (1.0 / tau)
    tempResult = arange(0, (5 * tau))
	
***************************************************	
sympy_sympy-1.0.0: 9	
===================================================================	
test_theano_function_numpy: 131	
----------------------------	

f = theano_function([x, y], [(x + y)], dim=1, dtypes={x: 'float64', y: 'float64'})
assert (numpy.linalg.norm((f([1, 2], [3, 4]) - numpy.asarray([4, 6]))) < 1e-09)
f = theano_function([x, y], [(x + y)], dtypes={x: 'float64', y: 'float64'}, dim=1)
tempResult = arange(3)
	
===================================================================	
test_theano_function_numpy: 132	
----------------------------	

f = theano_function([x, y], [(x + y)], dim=1, dtypes={x: 'float64', y: 'float64'})
assert (numpy.linalg.norm((f([1, 2], [3, 4]) - numpy.asarray([4, 6]))) < 1e-09)
f = theano_function([x, y], [(x + y)], dtypes={x: 'float64', y: 'float64'}, dim=1)
xx = np.arange(3).astype('float64')
tempResult = arange(3)
	
===================================================================	
test_theano_function_numpy: 133	
----------------------------	

f = theano_function([x, y], [(x + y)], dim=1, dtypes={x: 'float64', y: 'float64'})
assert (numpy.linalg.norm((f([1, 2], [3, 4]) - numpy.asarray([4, 6]))) < 1e-09)
f = theano_function([x, y], [(x + y)], dtypes={x: 'float64', y: 'float64'}, dim=1)
xx = np.arange(3).astype('float64')
yy = (2 * np.arange(3).astype('float64'))
tempResult = arange(3)
	
===================================================================	
test_BlockMatrix_Inverse_execution: 193	
----------------------------	

(k, n) = (2, 4)
dtype = 'float32'
A = sympy.MatrixSymbol('A', n, k)
B = sympy.MatrixSymbol('B', n, n)
inputs = (A, B)
output = (B.I * A)
cutsizes = {A: [((n // 2), (n // 2)), ((k // 2), (k // 2))], B: [((n // 2), (n // 2)), ((n // 2), (n // 2))]}
cutinputs = [sympy.blockcut(i, *cutsizes[i]) for i in inputs]
cutoutput = output.subs(dict(zip(inputs, cutinputs)))
dtypes = dict(zip(inputs, ([dtype] * len(inputs))))
f = theano_function(inputs, [output], dtypes=dtypes, cache={})
fblocked = theano_function(inputs, [sympy.block_collapse(cutoutput)], dtypes=dtypes, cache={})
ninputs = [np.random.rand(*x.shape).astype(dtype) for x in inputs]
tempResult = arange((n * k))
	
===================================================================	
test_theano_function_kwargs: 140	
----------------------------	

import numpy as np
f = theano_function([x, y, z], [(x + y)], dim=1, on_unused_input='ignore', dtypes={x: 'float64', y: 'float64', z: 'float64'})
assert (numpy.linalg.norm((f([1, 2], [3, 4], [0, 0]) - numpy.asarray([4, 6]))) < 1e-09)
f = theano_function([x, y, z], [(x + y)], dtypes={x: 'float64', y: 'float64', z: 'float64'}, dim=1, on_unused_input='ignore')
tempResult = arange(3)
	
===================================================================	
test_theano_function_kwargs: 141	
----------------------------	

import numpy as np
f = theano_function([x, y, z], [(x + y)], dim=1, on_unused_input='ignore', dtypes={x: 'float64', y: 'float64', z: 'float64'})
assert (numpy.linalg.norm((f([1, 2], [3, 4], [0, 0]) - numpy.asarray([4, 6]))) < 1e-09)
f = theano_function([x, y, z], [(x + y)], dtypes={x: 'float64', y: 'float64', z: 'float64'}, dim=1, on_unused_input='ignore')
xx = np.arange(3).astype('float64')
tempResult = arange(3)
	
===================================================================	
test_theano_function_kwargs: 142	
----------------------------	

import numpy as np
f = theano_function([x, y, z], [(x + y)], dim=1, on_unused_input='ignore', dtypes={x: 'float64', y: 'float64', z: 'float64'})
assert (numpy.linalg.norm((f([1, 2], [3, 4], [0, 0]) - numpy.asarray([4, 6]))) < 1e-09)
f = theano_function([x, y, z], [(x + y)], dtypes={x: 'float64', y: 'float64', z: 'float64'}, dim=1, on_unused_input='ignore')
xx = np.arange(3).astype('float64')
yy = (2 * np.arange(3).astype('float64'))
tempResult = arange(3)
	
===================================================================	
test_theano_function_kwargs: 143	
----------------------------	

import numpy as np
f = theano_function([x, y, z], [(x + y)], dim=1, on_unused_input='ignore', dtypes={x: 'float64', y: 'float64', z: 'float64'})
assert (numpy.linalg.norm((f([1, 2], [3, 4], [0, 0]) - numpy.asarray([4, 6]))) < 1e-09)
f = theano_function([x, y, z], [(x + y)], dtypes={x: 'float64', y: 'float64', z: 'float64'}, dim=1, on_unused_input='ignore')
xx = np.arange(3).astype('float64')
yy = (2 * np.arange(3).astype('float64'))
zz = (2 * np.arange(3).astype('float64'))
tempResult = arange(3)
	
===================================================================	
test_numpy_piecewise: 308	
----------------------------	

if (not numpy):
    skip('numpy not installed.')
pieces = Piecewise((x, (x < 3)), ((x ** 2), (x > 5)), (0, True))
f = lambdify(x, pieces, modules='numpy')
tempResult = arange(10)
	
***************************************************	
daducci_amico-dev: 0	
***************************************************	
aplpy_aplpy-1.1.1: 15	
===================================================================	
complete_range: 31	
----------------------------	

if ((xmax - xmin) < 1):
    spacing = 10
xstep = ((xmax - xmin) / float(spacing))
tempResult = arange(xmin, xmax, xstep)
	
===================================================================	
tick_positions: 344	
----------------------------	

"\n    Find positions of ticks along a given axis.\n\n    Parameters\n    ----------\n\n    wcs : ~aplpy.wcs_util.WCS\n       The WCS instance for the image.\n\n    spacing : float\n       The spacing along the axis.\n\n    axis : { 'x', 'y' }\n       The axis along which we are looking for ticks.\n\n    coord : { 'x', 'y' }\n       The coordinate for which we are looking for ticks.\n\n    farside : bool, optional\n       Whether we are looking on the left or bottom axes (False) or the\n       right or top axes (True).\n\n    xmin, xmax, ymin, ymax : float, optional\n       The range of pixel values covered by the image.\n\n    mode : { 'xy', 'xscaled' }, optional\n       If set to 'xy' the function returns the world coordinates of the\n       ticks. If 'xscaled', then only the coordinate requested is\n       returned, in units of the tick spacing.\n    "
(px, py, wx, wy) = axis_positions(wcs, axis, farside, xmin, xmax, ymin, ymax)
if (coord == 'x'):
    (warr, walt) = (wx, wy)
else:
    (warr, walt) = (wy, wx)
if (((coord == 'x') and (wcs.xaxis_coord_type == 'longitude')) or ((coord == 'y') and (wcs.yaxis_coord_type == 'longitude'))):
    for i in range(0, (len(warr) - 1)):
        if (abs((warr[i] - warr[(i + 1)])) > 180.0):
            if (warr[i] > warr[(i + 1)]):
                warr[(i + 1):] = (warr[(i + 1):] + 360.0)
            else:
                warr[(i + 1):] = (warr[(i + 1):] - 360.0)
warr = (warr / spacing)
iall = []
wall = []
tempResult = arange(numpy.floor(min(warr)), numpy.ceil(max(warr)), 1.0)
	
===================================================================	
WCS.__init__: 31	
----------------------------	

if ('slices' in kwargs):
    self._slices = kwargs.pop('slices')
else:
    self._slices = []
if ('dimensions' in kwargs):
    self._dimensions = kwargs.pop('dimensions')
else:
    self._dimensions = [0, 1]
astropy.wcs.WCS.__init__(self, *args, **kwargs)
self.wcs.unitfix()
if (len(self._slices) > 0):
    self.nx = args[0][('NAXIS%i' % (self._dimensions[0] + 1))]
    self.ny = args[0][('NAXIS%i' % (self._dimensions[1] + 1))]
    tempResult = arange(self.nx)
	
===================================================================	
WCS.__init__: 32	
----------------------------	

if ('slices' in kwargs):
    self._slices = kwargs.pop('slices')
else:
    self._slices = []
if ('dimensions' in kwargs):
    self._dimensions = kwargs.pop('dimensions')
else:
    self._dimensions = [0, 1]
astropy.wcs.WCS.__init__(self, *args, **kwargs)
self.wcs.unitfix()
if (len(self._slices) > 0):
    self.nx = args[0][('NAXIS%i' % (self._dimensions[0] + 1))]
    self.ny = args[0][('NAXIS%i' % (self._dimensions[1] + 1))]
    xpix = (numpy.arange(self.nx) + 1.0)
    tempResult = arange(self.ny)
	
===================================================================	
test_numpy_contour: 10	
----------------------------	

tempResult = arange(256)
	
===================================================================	
test_numpy_downsample: 9	
----------------------------	

tempResult = arange(256)
	
===================================================================	
TestBasic.test_contours: 103	
----------------------------	

tempResult = arange(256)
	
===================================================================	
module: 12	
----------------------------	

import os
import matplotlib
matplotlib.use('Agg')
import numpy as np
from astropy.table import Table
from astropy.tests.helper import pytest
from .helpers import generate_wcs
from .. import wcs_util
HEADER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data/2d_fits', '1904-66_TAN.hdr')
tab = Table({'RA': [347.0, 349.0], 'DEC': [(- 68.0), (- 68)]})
tempResult = arange(2)
	
===================================================================	
module: 12	
----------------------------	

import os
import matplotlib
matplotlib.use('Agg')
import numpy as np
from astropy.table import Table
from astropy.tests.helper import pytest
from .helpers import generate_wcs
from .. import wcs_util
HEADER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data/2d_fits', '1904-66_TAN.hdr')
tab = Table({'RA': [347.0, 349.0], 'DEC': [(- 68.0), (- 68)]})
tempResult = arange(2)
	
===================================================================	
module: 13	
----------------------------	

import os
import matplotlib
matplotlib.use('Agg')
import numpy as np
from astropy.table import Table
from astropy.tests.helper import pytest
from .helpers import generate_wcs
from .. import wcs_util
HEADER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data/2d_fits', '1904-66_TAN.hdr')
tab = Table({'RA': [347.0, 349.0], 'DEC': [(- 68.0), (- 68)]})
GOOD_INPUT = [[1.0, 2.0], [[1.0, 2.0], [3, 4]], [numpy.arange(2), numpy.arange(2)], [tab['RA'], tab['DEC']]]
tempResult = arange(2)
	
===================================================================	
test_returntypes: 29	
----------------------------	

wcs = generate_wcs(HEADER)
(ra, dec) = wcs_util.pix2world(wcs, 1.0, 2.0)
assert (numpy.isscalar(ra) and numpy.isscalar(dec))
(ra, dec) = wcs_util.pix2world(wcs, [1.0], [2.0])
assert ((type(ra) == list) and (type(dec) == list))
tempResult = arange(2)
	
===================================================================	
module: 13	
----------------------------	

import os
import matplotlib
matplotlib.use('Agg')
import numpy as np
from astropy.table import Table
from astropy.tests.helper import pytest
from astropy.io import fits
from .helpers import generate_wcs
from .. import FITSFigure
HEADER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data/2d_fits', '1904-66_TAN.hdr')
tab = Table({'RA': [347.0, 349.0], 'DEC': [(- 68.0), (- 68)]})
tempResult = arange(2)
	
===================================================================	
module: 13	
----------------------------	

import os
import matplotlib
matplotlib.use('Agg')
import numpy as np
from astropy.table import Table
from astropy.tests.helper import pytest
from astropy.io import fits
from .helpers import generate_wcs
from .. import FITSFigure
HEADER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data/2d_fits', '1904-66_TAN.hdr')
tab = Table({'RA': [347.0, 349.0], 'DEC': [(- 68.0), (- 68)]})
tempResult = arange(2)
	
===================================================================	
module: 14	
----------------------------	

import os
import matplotlib
matplotlib.use('Agg')
import numpy as np
from astropy.table import Table
from astropy.tests.helper import pytest
from astropy.io import fits
from .helpers import generate_wcs
from .. import FITSFigure
HEADER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data/2d_fits', '1904-66_TAN.hdr')
tab = Table({'RA': [347.0, 349.0], 'DEC': [(- 68.0), (- 68)]})
GOOD_INPUT = [[1, 2], [[1, 2], [3, 4]], [numpy.arange(2), numpy.arange(2)], [tab['RA'], tab['DEC']]]
tempResult = arange(2)
	
===================================================================	
module: 12	
----------------------------	

import numpy as np
from astropy.tests.helper import pytest, remote_data
from ..core import FITSFigure
from .test_images import BaseImageTests
from . import baseline_dir
x = numpy.linspace((- 1.0), 1.0, 10)
y = numpy.linspace((- 1.0), 1.0, 10)
(X, Y) = numpy.meshgrid(x, y)
numpy.random.seed(12345)
IMAGE = numpy.random.random((10, 10))
tempResult = arange(100)
	
***************************************************	
markovmodel_msmtools-1.0.2: 23	
===================================================================	
forward_committor_sensitivity: 12	
----------------------------	

' \n    calculate the sensitivity matrix for index of the forward committor from A to B given transition matrix T.\n    Parameters\n    ----------\n    T : numpy.ndarray shape = (n, n)\n        Transition matrix\n    A : array like\n        List of integer state labels for set A\n    B : array like\n        List of integer state labels for set B\n    index : entry of the committor for which the sensitivity is to be computed\n        \n    Returns\n    -------\n    x : ndarray, shape=(n, n)\n        Sensitivity matrix for entry index around transition matrix T. Reversibility is not assumed.\n    '
n = len(T)
tempResult = arange(n)
	
===================================================================	
random_nonempty_rows: 33	
----------------------------	

'Generate a random sparse matrix with nonempty rows'
N_el = int(((density * M) * N))
if (N_el < M):
    raise ValueError('Density too small to obtain nonempty rows')
else:
    rows = numpy.zeros(N_el)
    tempResult = arange(M)
	
===================================================================	
expected_counts: 21	
----------------------------	

'Compute expected transition counts for Markov chain after N steps.\n\n    Expected counts are computed according to ..math::\n\n    E[C_{ij}^{(n)}]=\\sum_{k=0}^{N-1} (p_0^T T^{k})_{i} p_{ij}\n\n    Parameters\n    ----------\n    p0 : (M,) ndarray\n        Starting (probability) vector of the chain.\n    T : (M, M) sparse matrix\n        Transition matrix of the chain.\n    N : int\n        Number of steps to take from initial state.\n\n    Returns\n    --------\n    EC : (M, M) sparse matrix\n        Expected value for transition counts after N steps. \n\n    '
if (N <= 0):
    EC = coo_matrix(T.shape, dtype=float)
    return EC
else:
    'Probability vector after (k=0) propagations'
    p_k = (1.0 * p0)
    'Sum of vectors after (k=0) propagations'
    p_sum = (1.0 * p_k)
    'Transpose T to use sparse dot product'
    Tt = T.transpose()
    tempResult = arange((N - 1))
	
===================================================================	
random_orthonormal_sparse_vectors: 18	
----------------------------	

'Generate a random set of k orthonormal sparse vectors \n\n    The algorithm draws random indices, {i_1,...,i_k}, from the set\n    of all possible indices, {0,...,d-1}, without replacement.\n    Random sparse vectors v are given by\n\n    v[i]=k^{-1/2} for i in {i_1,...,i_k} and zero elsewhere.\n\n    '
indices = choice(d, replace=False, size=(k * k))
tempResult = arange(0, (k * (k + 1)), k)
	
===================================================================	
random_nonempty_rows: 71	
----------------------------	

'Generate a random sparse matrix with nonempty rows'
N_el = int(((density * M) * N))
if (N_el < M):
    raise ValueError('Density too small to obtain nonempty rows')
else:
    rows = numpy.zeros(N_el)
    tempResult = arange(M)
	
===================================================================	
random_orthonormal_sparse_vectors: 87	
----------------------------	

'Generate a random set of k orthonormal sparse vectors \n\n    The algorithm draws random indices, {i_1,...,i_k}, from the set\n    of all possible indices, {0,...,d-1}, without replacement.\n    Random sparse vectors v are given by\n\n    v[i]=k^{-1/2} for i in {i_1,...,i_k} and zero elsewhere.\n\n    '
indices = choice(d, replace=False, size=(k * k))
tempResult = arange(0, (k * (k + 1)), k)
	
===================================================================	
TestWriteDiscreteTrajectory.setUp: 29	
----------------------------	

self.filename = (testpath + 'out_dtraj.dat')
tempResult = arange(10000)
	
===================================================================	
TestSaveDiscreteTrajectory.setUp: 56	
----------------------------	

self.filename = (testpath + 'out_dtraj.npy')
tempResult = arange(10000)
	
===================================================================	
connected_sets: 14	
----------------------------	

'Compute connected components for a directed graph with weights\n    represented by the given count matrix.\n\n    Parameters\n    ----------\n    C : scipy.sparse matrix or numpy ndarray \n        square matrix specifying edge weights.\n    directed : bool, optional\n       Whether to compute connected components for a directed  or\n       undirected graph. Default is True.       \n\n    Returns\n    -------\n    cc : list of arrays of integers\n        Each entry is an array containing all vertices (states) in \n        the corresponding connected component.\n\n    '
M = C.shape[0]
' Compute connected components of C. nc is the number of\n    components, indices contain the component labels of the states\n    '
(nc, indices) = scipy.sparse.csgraph.connected_components(C, directed=directed, connection='strong')
tempResult = arange(M)
	
===================================================================	
total_flux: 73	
----------------------------	

'Compute the total flux, or turnover flux, that is produced by the\n        flux sources and consumed by the flux sinks\n    \n    Parameters\n    ----------\n    F : (n, n) ndarray\n        Matrix of flux values between pairs of states.\n    A : array_like (optional)\n        List of integer state labels for set A (reactant)\n    \n    Returns\n    -------\n    F : float\n        The total flux, or turnover flux, that is produced by the\n        flux sources and consumed by the flux sinks\n    \n    '
if (A is None):
    prod = flux_production(F)
    zeros = numpy.zeros(len(prod))
    outflux = numpy.sum(numpy.maximum(prod, zeros))
    return outflux
else:
    tempResult = arange(F.shape[0])
	
===================================================================	
total_flux: 57	
----------------------------	

'Compute the total flux between reactant and product.\n    \n    Parameters\n    ----------\n    flux : (M, M) scipy.sparse matrix\n        Matrix of flux values between pairs of states.\n    A : array_like\n        List of integer state labels for set A (reactant)\n    \n    Returns\n    -------\n    F : float\n        The total flux between reactant and product\n    \n    '
tempResult = arange(flux.shape[0])
	
===================================================================	
TestSaveMatrixSparse.setUp: 311	
----------------------------	

self.filename_int = (testpath + 'spmatrix_int_out.coo.npy')
self.filename_float = (testpath + 'spmatrix_float_out.coo.npy')
self.filename_complex = (testpath + 'spmatrix_complex_out.coo.npy')
'Tri-diagonal test matrices'
dim = 10
tempResult = arange(0, dim)
	
===================================================================	
TestSaveMatrixSparse.setUp: 312	
----------------------------	

self.filename_int = (testpath + 'spmatrix_int_out.coo.npy')
self.filename_float = (testpath + 'spmatrix_float_out.coo.npy')
self.filename_complex = (testpath + 'spmatrix_complex_out.coo.npy')
'Tri-diagonal test matrices'
dim = 10
d0 = numpy.arange(0, dim)
tempResult = arange(dim, ((2 * dim) - 1))
	
===================================================================	
TestSaveMatrixSparse.setUp: 313	
----------------------------	

self.filename_int = (testpath + 'spmatrix_int_out.coo.npy')
self.filename_float = (testpath + 'spmatrix_float_out.coo.npy')
self.filename_complex = (testpath + 'spmatrix_complex_out.coo.npy')
'Tri-diagonal test matrices'
dim = 10
d0 = numpy.arange(0, dim)
d1 = numpy.arange(dim, ((2 * dim) - 1))
tempResult = arange((2 * dim), ((3 * dim) - 1))
	
===================================================================	
TestWriteMatrixSparse.setUp: 170	
----------------------------	

self.filename_int = (testpath + 'spmatrix_int_out.coo.dat')
self.filename_float = (testpath + 'spmatrix_float_out.coo.dat')
self.filename_complex = (testpath + 'spmatrix_complex_out.coo.dat')
'Tri-diagonal test matrices'
dim = 10
tempResult = arange(0, dim)
	
===================================================================	
TestWriteMatrixSparse.setUp: 171	
----------------------------	

self.filename_int = (testpath + 'spmatrix_int_out.coo.dat')
self.filename_float = (testpath + 'spmatrix_float_out.coo.dat')
self.filename_complex = (testpath + 'spmatrix_complex_out.coo.dat')
'Tri-diagonal test matrices'
dim = 10
d0 = numpy.arange(0, dim)
tempResult = arange(dim, ((2 * dim) - 1))
	
===================================================================	
TestWriteMatrixSparse.setUp: 172	
----------------------------	

self.filename_int = (testpath + 'spmatrix_int_out.coo.dat')
self.filename_float = (testpath + 'spmatrix_float_out.coo.dat')
self.filename_complex = (testpath + 'spmatrix_complex_out.coo.dat')
'Tri-diagonal test matrices'
dim = 10
d0 = numpy.arange(0, dim)
d1 = numpy.arange(dim, ((2 * dim) - 1))
tempResult = arange((2 * dim), ((3 * dim) - 1))
	
===================================================================	
TestSaveMatrixDense.setUp: 232	
----------------------------	

self.filename_int = (testpath + 'matrix_int_out.npy')
self.filename_float = (testpath + 'matrix_float_out.npy')
self.filename_complex = (testpath + 'matrix_complex_out.npy')
tempResult = arange((3 * 3))
	
===================================================================	
TestSaveMatrixDense.setUp: 234	
----------------------------	

self.filename_int = (testpath + 'matrix_int_out.npy')
self.filename_float = (testpath + 'matrix_float_out.npy')
self.filename_complex = (testpath + 'matrix_complex_out.npy')
self.A_int = np.arange((3 * 3)).reshape(3, 3)
self.A_float = (1.0 * self.A_int)
tempResult = arange((3 * 3))
	
===================================================================	
TestSaveMatrixDense.setUp: 234	
----------------------------	

self.filename_int = (testpath + 'matrix_int_out.npy')
self.filename_float = (testpath + 'matrix_float_out.npy')
self.filename_complex = (testpath + 'matrix_complex_out.npy')
self.A_int = np.arange((3 * 3)).reshape(3, 3)
self.A_float = (1.0 * self.A_int)
tempResult = arange(9, ((3 * 3) + 9))
	
===================================================================	
TestWriteMatrixDense.setUp: 91	
----------------------------	

self.filename_int = (testpath + 'matrix_int_out.dat')
self.filename_float = (testpath + 'matrix_float_out.dat')
self.filename_complex = (testpath + 'matrix_complex_out.dat')
tempResult = arange((3 * 3))
	
===================================================================	
TestWriteMatrixDense.setUp: 93	
----------------------------	

self.filename_int = (testpath + 'matrix_int_out.dat')
self.filename_float = (testpath + 'matrix_float_out.dat')
self.filename_complex = (testpath + 'matrix_complex_out.dat')
self.A_int = np.arange((3 * 3)).reshape(3, 3)
self.A_float = (1.0 * self.A_int)
tempResult = arange((3 * 3))
	
===================================================================	
TestWriteMatrixDense.setUp: 93	
----------------------------	

self.filename_int = (testpath + 'matrix_int_out.dat')
self.filename_float = (testpath + 'matrix_float_out.dat')
self.filename_complex = (testpath + 'matrix_complex_out.dat')
self.A_int = np.arange((3 * 3)).reshape(3, 3)
self.A_float = (1.0 * self.A_int)
tempResult = arange(9, ((3 * 3) + 9))
	
***************************************************	
nilearn_nilearn-0.4.0: 59	
===================================================================	
_detrend: 50	
----------------------------	

'Detrend columns of input array.\n\n    Signals are supposed to be columns of `signals`.\n    This function is significantly faster than scipy.signal.detrend on this\n    case and uses a lot less memory.\n\n    Parameters\n    ----------\n    signals : numpy.ndarray\n        This parameter must be two-dimensional.\n        Signals to detrend. A signal is a column.\n\n    inplace : bool, optional\n        Tells if the computation must be made inplace or not (default\n        False).\n\n    type : str, optional\n        Detrending type ("linear" or "constant").\n        See also scipy.signal.detrend.\n\n    n_batches : int, optional\n        number of batches to use in the computation. Tweaking this value\n        can lead to variation of memory usage and computation time. The higher\n        the value, the lower the memory consumption.\n\n    Returns\n    -------\n    detrended_signals: numpy.ndarray\n        Detrended signals. The shape is that of \'signals\'.\n\n    Notes\n    -----\n\n    If a signal of lenght 1 is given, it is returned unchanged.\n\n    '
signals = as_float_array(signals, copy=(not inplace))
if (signals.shape[0] == 1):
    warnings.warn('Detrending of 3D signal has been requested but would lead to zero values. Skipping.')
    return signals
signals -= numpy.mean(signals, axis=0)
if (type == 'linear'):
    tempResult = arange(signals.shape[0], dtype=signals.dtype)
	
===================================================================	
test_geometric_mean_geodesic: 94	
----------------------------	

n_matrices = 10
n_features = 6
tempResult = arange(n_features)
	
===================================================================	
test_geometric_mean_geodesic: 94	
----------------------------	

n_matrices = 10
n_features = 6
tempResult = arange(n_features)
	
===================================================================	
test_geometric_mean_geodesic: 96	
----------------------------	

n_matrices = 10
n_features = 6
sym = (numpy.arange(n_features) / numpy.linalg.norm(numpy.arange(n_features)))
sym = (sym * sym[:, numpy.newaxis])
tempResult = arange(n_matrices)
	
===================================================================	
fetch_haxby: 54	
----------------------------	

"Download and loads complete haxby dataset\n\n    Parameters\n    ----------\n    data_dir: string, optional\n        Path of the data directory. Used to force data storage in a specified\n        location. Default: None\n\n    n_subjects: int, optional\n        Number of subjects, from 1 to 6.\n\n        NOTE: n_subjects is deprecated from 0.2.6 and will be removed in next\n        release. Use `subjects` instead.\n\n    subjects : list or int, optional\n        Either a list of subjects or the number of subjects to load, from 1 to\n        6. By default, 2nd subject will be loaded. Empty list returns no subject\n        data.\n\n    fetch_stimuli: boolean, optional\n        Indicate if stimuli images must be downloaded. They will be presented\n        as a dictionnary of categories.\n\n    Returns\n    -------\n    data: sklearn.datasets.base.Bunch\n        Dictionary-like object, the interest attributes are :\n        'anat': string list. Paths to anatomic images.\n        'func': string list. Paths to nifti file with bold data.\n        'session_target': string list. Paths to text file containing\n        session and target data.\n        'mask': string. Path to fullbrain mask file.\n        'mask_vt': string list. Paths to nifti ventral temporal mask file.\n        'mask_face': string list. Paths to nifti ventral temporal mask file.\n        'mask_house': string list. Paths to nifti ventral temporal mask file.\n        'mask_face_little': string list. Paths to nifti ventral temporal\n        mask file.\n        'mask_house_little': string list. Paths to nifti ventral temporal\n        mask file.\n\n    References\n    ----------\n    `Haxby, J., Gobbini, M., Furey, M., Ishai, A., Schouten, J.,\n    and Pietrini, P. (2001). Distributed and overlapping representations of\n    faces and objects in ventral temporal cortex. Science 293, 2425-2430.`\n\n    Notes\n    -----\n    PyMVPA provides a tutorial making use of this dataset:\n    http://www.pymvpa.org/tutorial.html\n\n    More information about its structure:\n    http://dev.pymvpa.org/datadb/haxby2001.html\n\n    See `additional information\n    <http://www.sciencemag.org/content/293/5539/2425>`\n\n    Run 8 in subject 5 does not contain any task labels.\n    The anatomical image for subject 6 is unavailable.\n    "
if (n_subjects is not None):
    warnings.warn("The parameter 'n_subjects' is deprecated from 0.2.6 and will be removed in nilearn next release. Use parameter 'subjects' instead.")
    subjects = n_subjects
if (isinstance(subjects, numbers.Number) and (subjects > 6)):
    subjects = 6
if ((subjects is not None) and (isinstance(subjects, list) or isinstance(subjects, tuple))):
    for sub_id in subjects:
        if (sub_id not in [1, 2, 3, 4, 5, 6]):
            raise ValueError('You provided invalid subject id {0} in a list. Subjects must be selected in [1, 2, 3, 4, 5, 6]'.format(sub_id))
dataset_name = 'haxby2001'
data_dir = _get_dataset_dir(dataset_name, data_dir=data_dir, verbose=verbose)
url_mask = 'https://www.nitrc.org/frs/download.php/7868/mask.nii.gz'
mask = _fetch_files(data_dir, [('mask.nii.gz', url_mask, {})], verbose=verbose)[0]
if (url is None):
    url = 'http://data.pymvpa.org/datasets/haxby2001/'
md5sums = _fetch_files(data_dir, [('MD5SUMS', (url + 'MD5SUMS'), {})], verbose=verbose)[0]
md5sums = _read_md5_sum_file(md5sums)
sub_files = ['bold.nii.gz', 'labels.txt', 'mask4_vt.nii.gz', 'mask8b_face_vt.nii.gz', 'mask8b_house_vt.nii.gz', 'mask8_face_vt.nii.gz', 'mask8_house_vt.nii.gz', 'anat.nii.gz']
n_files = len(sub_files)
if (subjects is None):
    subjects = []
if isinstance(subjects, numbers.Number):
    tempResult = arange(1, (subjects + 1))
	
===================================================================	
_load_mixed_gambles: 336	
----------------------------	

'Ravel zmaps (one per subject) along time axis, resulting,\n    in a n_subjects * n_trials 3D niimgs and, and then make\n    gain vector y of same length.\n    '
X = []
y = []
mask = []
for zmap_img in zmap_imgs:
    this_X = zmap_img.get_data()
    affine = zmap_img.affine
    finite_mask = numpy.all(numpy.isfinite(this_X), axis=(- 1))
    this_mask = numpy.logical_and(numpy.all((this_X != 0), axis=(- 1)), finite_mask)
    tempResult = arange(1, 9)
	
===================================================================	
fetch_localizer_contrasts: 190	
----------------------------	

'Download and load Brainomics Localizer dataset (94 subjects).\n\n    "The Functional Localizer is a simple and fast acquisition\n    procedure based on a 5-minute functional magnetic resonance\n    imaging (fMRI) sequence that can be run as easily and as\n    systematically as an anatomical scan. This protocol captures the\n    cerebral bases of auditory and visual perception, motor actions,\n    reading, language comprehension and mental calculation at an\n    individual level. Individual functional maps are reliable and\n    quite precise. The procedure is decribed in more detail on the\n    Functional Localizer page."\n    (see http://brainomics.cea.fr/localizer/)\n\n    "Scientific results obtained using this dataset are described in\n    Pinel et al., 2007" [1]\n\n    Parameters\n    ----------\n    contrasts: list of str\n        The contrasts to be fetched (for all 94 subjects available).\n        Allowed values are::\n\n            {"checkerboard",\n            "horizontal checkerboard",\n            "vertical checkerboard",\n            "horizontal vs vertical checkerboard",\n            "vertical vs horizontal checkerboard",\n            "sentence listening",\n            "sentence reading",\n            "sentence listening and reading",\n            "sentence reading vs checkerboard",\n            "calculation (auditory cue)",\n            "calculation (visual cue)",\n            "calculation (auditory and visual cue)",\n            "calculation (auditory cue) vs sentence listening",\n            "calculation (visual cue) vs sentence reading",\n            "calculation vs sentences",\n            "calculation (auditory cue) and sentence listening",\n            "calculation (visual cue) and sentence reading",\n            "calculation and sentence listening/reading",\n            "calculation (auditory cue) and sentence listening vs "\n            "calculation (visual cue) and sentence reading",\n            "calculation (visual cue) and sentence reading vs checkerboard",\n            "calculation and sentence listening/reading vs button press",\n            "left button press (auditory cue)",\n            "left button press (visual cue)",\n            "left button press",\n            "left vs right button press",\n            "right button press (auditory cue)",\n            "right button press (visual cue)",\n            "right button press",\n            "right vs left button press",\n            "button press (auditory cue) vs sentence listening",\n            "button press (visual cue) vs sentence reading",\n            "button press vs calculation and sentence listening/reading"}\n\n        or equivalently on can use the original names::\n\n            {"checkerboard",\n            "horizontal checkerboard",\n            "vertical checkerboard",\n            "horizontal vs vertical checkerboard",\n            "vertical vs horizontal checkerboard",\n            "auditory sentences",\n            "visual sentences",\n            "auditory&visual sentences",\n            "visual sentences vs checkerboard",\n            "auditory calculation",\n            "visual calculation",\n            "auditory&visual calculation",\n            "auditory calculation vs auditory sentences",\n            "visual calculation vs sentences",\n            "auditory&visual calculation vs sentences",\n            "auditory processing",\n            "visual processing",\n            "visual processing vs auditory processing",\n            "auditory processing vs visual processing",\n            "visual processing vs checkerboard",\n            "cognitive processing vs motor",\n            "left auditory click",\n            "left visual click",\n            "left auditory&visual click",\n            "left auditory & visual click vs right auditory&visual click",\n            "right auditory click",\n            "right visual click",\n            "right auditory&visual click",\n            "right auditory & visual click vs left auditory&visual click",\n            "auditory click vs auditory sentences",\n            "visual click vs visual sentences",\n            "auditory&visual motor vs cognitive processing"}\n\n    n_subjects: int or list, optional\n        The number or list of subjects to load. If None is given,\n        all 94 subjects are used.\n\n    get_tmaps: boolean\n        Whether t maps should be fetched or not.\n\n    get_masks: boolean\n        Whether individual masks should be fetched or not.\n\n    get_anats: boolean\n        Whether individual structural images should be fetched or not.\n\n    data_dir: string, optional\n        Path of the data directory. Used to force data storage in a specified\n        location.\n\n    url: string, optional\n        Override download URL. Used for test only (or if you setup a mirror of\n        the data).\n\n    resume: bool\n        Whether to resume download of a partly-downloaded file.\n\n    verbose: int\n        Verbosity level (0 means no message).\n\n    Returns\n    -------\n    data: Bunch\n        Dictionary-like object, the interest attributes are :\n\n        - \'cmaps\': string list\n            Paths to nifti contrast maps\n        - \'tmaps\' string list (if \'get_tmaps\' set to True)\n            Paths to nifti t maps\n        - \'masks\': string list\n            Paths to nifti files corresponding to the subjects individual masks\n        - \'anats\': string\n            Path to nifti files corresponding to the subjects structural images\n\n    References\n    ----------\n    Pinel, Philippe, et al.\n    "Fast reproducible identification and large-scale databasing of\n    individual functional cognitive networks."\n    BMC neuroscience 8.1 (2007): 91.\n\n    See Also\n    ---------\n    nilearn.datasets.fetch_localizer_calculation_task\n    nilearn.datasets.fetch_localizer_button_task\n\n    '
if isinstance(contrasts, _basestring):
    raise ValueError(('Contrasts should be a list of strings, but a single string was given: "%s"' % contrasts))
if (n_subjects is None):
    n_subjects = 94
if (isinstance(n_subjects, numbers.Number) and ((n_subjects > 94) or (n_subjects < 1))):
    warnings.warn("Wrong value for 'n_subjects' (%d). The maximum value will be used instead ('n_subjects=94')")
    n_subjects = 94
contrast_name_wrapper = {'checkerboard': 'checkerboard', 'horizontal checkerboard': 'horizontal checkerboard', 'vertical checkerboard': 'vertical checkerboard', 'horizontal vs vertical checkerboard': 'horizontal vs vertical checkerboard', 'vertical vs horizontal checkerboard': 'vertical vs horizontal checkerboard', 'sentence listening': 'auditory sentences', 'sentence reading': 'visual sentences', 'sentence listening and reading': 'auditory&visual sentences', 'sentence reading vs checkerboard': 'visual sentences vs checkerboard', 'calculation (auditory cue)': 'auditory calculation', 'calculation (visual cue)': 'visual calculation', 'calculation (auditory and visual cue)': 'auditory&visual calculation', 'calculation (auditory cue) vs sentence listening': 'auditory calculation vs auditory sentences', 'calculation (visual cue) vs sentence reading': 'visual calculation vs sentences', 'calculation vs sentences': 'auditory&visual calculation vs sentences', 'calculation (auditory cue) and sentence listening': 'auditory processing', 'calculation (visual cue) and sentence reading': 'visual processing', 'calculation (visual cue) and sentence reading vs calculation (auditory cue) and sentence listening': 'visual processing vs auditory processing', 'calculation (auditory cue) and sentence listening vs calculation (visual cue) and sentence reading': 'auditory processing vs visual processing', 'calculation (visual cue) and sentence reading vs checkerboard': 'visual processing vs checkerboard', 'calculation and sentence listening/reading vs button press': 'cognitive processing vs motor', 'left button press (auditory cue)': 'left auditory click', 'left button press (visual cue)': 'left visual click', 'left button press': 'left auditory&visual click', 'left vs right button press': ('left auditory & visual click vs ' + 'right auditory&visual click'), 'right button press (auditory cue)': 'right auditory click', 'right button press (visual cue)': 'right visual click', 'right button press': 'right auditory & visual click', 'right vs left button press': ('right auditory & visual click ' + 'vs left auditory&visual click'), 'button press (auditory cue) vs sentence listening': 'auditory click vs auditory sentences', 'button press (visual cue) vs sentence reading': 'visual click vs visual sentences', 'button press vs calculation and sentence listening/reading': 'auditory&visual motor vs cognitive processing'}
allowed_contrasts = list(contrast_name_wrapper.values())
contrasts_wrapped = []
contrasts_indices = []
for contrast in contrasts:
    if (contrast in allowed_contrasts):
        contrasts_wrapped.append(contrast)
        contrasts_indices.append(allowed_contrasts.index(contrast))
    elif (contrast in contrast_name_wrapper):
        name = contrast_name_wrapper[contrast]
        contrasts_wrapped.append(name)
        contrasts_indices.append(allowed_contrasts.index(name))
    else:
        raise ValueError(("Contrast '%s' is not available" % contrast))
opts = {'uncompress': True}
if isinstance(n_subjects, numbers.Number):
    tempResult = arange(1, (n_subjects + 1))
	
===================================================================	
fetch_mixed_gambles: 368	
----------------------------	

'Fetch Jimura "mixed gambles" dataset.\n\n    Parameters\n    ----------\n    n_subjects: int, optional (default 1)\n        The number of subjects to load. If None is given, all the\n        subjects are used.\n\n    data_dir: string, optional (default None)\n        Path of the data directory. Used to force data storage in a specified\n        location. Default: None.\n\n    url: string, optional (default None)\n        Override download URL. Used for test only (or if you setup a mirror of\n        the data).\n\n    resume: bool, optional (default True)\n        If true, try resuming download if possible.\n\n    verbose: int, optional (default 0)\n        Defines the level of verbosity of the output.\n\n    return_raw_data: bool, optional (default True)\n        If false, then the data will transformed into and (X, y) pair, suitable\n        for machine learning routines. X is a list of n_subjects * 48\n        Nifti1Image objects (where 48 is the number of trials),\n        and y is an array of shape (n_subjects * 48,).\n\n    smooth: float, or list of 3 floats, optional (default 0.)\n        Size of smoothing kernel to apply to the loaded zmaps.\n\n    Returns\n    -------\n    data: Bunch\n        Dictionary-like object, the interest attributes are :\n        \'zmaps\': string list\n            Paths to realigned gain betamaps (one nifti per subject).\n        \'gain\': ..\n            If make_Xy is true, this is a list of n_subjects * 48\n            Nifti1Image objects, else it is None.\n        \'y\': array of shape (n_subjects * 48,) or None\n            If make_Xy is true, then this is an array of shape\n            (n_subjects * 48,), else it is None.\n\n    References\n    ----------\n    [1] K. Jimura and R. Poldrack, "Analyses of regional-average activation\n        and multivoxel pattern information tell complementary stories",\n        Neuropsychologia, vol. 50, page 544, 2012\n    '
if (n_subjects > 16):
    warnings.warn('Warning: there are only 16 subjects!')
    n_subjects = 16
if (url is None):
    url = 'https://www.nitrc.org/frs/download.php/7229/jimura_poldrack_2012_zmaps.zip'
opts = dict(uncompress=True)
files = [(('zmaps%ssub%03i_zmaps.nii.gz' % (os.sep, (j + 1))), url, opts) for j in range(n_subjects)]
data_dir = _get_dataset_dir('jimura_poldrack_2012_zmaps', data_dir=data_dir)
zmap_fnames = _fetch_files(data_dir, files, resume=resume, verbose=verbose)
tempResult = arange(n_subjects)
	
===================================================================	
test_fetch_atlas_surf_destrieux: 274	
----------------------------	

if (LooseVersion(nibabel.__version__) <= LooseVersion('1.2.0')):
    raise SkipTest
data_dir = os.path.join(test_utils.tmpdir, 'destrieux_surface')
os.mkdir(data_dir)
for hemi in ('left', 'right'):
    tempResult = arange(4)
	
===================================================================	
_get_small_fake_talairach: 286	
----------------------------	

labels = ['*', 'b', 'a']
all_labels = itertools.product(*((labels,) * 5))
labels_txt = '\n'.join(map('.'.join, all_labels))
extensions = nibabel.nifti1.Nifti1Extensions([nibabel.nifti1.Nifti1Extension('afni', labels_txt.encode('utf-8'))])
tempResult = arange(243)
	
===================================================================	
test_filter_columns: 174	
----------------------------	

tempResult = arange(500)
	
===================================================================	
GroupIterator.__iter__: 35	
----------------------------	

tempResult = arange(self.n_features)
	
===================================================================	
BaseSpaceNet.fit: 326	
----------------------------	

'Fit the learner\n\n        Parameters\n        ----------\n        X : list of Niimg-like objects\n            See http://nilearn.github.io/manipulating_images/input_output.html\n            Data on which model is to be fitted. If this is a list,\n            the affine is considered the same for all.\n\n        y : array or list of length n_samples\n            The dependent variable (age, sex, QI, etc.).\n\n        Notes\n        -----\n        self : `SpaceNet` object\n            Model selection is via cross-validation with bagging.\n        '
self.check_params()
if ((self.memory is None) or isinstance(self.memory, _basestring)):
    self.memory_ = Memory(self.memory, verbose=max(0, (self.verbose - 1)))
else:
    self.memory_ = self.memory
if self.verbose:
    tic = time.time()
self.masker_ = check_embedded_nifti_masker(self, multi_subject=False)
X = self.masker_.fit_transform(X)
(X, y) = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=numpy.float, multi_output=True, y_numeric=(not self.is_classif))
self.Xmean_ = X.mean(axis=0)
self.Xstd_ = X.std(axis=0)
self.Xstd_[(self.Xstd_ < 1e-08)] = 1
self.mask_img_ = self.masker_.mask_img_
self.mask_ = self.mask_img_.get_data().astype(numpy.bool)
(n_samples, _) = X.shape
y = np.array(y).copy()
l1_ratios = self.l1_ratios
if isinstance(l1_ratios, numbers.Number):
    l1_ratios = [l1_ratios]
alphas = self.alphas
if isinstance(alphas, numbers.Number):
    alphas = [alphas]
if (self.loss is not None):
    loss = self.loss
elif self.is_classif:
    loss = 'logistic'
else:
    loss = 'mse'
if (self.penalty.lower() == 'graph-net'):
    if ((not self.is_classif) or (loss == 'mse')):
        solver = _graph_net_squared_loss
    else:
        solver = _graph_net_logistic
elif ((not self.is_classif) or (loss == 'mse')):
    solver = partial(tvl1_solver, loss='mse')
else:
    solver = partial(tvl1_solver, loss='logistic')
case1 = ((None in [alphas, l1_ratios]) and (self.n_alphas > 1))
case2 = ((alphas is not None) and (min(len(l1_ratios), len(alphas)) > 1))
if (case1 or case2):
    if (LooseVersion(sklearn.__version__) >= LooseVersion('0.18')):
        self.cv_ = list(check_cv(self.cv, y=y, classifier=self.is_classif).split(X, y))
    else:
        self.cv_ = list(check_cv(self.cv, X=X, y=y, classifier=self.is_classif))
else:
    tempResult = arange(n_samples)
	
===================================================================	
test_1D__gradient_id: 21	
----------------------------	

for size in [1, 2, 10]:
    tempResult = arange(size)
	
===================================================================	
test_searchlight: 16	
----------------------------	

rand = numpy.random.RandomState(0)
frames = 30
data = rand.rand(5, 5, 5, frames)
mask = numpy.ones((5, 5, 5), numpy.bool)
mask_img = nibabel.Nifti1Image(mask.astype(numpy.int), numpy.eye(4))
tempResult = arange(frames, dtype=int)
	
===================================================================	
test_searchlight: 56	
----------------------------	

rand = numpy.random.RandomState(0)
frames = 30
data = rand.rand(5, 5, 5, frames)
mask = numpy.ones((5, 5, 5), numpy.bool)
mask_img = nibabel.Nifti1Image(mask.astype(numpy.int), numpy.eye(4))
cond = (numpy.arange(frames, dtype=int) > (frames // 2))
data[2, 2, 2, :] = 0
data[(2, 2, 2)][cond.astype(numpy.bool)] = 2
data_img = nibabel.Nifti1Image(data, numpy.eye(4))
if (LooseVersion(sklearn.__version__) >= LooseVersion('0.18')):
    from sklearn.model_selection import KFold
    cv = KFold(n_splits=4)
else:
    from sklearn.cross_validation import KFold
    cv = KFold(len(cond), 4)
n_jobs = 1
sl = nilearn.decoding.searchlight.SearchLight(mask_img, process_mask_img=mask_img, radius=0.5, n_jobs=n_jobs, scoring='accuracy', cv=cv)
sl.fit(data_img, cond)
assert_equal(np.where((sl.scores_ == 1))[0].size, 1)
assert_equal(sl.scores_[(2, 2, 2)], 1.0)
process_mask = numpy.zeros((5, 5, 5), numpy.bool)
process_mask[(0, 0, 0)] = True
process_mask_img = nibabel.Nifti1Image(process_mask.astype(numpy.int), numpy.eye(4))
sl = nilearn.decoding.searchlight.SearchLight(mask_img, process_mask_img=process_mask_img, radius=0.5, n_jobs=n_jobs, scoring='accuracy', cv=cv)
sl.fit(data_img, cond)
assert_equal(np.where((sl.scores_ == 1))[0].size, 0)
sl = nilearn.decoding.searchlight.SearchLight(mask_img, process_mask_img=mask_img, radius=1, n_jobs=n_jobs, scoring='accuracy', cv=cv)
sl.fit(data_img, cond)
assert_equal(np.where((sl.scores_ == 1))[0].size, 7)
assert_equal(sl.scores_[(2, 2, 2)], 1.0)
assert_equal(sl.scores_[(1, 2, 2)], 1.0)
assert_equal(sl.scores_[(2, 1, 2)], 1.0)
assert_equal(sl.scores_[(2, 2, 1)], 1.0)
assert_equal(sl.scores_[(3, 2, 2)], 1.0)
assert_equal(sl.scores_[(2, 3, 2)], 1.0)
assert_equal(sl.scores_[(2, 2, 3)], 1.0)
sl = nilearn.decoding.searchlight.SearchLight(mask_img, process_mask_img=mask_img, radius=2, n_jobs=n_jobs, scoring='accuracy', cv=cv)
sl.fit(data_img, cond)
assert_equal(np.where((sl.scores_ == 1))[0].size, 33)
assert_equal(sl.scores_[(2, 2, 2)], 1.0)
try:
    from sklearn.model_selection import LeaveOneGroupOut
    gcv = LeaveOneGroupOut()
except ImportError:
    gcv = cv
tempResult = arange(frames, dtype=int)
	
===================================================================	
test_squared_loss_path_scores: 98	
----------------------------	

iris = load_iris()
(X, y) = (iris.data, iris.target)
(_, mask) = to_niimgs(X, [2, 2, 2])
mask = mask.get_data().astype(numpy.bool)
alphas = [1.0, 0.1, 0.01]
tempResult = arange(len(X))
	
===================================================================	
test_squared_loss_path_scores: 98	
----------------------------	

iris = load_iris()
(X, y) = (iris.data, iris.target)
(_, mask) = to_niimgs(X, [2, 2, 2])
mask = mask.get_data().astype(numpy.bool)
alphas = [1.0, 0.1, 0.01]
tempResult = arange(len(X))
	
===================================================================	
test_space_net_alpha_grid: 29	
----------------------------	

rng = check_random_state(42)
X = rng.randn(n_samples, n_features)
tempResult = arange(n_samples)
	
===================================================================	
test_space_net_alpha_grid_pure_spatial: 212	
----------------------------	

rng = check_random_state(42)
X = rng.randn(10, 100)
tempResult = arange(X.shape[0])
	
===================================================================	
test_logistic_path_scores: 87	
----------------------------	

iris = load_iris()
(X, y) = (iris.data, iris.target)
(_, mask) = to_niimgs(X, [2, 2, 2])
mask = mask.get_data().astype(numpy.bool)
alphas = [1.0, 0.1, 0.01]
tempResult = arange(len(X))
	
===================================================================	
test_logistic_path_scores: 87	
----------------------------	

iris = load_iris()
(X, y) = (iris.data, iris.target)
(_, mask) = to_niimgs(X, [2, 2, 2])
mask = mask.get_data().astype(numpy.bool)
alphas = [1.0, 0.1, 0.01]
tempResult = arange(len(X))
	
===================================================================	
test_multi_pca: 27	
----------------------------	

shape = (6, 8, 10, 5)
affine = numpy.eye(4)
rng = numpy.random.RandomState(0)
data = []
for i in range(8):
    this_data = rng.normal(size=shape)
    this_data[2:4, 2:4, 2:4, :] += 10
    data.append(nibabel.Nifti1Image(this_data, affine))
mask_img = nibabel.Nifti1Image(numpy.ones(shape[:3], dtype=numpy.int8), affine)
multi_pca = MultiPCA(mask=mask_img, n_components=3, random_state=0)
components1 = multi_pca.fit(data).components_
components2 = multi_pca.fit(data).components_
components3 = multi_pca.fit((2 * data)).components_
numpy.testing.assert_array_equal(components1, components2)
numpy.testing.assert_array_almost_equal(components1, components3)
tempResult = arange(10)
	
===================================================================	
test_index_img: 200	
----------------------------	

img_3d = nibabel.Nifti1Image(numpy.ones((3, 4, 5)), numpy.eye(4))
nilearn._utils.testing.assert_raises_regex(TypeError, 'Input data has incompatible dimensionality: Expected dimension is 4D and you provided a 3D image.', nilearn.image.image.index_img, img_3d, 0)
affine = numpy.array([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0], [0.0, 0.0, 0.0, 1.0]])
(img_4d, _) = nilearn._utils.testing.generate_fake_fmri(affine=affine)
fourth_dim_size = img_4d.shape[3]
tempResult = arange(fourth_dim_size)
	
===================================================================	
test_resampling_nan: 214	
----------------------------	

for core_shape in [(3, 5, 4), (3, 5, 4, 2)]:
    tempResult = arange(numpy.prod(core_shape))
	
===================================================================	
test_resampling_result_axis_permutation: 189	
----------------------------	

core_shape = (3, 5, 4)
tempResult = arange(numpy.prod(core_shape))
	
===================================================================	
test_sided_test2: 272	
----------------------------	

'Check that two-sided can actually recover positive and negative effects.\n    '
tempResult = arange(0, 10)
	
===================================================================	
test_sided_test2: 274	
----------------------------	

'Check that two-sided can actually recover positive and negative effects.\n    '
target_var1 = np.arange(0, 10).reshape(((- 1), 1))
target_var = numpy.hstack((target_var1, (- target_var1)))
tempResult = arange(0, 20, 2)
	
===================================================================	
test_permuted_ols_check_h0_noeffect_labelswap: 83	
----------------------------	

rng = check_random_state(random_state)
n_samples = 100
target_var = rng.randn(n_samples, 1)
tempResult = arange(n_samples, dtype='f8')
	
===================================================================	
_edge_detect: 29	
----------------------------	

" Edge detection for 2D images based on Canny filtering.\n\n        Parameters\n        ----------\n        image: 2D array\n            The image on which edge detection is applied\n        high_threshold: float, optional\n            The quantile defining the upper threshold of the hysteries \n            thresholding: decrease this to keep more edges\n        low_threshold: float, optional\n            The quantile defining the lower threshold of the hysteries \n            thresholding: decrease this to extract wider edges\n\n        Returns\n        --------\n        grad_mag: 2D array of floats\n            The magnitude of the gradient\n        edge_mask: 2D array of booleans\n            A mask of where have edges been detected\n\n        Notes\n        ------\n        This function is based on a Canny filter, however it has been\n        taylored to visualization purposes on brain images: don't use it\n        in the general case.\n\n        It computes the norm of the gradient, extracts the ridge by\n        keeping only local maximum in each direction, and performs\n        hysteresis filtering to keep only edges with high gradient\n        magnitude.\n    "
np_err = numpy.seterr(all='ignore')
image = numpy.nan_to_num(image)
img = scipy.signal.wiener(image.astype(numpy.float))
numpy.seterr(**np_err)
img[numpy.isnan(img)] = image[numpy.isnan(img)]
img /= img.max()
grad_x = scipy.ndimage.sobel(img, mode='constant', axis=0)
grad_y = scipy.ndimage.sobel(img, mode='constant', axis=1)
grad_mag = numpy.sqrt(((grad_x ** 2) + (grad_y ** 2)))
grad_angle = numpy.arctan2(grad_y, grad_x)
grad_angle = ((grad_angle + numpy.pi) / numpy.pi)
thinner = numpy.zeros(grad_mag.shape, dtype=numpy.bool)
tempResult = arange(0, 2, 0.25)
	
===================================================================	
find_cut_slices: 111	
----------------------------	

' Find \'good\' cross-section slicing positions along a given axis.\n\n    Parameters\n    ----------\n    img: 3D Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        the brain map\n    direction: string, optional (default "z")\n        sectional direction; possible values are "x", "y", or "z"\n    n_cuts: int, optional (default 7)\n        number of cuts in the plot\n    spacing: \'auto\' or int, optional (default \'auto\')\n        minimum spacing between cuts (in voxels, not milimeters)\n        if \'auto\', the spacing is .5 / n_cuts * img_length\n\n    Returns\n    -------\n    cut_coords: 1D array of length n_cuts\n        the computed cut_coords\n\n    Notes\n    -----\n    This code works by iteratively locating peak activations that are\n    separated by a distance of at least \'spacing\'. If n_cuts is very\n    large and all the activated regions are covered, cuts with a spacing\n    less than \'spacing\' will be returned.\n    '
if (not (direction in 'xyz')):
    raise ValueError(("'direction' must be one of 'x', 'y', or 'z'. Got '%s'" % direction))
axis = 'xyz'.index(direction)
img = check_niimg_3d(img)
affine = img.affine
orig_data = numpy.abs(_safe_get_data(img))
this_shape = orig_data.shape[axis]
if (not isinstance(n_cuts, numbers.Number)):
    raise ValueError(('The number of cuts (n_cuts) must be an integer greater than or equal to 1. You provided a value of n_cuts=%s. ' % n_cuts))
if (n_cuts > this_shape):
    warnings.warn(('Too many cuts requested for the data: n_cuts=%i, data size=%i' % (n_cuts, this_shape)))
    tempResult = arange(this_shape)
	
===================================================================	
plot_matrix: 52	
----------------------------	

" Plot the given matrix.\n\n        Parameters\n        ----------\n        mat : 2-D numpy array\n            Matrix to be plotted.\n        title : string or None, optional\n            A text to add in the upper left corner.\n        labels : list of strings, optional\n            The label of each row and column\n        figure : figure instance, figsize tuple, or None\n            Sets the figure used. This argument can be either an existing\n            figure, or a pair (width, height) that gives the size of a\n            newly-created figure.\n            Specifying both axes and figure is not allowed.\n        axes : None or Axes, optional\n            Axes instance to be plotted on. Creates a new one if None.\n            Specifying both axes and figure is not allowed.\n        colorbar : boolean, optional\n            If True, an integrated colorbar is added.\n        cmap : matplotlib colormap, optional\n            The colormap for the matrix. Default is RdBu_r.\n        tri : {'lower', 'diag', 'full'}, optional\n            Which triangular part of the matrix to plot:\n            'lower' is the lower part, 'diag' is the lower including\n            diagonal, and 'full' is the full matrix.\n        auto_fit : boolean, optional\n            If auto_fit is True, the axes are dimensioned to give room\n            for the labels. This assumes that the labels are resting\n            against the bottom and left edges of the figure.\n        grid : color or False, optional\n            If not False, a grid is plotted to separate rows and columns\n            using the given color.\n        kwargs : extra keyword arguments\n            Extra keyword arguments are sent to pylab.imshow\n\n        Returns Matplotlib AxesImage instance\n    "
if (tri == 'lower'):
    mask = (numpy.tri(mat.shape[0], k=(- 1), dtype=numpy.bool) ^ True)
    mat = numpy.ma.masked_array(mat, mask)
elif (tri == 'diag'):
    mask = (numpy.tri(mat.shape[0], dtype=numpy.bool) ^ True)
    mat = numpy.ma.masked_array(mat, mask)
if ((axes is not None) and (figure is not None)):
    raise ValueError(("Parameters figure and axes cannot be specified together. You gave 'figure=%s, axes=%s'" % (figure, axes)))
if (figure is not None):
    if isinstance(figure, matplotlib.pyplot.Figure):
        fig = figure
    else:
        fig = matplotlib.pyplot.figure(figsize=figure)
    axes = matplotlib.pyplot.gca()
    own_fig = True
elif (axes is None):
    (fig, axes) = matplotlib.pyplot.subplots(1, 1, figsize=(7, 5))
    own_fig = True
else:
    fig = axes.figure
    own_fig = False
display = axes.imshow(mat, aspect='equal', interpolation='nearest', cmap=cmap, **kwargs)
axes.set_autoscale_on(False)
(ymin, ymax) = axes.get_ylim()
if (labels is False):
    axes.xaxis.set_major_formatter(matplotlib.pyplot.NullFormatter())
    axes.yaxis.set_major_formatter(matplotlib.pyplot.NullFormatter())
elif (labels is not None):
    tempResult = arange(len(labels))
	
===================================================================	
plot_matrix: 57	
----------------------------	

" Plot the given matrix.\n\n        Parameters\n        ----------\n        mat : 2-D numpy array\n            Matrix to be plotted.\n        title : string or None, optional\n            A text to add in the upper left corner.\n        labels : list of strings, optional\n            The label of each row and column\n        figure : figure instance, figsize tuple, or None\n            Sets the figure used. This argument can be either an existing\n            figure, or a pair (width, height) that gives the size of a\n            newly-created figure.\n            Specifying both axes and figure is not allowed.\n        axes : None or Axes, optional\n            Axes instance to be plotted on. Creates a new one if None.\n            Specifying both axes and figure is not allowed.\n        colorbar : boolean, optional\n            If True, an integrated colorbar is added.\n        cmap : matplotlib colormap, optional\n            The colormap for the matrix. Default is RdBu_r.\n        tri : {'lower', 'diag', 'full'}, optional\n            Which triangular part of the matrix to plot:\n            'lower' is the lower part, 'diag' is the lower including\n            diagonal, and 'full' is the full matrix.\n        auto_fit : boolean, optional\n            If auto_fit is True, the axes are dimensioned to give room\n            for the labels. This assumes that the labels are resting\n            against the bottom and left edges of the figure.\n        grid : color or False, optional\n            If not False, a grid is plotted to separate rows and columns\n            using the given color.\n        kwargs : extra keyword arguments\n            Extra keyword arguments are sent to pylab.imshow\n\n        Returns Matplotlib AxesImage instance\n    "
if (tri == 'lower'):
    mask = (numpy.tri(mat.shape[0], k=(- 1), dtype=numpy.bool) ^ True)
    mat = numpy.ma.masked_array(mat, mask)
elif (tri == 'diag'):
    mask = (numpy.tri(mat.shape[0], dtype=numpy.bool) ^ True)
    mat = numpy.ma.masked_array(mat, mask)
if ((axes is not None) and (figure is not None)):
    raise ValueError(("Parameters figure and axes cannot be specified together. You gave 'figure=%s, axes=%s'" % (figure, axes)))
if (figure is not None):
    if isinstance(figure, matplotlib.pyplot.Figure):
        fig = figure
    else:
        fig = matplotlib.pyplot.figure(figsize=figure)
    axes = matplotlib.pyplot.gca()
    own_fig = True
elif (axes is None):
    (fig, axes) = matplotlib.pyplot.subplots(1, 1, figsize=(7, 5))
    own_fig = True
else:
    fig = axes.figure
    own_fig = False
display = axes.imshow(mat, aspect='equal', interpolation='nearest', cmap=cmap, **kwargs)
axes.set_autoscale_on(False)
(ymin, ymax) = axes.get_ylim()
if (labels is False):
    axes.xaxis.set_major_formatter(matplotlib.pyplot.NullFormatter())
    axes.yaxis.set_major_formatter(matplotlib.pyplot.NullFormatter())
elif (labels is not None):
    axes.set_xticks(numpy.arange(len(labels)))
    axes.set_xticklabels(labels, size='x-small')
    for label in axes.get_xticklabels():
        label.set_ha('right')
        label.set_rotation(50)
    tempResult = arange(len(labels))
	
===================================================================	
test_tranform_cut_coords: 90	
----------------------------	

affine = numpy.eye(4)
for direction in 'xyz':
    assert_true(hasattr(_transform_cut_coords([4], direction, affine), '__iter__'))
n_cuts = 5
tempResult = arange(n_cuts)
	
===================================================================	
test_plot_connectome: 234	
----------------------------	

node_color = ['green', 'blue', 'k', 'cyan']
adjacency_matrix = numpy.array([[1.0, (- 2.0), 0.3, 0.0], [(- 2.002), 1, 0.0, 0.0], [0.3, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])
tempResult = arange((3 * 4))
	
===================================================================	
test_plot_connectome: 258	
----------------------------	

node_color = ['green', 'blue', 'k', 'cyan']
adjacency_matrix = numpy.array([[1.0, (- 2.0), 0.3, 0.0], [(- 2.002), 1, 0.0, 0.0], [0.3, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])
node_coords = np.arange((3 * 4)).reshape(4, 3)
args = (adjacency_matrix, node_coords)
kwargs = dict(edge_threshold=0.38, title='threshold=0.38', node_size=10, node_color=node_color)
plot_connectome(*args, **kwargs)
matplotlib.pyplot.close()
kwargs['display_mode'] = 'x'
plot_connectome(adjacency_matrix, [tuple(each) for each in node_coords], **kwargs)
filename = tempfile.mktemp(suffix='.png')
try:
    display = plot_connectome(*args, output_file=filename, **kwargs)
    assert_true((display is None))
    assert_true((os.path.isfile(filename) and (os.path.getsize(filename) > 0)))
finally:
    os.remove(filename)
matplotlib.pyplot.close()
plot_connectome(*args, edge_threshold='70%', node_size=[10, 20, 30, 40], node_color=numpy.zeros((4, 3)), edge_cmap='RdBu', colorbar=True, node_kwargs={'marker': 'v'}, edge_kwargs={'linewidth': 4})
matplotlib.pyplot.close()
masked_adjacency_matrix = numpy.ma.masked_array(adjacency_matrix, (numpy.abs(adjacency_matrix) < 0.5))
plot_connectome(masked_adjacency_matrix, node_coords, **kwargs)
matplotlib.pyplot.close()
sparse_adjacency_matrix = scipy.sparse.coo_matrix(adjacency_matrix)
plot_connectome(sparse_adjacency_matrix, node_coords, **kwargs)
matplotlib.pyplot.close()
nan_adjacency_matrix = numpy.array([[1.0, numpy.nan, 0.0], [numpy.nan, 1.0, 2.0], [numpy.nan, 2.0, 1.0]])
tempResult = arange((3 * 3))
	
===================================================================	
test_plot_connectome_exceptions: 269	
----------------------------	

tempResult = arange((2 * 3))
	
===================================================================	
connected_label_regions: 158	
----------------------------	

" Extract connected regions from a brain atlas image defined by labels\n    (integers).\n\n    For each label in an parcellations, separates out connected\n    components and assigns to each separated region a unique label.\n\n    Parameters\n    ----------\n\n    labels_img : Nifti-like image\n        A 3D image which contains regions denoted as labels. Each region\n        is assigned with integers.\n\n    min_size : float, in mm^3 optional (default None)\n        Minimum region size in volume required to keep after extraction.\n        Removes small or spurious regions.\n\n    connect_diag : bool (default True)\n        If 'connect_diag' is True, two voxels are considered in the same region\n        if they are connected along the diagonal (26-connectivity). If it is\n        False, two voxels are considered connected only if they are within the\n        same x, y, or z direction.\n\n    labels : 1D numpy array or list of str, (default None), optional\n        Each string in a list or array denote the name of the brain atlas\n        regions given in labels_img input. If provided, same names will be\n        re-assigned corresponding to each connected component based extraction\n        of regions relabelling. The total number of names should match with the\n        number of labels assigned in the image.\n\n        NOTE: The order of the names given in labels should be appropriately\n        matched with the unique labels (integers) assigned to each region\n        given in labels_img (also excluding 'Background' label).\n\n    Returns\n    -------\n    new_labels_img : Nifti-like image\n        A new image comprising of regions extracted on an input labels_img.\n\n    new_labels : list, optional\n        If labels are provided, new labels assigned to region extracted will\n        be returned. Otherwise, only new labels image will be returned.\n\n    See Also\n    --------\n    nilearn.datasets.fetch_atlas_harvard_oxford : For an example of atlas with\n        labels.\n\n    nilearn.regions.RegionExtractor : A class can be used for region extraction\n        on continuous type atlas images.\n\n    nilearn.regions.connected_regions : A function used for region extraction\n        on continuous type atlas images.\n\n    "
labels_img = check_niimg_3d(labels_img)
labels_data = _safe_get_data(labels_img, ensure_finite=True)
affine = labels_img.affine
check_unique_labels = numpy.unique(labels_data)
if ((min_size is not None) and (not isinstance(min_size, numbers.Number))):
    raise ValueError("Expected 'min_size' to be specified as integer. You provided {0}".format(min_size))
if (not isinstance(connect_diag, bool)):
    raise ValueError("'connect_diag' must be specified as True or False. You provided {0}".format(connect_diag))
if numpy.any((check_unique_labels < 0)):
    raise ValueError("The 'labels_img' you provided has unknown/negative integers as labels {0} assigned to regions. All regions in an image should have positive integers assigned as labels.".format(check_unique_labels))
unique_labels = set(check_unique_labels)
if numpy.any((check_unique_labels == 0)):
    unique_labels.remove(0)
if (labels is not None):
    if ((not isinstance(labels, collections.Iterable)) or isinstance(labels, _basestring)):
        labels = [labels]
    if (len(unique_labels) != len(labels)):
        raise ValueError('The number of labels: {0} provided as input in labels={1} does not match with the number of unique labels in labels_img: {2}. Please provide appropriate match with unique number of labels in labels_img.'.format(len(labels), labels, len(unique_labels)))
    new_names = []
if (labels is None):
    this_labels = ([None] * len(unique_labels))
else:
    this_labels = labels
new_labels_data = numpy.zeros(labels_data.shape, dtype=numpy.int)
current_max_label = 0
for (label_id, name) in zip(unique_labels, this_labels):
    this_label_mask = (labels_data == label_id)
    if connect_diag:
        structure = numpy.ones((3, 3, 3), dtype=numpy.int)
        (regions, this_n_labels) = scipy.ndimage.label(this_label_mask.astype(numpy.int), structure=structure)
    else:
        (regions, this_n_labels) = scipy.ndimage.label(this_label_mask.astype(numpy.int))
    if (min_size is not None):
        tempResult = arange((this_n_labels + 1))
	
===================================================================	
img_to_signals_maps: 96	
----------------------------	

'Extract region signals from image.\n\n    This function is applicable to regions defined by maps.\n\n    Parameters\n    ----------\n    imgs: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        Input images.\n\n    maps_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        regions definition as maps (array of weights).\n        shape: imgs.shape + (region number, )\n\n    mask_img: Niimg-like object\n        See http://nilearn.github.io/manipulating_images/input_output.html\n        mask to apply to regions before extracting signals. Every point\n        outside the mask is considered as background (i.e. outside of any\n        region).\n\n    order: str\n        ordering of output array ("C" or "F"). Defaults to "F".\n\n    Returns\n    -------\n    region_signals: numpy.ndarray\n        Signals extracted from each region.\n        Shape is: (scans number, number of regions intersecting mask)\n\n    labels: list\n        maps_img[..., labels[n]] is the region that has been used to extract\n        signal region_signals[:, n].\n\n    See also\n    --------\n    nilearn.regions.img_to_signals_labels\n    nilearn.regions.signals_to_img_maps\n    '
maps_img = _utils.check_niimg_4d(maps_img)
imgs = _utils.check_niimg_4d(imgs)
affine = imgs.affine
shape = imgs.shape[:3]
if (maps_img.shape[:3] != shape):
    raise ValueError('maps_img and imgs shapes must be identical.')
if (abs((maps_img.affine - affine)).max() > 1e-09):
    raise ValueError('maps_img and imgs affines must be identical')
maps_data = _safe_get_data(maps_img, ensure_finite=True)
if (mask_img is not None):
    mask_img = _utils.check_niimg_3d(mask_img)
    if (mask_img.shape != shape):
        raise ValueError('mask_img and imgs shapes must be identical.')
    if (abs((mask_img.affine - affine)).max() > 1e-09):
        raise ValueError('mask_img and imgs affines must be identical')
    (maps_data, maps_mask, labels) = _trim_maps(maps_data, _safe_get_data(mask_img, ensure_finite=True), keep_empty=True)
    maps_mask = _utils.as_ndarray(maps_mask, dtype=numpy.bool)
else:
    maps_mask = numpy.ones(maps_data.shape[:3], dtype=numpy.bool)
    tempResult = arange(maps_data.shape[(- 1)], dtype=numpy.int)
	
===================================================================	
_trim_maps: 140	
----------------------------	

'Crop maps using a mask.\n\n    No consistency check is performed (esp. on affine). Every required check\n    must be performed before calling this function.\n\n    Parameters\n    ----------\n    maps: numpy.ndarray\n        Set of maps, defining some regions.\n\n    mask: numpy.ndarray\n        Definition of a mask. The shape must match that of a single map.\n\n    keep_empty: bool\n        If False, maps that lie completely outside the mask are dropped from\n        the output. If True, they are kept, meaning that maps that are\n        completely zero can occur in the output.\n\n    order: "F" or "C"\n        Ordering of the output maps array (trimmed_maps).\n\n    Returns\n    -------\n    trimmed_maps: numpy.ndarray\n        New set of maps, computed as intersection of each input map and mask.\n        Empty maps are discarded if keep_empty is False, thus the number of\n        output maps is not necessarily the same as the number of input maps.\n        shape: mask.shape + (output maps number,). Data ordering depends\n        on the "order" parameter.\n\n    maps_mask: numpy.ndarray\n        Union of all output maps supports. One non-zero value in this\n        array guarantees that there is at least one output map that is\n        non-zero at this voxel.\n        shape: mask.shape. Order is always C.\n\n    indices: numpy.ndarray\n        indices of regions that have an non-empty intersection with the\n        given mask. len(indices) == trimmed_maps.shape[-1]\n    '
maps = maps.copy()
sums = abs(maps[_utils.as_ndarray(mask, dtype=np.bool), :]).sum(axis=0)
if keep_empty:
    n_regions = maps.shape[(- 1)]
else:
    n_regions = (sums > 0).sum()
trimmed_maps = numpy.zeros((maps.shape[:3] + (n_regions,)), dtype=maps.dtype, order=order)
maps_mask = numpy.zeros(mask.shape, dtype=numpy.int8)
p = 0
mask = _utils.as_ndarray(mask, dtype=numpy.bool, order='C')
for (n, m) in enumerate(numpy.rollaxis(maps, (- 1))):
    if ((not keep_empty) and (sums[n] == 0)):
        continue
    trimmed_maps[(mask, p)] = maps[(mask, n)]
    maps_mask[(trimmed_maps[(..., p)] > 0)] = 1
    p += 1
if keep_empty:
    tempResult = arange(trimmed_maps.shape[(- 1)], dtype=numpy.int)
	
===================================================================	
test_remove_small_regions: 130	
----------------------------	

data = numpy.array([[[0.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]])
(label_map, n_labels) = scipy.ndimage.label(data)
sum_label_data = numpy.sum(label_map)
affine = numpy.eye(4)
min_size = 10
tempResult = arange((n_labels + 1))
	
===================================================================	
_surrounding_faces: 50	
----------------------------	

'Get matrix indicating which faces the nodes belong to.\n\n    i, j is set if node i is a vertex of triangle j.\n    '
(vertices, faces) = load_surf_mesh(mesh)
n_faces = faces.shape[0]
tempResult = arange(n_faces)
	
===================================================================	
_interpolation_sampling: 133	
----------------------------	

'In each image, measure the intensity at each node of the mesh.\n\n    Image intensity at each sample point is computed with trilinear\n    interpolation.\n    A 2-d array is returned, where each row corresponds to an image and each\n    column to a mesh vertex.\n    See documentation of vol_to_surf for details.\n\n    '
sample_locations = _sample_locations(mesh, affine, kind=kind, radius=radius, n_points=n_points)
(n_vertices, n_points, img_dim) = sample_locations.shape
tempResult = arange(size)
	
===================================================================	
_z_const_img: 163	
----------------------------	

tempResult = arange((x_s * y_s))
	
===================================================================	
test_is_spd_with_non_symmetrical_matrix: 16	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_is_spd_with_symmetrical_matrix: 30	
----------------------------	

matrix = numpy.array([[0, 1], [1, 0]])
assert (not is_spd(matrix, verbose=0))
tempResult = arange(4)
	
===================================================================	
test_fast_abs_percentile_no_index_error: 36	
----------------------------	

tempResult = arange(4)
	
===================================================================	
test_fast_abs_percentile: 10	
----------------------------	

rng = check_random_state(1)
tempResult = arange(100)
	
===================================================================	
test_apply_mask: 94	
----------------------------	

' Test smoothing of timeseries extraction\n    '
data = numpy.zeros((40, 40, 40, 2))
data[(20, 20, 20)] = 1
mask = numpy.ones((40, 40, 40))
full_mask = numpy.zeros((40, 40, 40))
for create_files in (False, True):
    for affine in (numpy.eye(4), numpy.diag((1, 1, (- 1), 1)), numpy.diag((0.5, 1, 0.5, 1))):
        data_img = Nifti1Image(data, affine)
        mask_img = Nifti1Image(mask, affine)
        with write_tmp_imgs(data_img, mask_img, create_files=create_files) as filenames:
            series = nilearn.masking.apply_mask(filenames[0], filenames[1], smoothing_fwhm=9)
        series = numpy.reshape(series[0, :], (40, 40, 40))
        vmax = series.max()
        above_half_max = (series > (0.5 * vmax))
        for axis in (0, 1, 2):
            proj = numpy.any(numpy.any(numpy.rollaxis(above_half_max, axis=axis), axis=(- 1)), axis=(- 1))
            numpy.testing.assert_equal(proj.sum(), (9 / numpy.abs(affine[(axis, axis)])))
data[(10, 10, 10)] = numpy.NaN
data_img = Nifti1Image(data, affine)
mask_img = Nifti1Image(mask, affine)
full_mask_img = Nifti1Image(full_mask, affine)
series = nilearn.masking.apply_mask(data_img, mask_img, smoothing_fwhm=9)
assert_true(numpy.all(numpy.isfinite(series)))
mask_img_4d = Nifti1Image(numpy.ones((40, 40, 40, 2)), numpy.eye(4))
assert_raises_regex(DimensionError, (_TEST_DIM_ERROR_MSG % '4D'), nilearn.masking.apply_mask, data_img, mask_img_4d)
tempResult = arange(27)
	
===================================================================	
test_bad_inputs: 27	
----------------------------	

img = numpy.ones(10)
tempResult = arange(10)
	
===================================================================	
test_bad_inputs: 31	
----------------------------	

img = numpy.ones(10)
labels = numpy.arange(10)
numpy.testing.assert_raises(ValueError, _random_walker, img, labels)
numpy.random.seed(42)
img = numpy.random.normal(size=(3, 3, 3, 3, 3))
tempResult = arange((3 ** 5))
	
===================================================================	
test_clean_confounds: 160	
----------------------------	

(signals, noises, confounds) = generate_signals(n_features=41, n_confounds=5, length=45)
eps = np.finfo(np.float).eps
noises1 = noises.copy()
cleaned_signals = nilearn.signal.clean(noises, confounds=confounds, detrend=True, standardize=False)
assert_true((abs(cleaned_signals).max() < (100.0 * eps)))
numpy.testing.assert_almost_equal(noises, noises1, decimal=12)
cleaned_signals = nilearn.signal.clean((signals + noises), confounds=confounds, detrend=False, standardize=True)
assert_true((abs(np.dot(confounds.T, cleaned_signals)).max() < (1000.0 * eps)))
confounds1 = numpy.hstack((numpy.ones((45, 1)), confounds))
cleaned_signals1 = nilearn.signal.clean((signals + noises), confounds=confounds1, detrend=False, standardize=True)
numpy.testing.assert_almost_equal(cleaned_signals1, cleaned_signals)
temp = confounds.T
tempResult = arange(confounds.shape[0])
	
===================================================================	
test_clean_confounds: 162	
----------------------------	

(signals, noises, confounds) = generate_signals(n_features=41, n_confounds=5, length=45)
eps = np.finfo(np.float).eps
noises1 = noises.copy()
cleaned_signals = nilearn.signal.clean(noises, confounds=confounds, detrend=True, standardize=False)
assert_true((abs(cleaned_signals).max() < (100.0 * eps)))
numpy.testing.assert_almost_equal(noises, noises1, decimal=12)
cleaned_signals = nilearn.signal.clean((signals + noises), confounds=confounds, detrend=False, standardize=True)
assert_true((abs(np.dot(confounds.T, cleaned_signals)).max() < (1000.0 * eps)))
confounds1 = numpy.hstack((numpy.ones((45, 1)), confounds))
cleaned_signals1 = nilearn.signal.clean((signals + noises), confounds=confounds1, detrend=False, standardize=True)
numpy.testing.assert_almost_equal(cleaned_signals1, cleaned_signals)
temp = confounds.T
temp += numpy.arange(confounds.shape[0])
cleaned_signals = nilearn.signal.clean((signals + noises), confounds=confounds, detrend=False, standardize=False)
tempResult = arange(cleaned_signals.shape[0])
	
===================================================================	
test_clean_confounds: 165	
----------------------------	

(signals, noises, confounds) = generate_signals(n_features=41, n_confounds=5, length=45)
eps = np.finfo(np.float).eps
noises1 = noises.copy()
cleaned_signals = nilearn.signal.clean(noises, confounds=confounds, detrend=True, standardize=False)
assert_true((abs(cleaned_signals).max() < (100.0 * eps)))
numpy.testing.assert_almost_equal(noises, noises1, decimal=12)
cleaned_signals = nilearn.signal.clean((signals + noises), confounds=confounds, detrend=False, standardize=True)
assert_true((abs(np.dot(confounds.T, cleaned_signals)).max() < (1000.0 * eps)))
confounds1 = numpy.hstack((numpy.ones((45, 1)), confounds))
cleaned_signals1 = nilearn.signal.clean((signals + noises), confounds=confounds1, detrend=False, standardize=True)
numpy.testing.assert_almost_equal(cleaned_signals1, cleaned_signals)
temp = confounds.T
temp += numpy.arange(confounds.shape[0])
cleaned_signals = nilearn.signal.clean((signals + noises), confounds=confounds, detrend=False, standardize=False)
coeffs = numpy.polyfit(numpy.arange(cleaned_signals.shape[0]), cleaned_signals, 1)
assert_true((abs(coeffs) > 0.001).any())
cleaned_signals = nilearn.signal.clean((signals + noises), confounds=confounds, detrend=True, standardize=False)
tempResult = arange(cleaned_signals.shape[0])
	
===================================================================	
_mask_edges_weights: 76	
----------------------------	

'\n    Remove edges of the graph connected to masked nodes, as well as\n    corresponding weights of the edges.\n    '
mask0 = numpy.hstack((mask[:, :, :(- 1)].ravel(), mask[:, :(- 1)].ravel(), mask[:(- 1)].ravel()))
mask1 = numpy.hstack((mask[:, :, 1:].ravel(), mask[:, 1:].ravel(), mask[1:].ravel()))
ind_mask = numpy.logical_and(mask0, mask1)
(edges, weights) = (edges[:, ind_mask], weights[ind_mask])
max_node_index = edges.max()
tempResult = arange((max_node_index + 1))
	
===================================================================	
_buildAB: 55	
----------------------------	

'\n    Build the matrix A and rhs B of the linear system to solve.\n    A and B are two block of the laplacian of the image graph.\n    '
labels = labels[(labels >= 0)]
tempResult = arange(labels.size)
	
===================================================================	
_make_laplacian_sparse: 37	
----------------------------	

'\n    Sparse implementation\n    '
pixel_nb = (edges.max() + 1)
tempResult = arange(pixel_nb)
	
===================================================================	
_make_graph_edges_3d: 11	
----------------------------	

'Returns a list of edges for a 3D image.\n\n    Parameters\n    ----------\n    n_x: integer\n        The size of the grid in the x direction.\n    n_y: integer\n        The size of the grid in the y direction\n    n_z: integer\n        The size of the grid in the z direction\n\n    Returns\n    -------\n    edges : (2, N) ndarray\n        with the total number of edges::\n\n            N = n_x * n_y * (nz - 1) +\n                n_x * (n_y - 1) * nz +\n                (n_x - 1) * n_y * nz\n\n        Graph edges with each column describing a node-id pair.\n    '
tempResult = arange(((n_x * n_y) * n_z))
	
===================================================================	
generate_regions_ts: 178	
----------------------------	

'Generate some regions as timeseries.\n\n    Parameters\n    ----------\n    overlap: int\n        Number of overlapping voxels between two regions (more or less)\n    window: str\n        Name of a window in scipy.signal. e.g. "hamming".\n\n    Returns\n    -------\n    regions: numpy.ndarray\n        regions, nepresented as signals.\n        shape (n_features, n_regions)\n    '
if (rand_gen is None):
    rand_gen = numpy.random.RandomState(0)
if (window is None):
    window = 'boxcar'
assert (n_features > n_regions)
boundaries = numpy.zeros((n_regions + 1))
boundaries[(- 1)] = n_features
tempResult = arange(1, n_features)
	
***************************************************	
poliastro_poliastro-0.8.0: 1	
===================================================================	
time_range: 28	
----------------------------	

'Generates range of astronomical times.\n\n    .. versionadded:: 0.8.0\n\n    Parameters\n    ----------\n    periods : int, optional\n        Number of periods, default to 50.\n    spacing : Time or Quantity, optional\n        Spacing between periods, optional.\n    end : Time or equivalent, optional\n        End date.\n\n    Returns\n    -------\n    Time\n        Array of time values.\n\n    '
start = Time(start)
if ((spacing is not None) and (end is None)):
    tempResult = arange(0, periods)
	
***************************************************	
skimage_skimage-0.13.0: 201	
===================================================================	
TestColorconv.test_lab_full_gamut: 236	
----------------------------	

tempResult = arange((- 100), 100)
	
===================================================================	
TestColorconv.test_lab_full_gamut: 236	
----------------------------	

tempResult = arange((- 100), 100)
	
===================================================================	
test_bg_and_color_cycle: 51	
----------------------------	

image = numpy.zeros((1, 10))
tempResult = arange(10)
	
===================================================================	
test_color_names: 43	
----------------------------	

image = numpy.ones((1, 3))
tempResult = arange(3)
	
===================================================================	
test_no_input_image: 29	
----------------------------	

tempResult = arange(3)
	
===================================================================	
test_negative_intensity: 107	
----------------------------	

tempResult = arange(100)
	
===================================================================	
test_image_alpha: 36	
----------------------------	

image = numpy.random.uniform(size=(1, 3))
tempResult = arange(3)
	
===================================================================	
test_label_consistency: 72	
----------------------------	

'Assert that the same labels map to the same colors.'
tempResult = arange(5)
	
===================================================================	
test_rgb: 15	
----------------------------	

image = numpy.ones((1, 3))
tempResult = arange(3)
	
===================================================================	
ellipsoid: 15	
----------------------------	

'\n    Generates ellipsoid with semimajor axes aligned with grid dimensions\n    on grid with specified `spacing`.\n\n    Parameters\n    ----------\n    a : float\n        Length of semimajor axis aligned with x-axis.\n    b : float\n        Length of semimajor axis aligned with y-axis.\n    c : float\n        Length of semimajor axis aligned with z-axis.\n    spacing : tuple of floats, length 3\n        Spacing in (x, y, z) spatial dimensions.\n    levelset : bool\n        If True, returns the level set for this ellipsoid (signed level\n        set about zero, with positive denoting interior) as np.float64.\n        False returns a binarized version of said level set.\n\n    Returns\n    -------\n    ellip : (N, M, P) array\n        Ellipsoid centered in a correctly sized array for given `spacing`.\n        Boolean dtype unless `levelset=True`, in which case a float array is\n        returned with the level set above 0.0 representing the ellipsoid.\n\n    '
if ((a <= 0) or (b <= 0) or (c <= 0)):
    raise ValueError('Parameters a, b, and c must all be > 0')
offset = (numpy.r_[(1, 1, 1)] * numpy.r_[spacing])
low = numpy.ceil(((- numpy.r_[(a, b, c)]) - offset))
high = numpy.floor(((numpy.r_[(a, b, c)] + offset) + 1))
for dim in range(3):
    if (((high[dim] - low[dim]) % 2) == 0):
        low[dim] -= 1
    tempResult = arange(low[dim], high[dim], spacing[dim])
	
===================================================================	
histogram: 28	
----------------------------	

'Return histogram of image.\n\n    Unlike `numpy.histogram`, this function returns the centers of bins and\n    does not rebin integer arrays. For integer arrays, each integer value has\n    its own bin, which improves speed and intensity-resolution.\n\n    The histogram is computed on the flattened image: for color images, the\n    function should be used separately on each channel to obtain a histogram\n    for each color channel.\n\n    Parameters\n    ----------\n    image : array\n        Input image.\n    nbins : int\n        Number of bins used to calculate histogram. This value is ignored for\n        integer arrays.\n\n    Returns\n    -------\n    hist : array\n        The values of the histogram.\n    bin_centers : array\n        The values at the center of the bins.\n\n    See Also\n    --------\n    cumulative_distribution\n\n    Examples\n    --------\n    >>> from skimage import data, exposure, img_as_float\n    >>> image = img_as_float(data.camera())\n    >>> np.histogram(image, bins=2)\n    (array([107432, 154712]), array([ 0. ,  0.5,  1. ]))\n    >>> exposure.histogram(image, nbins=2)\n    (array([107432, 154712]), array([ 0.25,  0.75]))\n    '
sh = image.shape
if ((len(sh) == 3) and (sh[(- 1)] < 4)):
    warn('This might be a color image. The histogram will be computed on the flattened image. You can instead apply this function to each color channel.')
if numpy.issubdtype(image.dtype, numpy.integer):
    offset = 0
    image_min = numpy.min(image)
    if (image_min < 0):
        offset = image_min
        image_range = (np.max(image).astype(numpy.int64) - image_min)
        offset_dtype = numpy.promote_types(numpy.min_scalar_type(image_range), numpy.min_scalar_type(image_min))
        if (image.dtype != offset_dtype):
            image = image.astype(offset_dtype)
        image = (image - offset)
    hist = numpy.bincount(image.ravel())
    tempResult = arange(len(hist))
	
===================================================================	
interpolate: 142	
----------------------------	

'Find the new grayscale level for a region using bilinear interpolation.\n\n    Parameters\n    ----------\n    image : ndarray\n        Full image.\n    xslice, yslice : array-like\n       Indices of the region.\n    map* : ndarray\n        Mappings of greylevels from histograms.\n    lut : ndarray\n        Maps grayscale levels in image to histogram levels.\n\n    Returns\n    -------\n    out : ndarray\n        Original image with the subregion replaced.\n\n    Notes\n    -----\n    This function calculates the new greylevel assignments of pixels within\n    a submatrix of the image. This is done by a bilinear interpolation between\n    four different mappings in order to eliminate boundary artifacts.\n    '
norm = (xslice.size * yslice.size)
tempResult = arange(xslice.size)
	
===================================================================	
interpolate: 142	
----------------------------	

'Find the new grayscale level for a region using bilinear interpolation.\n\n    Parameters\n    ----------\n    image : ndarray\n        Full image.\n    xslice, yslice : array-like\n       Indices of the region.\n    map* : ndarray\n        Mappings of greylevels from histograms.\n    lut : ndarray\n        Maps grayscale levels in image to histogram levels.\n\n    Returns\n    -------\n    out : ndarray\n        Original image with the subregion replaced.\n\n    Notes\n    -----\n    This function calculates the new greylevel assignments of pixels within\n    a submatrix of the image. This is done by a bilinear interpolation between\n    four different mappings in order to eliminate boundary artifacts.\n    '
norm = (xslice.size * yslice.size)
tempResult = arange(yslice.size)
	
===================================================================	
clip_histogram: 119	
----------------------------	

'Perform clipping of the histogram and redistribution of bins.\n\n    The histogram is clipped and the number of excess pixels is counted.\n    Afterwards the excess pixels are equally redistributed across the\n    whole histogram (providing the bin count is smaller than the cliplimit).\n\n    Parameters\n    ----------\n    hist : ndarray\n        Histogram array.\n    clip_limit : int\n        Maximum allowed bin count.\n\n    Returns\n    -------\n    hist : ndarray\n        Clipped histogram.\n    '
excess_mask = (hist > clip_limit)
excess = hist[excess_mask]
n_excess = (excess.sum() - (excess.size * clip_limit))
bin_incr = int((n_excess / hist.size))
upper = (clip_limit - bin_incr)
hist[excess_mask] = clip_limit
low_mask = (hist < upper)
n_excess -= (hist[low_mask].size * bin_incr)
hist[low_mask] += bin_incr
mid_mask = ((hist >= upper) & (hist < clip_limit))
mid = hist[mid_mask]
n_excess -= ((mid.size * clip_limit) - mid.sum())
hist[mid_mask] = clip_limit
prev_n_excess = n_excess
while (n_excess > 0):
    index = 0
    while ((n_excess > 0) and (index < hist.size)):
        under_mask = (hist < 0)
        step_size = int((hist[(hist < clip_limit)].size / n_excess))
        step_size = max(step_size, 1)
        tempResult = arange(index, hist.size, step_size)
	
===================================================================	
_clahe: 40	
----------------------------	

'Contrast Limited Adaptive Histogram Equalization.\n\n    Parameters\n    ----------\n    image : (M, N) ndarray\n        Input image.\n    kernel_size: 2-tuple of int\n        Defines the shape of contextual regions used in the algorithm.\n    clip_limit : float\n        Normalized clipping limit (higher values give more contrast).\n    nbins : int, optional\n        Number of gray bins for histogram ("data range").\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        Equalized image.\n\n    The number of "effective" greylevels in the output image is set by `nbins`;\n    selecting a small value (eg. 128) speeds up processing and still produce\n    an output image of good quality. The output image will have the same\n    minimum and maximum value as the input image. A clip limit smaller than 1\n    results in standard (non-contrast limited) AHE.\n    '
if (clip_limit == 1.0):
    return image
nr = int(numpy.ceil((image.shape[0] / kernel_size[0])))
nc = int(numpy.ceil((image.shape[1] / kernel_size[1])))
row_step = int(numpy.floor((image.shape[0] / nr)))
col_step = int(numpy.floor((image.shape[1] / nc)))
bin_size = (1 + (NR_OF_GREY // nbins))
tempResult = arange(NR_OF_GREY)
	
===================================================================	
_clahe: 90	
----------------------------	

'Contrast Limited Adaptive Histogram Equalization.\n\n    Parameters\n    ----------\n    image : (M, N) ndarray\n        Input image.\n    kernel_size: 2-tuple of int\n        Defines the shape of contextual regions used in the algorithm.\n    clip_limit : float\n        Normalized clipping limit (higher values give more contrast).\n    nbins : int, optional\n        Number of gray bins for histogram ("data range").\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        Equalized image.\n\n    The number of "effective" greylevels in the output image is set by `nbins`;\n    selecting a small value (eg. 128) speeds up processing and still produce\n    an output image of good quality. The output image will have the same\n    minimum and maximum value as the input image. A clip limit smaller than 1\n    results in standard (non-contrast limited) AHE.\n    '
if (clip_limit == 1.0):
    return image
nr = int(numpy.ceil((image.shape[0] / kernel_size[0])))
nc = int(numpy.ceil((image.shape[1] / kernel_size[1])))
row_step = int(numpy.floor((image.shape[0] / nr)))
col_step = int(numpy.floor((image.shape[1] / nc)))
bin_size = (1 + (NR_OF_GREY // nbins))
lut = numpy.arange(NR_OF_GREY)
lut //= bin_size
map_array = numpy.zeros((nr, nc, nbins), dtype=int)
for r in range(nr):
    for c in range(nc):
        sub_img = image[(r * row_step):((r + 1) * row_step), (c * col_step):((c + 1) * col_step)]
        if (clip_limit > 0.0):
            clim = int(((clip_limit * sub_img.size) / nbins))
            if (clim < 1):
                clim = 1
        else:
            clim = NR_OF_GREY
        hist = lut[sub_img.ravel()]
        hist = numpy.bincount(hist)
        hist = numpy.append(hist, numpy.zeros((nbins - hist.size), dtype=int))
        hist = clip_histogram(hist, clim)
        hist = map_histogram(hist, 0, (NR_OF_GREY - 1), sub_img.size)
        map_array[(r, c)] = hist
rstart = 0
for r in range((nr + 1)):
    cstart = 0
    if (r == 0):
        r_offset = (row_step / 2.0)
        rU = 0
        rB = 0
    elif (r == nr):
        r_offset = (row_step / 2.0)
        rU = (nr - 1)
        rB = rU
    else:
        r_offset = row_step
        rU = (r - 1)
        rB = (rB + 1)
    for c in range((nc + 1)):
        if (c == 0):
            c_offset = (col_step / 2.0)
            cL = 0
            cR = 0
        elif (c == nc):
            c_offset = (col_step / 2.0)
            cL = (nc - 1)
            cR = cL
        else:
            c_offset = col_step
            cL = (c - 1)
            cR = (cL + 1)
        mapLU = map_array[(rU, cL)]
        mapRU = map_array[(rU, cR)]
        mapLB = map_array[(rB, cL)]
        mapRB = map_array[(rB, cR)]
        tempResult = arange(cstart, (cstart + c_offset))
	
===================================================================	
_clahe: 91	
----------------------------	

'Contrast Limited Adaptive Histogram Equalization.\n\n    Parameters\n    ----------\n    image : (M, N) ndarray\n        Input image.\n    kernel_size: 2-tuple of int\n        Defines the shape of contextual regions used in the algorithm.\n    clip_limit : float\n        Normalized clipping limit (higher values give more contrast).\n    nbins : int, optional\n        Number of gray bins for histogram ("data range").\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        Equalized image.\n\n    The number of "effective" greylevels in the output image is set by `nbins`;\n    selecting a small value (eg. 128) speeds up processing and still produce\n    an output image of good quality. The output image will have the same\n    minimum and maximum value as the input image. A clip limit smaller than 1\n    results in standard (non-contrast limited) AHE.\n    '
if (clip_limit == 1.0):
    return image
nr = int(numpy.ceil((image.shape[0] / kernel_size[0])))
nc = int(numpy.ceil((image.shape[1] / kernel_size[1])))
row_step = int(numpy.floor((image.shape[0] / nr)))
col_step = int(numpy.floor((image.shape[1] / nc)))
bin_size = (1 + (NR_OF_GREY // nbins))
lut = numpy.arange(NR_OF_GREY)
lut //= bin_size
map_array = numpy.zeros((nr, nc, nbins), dtype=int)
for r in range(nr):
    for c in range(nc):
        sub_img = image[(r * row_step):((r + 1) * row_step), (c * col_step):((c + 1) * col_step)]
        if (clip_limit > 0.0):
            clim = int(((clip_limit * sub_img.size) / nbins))
            if (clim < 1):
                clim = 1
        else:
            clim = NR_OF_GREY
        hist = lut[sub_img.ravel()]
        hist = numpy.bincount(hist)
        hist = numpy.append(hist, numpy.zeros((nbins - hist.size), dtype=int))
        hist = clip_histogram(hist, clim)
        hist = map_histogram(hist, 0, (NR_OF_GREY - 1), sub_img.size)
        map_array[(r, c)] = hist
rstart = 0
for r in range((nr + 1)):
    cstart = 0
    if (r == 0):
        r_offset = (row_step / 2.0)
        rU = 0
        rB = 0
    elif (r == nr):
        r_offset = (row_step / 2.0)
        rU = (nr - 1)
        rB = rU
    else:
        r_offset = row_step
        rU = (r - 1)
        rB = (rB + 1)
    for c in range((nc + 1)):
        if (c == 0):
            c_offset = (col_step / 2.0)
            cL = 0
            cR = 0
        elif (c == nc):
            c_offset = (col_step / 2.0)
            cL = (nc - 1)
            cR = cL
        else:
            c_offset = col_step
            cL = (c - 1)
            cR = (cL + 1)
        mapLU = map_array[(rU, cL)]
        mapRU = map_array[(rU, cR)]
        mapLB = map_array[(rB, cL)]
        mapRB = map_array[(rB, cR)]
        cslice = numpy.arange(cstart, (cstart + c_offset))
        tempResult = arange(rstart, (rstart + r_offset))
	
===================================================================	
test_negative_overflow: 16	
----------------------------	

im = numpy.array([(- 1), 127], dtype=numpy.int8)
(frequencies, bin_centers) = skimage.exposure.histogram(im)
tempResult = arange((- 1), 128)
	
===================================================================	
test_all_negative_image: 24	
----------------------------	

im = numpy.array([(- 128), (- 1)], dtype=numpy.int8)
(frequencies, bin_centers) = skimage.exposure.histogram(im)
tempResult = arange((- 128), 0)
	
===================================================================	
test_adjust_sigmoid_cutoff_one: 262	
----------------------------	

'Verifying the output with expected results for sigmoid correction\n    with cutoff equal to one and gain of 5'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_negative: 289	
----------------------------	

tempResult = arange((- 10), 245, 4)
	
===================================================================	
test_adjust_gamma_greater_one: 237	
----------------------------	

'Verifying the output with expected results for gamma\n    correction with gamma equal to two'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_adjust_inv_sigmoid_cutoff_half: 283	
----------------------------	

'Verifying the output with expected results for inverse sigmoid\n    correction with cutoff equal to half and gain of 10'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_adjust_gamma_less_one: 230	
----------------------------	

'Verifying the output with expected results for gamma\n    correction with gamma equal to half'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_adjust_inv_log: 255	
----------------------------	

'Verifying the output with expected results for inverse logarithmic\n    correction with multiplier constant multiplier equal to unity'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_adjust_sigmoid_cutoff_half: 276	
----------------------------	

'Verifying the output with expected results for sigmoid correction\n    with cutoff equal to half and gain of 10'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_adjust_log: 248	
----------------------------	

'Verifying the output with expected results for logarithmic\n    correction with multiplier constant multiplier equal to unity'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_adjust_sigmoid_cutoff_zero: 269	
----------------------------	

'Verifying the output with expected results for sigmoid correction\n    with cutoff equal to zero and gain of 10'
tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
test_adjust_gamma_neggative: 243	
----------------------------	

tempResult = arange(0, 255, 4, numpy.uint8)
	
===================================================================	
TiffFile._ome_series: 1026	
----------------------------	

'Return image series in OME-TIFF file(s).'
omexml = self.pages[0].tags['image_description'].value
try:
    root = xml.etree.cElementTree.fromstring(omexml)
except xml.etree.cElementTree.ParseError as e:
    warnings.warn(('ome-xml: %s' % e))
    omexml = omexml.decode('utf-8', 'ignore').encode('utf-8')
    root = xml.etree.cElementTree.fromstring(omexml)
uuid = root.attrib.get('UUID', None)
self._files = {uuid: self}
dirname = self._fh.dirname
modulo = {}
series = []
for element in root:
    if element.tag.endswith('BinaryOnly'):
        warnings.warn('ome-xml: not an ome-tiff master file')
        break
    if element.tag.endswith('StructuredAnnotations'):
        for annot in element:
            if (not annot.attrib.get('Namespace', '').endswith('modulo')):
                continue
            for value in annot:
                for modul in value:
                    for along in modul:
                        if (not along.tag[:(- 1)].endswith('Along')):
                            continue
                        axis = along.tag[(- 1)]
                        newaxis = along.attrib.get('Type', 'other')
                        newaxis = AXES_LABELS[newaxis]
                        if ('Start' in along.attrib):
                            step = float(along.attrib.get('Step', 1))
                            start = float(along.attrib['Start'])
                            stop = (float(along.attrib['End']) + step)
                            tempResult = arange(start, stop, step)
	
===================================================================	
match_descriptors: 15	
----------------------------	

"Brute-force matching of descriptors.\n\n    For each descriptor in the first set this matcher finds the closest\n    descriptor in the second set (and vice-versa in the case of enabled\n    cross-checking).\n\n    Parameters\n    ----------\n    descriptors1 : (M, P) array\n        Binary descriptors of size P about M keypoints in the first image.\n    descriptors2 : (N, P) array\n        Binary descriptors of size P about N keypoints in the second image.\n    metric : {'euclidean', 'cityblock', 'minkowski', 'hamming', ...}\n        The metric to compute the distance between two descriptors. See\n        `scipy.spatial.distance.cdist` for all possible types. The hamming\n        distance should be used for binary descriptors. By default the L2-norm\n        is used for all descriptors of dtype float or double and the Hamming\n        distance is used for binary descriptors automatically.\n    p : int\n        The p-norm to apply for ``metric='minkowski'``.\n    max_distance : float\n        Maximum allowed distance between descriptors of two keypoints\n        in separate images to be regarded as a match.\n    cross_check : bool\n        If True, the matched keypoints are returned after cross checking i.e. a\n        matched pair (keypoint1, keypoint2) is returned if keypoint2 is the\n        best match for keypoint1 in second image and keypoint1 is the best\n        match for keypoint2 in first image.\n\n    Returns\n    -------\n    matches : (Q, 2) array\n        Indices of corresponding matches in first and second set of\n        descriptors, where ``matches[:, 0]`` denote the indices in the first\n        and ``matches[:, 1]`` the indices in the second set of descriptors.\n\n    "
if (descriptors1.shape[1] != descriptors2.shape[1]):
    raise ValueError('Descriptor length must equal.')
if (metric is None):
    if numpy.issubdtype(descriptors1.dtype, numpy.bool):
        metric = 'hamming'
    else:
        metric = 'euclidean'
distances = cdist(descriptors1, descriptors2, metric=metric, p=p)
tempResult = arange(descriptors1.shape[0])
	
===================================================================	
_upsampled_dft: 15	
----------------------------	

'\n    Upsampled DFT by matrix multiplication.\n\n    This code is intended to provide the same result as if the following\n    operations were performed:\n        - Embed the array "data" in an array that is ``upsample_factor`` times\n          larger in each dimension.  ifftshift to bring the center of the\n          image to (1,1).\n        - Take the FFT of the larger array.\n        - Extract an ``[upsampled_region_size]`` region of the result, starting\n          with the ``[axis_offsets+1]`` element.\n\n    It achieves this result by computing the DFT in the output array without\n    the need to zeropad. Much faster and memory efficient than the zero-padded\n    FFT approach if ``upsampled_region_size`` is much smaller than\n    ``data.size * upsample_factor``.\n\n    Parameters\n    ----------\n    data : 2D ndarray\n        The input data array (DFT of original data) to upsample.\n    upsampled_region_size : integer or tuple of integers, optional\n        The size of the region to be sampled.  If one integer is provided, it\n        is duplicated up to the dimensionality of ``data``.\n    upsample_factor : integer, optional\n        The upsampling factor.  Defaults to 1.\n    axis_offsets : tuple of integers, optional\n        The offsets of the region to be sampled.  Defaults to None (uses\n        image center)\n\n    Returns\n    -------\n    output : 2D ndarray\n            The upsampled DFT of the specified region.\n    '
if (not hasattr(upsampled_region_size, '__iter__')):
    upsampled_region_size = ([upsampled_region_size] * data.ndim)
elif (len(upsampled_region_size) != data.ndim):
    raise ValueError("shape of upsampled region sizes must be equal to input data's number of dimensions.")
if (axis_offsets is None):
    axis_offsets = ([0] * data.ndim)
elif (len(axis_offsets) != data.ndim):
    raise ValueError("number of axis offsets must be equal to input data's number of dimensions.")
tempResult = arange(data.shape[1])
	
===================================================================	
_upsampled_dft: 15	
----------------------------	

'\n    Upsampled DFT by matrix multiplication.\n\n    This code is intended to provide the same result as if the following\n    operations were performed:\n        - Embed the array "data" in an array that is ``upsample_factor`` times\n          larger in each dimension.  ifftshift to bring the center of the\n          image to (1,1).\n        - Take the FFT of the larger array.\n        - Extract an ``[upsampled_region_size]`` region of the result, starting\n          with the ``[axis_offsets+1]`` element.\n\n    It achieves this result by computing the DFT in the output array without\n    the need to zeropad. Much faster and memory efficient than the zero-padded\n    FFT approach if ``upsampled_region_size`` is much smaller than\n    ``data.size * upsample_factor``.\n\n    Parameters\n    ----------\n    data : 2D ndarray\n        The input data array (DFT of original data) to upsample.\n    upsampled_region_size : integer or tuple of integers, optional\n        The size of the region to be sampled.  If one integer is provided, it\n        is duplicated up to the dimensionality of ``data``.\n    upsample_factor : integer, optional\n        The upsampling factor.  Defaults to 1.\n    axis_offsets : tuple of integers, optional\n        The offsets of the region to be sampled.  Defaults to None (uses\n        image center)\n\n    Returns\n    -------\n    output : 2D ndarray\n            The upsampled DFT of the specified region.\n    '
if (not hasattr(upsampled_region_size, '__iter__')):
    upsampled_region_size = ([upsampled_region_size] * data.ndim)
elif (len(upsampled_region_size) != data.ndim):
    raise ValueError("shape of upsampled region sizes must be equal to input data's number of dimensions.")
if (axis_offsets is None):
    axis_offsets = ([0] * data.ndim)
elif (len(axis_offsets) != data.ndim):
    raise ValueError("number of axis offsets must be equal to input data's number of dimensions.")
tempResult = arange(upsampled_region_size[1])
	
===================================================================	
_upsampled_dft: 16	
----------------------------	

'\n    Upsampled DFT by matrix multiplication.\n\n    This code is intended to provide the same result as if the following\n    operations were performed:\n        - Embed the array "data" in an array that is ``upsample_factor`` times\n          larger in each dimension.  ifftshift to bring the center of the\n          image to (1,1).\n        - Take the FFT of the larger array.\n        - Extract an ``[upsampled_region_size]`` region of the result, starting\n          with the ``[axis_offsets+1]`` element.\n\n    It achieves this result by computing the DFT in the output array without\n    the need to zeropad. Much faster and memory efficient than the zero-padded\n    FFT approach if ``upsampled_region_size`` is much smaller than\n    ``data.size * upsample_factor``.\n\n    Parameters\n    ----------\n    data : 2D ndarray\n        The input data array (DFT of original data) to upsample.\n    upsampled_region_size : integer or tuple of integers, optional\n        The size of the region to be sampled.  If one integer is provided, it\n        is duplicated up to the dimensionality of ``data``.\n    upsample_factor : integer, optional\n        The upsampling factor.  Defaults to 1.\n    axis_offsets : tuple of integers, optional\n        The offsets of the region to be sampled.  Defaults to None (uses\n        image center)\n\n    Returns\n    -------\n    output : 2D ndarray\n            The upsampled DFT of the specified region.\n    '
if (not hasattr(upsampled_region_size, '__iter__')):
    upsampled_region_size = ([upsampled_region_size] * data.ndim)
elif (len(upsampled_region_size) != data.ndim):
    raise ValueError("shape of upsampled region sizes must be equal to input data's number of dimensions.")
if (axis_offsets is None):
    axis_offsets = ([0] * data.ndim)
elif (len(axis_offsets) != data.ndim):
    raise ValueError("number of axis offsets must be equal to input data's number of dimensions.")
col_kernel = numpy.exp((((((- 1j) * 2) * numpy.pi) / (data.shape[1] * upsample_factor)) * (np.fft.ifftshift(np.arange(data.shape[1]))[:, None] - np.floor((data.shape[1] / 2))).dot((numpy.arange(upsampled_region_size[1])[None, :] - axis_offsets[1]))))
tempResult = arange(upsampled_region_size[0])
	
===================================================================	
_upsampled_dft: 16	
----------------------------	

'\n    Upsampled DFT by matrix multiplication.\n\n    This code is intended to provide the same result as if the following\n    operations were performed:\n        - Embed the array "data" in an array that is ``upsample_factor`` times\n          larger in each dimension.  ifftshift to bring the center of the\n          image to (1,1).\n        - Take the FFT of the larger array.\n        - Extract an ``[upsampled_region_size]`` region of the result, starting\n          with the ``[axis_offsets+1]`` element.\n\n    It achieves this result by computing the DFT in the output array without\n    the need to zeropad. Much faster and memory efficient than the zero-padded\n    FFT approach if ``upsampled_region_size`` is much smaller than\n    ``data.size * upsample_factor``.\n\n    Parameters\n    ----------\n    data : 2D ndarray\n        The input data array (DFT of original data) to upsample.\n    upsampled_region_size : integer or tuple of integers, optional\n        The size of the region to be sampled.  If one integer is provided, it\n        is duplicated up to the dimensionality of ``data``.\n    upsample_factor : integer, optional\n        The upsampling factor.  Defaults to 1.\n    axis_offsets : tuple of integers, optional\n        The offsets of the region to be sampled.  Defaults to None (uses\n        image center)\n\n    Returns\n    -------\n    output : 2D ndarray\n            The upsampled DFT of the specified region.\n    '
if (not hasattr(upsampled_region_size, '__iter__')):
    upsampled_region_size = ([upsampled_region_size] * data.ndim)
elif (len(upsampled_region_size) != data.ndim):
    raise ValueError("shape of upsampled region sizes must be equal to input data's number of dimensions.")
if (axis_offsets is None):
    axis_offsets = ([0] * data.ndim)
elif (len(axis_offsets) != data.ndim):
    raise ValueError("number of axis offsets must be equal to input data's number of dimensions.")
col_kernel = numpy.exp((((((- 1j) * 2) * numpy.pi) / (data.shape[1] * upsample_factor)) * (np.fft.ifftshift(np.arange(data.shape[1]))[:, None] - np.floor((data.shape[1] / 2))).dot((numpy.arange(upsampled_region_size[1])[None, :] - axis_offsets[1]))))
tempResult = arange(data.shape[0])
	
===================================================================	
canny: 105	
----------------------------	

"Edge filter an image using the Canny algorithm.\n\n    Parameters\n    -----------\n    image : 2D array\n        Greyscale input image to detect edges on; can be of any dtype.\n    sigma : float\n        Standard deviation of the Gaussian filter.\n    low_threshold : float\n        Lower bound for hysteresis thresholding (linking edges).\n        If None, low_threshold is set to 10% of dtype's max.\n    high_threshold : float\n        Upper bound for hysteresis thresholding (linking edges).\n        If None, high_threshold is set to 20% of dtype's max.\n    mask : array, dtype=bool, optional\n        Mask to limit the application of Canny to a certain area.\n    use_quantiles : bool, optional\n        If True then treat low_threshold and high_threshold as quantiles of the\n        edge magnitude image, rather than absolute edge magnitude values. If True\n        then the thresholds must be in the range [0, 1].\n\n    Returns\n    -------\n    output : 2D array (image)\n        The binary edge map.\n\n    See also\n    --------\n    skimage.sobel\n\n    Notes\n    -----\n    The steps of the algorithm are as follows:\n\n    * Smooth the image using a Gaussian with ``sigma`` width.\n\n    * Apply the horizontal and vertical Sobel operators to get the gradients\n      within the image. The edge strength is the norm of the gradient.\n\n    * Thin potential edges to 1-pixel wide curves. First, find the normal\n      to the edge at each point. This is done by looking at the\n      signs and the relative magnitude of the X-Sobel and Y-Sobel\n      to sort the points into 4 categories: horizontal, vertical,\n      diagonal and antidiagonal. Then look in the normal and reverse\n      directions to see if the values in either of those directions are\n      greater than the point in question. Use interpolation to get a mix of\n      points instead of picking the one that's the closest to the normal.\n\n    * Perform a hysteresis thresholding: first label all points above the\n      high threshold as edges. Then recursively label any point above the\n      low threshold that is 8-connected to a labeled point as an edge.\n\n    References\n    -----------\n    .. [1] Canny, J., A Computational Approach To Edge Detection, IEEE Trans.\n           Pattern Analysis and Machine Intelligence, 8:679-714, 1986\n    .. [2] William Green's Canny tutorial\n           http://dasl.mem.drexel.edu/alumni/bGreen/www.pages.drexel.edu/_weg22/can_tut.html\n\n    Examples\n    --------\n    >>> from skimage import feature\n    >>> # Generate noisy image of a square\n    >>> im = np.zeros((256, 256))\n    >>> im[64:-64, 64:-64] = 1\n    >>> im += 0.2 * np.random.rand(*im.shape)\n    >>> # First trial with the Canny filter, with the default smoothing\n    >>> edges1 = feature.canny(im)\n    >>> # Increase the smoothing for better results\n    >>> edges2 = feature.canny(im, sigma=3)\n    "
assert_nD(image, 2)
if (low_threshold is None):
    low_threshold = (0.1 * dtype_limits(image, clip_negative=False)[1])
if (high_threshold is None):
    high_threshold = (0.2 * dtype_limits(image, clip_negative=False)[1])
if (mask is None):
    mask = numpy.ones(image.shape, dtype=bool)

def fsmooth(x):
    return gaussian_filter(x, sigma, mode='constant')
smoothed = smooth_with_function_and_mask(image, fsmooth, mask)
jsobel = scipy.ndimage.sobel(smoothed, axis=1)
isobel = scipy.ndimage.sobel(smoothed, axis=0)
abs_isobel = numpy.abs(isobel)
abs_jsobel = numpy.abs(jsobel)
magnitude = numpy.hypot(isobel, jsobel)
s = generate_binary_structure(2, 2)
eroded_mask = binary_erosion(mask, s, border_value=0)
eroded_mask = (eroded_mask & (magnitude > 0))
local_maxima = numpy.zeros(image.shape, bool)
pts_plus = (((isobel >= 0) & (jsobel >= 0)) & (abs_isobel >= abs_jsobel))
pts_minus = (((isobel <= 0) & (jsobel <= 0)) & (abs_isobel >= abs_jsobel))
pts = (pts_plus | pts_minus)
pts = (eroded_mask & pts)
c1 = magnitude[1:, :][pts[:(- 1), :]]
c2 = magnitude[1:, 1:][pts[:(- 1), :(- 1)]]
m = magnitude[pts]
w = (abs_jsobel[pts] / abs_isobel[pts])
c_plus = (((c2 * w) + (c1 * (1 - w))) <= m)
c1 = magnitude[:(- 1), :][pts[1:, :]]
c2 = magnitude[:(- 1), :(- 1)][pts[1:, 1:]]
c_minus = (((c2 * w) + (c1 * (1 - w))) <= m)
local_maxima[pts] = (c_plus & c_minus)
pts_plus = (((isobel >= 0) & (jsobel >= 0)) & (abs_isobel <= abs_jsobel))
pts_minus = (((isobel <= 0) & (jsobel <= 0)) & (abs_isobel <= abs_jsobel))
pts = (pts_plus | pts_minus)
pts = (eroded_mask & pts)
c1 = magnitude[:, 1:][pts[:, :(- 1)]]
c2 = magnitude[1:, 1:][pts[:(- 1), :(- 1)]]
m = magnitude[pts]
w = (abs_isobel[pts] / abs_jsobel[pts])
c_plus = (((c2 * w) + (c1 * (1 - w))) <= m)
c1 = magnitude[:, :(- 1)][pts[:, 1:]]
c2 = magnitude[:(- 1), :(- 1)][pts[1:, 1:]]
c_minus = (((c2 * w) + (c1 * (1 - w))) <= m)
local_maxima[pts] = (c_plus & c_minus)
pts_plus = (((isobel <= 0) & (jsobel >= 0)) & (abs_isobel <= abs_jsobel))
pts_minus = (((isobel >= 0) & (jsobel <= 0)) & (abs_isobel <= abs_jsobel))
pts = (pts_plus | pts_minus)
pts = (eroded_mask & pts)
c1a = magnitude[:, 1:][pts[:, :(- 1)]]
c2a = magnitude[:(- 1), 1:][pts[1:, :(- 1)]]
m = magnitude[pts]
w = (abs_isobel[pts] / abs_jsobel[pts])
c_plus = (((c2a * w) + (c1a * (1.0 - w))) <= m)
c1 = magnitude[:, :(- 1)][pts[:, 1:]]
c2 = magnitude[1:, :(- 1)][pts[:(- 1), 1:]]
c_minus = (((c2 * w) + (c1 * (1.0 - w))) <= m)
local_maxima[pts] = (c_plus & c_minus)
pts_plus = (((isobel <= 0) & (jsobel >= 0)) & (abs_isobel >= abs_jsobel))
pts_minus = (((isobel >= 0) & (jsobel <= 0)) & (abs_isobel >= abs_jsobel))
pts = (pts_plus | pts_minus)
pts = (eroded_mask & pts)
c1 = magnitude[:(- 1), :][pts[1:, :]]
c2 = magnitude[:(- 1), 1:][pts[1:, :(- 1)]]
m = magnitude[pts]
w = (abs_jsobel[pts] / abs_isobel[pts])
c_plus = (((c2 * w) + (c1 * (1 - w))) <= m)
c1 = magnitude[1:, :][pts[:(- 1), :]]
c2 = magnitude[1:, :(- 1)][pts[:(- 1), 1:]]
c_minus = (((c2 * w) + (c1 * (1 - w))) <= m)
local_maxima[pts] = (c_plus & c_minus)
if use_quantiles:
    if ((high_threshold > 1.0) or (low_threshold > 1.0)):
        raise ValueError('Quantile thresholds must not be > 1.0')
    if ((high_threshold < 0.0) or (low_threshold < 0.0)):
        raise ValueError('Quantile thresholds must not be < 0.0')
    high_threshold = numpy.percentile(magnitude, (100.0 * high_threshold))
    low_threshold = numpy.percentile(magnitude, (100.0 * low_threshold))
high_mask = (local_maxima & (magnitude >= high_threshold))
low_mask = (local_maxima & (magnitude >= low_threshold))
strel = numpy.ones((3, 3), bool)
(labels, count) = label(low_mask, strel)
if (count == 0):
    return low_mask
tempResult = arange(count, dtype=numpy.int32)
	
===================================================================	
hog: 50	
----------------------------	

"Extract Histogram of Oriented Gradients (HOG) for a given image.\n\n    Compute a Histogram of Oriented Gradients (HOG) by\n\n        1. (optional) global image normalization\n        2. computing the gradient image in x and y\n        3. computing gradient histograms\n        4. normalizing across blocks\n        5. flattening into a feature vector\n\n    Parameters\n    ----------\n    image : (M, N) ndarray\n        Input image (greyscale).\n    orientations : int, optional\n        Number of orientation bins.\n    pixels_per_cell : 2-tuple (int, int), optional\n        Size (in pixels) of a cell.\n    cells_per_block : 2-tuple (int, int), optional\n        Number of cells in each block.\n    block_norm : str {'L1', 'L1-sqrt', 'L2', 'L2-Hys'}, optional\n        Block normalization method:\n\n        ``L1``\n           Normalization using L1-norm. (default)\n        ``L1-sqrt``\n           Normalization using L1-norm, followed by square root.\n        ``L2``\n           Normalization using L2-norm.\n        ``L2-Hys``\n           Normalization using L2-norm, followed by limiting the\n           maximum values to 0.2 (`Hys` stands for `hysteresis`) and\n           renormalization using L2-norm.\n           For details, see [3]_, [4]_.\n\n    visualise : bool, optional\n        Also return an image of the HOG.\n    transform_sqrt : bool, optional\n        Apply power law compression to normalize the image before\n        processing. DO NOT use this if the image contains negative\n        values. Also see `notes` section below.\n    feature_vector : bool, optional\n        Return the data as a feature vector by calling .ravel() on the result\n        just before returning.\n    normalise : bool, deprecated\n        The parameter is deprecated. Use `transform_sqrt` for power law\n        compression. `normalise` has been deprecated.\n\n    Returns\n    -------\n    newarr : ndarray\n        HOG for the image as a 1D (flattened) array.\n    hog_image : ndarray (if visualise=True)\n        A visualisation of the HOG image.\n\n    References\n    ----------\n    .. [1] http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n\n    .. [2] Dalal, N and Triggs, B, Histograms of Oriented Gradients for\n           Human Detection, IEEE Computer Society Conference on Computer\n           Vision and Pattern Recognition 2005 San Diego, CA, USA,\n           https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf,\n           DOI:10.1109/CVPR.2005.177\n\n    .. [3] Lowe, D.G., Distinctive image features from scale-invatiant\n           keypoints, International Journal of Computer Vision (2004) 60: 91,\n           http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf,\n           DOI:10.1023/B:VISI.0000029664.99615.94\n\n    .. [4] Dalal, N, Finding People in Images and Videos,\n           Human-Computer Interaction [cs.HC], Institut National Polytechnique\n           de Grenoble - INPG, 2006,\n           https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf\n\n    Notes\n    -----\n    The presented code implements the HOG extraction method from [2]_ with\n    the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the\n    paper; (II) no smoothing within cells (Gaussian spatial window with sigma=8pix\n    in the paper); (III) L1 block normalization is used (L2-Hys in the paper).\n\n    Power law compression, also known as Gamma correction, is used to reduce\n    the effects of shadowing and illumination variations. The compression makes\n    the dark regions lighter. When the kwarg `transform_sqrt` is set to\n    ``True``, the function computes the square root of each color channel\n    and then applies the hog algorithm to the image.\n    "
if (block_norm == 'L1'):
    warn('Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15', skimage_deprecation)
image = numpy.atleast_2d(image)
'\n    The first stage applies an optional global image normalization\n    equalisation that is designed to reduce the influence of illumination\n    effects. In practice we use gamma (power law) compression, either\n    computing the square root or the log of each colour channel.\n    Image texture strength is typically proportional to the local surface\n    illumination so this compression helps to reduce the effects of local\n    shadowing and illumination variations.\n    '
assert_nD(image, 2)
if (normalise is not None):
    raise ValueError('The ``normalise`` parameter was removed due to incorrect behavior: it only applied a square root instead of a true normalization. If you wish to duplicate the old behavior, set ``transform_sqrt=True``. ``normalise`` will be completely removed in v0.14.')
if transform_sqrt:
    image = numpy.sqrt(image)
'\n    The second stage computes first order image gradients. These capture\n    contour, silhouette and some texture information, while providing\n    further resistance to illumination variations. The locally dominant\n    colour channel is used, which provides colour invariance to a large\n    extent. Variant methods may also include second order image derivatives,\n    which act as primitive bar detectors - a useful feature for capturing,\n    e.g. bar like structures in bicycles and limbs in humans.\n    '
if (image.dtype.kind == 'u'):
    image = image.astype('float')
(gy, gx) = [numpy.ascontiguousarray(g, dtype=numpy.double) for g in numpy.gradient(image)]
'\n    The third stage aims to produce an encoding that is sensitive to\n    local image content while remaining resistant to small changes in\n    pose or appearance. The adopted method pools gradient orientation\n    information locally in the same way as the SIFT [Lowe 2004]\n    feature. The image window is divided into small spatial regions,\n    called "cells". For each cell we accumulate a local 1-D histogram\n    of gradient or edge orientations over all the pixels in the\n    cell. This combined cell-level 1-D histogram forms the basic\n    "orientation histogram" representation. Each orientation histogram\n    divides the gradient angle range into a fixed number of\n    predetermined bins. The gradient magnitudes of the pixels in the\n    cell are used to vote into the orientation histogram.\n    '
(sy, sx) = image.shape
(cx, cy) = pixels_per_cell
(bx, by) = cells_per_block
n_cellsx = int((sx // cx))
n_cellsy = int((sy // cy))
orientation_histogram = numpy.zeros((n_cellsy, n_cellsx, orientations))
_hoghistogram.hog_histograms(gx, gy, cx, cy, sx, sy, n_cellsx, n_cellsy, orientations, orientation_histogram)
hog_image = None
if visualise:
    from .. import draw
    radius = ((min(cx, cy) // 2) - 1)
    tempResult = arange(orientations)
	
===================================================================	
TestPeakLocalMax.test_num_peaks3D: 113	
----------------------------	

image = numpy.zeros((10, 10, 100))
tempResult = arange(20)
	
===================================================================	
threshold_triangle: 203	
----------------------------	

'Return threshold value based on the triangle algorithm.\n\n    Parameters\n    ----------\n    image : (N, M[, ..., P]) ndarray\n        Grayscale input image.\n    nbins : int, optional\n        Number of bins used to calculate histogram. This value is ignored for\n        integer arrays.\n\n    Returns\n    -------\n    threshold : float\n        Upper threshold value. All pixels with an intensity higher than\n        this value are assumed to be foreground.\n\n    References\n    ----------\n    .. [1] Zack, G. W., Rogers, W. E. and Latt, S. A., 1977,\n       Automatic Measurement of Sister Chromatid Exchange Frequency,\n       Journal of Histochemistry and Cytochemistry 25 (7), pp. 741-753\n       DOI:10.1177/25.7.70454\n    .. [2] ImageJ AutoThresholder code,\n       http://fiji.sc/wiki/index.php/Auto_Threshold\n\n    Examples\n    --------\n    >>> from skimage.data import camera\n    >>> image = camera()\n    >>> thresh = threshold_triangle(image)\n    >>> binary = image > thresh\n    '
(hist, bin_centers) = histogram(image.ravel(), nbins)
nbins = len(hist)
arg_peak_height = numpy.argmax(hist)
peak_height = hist[arg_peak_height]
(arg_low_level, arg_high_level) = numpy.where((hist > 0))[0][[0, (- 1)]]
flip = ((arg_peak_height - arg_low_level) < (arg_high_level - arg_peak_height))
if flip:
    hist = hist[::(- 1)]
    arg_low_level = ((nbins - arg_high_level) - 1)
    arg_peak_height = ((nbins - arg_peak_height) - 1)
del arg_high_level
width = (arg_peak_height - arg_low_level)
tempResult = arange(width)
	
===================================================================	
_frangi_hessian_common_filter: 8	
----------------------------	

"This is an intermediate function for Frangi and Hessian filters.\n\n    Shares the common code for Frangi and Hessian functions.\n\n    Parameters\n    ----------\n    image : (N, M) ndarray\n        Array with input image data.\n    scale_range : 2-tuple of floats, optional\n        The range of sigmas used.\n    scale_step : float, optional\n        Step size between sigmas.\n    beta1 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to deviation from a blob-like structure.\n    beta2 : float, optional\n        Frangi correction constant that adjusts the filter's\n        sensitivity to areas of high variance/texture/structure.\n\n    Returns\n    -------\n    filtered_list : list\n        List of pre-filtered images.\n\n    "
from ..feature import hessian_matrix, hessian_matrix_eigvals
tempResult = arange(scale_range[0], scale_range[1], scale_step)
	
===================================================================	
TestRank.test_entropy: 312	
----------------------------	

selem = numpy.ones((16, 16), dtype=numpy.uint8)
data = np.tile(np.asarray([0, 1]), (100, 100)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 1)
data = np.tile(np.asarray([[0, 1], [2, 3]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 2)
data = np.tile(np.asarray([[0, 1, 2, 3], [4, 5, 6, 7]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 3)
tempResult = arange(16)
	
===================================================================	
TestRank.test_entropy: 314	
----------------------------	

selem = numpy.ones((16, 16), dtype=numpy.uint8)
data = np.tile(np.asarray([0, 1]), (100, 100)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 1)
data = np.tile(np.asarray([[0, 1], [2, 3]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 2)
data = np.tile(np.asarray([[0, 1, 2, 3], [4, 5, 6, 7]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 3)
data = np.tile(np.reshape(np.arange(16), (4, 4)), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 4)
tempResult = arange(64)
	
===================================================================	
TestRank.test_entropy: 316	
----------------------------	

selem = numpy.ones((16, 16), dtype=numpy.uint8)
data = np.tile(np.asarray([0, 1]), (100, 100)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 1)
data = np.tile(np.asarray([[0, 1], [2, 3]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 2)
data = np.tile(np.asarray([[0, 1, 2, 3], [4, 5, 6, 7]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 3)
data = np.tile(np.reshape(np.arange(16), (4, 4)), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 4)
data = np.tile(np.reshape(np.arange(64), (8, 8)), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 6)
tempResult = arange(256)
	
===================================================================	
TestRank.test_entropy: 320	
----------------------------	

selem = numpy.ones((16, 16), dtype=numpy.uint8)
data = np.tile(np.asarray([0, 1]), (100, 100)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 1)
data = np.tile(np.asarray([[0, 1], [2, 3]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 2)
data = np.tile(np.asarray([[0, 1, 2, 3], [4, 5, 6, 7]]), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 3)
data = np.tile(np.reshape(np.arange(16), (4, 4)), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 4)
data = np.tile(np.reshape(np.arange(64), (8, 8)), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 6)
data = np.tile(np.reshape(np.arange(256), (16, 16)), (10, 10)).astype(numpy.uint8)
assert (numpy.max(skimage.filters.rank.entropy(data, selem)) == 8)
selem = numpy.ones((64, 64), dtype=numpy.uint8)
data = numpy.zeros((65, 65), dtype=numpy.uint16)
tempResult = arange(4096)
	
===================================================================	
test_multichannel: 32	
----------------------------	

a = numpy.zeros((5, 5, 3))
tempResult = arange(1, 4)
	
===================================================================	
test_threshold_minimum_synthetic: 240	
----------------------------	

tempResult = arange((25 * 25), dtype=numpy.uint8)
	
===================================================================	
TestSimpleImage.test_yen_arange: 52	
----------------------------	

tempResult = arange(256)
	
===================================================================	
cut_threshold: 19	
----------------------------	

'Combine regions separated by weight less than threshold.\n\n    Given an image\'s labels and its RAG, output new labels by\n    combining regions whose nodes are separated by a weight less\n    than the given threshold.\n\n    Parameters\n    ----------\n    labels : ndarray\n        The array of labels.\n    rag : RAG\n        The region adjacency graph.\n    thresh : float\n        The threshold. Regions connected by edges with smaller weights are\n        combined.\n    in_place : bool\n        If set, modifies `rag` in place. The function will remove the edges\n        with weights less that `thresh`. If set to `False` the function\n        makes a copy of `rag` before proceeding.\n\n    Returns\n    -------\n    out : ndarray\n        The new labelled array.\n\n    Examples\n    --------\n    >>> from skimage import data, segmentation\n    >>> from skimage.future import graph\n    >>> img = data.astronaut()\n    >>> labels = segmentation.slic(img)\n    >>> rag = graph.rag_mean_color(img, labels)\n    >>> new_labels = graph.cut_threshold(labels, rag, 10)\n\n    References\n    ----------\n    .. [1] Alain Tremeau and Philippe Colantoni\n           "Regions Adjacency Graph Applied To Color Image Segmentation"\n           http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.5274\n\n    '
if (not in_place):
    rag = rag.copy()
to_remove = [(x, y) for (x, y, d) in rag.edges_iter(data=True) if (d['weight'] >= thresh)]
rag.remove_edges_from(to_remove)
comps = networkx.connected_components(rag)
tempResult = arange((labels.max() + 1), dtype=labels.dtype)
	
===================================================================	
merge_hierarchical: 58	
----------------------------	

'Perform hierarchical merging of a RAG.\n\n    Greedily merges the most similar pair of nodes until no edges lower than\n    `thresh` remain.\n\n    Parameters\n    ----------\n    labels : ndarray\n        The array of labels.\n    rag : RAG\n        The Region Adjacency Graph.\n    thresh : float\n        Regions connected by an edge with weight smaller than `thresh` are\n        merged.\n    rag_copy : bool\n        If set, the RAG copied before modifying.\n    in_place_merge : bool\n        If set, the nodes are merged in place. Otherwise, a new node is\n        created for each merge..\n    merge_func : callable\n        This function is called before merging two nodes. For the RAG `graph`\n        while merging `src` and `dst`, it is called as follows\n        ``merge_func(graph, src, dst)``.\n    weight_func : callable\n        The function to compute the new weights of the nodes adjacent to the\n        merged node. This is directly supplied as the argument `weight_func`\n        to `merge_nodes`.\n\n    Returns\n    -------\n    out : ndarray\n        The new labeled array.\n\n    '
if rag_copy:
    rag = rag.copy()
edge_heap = []
for (n1, n2, data) in rag.edges_iter(data=True):
    wt = data['weight']
    heap_item = [wt, n1, n2, True]
    heapq.heappush(edge_heap, heap_item)
    data['heap item'] = heap_item
while ((len(edge_heap) > 0) and (edge_heap[0][0] < thresh)):
    (_, n1, n2, valid) = heapq.heappop(edge_heap)
    if valid:
        for nbr in rag.neighbors(n1):
            _invalidate_edge(rag, n1, nbr)
        for nbr in rag.neighbors(n2):
            _invalidate_edge(rag, n2, nbr)
        if (not in_place_merge):
            next_id = rag.next_id()
            _rename_node(rag, n2, next_id)
            (src, dst) = (n1, next_id)
        else:
            (src, dst) = (n1, n2)
        merge_func(rag, src, dst)
        new_id = rag.merge_nodes(src, dst, weight_func)
        _revalidate_node_edges(rag, new_id, edge_heap)
tempResult = arange((labels.max() + 1))
	
===================================================================	
show_rag: 180	
----------------------------	

"Show a Region Adjacency Graph on an image.\n\n    Given a labelled image and its corresponding RAG, show the nodes and edges\n    of the RAG on the image with the specified colors. Edges are displayed between\n    the centroid of the 2 adjacent regions in the image.\n\n    Parameters\n    ----------\n    labels : ndarray, shape (M, N)\n        The labelled image.\n    rag : RAG\n        The Region Adjacency Graph.\n    img : ndarray, shape (M, N[, 3])\n        Input image. If `colormap` is `None`, the image should be in RGB\n        format.\n    border_color : color spec, optional\n        Color with which the borders between regions are drawn.\n    edge_width : float, optional\n        The thickness with which the RAG edges are drawn.\n    edge_cmap : :py:class:`matplotlib.colors.Colormap`, optional\n        Any matplotlib colormap with which the edges are drawn.\n    img_cmap : :py:class:`matplotlib.colors.Colormap`, optional\n        Any matplotlib colormap with which the image is draw. If set to `None`\n        the image is drawn as it is.\n    in_place : bool, optional\n        If set, the RAG is modified in place. For each node `n` the function\n        will set a new attribute ``rag.node[n]['centroid']``.\n    ax : :py:class:`matplotlib.axes.Axes`, optional\n        The axes to draw on. If not specified, new axes are created and drawn\n        on.\n\n    Returns\n    -------\n    lc : :py:class:`matplotlib.collections.LineCollection`\n         A colection of lines that represent the edges of the graph. It can be\n         passed to the :meth:`matplotlib.figure.Figure.colorbar` function.\n\n    Examples\n    --------\n    >>> from skimage import data, segmentation\n    >>> from skimage.future import graph\n    >>> img = data.coffee()\n    >>> labels = segmentation.slic(img)\n    >>> g =  graph.rag_mean_color(img, labels)\n    >>> lc = graph.show_rag(labels, g, img)\n    >>> cbar = plt.colorbar(lc)\n    "
if (not in_place):
    rag = rag.copy()
if (ax is None):
    (fig, ax) = matplotlib.pyplot.subplots()
out = util.img_as_float(img, force_copy=True)
if (img_cmap is None):
    if ((img.ndim < 3) or (img.shape[2] not in [3, 4])):
        msg = 'If colormap is `None`, an RGB or RGBA image should be given'
        raise ValueError(msg)
    out = img[:, :, :3]
else:
    img_cmap = matplotlib.cm.get_cmap(img_cmap)
    out = color.rgb2gray(img)
    out = img_cmap(out)[:, :, :3]
edge_cmap = matplotlib.cm.get_cmap(edge_cmap)
offset = 1
tempResult = arange((labels.max() + 1))
	
===================================================================	
test_generic_rag_3d: 140	
----------------------------	

tempResult = arange(8, dtype=numpy.uint8)
	
===================================================================	
TestHistogram.test_counts: 15	
----------------------------	

tempResult = arange(255)
	
===================================================================	
test_stack_basic: 10	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestPrepareForDisplay.test_grey: 21	
----------------------------	

with expected_warnings(['precision loss']):
    tempResult = arange(12, dtype=float)
	
===================================================================	
test_block_reduce_max: 50	
----------------------------	

tempResult = arange((4 * 6))
	
===================================================================	
test_block_reduce_max: 54	
----------------------------	

image1 = np.arange((4 * 6)).reshape(4, 6)
out1 = block_reduce(image1, (2, 3), func=numpy.max)
expected1 = numpy.array([[8, 11], [20, 23]])
assert_equal(expected1, out1)
tempResult = arange((5 * 8))
	
===================================================================	
test_block_reduce_mean: 17	
----------------------------	

tempResult = arange((4 * 6))
	
===================================================================	
test_block_reduce_mean: 21	
----------------------------	

image1 = np.arange((4 * 6)).reshape(4, 6)
out1 = block_reduce(image1, (2, 3), func=numpy.mean)
expected1 = numpy.array([[4.0, 7.0], [16.0, 19.0]])
assert_equal(expected1, out1)
tempResult = arange((5 * 8))
	
===================================================================	
test_block_reduce_sum: 7	
----------------------------	

tempResult = arange((4 * 6))
	
===================================================================	
test_block_reduce_sum: 11	
----------------------------	

image1 = np.arange((4 * 6)).reshape(4, 6)
out1 = block_reduce(image1, (2, 3))
expected1 = numpy.array([[24, 42], [96, 114]])
assert_equal(expected1, out1)
tempResult = arange((5 * 8))
	
===================================================================	
test_block_reduce_median: 27	
----------------------------	

tempResult = arange((4 * 6))
	
===================================================================	
test_block_reduce_median: 31	
----------------------------	

image1 = np.arange((4 * 6)).reshape(4, 6)
out1 = block_reduce(image1, (2, 3), func=numpy.median)
expected1 = numpy.array([[4.0, 7.0], [16.0, 19.0]])
assert_equal(expected1, out1)
tempResult = arange((5 * 8))
	
===================================================================	
test_invalid_block_size: 60	
----------------------------	

tempResult = arange((4 * 6))
	
===================================================================	
test_block_reduce_min: 40	
----------------------------	

tempResult = arange((4 * 6))
	
===================================================================	
test_block_reduce_min: 44	
----------------------------	

image1 = np.arange((4 * 6)).reshape(4, 6)
out1 = block_reduce(image1, (2, 3), func=numpy.min)
expected1 = numpy.array([[0, 3], [12, 15]])
assert_equal(expected1, out1)
tempResult = arange((5 * 8))
	
===================================================================	
test_line_modelND_estimate: 59	
----------------------------	

model0 = LineModelND()
model0.params = (numpy.array([0, 0, 0], dtype='float'), (numpy.array([1, 1, 1], dtype='float') / numpy.sqrt(3)))
tempResult = arange((- 100), 100)
	
===================================================================	
test_circle_model_predict: 88	
----------------------------	

model = CircleModel()
r = 5
model.params = (0, 0, r)
tempResult = arange(0, (2 * numpy.pi), (numpy.pi / 2))
	
===================================================================	
test_line_modelND_predict: 52	
----------------------------	

model = LineModelND()
model.params = (numpy.array([0, 0]), numpy.array([0.2, 0.98]))
tempResult = arange((- 10), 10)
	
===================================================================	
test_line_model_predict: 16	
----------------------------	

model = LineModelND()
model.params = ((0, 0), (1, 1))
tempResult = arange((- 10), 10)
	
===================================================================	
test_line_model_estimate: 23	
----------------------------	

model0 = LineModelND()
model0.params = ((0, 0), (1, 1))
tempResult = arange((- 100), 100)
	
===================================================================	
test_ellipse_model_predict: 116	
----------------------------	

model = EllipseModel()
model.params = (0, 0, 5, 10, 0)
tempResult = arange(0, (2 * numpy.pi), (numpy.pi / 2))
	
===================================================================	
module: 5	
----------------------------	

from numpy.testing import assert_equal, assert_almost_equal
import numpy as np
from skimage.measure import profile_line
tempResult = arange(100)
	
===================================================================	
test_vertical_upward: 24	
----------------------------	

prof = profile_line(image, (8, 5), (2, 5), order=0)
tempResult = arange(85, 15, (- 10))
	
===================================================================	
test_45deg_right_upward: 39	
----------------------------	

prof = profile_line(image, (8, 2), (2, 8), order=1)
tempResult = arange(82, 27, (- 6))
	
===================================================================	
test_horizontal_rightward: 9	
----------------------------	

prof = profile_line(image, (0, 2), (0, 8), order=0)
tempResult = arange(2, 9)
	
===================================================================	
test_horizontal_leftward: 14	
----------------------------	

prof = profile_line(image, (0, 8), (0, 2), order=0)
tempResult = arange(8, 1, (- 1))
	
===================================================================	
test_vertical_downward: 19	
----------------------------	

prof = profile_line(image, (2, 5), (8, 5), order=0)
tempResult = arange(25, 95, 10)
	
===================================================================	
test_45deg_left_upward: 44	
----------------------------	

prof = profile_line(image, (8, 8), (2, 2), order=1)
tempResult = arange(88, 21, ((- 22.0) / 3))
	
===================================================================	
test_45deg_left_downward: 49	
----------------------------	

prof = profile_line(image, (2, 8), (8, 2), order=1)
tempResult = arange(28, 83, 6)
	
===================================================================	
diamond: 16	
----------------------------	

'Generates a flat, diamond-shaped structuring element.\n\n    A pixel is part of the neighborhood (i.e. labeled 1) if\n    the city block/Manhattan distance between it and the center of\n    the neighborhood is no greater than radius.\n\n    Parameters\n    ----------\n    radius : int\n        The radius of the diamond-shaped structuring element.\n\n    Other Parameters\n    ----------------\n    dtype : data-type\n        The data type of the structuring element.\n\n    Returns\n    -------\n\n    selem : ndarray\n        The structuring element where elements of the neighborhood\n        are 1 and 0 otherwise.\n    '
tempResult = arange(0, ((radius * 2) + 1))
	
===================================================================	
disk: 22	
----------------------------	

'Generates a flat, disk-shaped structuring element.\n\n    A pixel is within the neighborhood if the euclidean distance between\n    it and the origin is no greater than radius.\n\n    Parameters\n    ----------\n    radius : int\n        The radius of the disk-shaped structuring element.\n\n    Other Parameters\n    ----------------\n    dtype : data-type\n        The data type of the structuring element.\n\n    Returns\n    -------\n    selem : ndarray\n        The structuring element where elements of the neighborhood\n        are 1 and 0 otherwise.\n    '
tempResult = arange((- radius), (radius + 1))
	
===================================================================	
medial_axis: 87	
----------------------------	

'\n    Compute the medial axis transform of a binary image\n\n    Parameters\n    ----------\n    image : binary ndarray, shape (M, N)\n        The image of the shape to be skeletonized.\n    mask : binary ndarray, shape (M, N), optional\n        If a mask is given, only those elements in `image` with a true\n        value in `mask` are used for computing the medial axis.\n    return_distance : bool, optional\n        If true, the distance transform is returned as well as the skeleton.\n\n    Returns\n    -------\n    out : ndarray of bools\n        Medial axis transform of the image\n    dist : ndarray of ints, optional\n        Distance transform of the image (only returned if `return_distance`\n        is True)\n\n    See also\n    --------\n    skeletonize\n\n    Notes\n    -----\n    This algorithm computes the medial axis transform of an image\n    as the ridges of its distance transform.\n\n    The different steps of the algorithm are as follows\n     * A lookup table is used, that assigns 0 or 1 to each configuration of\n       the 3x3 binary square, whether the central pixel should be removed\n       or kept. We want a point to be removed if it has more than one neighbor\n       and if removing it does not change the number of connected components.\n\n     * The distance transform to the background is computed, as well as\n       the cornerness of the pixel.\n\n     * The foreground (value of 1) points are ordered by\n       the distance transform, then the cornerness.\n\n     * A cython function is called to reduce the image to its skeleton. It\n       processes pixels in the order determined at the previous step, and\n       removes or maintains a pixel according to the lookup table. Because\n       of the ordering, it is possible to process all pixels in only one\n       pass.\n\n    Examples\n    --------\n    >>> square = np.zeros((7, 7), dtype=np.uint8)\n    >>> square[1:-1, 2:-2] = 1\n    >>> square\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n    >>> medial_axis(square).astype(np.uint8)\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 1, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 1, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n\n    '
global _eight_connect
if (mask is None):
    masked_image = image.astype(numpy.bool)
else:
    masked_image = image.astype(bool).copy()
    masked_image[(~ mask)] = False
tempResult = arange(512)
	
===================================================================	
medial_axis: 101	
----------------------------	

'\n    Compute the medial axis transform of a binary image\n\n    Parameters\n    ----------\n    image : binary ndarray, shape (M, N)\n        The image of the shape to be skeletonized.\n    mask : binary ndarray, shape (M, N), optional\n        If a mask is given, only those elements in `image` with a true\n        value in `mask` are used for computing the medial axis.\n    return_distance : bool, optional\n        If true, the distance transform is returned as well as the skeleton.\n\n    Returns\n    -------\n    out : ndarray of bools\n        Medial axis transform of the image\n    dist : ndarray of ints, optional\n        Distance transform of the image (only returned if `return_distance`\n        is True)\n\n    See also\n    --------\n    skeletonize\n\n    Notes\n    -----\n    This algorithm computes the medial axis transform of an image\n    as the ridges of its distance transform.\n\n    The different steps of the algorithm are as follows\n     * A lookup table is used, that assigns 0 or 1 to each configuration of\n       the 3x3 binary square, whether the central pixel should be removed\n       or kept. We want a point to be removed if it has more than one neighbor\n       and if removing it does not change the number of connected components.\n\n     * The distance transform to the background is computed, as well as\n       the cornerness of the pixel.\n\n     * The foreground (value of 1) points are ordered by\n       the distance transform, then the cornerness.\n\n     * A cython function is called to reduce the image to its skeleton. It\n       processes pixels in the order determined at the previous step, and\n       removes or maintains a pixel according to the lookup table. Because\n       of the ordering, it is possible to process all pixels in only one\n       pass.\n\n    Examples\n    --------\n    >>> square = np.zeros((7, 7), dtype=np.uint8)\n    >>> square[1:-1, 2:-2] = 1\n    >>> square\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 1, 1, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n    >>> medial_axis(square).astype(np.uint8)\n    array([[0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 1, 0, 1, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 0, 1, 0, 0, 0],\n           [0, 0, 1, 0, 1, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\n\n    '
global _eight_connect
if (mask is None):
    masked_image = image.astype(numpy.bool)
else:
    masked_image = image.astype(bool).copy()
    masked_image[(~ mask)] = False
center_is_foreground = (np.arange(512) & (2 ** 4)).astype(bool)
table = (center_is_foreground & (numpy.array([(scipy.ndimage.label(_pattern_of(index), _eight_connect)[1] != scipy.ndimage.label(_pattern_of((index & (~ (2 ** 4)))), _eight_connect)[1]) for index in range(512)]) | numpy.array([(numpy.sum(_pattern_of(index)) < 3) for index in range(512)])))
distance = scipy.ndimage.distance_transform_edt(masked_image)
if return_distance:
    store_distance = distance.copy()
cornerness_table = numpy.array([(9 - numpy.sum(_pattern_of(index))) for index in range(512)])
corner_score = _table_lookup(masked_image, cornerness_table)
(i, j) = numpy.mgrid[0:image.shape[0], 0:image.shape[1]]
result = masked_image.copy()
distance = distance[result]
i = numpy.ascontiguousarray(i[result], dtype=numpy.intp)
j = numpy.ascontiguousarray(j[result], dtype=numpy.intp)
result = numpy.ascontiguousarray(result, numpy.uint8)
generator = numpy.random.RandomState(0)
tempResult = arange(masked_image.sum())
	
===================================================================	
test_pixel_rgb: 60	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
tempResult = arange(3)
	
===================================================================	
test_pixel_rgb: 61	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
pixel.rgb = numpy.arange(3)
tempResult = arange(3)
	
===================================================================	
test_pixel_rgb: 67	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
pixel.rgb = numpy.arange(3)
assert_equal(pixel.rgb, numpy.arange(3))
for (i, channel) in enumerate((pixel.red, pixel.green, pixel.blue)):
    assert_equal(channel, i)
pixel.red = 3
pixel.green = 4
pixel.blue = 5
tempResult = arange(3)
	
===================================================================	
test_pixel_rgb: 70	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
pixel.rgb = numpy.arange(3)
assert_equal(pixel.rgb, numpy.arange(3))
for (i, channel) in enumerate((pixel.red, pixel.green, pixel.blue)):
    assert_equal(channel, i)
pixel.red = 3
pixel.green = 4
pixel.blue = 5
assert_equal(pixel.rgb, (numpy.arange(3) + 3))
for (i, channel) in enumerate((pixel.red, pixel.green, pixel.blue)):
    assert_equal(channel, (i + 3))
tempResult = arange(4)
	
===================================================================	
test_pixel_rgb: 71	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
pixel.rgb = numpy.arange(3)
assert_equal(pixel.rgb, numpy.arange(3))
for (i, channel) in enumerate((pixel.red, pixel.green, pixel.blue)):
    assert_equal(channel, i)
pixel.red = 3
pixel.green = 4
pixel.blue = 5
assert_equal(pixel.rgb, (numpy.arange(3) + 3))
for (i, channel) in enumerate((pixel.red, pixel.green, pixel.blue)):
    assert_equal(channel, (i + 3))
pixel.rgb = numpy.arange(4)
tempResult = arange(3)
	
===================================================================	
test_xy_to_array_origin: 19	
----------------------------	

(h, w) = (3, 5)
tempResult = arange((h * w))
	
===================================================================	
test_negative_index: 192	
----------------------------	

n = 10
tempResult = arange(0, n)
	
===================================================================	
test_negative_slice: 200	
----------------------------	

n = 10
tempResult = arange(0, n)
	
===================================================================	
test_picture_slice: 169	
----------------------------	

tempResult = arange(0, 10)
	
===================================================================	
test_pixel_rgba: 77	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
tempResult = arange(4)
	
===================================================================	
test_pixel_rgba: 78	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
pixel.rgba = numpy.arange(4)
tempResult = arange(4)
	
===================================================================	
test_pixel_rgba: 85	
----------------------------	

pic = skimage.novice.Picture.from_size((3, 3), color=(10, 10, 10))
pixel = pic[(0, 0)]
pixel.rgba = numpy.arange(4)
assert_equal(pixel.rgba, numpy.arange(4))
for (i, channel) in enumerate((pixel.red, pixel.green, pixel.blue, pixel.alpha)):
    assert_equal(channel, i)
pixel.red = 3
pixel.green = 4
pixel.blue = 5
pixel.alpha = 6
tempResult = arange(4)
	
===================================================================	
active_contour: 44	
----------------------------	

'Active contour model.\n\n    Active contours by fitting snakes to features of images. Supports single\n    and multichannel 2D images. Snakes can be periodic (for segmentation) or\n    have fixed and/or free ends.\n    The output snake has the same length as the input boundary.\n    As the number of points is constant, make sure that the initial snake\n    has enough points to capture the details of the final contour.\n\n    Parameters\n    ----------\n    image : (N, M) or (N, M, 3) ndarray\n        Input image.\n    snake : (N, 2) ndarray\n        Initialisation coordinates of snake. For periodic snakes, it should\n        not include duplicate endpoints.\n    alpha : float, optional\n        Snake length shape parameter. Higher values makes snake contract\n        faster.\n    beta : float, optional\n        Snake smoothness shape parameter. Higher values makes snake smoother.\n    w_line : float, optional\n        Controls attraction to brightness. Use negative values to attract to\n        dark regions.\n    w_edge : float, optional\n        Controls attraction to edges. Use negative values to repel snake from\n        edges.\n    gamma : float, optional\n        Explicit time stepping parameter.\n    bc : {\'periodic\', \'free\', \'fixed\'}, optional\n        Boundary conditions for worm. \'periodic\' attaches the two ends of the\n        snake, \'fixed\' holds the end-points in place, and\'free\' allows free\n        movement of the ends. \'fixed\' and \'free\' can be combined by parsing\n        \'fixed-free\', \'free-fixed\'. Parsing \'fixed-fixed\' or \'free-free\'\n        yields same behaviour as \'fixed\' and \'free\', respectively.\n    max_px_move : float, optional\n        Maximum pixel distance to move per iteration.\n    max_iterations : int, optional\n        Maximum iterations to optimize snake shape.\n    convergence: float, optional\n        Convergence criteria.\n\n    Returns\n    -------\n    snake : (N, 2) ndarray\n        Optimised snake, same shape as input parameter.\n\n    References\n    ----------\n    .. [1]  Kass, M.; Witkin, A.; Terzopoulos, D. "Snakes: Active contour\n            models". International Journal of Computer Vision 1 (4): 321\n            (1988).\n\n    Examples\n    --------\n    >>> from skimage.draw import circle_perimeter\n    >>> from skimage.filters import gaussian\n\n    Create and smooth image:\n\n    >>> img = np.zeros((100, 100))\n    >>> rr, cc = circle_perimeter(35, 45, 25)\n    >>> img[rr, cc] = 1\n    >>> img = gaussian(img, 2)\n\n    Initiliaze spline:\n\n    >>> s = np.linspace(0, 2*np.pi,100)\n    >>> init = 50*np.array([np.cos(s), np.sin(s)]).T+50\n\n    Fit spline to image:\n\n    >>> snake = active_contour(img, init, w_edge=0, w_line=1) #doctest: +SKIP\n    >>> dist = np.sqrt((45-snake[:, 0])**2 +(35-snake[:, 1])**2) #doctest: +SKIP\n    >>> int(np.mean(dist)) #doctest: +SKIP\n    25\n\n    '
split_version = scipy.__version__.split('.')
if (not split_version[(- 1)].isdigit()):
    split_version.pop()
scipy_version = list(map(int, split_version))
new_scipy = ((scipy_version[0] > 0) or ((scipy_version[0] == 0) and (scipy_version[1] >= 14)))
if (not new_scipy):
    raise NotImplementedError('You are using an old version of scipy. Active contours is implemented for scipy versions 0.14.0 and above.')
max_iterations = int(max_iterations)
if (max_iterations <= 0):
    raise ValueError('max_iterations should be >0.')
convergence_order = 10
valid_bcs = ['periodic', 'free', 'fixed', 'free-fixed', 'fixed-free', 'fixed-fixed', 'free-free']
if (bc not in valid_bcs):
    raise ValueError(((('Invalid boundary condition.\n' + 'Should be one of: ') + ', '.join(valid_bcs)) + '.'))
img = img_as_float(image)
RGB = (img.ndim == 3)
if (w_edge != 0):
    if RGB:
        edge = [sobel(img[:, :, 0]), sobel(img[:, :, 1]), sobel(img[:, :, 2])]
    else:
        edge = [sobel(img)]
    for i in range((3 if RGB else 1)):
        edge[i][0, :] = edge[i][1, :]
        edge[i][(- 1), :] = edge[i][(- 2), :]
        edge[i][:, 0] = edge[i][:, 1]
        edge[i][:, (- 1)] = edge[i][:, (- 2)]
else:
    edge = [0]
if RGB:
    img = ((w_line * numpy.sum(img, axis=2)) + (w_edge * sum(edge)))
else:
    img = ((w_line * img) + (w_edge * edge[0]))
if new_scipy:
    tempResult = arange(img.shape[1])
	
===================================================================	
active_contour: 44	
----------------------------	

'Active contour model.\n\n    Active contours by fitting snakes to features of images. Supports single\n    and multichannel 2D images. Snakes can be periodic (for segmentation) or\n    have fixed and/or free ends.\n    The output snake has the same length as the input boundary.\n    As the number of points is constant, make sure that the initial snake\n    has enough points to capture the details of the final contour.\n\n    Parameters\n    ----------\n    image : (N, M) or (N, M, 3) ndarray\n        Input image.\n    snake : (N, 2) ndarray\n        Initialisation coordinates of snake. For periodic snakes, it should\n        not include duplicate endpoints.\n    alpha : float, optional\n        Snake length shape parameter. Higher values makes snake contract\n        faster.\n    beta : float, optional\n        Snake smoothness shape parameter. Higher values makes snake smoother.\n    w_line : float, optional\n        Controls attraction to brightness. Use negative values to attract to\n        dark regions.\n    w_edge : float, optional\n        Controls attraction to edges. Use negative values to repel snake from\n        edges.\n    gamma : float, optional\n        Explicit time stepping parameter.\n    bc : {\'periodic\', \'free\', \'fixed\'}, optional\n        Boundary conditions for worm. \'periodic\' attaches the two ends of the\n        snake, \'fixed\' holds the end-points in place, and\'free\' allows free\n        movement of the ends. \'fixed\' and \'free\' can be combined by parsing\n        \'fixed-free\', \'free-fixed\'. Parsing \'fixed-fixed\' or \'free-free\'\n        yields same behaviour as \'fixed\' and \'free\', respectively.\n    max_px_move : float, optional\n        Maximum pixel distance to move per iteration.\n    max_iterations : int, optional\n        Maximum iterations to optimize snake shape.\n    convergence: float, optional\n        Convergence criteria.\n\n    Returns\n    -------\n    snake : (N, 2) ndarray\n        Optimised snake, same shape as input parameter.\n\n    References\n    ----------\n    .. [1]  Kass, M.; Witkin, A.; Terzopoulos, D. "Snakes: Active contour\n            models". International Journal of Computer Vision 1 (4): 321\n            (1988).\n\n    Examples\n    --------\n    >>> from skimage.draw import circle_perimeter\n    >>> from skimage.filters import gaussian\n\n    Create and smooth image:\n\n    >>> img = np.zeros((100, 100))\n    >>> rr, cc = circle_perimeter(35, 45, 25)\n    >>> img[rr, cc] = 1\n    >>> img = gaussian(img, 2)\n\n    Initiliaze spline:\n\n    >>> s = np.linspace(0, 2*np.pi,100)\n    >>> init = 50*np.array([np.cos(s), np.sin(s)]).T+50\n\n    Fit spline to image:\n\n    >>> snake = active_contour(img, init, w_edge=0, w_line=1) #doctest: +SKIP\n    >>> dist = np.sqrt((45-snake[:, 0])**2 +(35-snake[:, 1])**2) #doctest: +SKIP\n    >>> int(np.mean(dist)) #doctest: +SKIP\n    25\n\n    '
split_version = scipy.__version__.split('.')
if (not split_version[(- 1)].isdigit()):
    split_version.pop()
scipy_version = list(map(int, split_version))
new_scipy = ((scipy_version[0] > 0) or ((scipy_version[0] == 0) and (scipy_version[1] >= 14)))
if (not new_scipy):
    raise NotImplementedError('You are using an old version of scipy. Active contours is implemented for scipy versions 0.14.0 and above.')
max_iterations = int(max_iterations)
if (max_iterations <= 0):
    raise ValueError('max_iterations should be >0.')
convergence_order = 10
valid_bcs = ['periodic', 'free', 'fixed', 'free-fixed', 'fixed-free', 'fixed-fixed', 'free-free']
if (bc not in valid_bcs):
    raise ValueError(((('Invalid boundary condition.\n' + 'Should be one of: ') + ', '.join(valid_bcs)) + '.'))
img = img_as_float(image)
RGB = (img.ndim == 3)
if (w_edge != 0):
    if RGB:
        edge = [sobel(img[:, :, 0]), sobel(img[:, :, 1]), sobel(img[:, :, 2])]
    else:
        edge = [sobel(img)]
    for i in range((3 if RGB else 1)):
        edge[i][0, :] = edge[i][1, :]
        edge[i][(- 1), :] = edge[i][(- 2), :]
        edge[i][:, 0] = edge[i][:, 1]
        edge[i][:, (- 1)] = edge[i][:, (- 2)]
else:
    edge = [0]
if RGB:
    img = ((w_line * numpy.sum(img, axis=2)) + (w_edge * sum(edge)))
else:
    img = ((w_line * img) + (w_edge * edge[0]))
if new_scipy:
    tempResult = arange(img.shape[0])
	
===================================================================	
active_contour: 46	
----------------------------	

'Active contour model.\n\n    Active contours by fitting snakes to features of images. Supports single\n    and multichannel 2D images. Snakes can be periodic (for segmentation) or\n    have fixed and/or free ends.\n    The output snake has the same length as the input boundary.\n    As the number of points is constant, make sure that the initial snake\n    has enough points to capture the details of the final contour.\n\n    Parameters\n    ----------\n    image : (N, M) or (N, M, 3) ndarray\n        Input image.\n    snake : (N, 2) ndarray\n        Initialisation coordinates of snake. For periodic snakes, it should\n        not include duplicate endpoints.\n    alpha : float, optional\n        Snake length shape parameter. Higher values makes snake contract\n        faster.\n    beta : float, optional\n        Snake smoothness shape parameter. Higher values makes snake smoother.\n    w_line : float, optional\n        Controls attraction to brightness. Use negative values to attract to\n        dark regions.\n    w_edge : float, optional\n        Controls attraction to edges. Use negative values to repel snake from\n        edges.\n    gamma : float, optional\n        Explicit time stepping parameter.\n    bc : {\'periodic\', \'free\', \'fixed\'}, optional\n        Boundary conditions for worm. \'periodic\' attaches the two ends of the\n        snake, \'fixed\' holds the end-points in place, and\'free\' allows free\n        movement of the ends. \'fixed\' and \'free\' can be combined by parsing\n        \'fixed-free\', \'free-fixed\'. Parsing \'fixed-fixed\' or \'free-free\'\n        yields same behaviour as \'fixed\' and \'free\', respectively.\n    max_px_move : float, optional\n        Maximum pixel distance to move per iteration.\n    max_iterations : int, optional\n        Maximum iterations to optimize snake shape.\n    convergence: float, optional\n        Convergence criteria.\n\n    Returns\n    -------\n    snake : (N, 2) ndarray\n        Optimised snake, same shape as input parameter.\n\n    References\n    ----------\n    .. [1]  Kass, M.; Witkin, A.; Terzopoulos, D. "Snakes: Active contour\n            models". International Journal of Computer Vision 1 (4): 321\n            (1988).\n\n    Examples\n    --------\n    >>> from skimage.draw import circle_perimeter\n    >>> from skimage.filters import gaussian\n\n    Create and smooth image:\n\n    >>> img = np.zeros((100, 100))\n    >>> rr, cc = circle_perimeter(35, 45, 25)\n    >>> img[rr, cc] = 1\n    >>> img = gaussian(img, 2)\n\n    Initiliaze spline:\n\n    >>> s = np.linspace(0, 2*np.pi,100)\n    >>> init = 50*np.array([np.cos(s), np.sin(s)]).T+50\n\n    Fit spline to image:\n\n    >>> snake = active_contour(img, init, w_edge=0, w_line=1) #doctest: +SKIP\n    >>> dist = np.sqrt((45-snake[:, 0])**2 +(35-snake[:, 1])**2) #doctest: +SKIP\n    >>> int(np.mean(dist)) #doctest: +SKIP\n    25\n\n    '
split_version = scipy.__version__.split('.')
if (not split_version[(- 1)].isdigit()):
    split_version.pop()
scipy_version = list(map(int, split_version))
new_scipy = ((scipy_version[0] > 0) or ((scipy_version[0] == 0) and (scipy_version[1] >= 14)))
if (not new_scipy):
    raise NotImplementedError('You are using an old version of scipy. Active contours is implemented for scipy versions 0.14.0 and above.')
max_iterations = int(max_iterations)
if (max_iterations <= 0):
    raise ValueError('max_iterations should be >0.')
convergence_order = 10
valid_bcs = ['periodic', 'free', 'fixed', 'free-fixed', 'fixed-free', 'fixed-fixed', 'free-free']
if (bc not in valid_bcs):
    raise ValueError(((('Invalid boundary condition.\n' + 'Should be one of: ') + ', '.join(valid_bcs)) + '.'))
img = img_as_float(image)
RGB = (img.ndim == 3)
if (w_edge != 0):
    if RGB:
        edge = [sobel(img[:, :, 0]), sobel(img[:, :, 1]), sobel(img[:, :, 2])]
    else:
        edge = [sobel(img)]
    for i in range((3 if RGB else 1)):
        edge[i][0, :] = edge[i][1, :]
        edge[i][(- 1), :] = edge[i][(- 2), :]
        edge[i][:, 0] = edge[i][:, 1]
        edge[i][:, (- 1)] = edge[i][:, (- 2)]
else:
    edge = [0]
if RGB:
    img = ((w_line * numpy.sum(img, axis=2)) + (w_edge * sum(edge)))
else:
    img = ((w_line * img) + (w_edge * edge[0]))
if new_scipy:
    intp = RectBivariateSpline(numpy.arange(img.shape[1]), numpy.arange(img.shape[0]), img.T, kx=2, ky=2, s=0)
else:
    tempResult = arange(img.shape[1])
	
===================================================================	
active_contour: 46	
----------------------------	

'Active contour model.\n\n    Active contours by fitting snakes to features of images. Supports single\n    and multichannel 2D images. Snakes can be periodic (for segmentation) or\n    have fixed and/or free ends.\n    The output snake has the same length as the input boundary.\n    As the number of points is constant, make sure that the initial snake\n    has enough points to capture the details of the final contour.\n\n    Parameters\n    ----------\n    image : (N, M) or (N, M, 3) ndarray\n        Input image.\n    snake : (N, 2) ndarray\n        Initialisation coordinates of snake. For periodic snakes, it should\n        not include duplicate endpoints.\n    alpha : float, optional\n        Snake length shape parameter. Higher values makes snake contract\n        faster.\n    beta : float, optional\n        Snake smoothness shape parameter. Higher values makes snake smoother.\n    w_line : float, optional\n        Controls attraction to brightness. Use negative values to attract to\n        dark regions.\n    w_edge : float, optional\n        Controls attraction to edges. Use negative values to repel snake from\n        edges.\n    gamma : float, optional\n        Explicit time stepping parameter.\n    bc : {\'periodic\', \'free\', \'fixed\'}, optional\n        Boundary conditions for worm. \'periodic\' attaches the two ends of the\n        snake, \'fixed\' holds the end-points in place, and\'free\' allows free\n        movement of the ends. \'fixed\' and \'free\' can be combined by parsing\n        \'fixed-free\', \'free-fixed\'. Parsing \'fixed-fixed\' or \'free-free\'\n        yields same behaviour as \'fixed\' and \'free\', respectively.\n    max_px_move : float, optional\n        Maximum pixel distance to move per iteration.\n    max_iterations : int, optional\n        Maximum iterations to optimize snake shape.\n    convergence: float, optional\n        Convergence criteria.\n\n    Returns\n    -------\n    snake : (N, 2) ndarray\n        Optimised snake, same shape as input parameter.\n\n    References\n    ----------\n    .. [1]  Kass, M.; Witkin, A.; Terzopoulos, D. "Snakes: Active contour\n            models". International Journal of Computer Vision 1 (4): 321\n            (1988).\n\n    Examples\n    --------\n    >>> from skimage.draw import circle_perimeter\n    >>> from skimage.filters import gaussian\n\n    Create and smooth image:\n\n    >>> img = np.zeros((100, 100))\n    >>> rr, cc = circle_perimeter(35, 45, 25)\n    >>> img[rr, cc] = 1\n    >>> img = gaussian(img, 2)\n\n    Initiliaze spline:\n\n    >>> s = np.linspace(0, 2*np.pi,100)\n    >>> init = 50*np.array([np.cos(s), np.sin(s)]).T+50\n\n    Fit spline to image:\n\n    >>> snake = active_contour(img, init, w_edge=0, w_line=1) #doctest: +SKIP\n    >>> dist = np.sqrt((45-snake[:, 0])**2 +(35-snake[:, 1])**2) #doctest: +SKIP\n    >>> int(np.mean(dist)) #doctest: +SKIP\n    25\n\n    '
split_version = scipy.__version__.split('.')
if (not split_version[(- 1)].isdigit()):
    split_version.pop()
scipy_version = list(map(int, split_version))
new_scipy = ((scipy_version[0] > 0) or ((scipy_version[0] == 0) and (scipy_version[1] >= 14)))
if (not new_scipy):
    raise NotImplementedError('You are using an old version of scipy. Active contours is implemented for scipy versions 0.14.0 and above.')
max_iterations = int(max_iterations)
if (max_iterations <= 0):
    raise ValueError('max_iterations should be >0.')
convergence_order = 10
valid_bcs = ['periodic', 'free', 'fixed', 'free-fixed', 'fixed-free', 'fixed-fixed', 'free-free']
if (bc not in valid_bcs):
    raise ValueError(((('Invalid boundary condition.\n' + 'Should be one of: ') + ', '.join(valid_bcs)) + '.'))
img = img_as_float(image)
RGB = (img.ndim == 3)
if (w_edge != 0):
    if RGB:
        edge = [sobel(img[:, :, 0]), sobel(img[:, :, 1]), sobel(img[:, :, 2])]
    else:
        edge = [sobel(img)]
    for i in range((3 if RGB else 1)):
        edge[i][0, :] = edge[i][1, :]
        edge[i][(- 1), :] = edge[i][(- 2), :]
        edge[i][:, 0] = edge[i][:, 1]
        edge[i][:, (- 1)] = edge[i][:, (- 2)]
else:
    edge = [0]
if RGB:
    img = ((w_line * numpy.sum(img, axis=2)) + (w_edge * sum(edge)))
else:
    img = ((w_line * img) + (w_edge * edge[0]))
if new_scipy:
    intp = RectBivariateSpline(numpy.arange(img.shape[1]), numpy.arange(img.shape[0]), img.T, kx=2, ky=2, s=0)
else:
    tempResult = arange(img.shape[0])
	
===================================================================	
_make_graph_edges_3d: 30	
----------------------------	

'Returns a list of edges for a 3D image.\n\n    Parameters\n    ----------\n    n_x: integer\n        The size of the grid in the x direction.\n    n_y: integer\n        The size of the grid in the y direction\n    n_z: integer\n        The size of the grid in the z direction\n\n    Returns\n    -------\n    edges : (2, N) ndarray\n        with the total number of edges::\n\n            N = n_x * n_y * (nz - 1) +\n                n_x * (n_y - 1) * nz +\n                (n_x - 1) * n_y * nz\n\n        Graph edges with each column describing a node-id pair.\n    '
tempResult = arange(((n_x * n_y) * n_z))
	
===================================================================	
_buildAB: 78	
----------------------------	

'\n    Build the matrix A and rhs B of the linear system to solve.\n    A and B are two block of the laplacian of the image graph.\n    '
labels = labels[(labels >= 0)]
tempResult = arange(labels.size)
	
===================================================================	
_make_laplacian_sparse: 58	
----------------------------	

'\n    Sparse implementation\n    '
pixel_nb = (edges.max() + 1)
tempResult = arange(pixel_nb)
	
===================================================================	
_mask_edges_weights: 99	
----------------------------	

'\n    Remove edges of the graph connected to masked nodes, as well as\n    corresponding weights of the edges.\n    '
mask0 = numpy.hstack((mask[:, :, :(- 1)].ravel(), mask[:, :(- 1)].ravel(), mask[:(- 1)].ravel()))
mask1 = numpy.hstack((mask[:, :, 1:].ravel(), mask[:, 1:].ravel(), mask[1:].ravel()))
ind_mask = numpy.logical_and(mask0, mask1)
(edges, weights) = (edges[:, ind_mask], weights[ind_mask])
max_node_index = edges.max()
tempResult = arange((max_node_index + 1))
	
===================================================================	
clear_border: 24	
----------------------------	

'Clear objects connected to the label image border.\n\n    Parameters\n    ----------\n    labels : (M[, N[, ..., P]]) array of int or bool\n        Imaging data labels.\n    buffer_size : int, optional\n        The width of the border examined.  By default, only objects\n        that touch the outside of the image are removed.\n    bgval : float or int, optional\n        Cleared objects are set to this value.\n    in_place : bool, optional\n        Whether or not to manipulate the labels array in-place.\n\n    Returns\n    -------\n    out : (M[, N[, ..., P]]) array\n        Imaging data labels with cleared borders\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from skimage.segmentation import clear_border\n    >>> labels = np.array([[0, 0, 0, 0, 0, 0, 0, 1, 0],\n    ...                    [0, 0, 0, 0, 1, 0, 0, 0, 0],\n    ...                    [1, 0, 0, 1, 0, 1, 0, 0, 0],\n    ...                    [0, 0, 1, 1, 1, 1, 1, 0, 0],\n    ...                    [0, 1, 1, 1, 1, 1, 1, 1, 0],\n    ...                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n    >>> clear_border(labels)\n    array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 1, 0, 0, 0, 0],\n           [0, 0, 0, 1, 0, 1, 0, 0, 0],\n           [0, 0, 1, 1, 1, 1, 1, 0, 0],\n           [0, 1, 1, 1, 1, 1, 1, 1, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n    '
image = labels
if any(((buffer_size >= s) for s in image.shape)):
    raise ValueError('buffer size may not be greater than image size')
borders = numpy.zeros_like(image, dtype=numpy.bool_)
ext = (buffer_size + 1)
slstart = slice(ext)
slend = slice((- ext), None)
slices = [slice(s) for s in image.shape]
for d in range(image.ndim):
    slicedim = list(slices)
    slicedim[d] = slstart
    borders[slicedim] = True
    slicedim[d] = slend
    borders[slicedim] = True
labels = label(image, background=0)
number = (numpy.max(labels) + 1)
borders_indices = numpy.unique(labels[borders])
tempResult = arange((number + 1))
	
===================================================================	
relabel_sequential: 32	
----------------------------	

'Relabel arbitrary labels to {`offset`, ... `offset` + number_of_labels}.\n\n    This function also returns the forward map (mapping the original labels to\n    the reduced labels) and the inverse map (mapping the reduced labels back\n    to the original ones).\n\n    Parameters\n    ----------\n    label_field : numpy array of int, arbitrary shape\n        An array of labels.\n    offset : int, optional\n        The return labels will start at `offset`, which should be\n        strictly positive.\n\n    Returns\n    -------\n    relabeled : numpy array of int, same shape as `label_field`\n        The input label field with labels mapped to\n        {offset, ..., number_of_labels + offset - 1}.\n    forward_map : numpy array of int, shape ``(label_field.max() + 1,)``\n        The map from the original label space to the returned label\n        space. Can be used to re-apply the same mapping. See examples\n        for usage.\n    inverse_map : 1D numpy array of int, of length offset + number of labels\n        The map from the new label space to the original space. This\n        can be used to reconstruct the original label field from the\n        relabeled one.\n\n    Notes\n    -----\n    The label 0 is assumed to denote the background and is never remapped.\n\n    The forward map can be extremely big for some inputs, since its\n    length is given by the maximum of the label field. However, in most\n    situations, ``label_field.max()`` is much smaller than\n    ``label_field.size``, and in these cases the forward map is\n    guaranteed to be smaller than either the input or output images.\n\n    Examples\n    --------\n    >>> from skimage.segmentation import relabel_sequential\n    >>> label_field = np.array([1, 1, 5, 5, 8, 99, 42])\n    >>> relab, fw, inv = relabel_sequential(label_field)\n    >>> relab\n    array([1, 1, 2, 2, 3, 5, 4])\n    >>> fw\n    array([0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 5])\n    >>> inv\n    array([ 0,  1,  5,  8, 42, 99])\n    >>> (fw[label_field] == relab).all()\n    True\n    >>> (inv[relab] == label_field).all()\n    True\n    >>> relab, fw, inv = relabel_sequential(label_field, offset=5)\n    >>> relab\n    array([5, 5, 6, 6, 7, 9, 8])\n    '
m = label_field.max()
if (not numpy.issubdtype(label_field.dtype, numpy.int)):
    new_type = numpy.min_scalar_type(int(m))
    label_field = label_field.astype(new_type)
    m = m.astype(new_type)
labels = numpy.unique(label_field)
labels0 = labels[(labels != 0)]
if (m == len(labels0)):
    return (label_field, labels, labels)
forward_map = numpy.zeros((m + 1), int)
tempResult = arange(offset, (offset + len(labels0)))
	
===================================================================	
test_minsize: 22	
----------------------------	

img = skimage.data.coins()[20:168, 0:128]
tempResult = arange(10, 100, 10)
	
===================================================================	
test_minsize: 27	
----------------------------	

img = skimage.data.coins()[20:168, 0:128]
for min_size in numpy.arange(10, 100, 10):
    segments = felzenszwalb(img, min_size=min_size, sigma=3)
    counts = numpy.bincount(segments.ravel())
    assert_greater((counts.min() + 1), min_size)
coffee = skimage.data.coffee()[::4, ::4]
tempResult = arange(10, 100, 10)
	
===================================================================	
test_bad_inputs: 232	
----------------------------	

img = numpy.ones(10)
tempResult = arange(10)
	
===================================================================	
test_bad_inputs: 237	
----------------------------	

img = numpy.ones(10)
labels = numpy.arange(10)
numpy.testing.assert_raises(ValueError, random_walker, img, labels)
numpy.testing.assert_raises(ValueError, random_walker, img, labels, multichannel=True)
numpy.random.seed(42)
img = numpy.random.normal(size=(3, 3, 3, 3, 3))
tempResult = arange((3 ** 5))
	
===================================================================	
test_gray_3d: 87	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21, 22))
slices = []
for dim_size in img.shape:
    midpoint = (dim_size // 2)
    slices.append((slice(None, midpoint), slice(midpoint, None)))
slices = list(itertools.product(*slices))
tempResult = arange(0, 1.000001, (1.0 / 7))
	
===================================================================	
test_more_segments_than_pixels: 159	
----------------------------	

rnd = numpy.random.RandomState(0)
img = numpy.zeros((20, 21))
img[:10, :10] = 0.33
img[10:, :10] = 0.67
img[10:, 10:] = 1.0
img += (0.0033 * rnd.normal(size=img.shape))
img[(img > 1)] = 1
img[(img < 0)] = 0
seg = slic(img, sigma=0, n_segments=500, compactness=1, multichannel=False, convert2lab=False)
tempResult = arange(seg.size)
	
===================================================================	
probabilistic_hough_line: 35	
----------------------------	

'Return lines from a progressive probabilistic line Hough transform.\n\n    Parameters\n    ----------\n    img : (M, N) ndarray\n        Input image with nonzero values representing edges.\n    threshold : int, optional\n        Threshold\n    line_length : int, optional\n        Minimum accepted length of detected lines.\n        Increase the parameter to extract longer lines.\n    line_gap : int, optional\n        Maximum gap between pixels to still form a line.\n        Increase the parameter to merge broken lines more aggresively.\n    theta : 1D ndarray, dtype=double, optional\n        Angles at which to compute the transform, in radians.\n        If None, use a range from -pi/2 to pi/2.\n\n    Returns\n    -------\n    lines : list\n      List of lines identified, lines in format ((x0, y0), (x1, y1)),\n      indicating line start and end.\n\n    References\n    ----------\n    .. [1] C. Galamhos, J. Matas and J. Kittler, "Progressive probabilistic\n           Hough transform for line detection", in IEEE Computer Society\n           Conference on Computer Vision and Pattern Recognition, 1999.\n    '
if (img.ndim != 2):
    raise ValueError('The input image `img` must be 2D.')
if (theta is None):
    tempResult = arange(180)
	
===================================================================	
iradon: 123	
----------------------------	

'\n    Inverse radon transform.\n\n    Reconstruct an image from the radon transform, using the filtered\n    back projection algorithm.\n\n    Parameters\n    ----------\n    radon_image : array_like, dtype=float\n        Image containing radon transform (sinogram). Each column of\n        the image corresponds to a projection along a different angle. The\n        tomography rotation axis should lie at the pixel index\n        ``radon_image.shape[0] // 2`` along the 0th dimension of\n        ``radon_image``.\n    theta : array_like, dtype=float, optional\n        Reconstruction angles (in degrees). Default: m angles evenly spaced\n        between 0 and 180 (if the shape of `radon_image` is (N, M)).\n    output_size : int\n        Number of rows and columns in the reconstruction.\n    filter : str, optional (default ramp)\n        Filter used in frequency domain filtering. Ramp filter used by default.\n        Filters available: ramp, shepp-logan, cosine, hamming, hann.\n        Assign None to use no filter.\n    interpolation : str, optional (default \'linear\')\n        Interpolation method used in reconstruction. Methods available:\n        \'linear\', \'nearest\', and \'cubic\' (\'cubic\' is slow).\n    circle : boolean, optional\n        Assume the reconstructed image is zero outside the inscribed circle.\n        Also changes the default output_size to match the behaviour of\n        ``radon`` called with ``circle=True``.\n        The default behavior (None) is equivalent to False.\n\n    Returns\n    -------\n    reconstructed : ndarray\n        Reconstructed image. The rotation axis will be located in the pixel\n        with indices\n        ``(reconstructed.shape[0] // 2, reconstructed.shape[1] // 2)``.\n\n    References\n    ----------\n    .. [1] AC Kak, M Slaney, "Principles of Computerized Tomographic\n           Imaging", IEEE Press 1988.\n    .. [2] B.R. Ramesh, N. Srinivasa, K. Rajgopal, "An Algorithm for Computing\n           the Discrete Radon Transform With Some Applications", Proceedings of\n           the Fourth IEEE Region 10 International Conference, TENCON \'89, 1989\n\n    Notes\n    -----\n    It applies the Fourier slice theorem to reconstruct an image by\n    multiplying the frequency domain of the filter with the FFT of the\n    projection data. This algorithm is called filtered back projection.\n\n    '
if (radon_image.ndim != 2):
    raise ValueError('The input image must be 2-D')
if (theta is None):
    (m, n) = radon_image.shape
    theta = numpy.linspace(0, 180, n, endpoint=False)
else:
    theta = numpy.asarray(theta)
if (len(theta) != radon_image.shape[1]):
    raise ValueError('The given ``theta`` does not match the number of projections in ``radon_image``.')
interpolation_types = ('linear', 'nearest', 'cubic')
if (interpolation not in interpolation_types):
    raise ValueError(('Unknown interpolation: %s' % interpolation))
if (not output_size):
    if circle:
        output_size = radon_image.shape[0]
    else:
        output_size = int(numpy.floor(numpy.sqrt(((radon_image.shape[0] ** 2) / 2.0))))
if (circle is None):
    warn('The default of `circle` in `skimage.transform.iradon` will change to `True` in version 0.15.')
    circle = False
if circle:
    radon_image = _sinogram_circle_to_square(radon_image)
th = ((numpy.pi / 180.0) * theta)
projection_size_padded = max(64, int((2 ** numpy.ceil(numpy.log2((2 * radon_image.shape[0]))))))
pad_width = ((0, (projection_size_padded - radon_image.shape[0])), (0, 0))
img = numpy.pad(radon_image, pad_width, mode='constant', constant_values=0)
f = fftfreq(projection_size_padded).reshape((- 1), 1)
omega = ((2 * numpy.pi) * f)
fourier_filter = (2 * numpy.abs(f))
if (filter == 'ramp'):
    pass
elif (filter == 'shepp-logan'):
    fourier_filter[1:] = ((fourier_filter[1:] * numpy.sin(omega[1:])) / omega[1:])
elif (filter == 'cosine'):
    fourier_filter *= numpy.cos(omega)
elif (filter == 'hamming'):
    fourier_filter *= (0.54 + (0.46 * numpy.cos((omega / 2))))
elif (filter == 'hann'):
    fourier_filter *= ((1 + numpy.cos((omega / 2))) / 2)
elif (filter is None):
    fourier_filter[:] = 1
else:
    raise ValueError(('Unknown filter: %s' % filter))
projection = (fft(img, axis=0) * fourier_filter)
radon_filtered = numpy.real(ifft(projection, axis=0))
radon_filtered = radon_filtered[:radon_image.shape[0], :]
reconstructed = numpy.zeros((output_size, output_size))
mid_index = (radon_image.shape[0] // 2)
[X, Y] = numpy.mgrid[0:output_size, 0:output_size]
xpr = (X - (int(output_size) // 2))
ypr = (Y - (int(output_size) // 2))
for i in range(len(theta)):
    t = ((ypr * numpy.cos(th[i])) - (xpr * numpy.sin(th[i])))
    tempResult = arange(radon_filtered.shape[0])
	
===================================================================	
radon: 16	
----------------------------	

'\n    Calculates the radon transform of an image given specified\n    projection angles.\n\n    Parameters\n    ----------\n    image : array_like, dtype=float\n        Input image. The rotation axis will be located in the pixel with\n        indices ``(image.shape[0] // 2, image.shape[1] // 2)``.\n    theta : array_like, dtype=float, optional (default np.arange(180))\n        Projection angles (in degrees).\n    circle : boolean, optional\n        Assume image is zero outside the inscribed circle, making the\n        width of each projection (the first dimension of the sinogram)\n        equal to ``min(image.shape)``.\n        The default behavior (None) is equivalent to False.\n\n    Returns\n    -------\n    radon_image : ndarray\n        Radon transform (sinogram).  The tomography rotation axis will lie\n        at the pixel index ``radon_image.shape[0] // 2`` along the 0th\n        dimension of ``radon_image``.\n\n    References\n    ----------\n    .. [1] AC Kak, M Slaney, "Principles of Computerized Tomographic\n           Imaging", IEEE Press 1988.\n    .. [2] B.R. Ramesh, N. Srinivasa, K. Rajgopal, "An Algorithm for Computing\n           the Discrete Radon Transform With Some Applications", Proceedings of\n           the Fourth IEEE Region 10 International Conference, TENCON \'89, 1989\n\n    Notes\n    -----\n    Based on code of Justin K. Romberg\n    (http://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html)\n\n    '
if (image.ndim != 2):
    raise ValueError('The input image must be 2-D')
if (theta is None):
    tempResult = arange(180)
	
===================================================================	
test_validity: 10	
----------------------------	

tempResult = arange(12)
	
===================================================================	
check_radon_iradon_minimal: 140	
----------------------------	

debug = False
tempResult = arange(180)
	
===================================================================	
test_iradon_sart: 284	
----------------------------	

debug = False
image = rescale(PHANTOM, 0.8, mode='reflect')
theta_ordered = numpy.linspace(0.0, 180.0, image.shape[0], endpoint=False)
theta_missing_wedge = numpy.linspace(0.0, 150.0, image.shape[0], endpoint=True)
for (theta, error_factor) in ((theta_ordered, 1.0), (theta_missing_wedge, 2.0)):
    sinogram = radon(image, theta, circle=True)
    reconstructed = iradon_sart(sinogram, theta)
    if debug:
        from matplotlib import pyplot as plt
        matplotlib.pyplot.figure()
        matplotlib.pyplot.subplot(221)
        matplotlib.pyplot.imshow(image, interpolation='nearest')
        matplotlib.pyplot.subplot(222)
        matplotlib.pyplot.imshow(sinogram, interpolation='nearest')
        matplotlib.pyplot.subplot(223)
        matplotlib.pyplot.imshow(reconstructed, interpolation='nearest')
        matplotlib.pyplot.subplot(224)
        matplotlib.pyplot.imshow((reconstructed - image), interpolation='nearest')
        matplotlib.pyplot.show()
    delta = numpy.mean(numpy.abs((reconstructed - image)))
    print('delta (1 iteration) =', delta)
    assert (delta < (0.02 * error_factor))
    reconstructed = iradon_sart(sinogram, theta, reconstructed)
    delta = numpy.mean(numpy.abs((reconstructed - image)))
    print('delta (2 iterations) =', delta)
    assert (delta < (0.014 * error_factor))
    reconstructed = iradon_sart(sinogram, theta, clip=(0, 1))
    delta = numpy.mean(numpy.abs((reconstructed - image)))
    print('delta (1 iteration, clip) =', delta)
    assert (delta < (0.018 * error_factor))
    numpy.random.seed(1239867)
    shifts = numpy.random.uniform((- 3), 3, sinogram.shape[1])
    tempResult = arange(sinogram.shape[0])
	
===================================================================	
test_downscale_local_mean: 222	
----------------------------	

tempResult = arange((4 * 6))
	
===================================================================	
test_downscale_local_mean: 226	
----------------------------	

image1 = np.arange((4 * 6)).reshape(4, 6)
out1 = downscale_local_mean(image1, (2, 3))
expected1 = numpy.array([[4.0, 7.0], [16.0, 19.0]])
assert_equal(expected1, out1)
tempResult = arange((5 * 8))
	
===================================================================	
test_inverse: 237	
----------------------------	

tform = SimilarityTransform(scale=0.5, rotation=0.1)
inverse_tform = SimilarityTransform(matrix=numpy.linalg.inv(tform.params))
tempResult = arange((10 * 10))
	
===================================================================	
regular_seeds: 31	
----------------------------	

'Return an image with ~`n_points` regularly-spaced nonzero pixels.\n\n    Parameters\n    ----------\n    ar_shape : tuple of int\n        The shape of the desired output image.\n    n_points : int\n        The desired number of nonzero points.\n    dtype : numpy data type, optional\n        The desired data type of the output.\n\n    Returns\n    -------\n    seed_img : array of int or bool\n        The desired image.\n\n    Examples\n    --------\n    >>> regular_seeds((5, 5), 4)\n    array([[0, 0, 0, 0, 0],\n           [0, 1, 0, 2, 0],\n           [0, 0, 0, 0, 0],\n           [0, 3, 0, 4, 0],\n           [0, 0, 0, 0, 0]])\n    '
grid = regular_grid(ar_shape, n_points)
seed_img = numpy.zeros(ar_shape, dtype=dtype)
tempResult = arange(seed_img[grid].size)
	
===================================================================	
Get no callers of function numpy.arange at line 37 col 8.	
===================================================================	
test_apply_parallel: 11	
----------------------------	

tempResult = arange(144)
	
===================================================================	
test_apply_parallel_nearest: 47	
----------------------------	


def wrapped(arr):
    return gaussian(arr, 1, mode='nearest')
tempResult = arange(144)
	
===================================================================	
test_zero_crop: 42	
----------------------------	

tempResult = arange(45)
	
===================================================================	
test_multi_crop: 9	
----------------------------	

tempResult = arange(45)
	
===================================================================	
test_pair_crop: 16	
----------------------------	

tempResult = arange(45)
	
===================================================================	
test_copy_crop: 30	
----------------------------	

tempResult = arange(45)
	
===================================================================	
test_int_crop: 23	
----------------------------	

tempResult = arange(45)
	
===================================================================	
TypeError1.test_str: 386	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestConstant.test_check_constant_zeros: 122	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestNdarrayPadWidth.test_check_simple: 321	
----------------------------	

tempResult = arange(12)
	
===================================================================	
ValueError1.test_check_simple: 330	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestStatistic.test_check_minimum_stat_length: 70	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestLegacyVectorFunction.test_legacy_vector_functionality: 313	
----------------------------	


def _padwithtens(vector, pad_width, iaxis, kwargs):
    vector[:pad_width[0]] = 10
    vector[(- pad_width[1]):] = 10
    return vector
tempResult = arange(6)
	
===================================================================	
TestWrap.test_check_large_pad: 263	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestWrap.test_check_simple: 257	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestConditionalShortcuts.test_clip_statistic_range: 25	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TypeError1.test_float: 381	
----------------------------	

tempResult = arange(30)
	
===================================================================	
ValueError3.test_check_kwarg_not_allowed: 358	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestReflect.test_check_simple: 169	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestStatistic.test_check_minimum_2: 64	
----------------------------	

tempResult = arange(100)
	
===================================================================	
ValueError3.test_malformed_pad_amount: 366	
----------------------------	

tempResult = arange(30)
	
===================================================================	
ValueError3.test_pad_too_many_axes: 374	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestStatistic.test_check_maximum_1: 40	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestStatLen.test_check_simple: 282	
----------------------------	

tempResult = arange(30)
	
===================================================================	
ValueError2.test_check_negative_pad_amount: 350	
----------------------------	

tempResult = arange(30)
	
===================================================================	
ValueError1.test_check_negative_stat_length: 336	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestConditionalShortcuts.test_shallow_statistic_range: 18	
----------------------------	

tempResult = arange(120)
	
===================================================================	
TypeError1.test_check_wrong_pad_amount: 403	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestStatistic.test_check_maximum_2: 46	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestReflect.test_check_odd_method: 175	
----------------------------	

tempResult = arange(100)
	
===================================================================	
ValueError3.test_mode_not_set: 362	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TypeError1.test_complex: 398	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestLinearRamp.test_check_2d: 161	
----------------------------	

tempResult = arange(20)
	
===================================================================	
TestStatistic.test_check_minimum_1: 58	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestConstant.test_check_constant_float: 128	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestSymmetric.test_check_simple: 210	
----------------------------	

tempResult = arange(100)
	
===================================================================	
ValueError1.test_check_negative_pad_width: 342	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestStatistic.test_check_maximum_stat_length: 52	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestConstant.test_check_constant_float3: 141	
----------------------------	

tempResult = arange(100, dtype=float)
	
===================================================================	
TestConstant.test_check_constant_float2: 134	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestStatistic.test_check_mean_stat_length: 34	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestStatistic.test_check_mean_2: 108	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestLinearRamp.test_check_simple: 155	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestStatistic.test_check_median: 76	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestConstant.test_check_constant_odd_pad_amount: 147	
----------------------------	

tempResult = arange(30)
	
===================================================================	
ValueError3.test_malformed_pad_amount2: 370	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestZeroPadWidth.test_zero_pad_width: 300	
----------------------------	

tempResult = arange(30)
	
===================================================================	
TestSymmetric.test_check_odd_method: 216	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestStatistic.test_check_median_stat_length: 94	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestConstant.test_check_constant: 116	
----------------------------	

tempResult = arange(100)
	
===================================================================	
TestEdge.test_check_simple: 291	
----------------------------	

tempResult = arange(12)
	
===================================================================	
TestConditionalShortcuts.test_zero_padding_shortcuts: 11	
----------------------------	

tempResult = arange(120)
	
===================================================================	
TypeError1.test_object: 394	
----------------------------	


class FooBar(object):
    pass
tempResult = arange(30)
	
===================================================================	
test_downcast: 41	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_grid_shape: 37	
----------------------------	

n_images = 6
(height, width) = (2, 2)
tempResult = arange(((n_images * height) * width), dtype=numpy.float32)
	
===================================================================	
test_fill: 19	
----------------------------	

n_images = 3
(height, width) = (2, 3)
tempResult = arange(((n_images * height) * width))
	
===================================================================	
test_rescale_intensity: 46	
----------------------------	

n_images = 4
(height, width) = (3, 3)
tempResult = arange(((n_images * height) * width), dtype=numpy.float32)
	
===================================================================	
test_simple: 10	
----------------------------	

n_images = 3
(height, width) = (2, 3)
tempResult = arange(((n_images * height) * width))
	
===================================================================	
test_shape: 28	
----------------------------	

n_images = 15
(height, width) = (11, 7)
tempResult = arange(((n_images * height) * width))
	
===================================================================	
test_view_as_windows_step_tuple: 102	
----------------------------	

tempResult = arange(24)
	
===================================================================	
test_view_as_blocks_1D_array_wrong_block_shape: 30	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_blocks_block_too_large: 20	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_blocks_negative_shape: 15	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_blocks_3D_array: 45	
----------------------------	

tempResult = arange(((4 * 4) * 6))
	
===================================================================	
test_view_as_blocks_1D_array: 34	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_windows_step_below_one: 72	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_windows_2D: 82	
----------------------------	

tempResult = arange((5 * 4))
	
===================================================================	
test_view_as_windows_window_too_large: 67	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_windows_wrong_window_dimension: 57	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_windows_with_skip: 89	
----------------------------	

tempResult = arange(20)
	
===================================================================	
test_view_as_blocks_2D_array: 39	
----------------------------	

tempResult = arange((4 * 4))
	
===================================================================	
test_view_as_windows_1D: 76	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_blocks_block_not_a_tuple: 10	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_views_non_contiguous: 96	
----------------------------	

tempResult = arange(16)
	
===================================================================	
test_view_as_blocks_wrong_block_dimension: 25	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_view_as_windows_negative_window_length: 62	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_3d_array: 30	
----------------------------	

tempResult = arange(8)
	
===================================================================	
ColorHistogram._on_new_image: 27	
----------------------------	

self.lab_image = color.rgb2lab(image)
(L, a, b) = self.lab_image.T
(left, right) = ((- 100), 100)
ab_extents = [left, right, right, left]
self.mask = numpy.ones(L.shape, bool)
tempResult = arange(left, right)
	
===================================================================	
LineProfile._update_data: 77	
----------------------------	

scan = measure.profile_line(self.image_viewer.image, *self.line_tool.end_points[:, ::(- 1)], linewidth=self.line_tool.linewidth)
self.scan_data = scan
if (scan.ndim == 1):
    scan = scan[:, numpy.newaxis]
if (scan.shape[1] != len(self.profile)):
    self.reset_axes(scan)
for i in range(len(scan[0])):
    tempResult = arange(scan.shape[0])
	
***************************************************	
sunpy_sunpy-0.8.0: 63	
===================================================================	
module: 31	
----------------------------	

'\nNothing here but dictionaries for generating LinearSegmentedColormaps,\nand a dictionary of these dictionaries.\n'
from __future__ import absolute_import, division, print_function
import numpy as np
import matplotlib.colors as colors
from sunpy.extern.six.moves import range, zip
__all__ = ['aia_color_table', 'lasco_color_table', 'eit_color_table', 'sxt_color_table', 'xrt_color_table', 'trace_color_table', 'sot_color_table', 'hmi_mag_color_table']

def _mkx(i, steps, n):
    ' Generate list according to pattern of g0 and b0. '
    x = []
    for step in steps:
        x.extend(list(range(i, (step + n), n)))
        i = (step + (n - 1))
    return x

def padfr(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the front. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (([pad] * diff) + lst)

def paden(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the end. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (lst + ([pad] * diff))
r0 = numpy.array(paden([0, 1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 212, 214, 215, 217, 218, 220, 221, 223, 224, 226, 227, 228, 230, 231, 233, 234, 236, 237, 239, 240, 241, 243, 244, 246, 247, 249, 250, 252, 253], 256, 255))
g0 = numpy.array(padfr(_mkx(1, range(17, 256, 17), 2), 256))
b0 = numpy.array(padfr(_mkx(3, range(51, 256, 51), 4), 256))
tempResult = arange(256, dtype='f')
	
===================================================================	
module: 33	
----------------------------	

'\nNothing here but dictionaries for generating LinearSegmentedColormaps,\nand a dictionary of these dictionaries.\n'
from __future__ import absolute_import, division, print_function
import numpy as np
import matplotlib.colors as colors
from sunpy.extern.six.moves import range, zip
__all__ = ['aia_color_table', 'lasco_color_table', 'eit_color_table', 'sxt_color_table', 'xrt_color_table', 'trace_color_table', 'sot_color_table', 'hmi_mag_color_table']

def _mkx(i, steps, n):
    ' Generate list according to pattern of g0 and b0. '
    x = []
    for step in steps:
        x.extend(list(range(i, (step + n), n)))
        i = (step + (n - 1))
    return x

def padfr(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the front. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (([pad] * diff) + lst)

def paden(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the end. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (lst + ([pad] * diff))
r0 = numpy.array(paden([0, 1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 212, 214, 215, 217, 218, 220, 221, 223, 224, 226, 227, 228, 230, 231, 233, 234, 236, 237, 239, 240, 241, 243, 244, 246, 247, 249, 250, 252, 253], 256, 255))
g0 = numpy.array(padfr(_mkx(1, range(17, 256, 17), 2), 256))
b0 = numpy.array(padfr(_mkx(3, range(51, 256, 51), 4), 256))
c0 = numpy.arange(256, dtype='f')
c1 = (np.sqrt(c0) * np.sqrt(255.0)).astype('f')
tempResult = arange(256)
	
===================================================================	
module: 82	
----------------------------	

'\nNothing here but dictionaries for generating LinearSegmentedColormaps,\nand a dictionary of these dictionaries.\n'
from __future__ import absolute_import, division, print_function
import numpy as np
import matplotlib.colors as colors
from sunpy.extern.six.moves import range, zip
__all__ = ['aia_color_table', 'lasco_color_table', 'eit_color_table', 'sxt_color_table', 'xrt_color_table', 'trace_color_table', 'sot_color_table', 'hmi_mag_color_table']

def _mkx(i, steps, n):
    ' Generate list according to pattern of g0 and b0. '
    x = []
    for step in steps:
        x.extend(list(range(i, (step + n), n)))
        i = (step + (n - 1))
    return x

def padfr(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the front. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (([pad] * diff) + lst)

def paden(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the end. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (lst + ([pad] * diff))
r0 = numpy.array(paden([0, 1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 212, 214, 215, 217, 218, 220, 221, 223, 224, 226, 227, 228, 230, 231, 233, 234, 236, 237, 239, 240, 241, 243, 244, 246, 247, 249, 250, 252, 253], 256, 255))
g0 = numpy.array(padfr(_mkx(1, range(17, 256, 17), 2), 256))
b0 = numpy.array(padfr(_mkx(3, range(51, 256, 51), 4), 256))
c0 = numpy.arange(256, dtype='f')
c1 = (np.sqrt(c0) * np.sqrt(255.0)).astype('f')
c2 = ((np.arange(256) ** 2) / 255.0).astype('f')
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (c1.max() + (c2.max() / 2.0))).astype('f')
aia_wave_dict = {1600: (c3, c3, c2), 1700: (c1, c0, c0), 4500: (c0, c0, (b0 / 2.0)), 94: (c2, c3, c0), 131: (g0, r0, r0), 171: (r0, c0, b0), 193: (c1, c0, c2), 211: (c1, c0, c3), 304: (r0, g0, b0), 335: (c2, c0, c1)}

def aia_color_table(wavelength):
    'Returns one of the fundamental color tables for SDO AIA images.\n       Based on aia_lct.pro part of SDO/AIA on SSWIDL written by\n       Karel Schrijver (2010/04/12).\n    '
    try:
        (r, g, b) = aia_wave_dict[wavelength]
    except KeyError:
        raise ValueError('Invalid AIA wavelength. Valid values are 1600,1700,4500,94,131,171,193,211,304,335.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SDO AIA {:s}'.format(str(wavelength)), cdict)
eit_yellow_r = numpy.array([0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 28, 29, 30, 31, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 47, 48, 49, 51, 52, 53, 54, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 89, 90, 91, 93, 94, 95, 96, 98, 99, 100, 102, 103, 104, 105, 107, 108, 109, 110, 112, 113, 114, 116, 117, 118, 119, 121, 122, 123, 124, 126, 127, 128, 130, 131, 132, 133, 135, 136, 137, 138, 140, 141, 142, 144, 145, 146, 147, 149, 150, 151, 153, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 167, 168, 169, 170, 172, 173, 174, 175, 177, 178, 179, 181, 182, 183, 184, 186, 187, 188, 189, 191, 192, 193, 195, 196, 197, 198, 200, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 214, 215, 216, 218, 219, 220, 221, 223, 224, 225, 226, 228, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 242, 243, 244, 246, 247, 248, 249, 251, 252, 253, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255])
eit_yellow_g = numpy.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 206, 207, 209, 209, 210, 210, 212, 213, 213, 215, 216, 216, 218, 219, 219, 221, 221, 222, 224, 224, 225, 227, 227, 228, 230, 230, 231, 231, 232, 234, 234, 235, 237, 237, 238, 240, 240, 241, 241, 243, 244, 244, 246, 247, 247, 249, 250, 250, 252, 252, 253, 255, 255, 255])
eit_yellow_b = numpy.concatenate((np.zeros(201).astype('int'), numpy.array([7, 7, 15, 22, 22, 30, 30, 37, 45, 45, 52, 60, 60, 67, 75, 75, 82, 82, 90, 97, 97, 105, 112, 112, 120, 127, 127, 135, 135, 142, 150, 150, 157, 165, 165, 172, 180, 180, 187, 187, 195, 202, 202, 210, 217, 217, 225, 232, 232, 240, 240, 247, 255, 255, 255])))
eit_dark_blue_r = numpy.concatenate((np.zeros(206).astype('int'), numpy.array([9, 13, 21, 25, 25, 29, 33, 41, 49, 53, 57, 65, 69, 73, 77, 77, 85, 94, 98, 102, 110, 114, 118, 122, 134, 134, 138, 142, 146, 154, 158, 162, 166, 179, 183, 183, 187, 191, 199, 203, 207, 215, 223, 227, 231, 231, 235, 243, 247, 255])))
eit_dark_blue_g = numpy.concatenate((np.zeros(128).astype('int'), numpy.array([2, 2, 4, 5, 7, 12, 13, 15, 17, 20, 21, 21, 23, 25, 29, 31, 33, 34, 37, 39, 41, 41, 44, 47, 49, 50, 52, 55, 57, 60, 61, 61, 65, 66, 68, 69, 73, 76, 77, 79, 82, 82, 84, 85, 87, 92, 94, 95, 97, 100, 102, 103, 103, 105, 110, 111, 113, 114, 118, 119, 121, 122, 122, 127, 129, 130, 132, 135, 137, 138, 142, 145, 145, 146, 148, 150, 153, 154, 158, 159, 162, 164, 164, 166, 167, 170, 174, 175, 177, 180, 182, 183, 185, 185, 188, 191, 193, 195, 198, 199, 201, 203, 207, 207, 209, 211, 212, 215, 217, 219, 220, 225, 227, 227, 228, 230, 233, 235, 236, 239, 243, 244, 246, 246, 247, 251, 252, 255])))
eit_dark_blue_b = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 4, 5, 6, 8, 8, 10, 12, 14, 16, 18, 20, 21, 23, 25, 25, 28, 29, 31, 33, 35, 36, 37, 42, 43, 43, 44, 46, 48, 50, 51, 52, 56, 58, 59, 61, 61, 63, 65, 66, 69, 71, 73, 74, 75, 78, 78, 80, 81, 84, 86, 88, 89, 90, 93, 94, 94, 97, 99, 101, 103, 104, 105, 108, 111, 112, 112, 113, 116, 118, 119, 120, 124, 126, 127, 128, 131, 131, 132, 134, 135, 139, 141, 142, 143, 146, 147, 147, 149, 150, 154, 155, 157, 158, 161, 162, 164, 164, 166, 169, 170, 172, 173, 176, 177, 180, 181, 181, 184, 185, 187, 188, 191, 193, 195, 196, 199, 199, 200, 202, 203, 207, 208, 210, 211, 214, 215, 217, 217, 218, 222, 223, 225, 226, 229, 230, 231, 233, 233, 237, 238, 240, 241, 244, 245, 246, 249, 252, 252, 253, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255])))
eit_dark_green_r = numpy.concatenate((np.zeros(130).astype('int'), numpy.array([1, 3, 4, 9, 11, 12, 14, 17, 19, 19, 20, 22, 27, 29, 30, 32, 35, 37, 38, 38, 41, 45, 46, 48, 50, 53, 54, 58, 59, 59, 62, 64, 66, 67, 71, 74, 75, 77, 80, 80, 82, 83, 85, 90, 91, 93, 95, 98, 100, 101, 101, 103, 108, 109, 111, 112, 116, 117, 119, 121, 121, 125, 127, 129, 130, 133, 135, 137, 140, 143, 143, 145, 146, 148, 151, 153, 156, 158, 161, 163, 163, 164, 166, 169, 172, 174, 175, 179, 180, 182, 183, 183, 187, 190, 192, 193, 196, 198, 200, 201, 206, 206, 208, 209, 211, 214, 216, 217, 219, 224, 225, 225, 227, 229, 232, 234, 235, 238, 242, 243, 245, 245, 246, 250, 251, 255])))
eit_dark_green_g = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 3, 4, 5, 6, 6, 8, 9, 11, 12, 14, 15, 16, 17, 19, 19, 21, 22, 23, 25, 26, 27, 28, 31, 32, 32, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 45, 47, 48, 49, 51, 53, 54, 55, 56, 58, 58, 59, 60, 62, 64, 65, 66, 67, 69, 70, 70, 72, 73, 75, 76, 77, 78, 80, 82, 83, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 97, 97, 98, 99, 100, 103, 104, 105, 106, 108, 109, 109, 110, 111, 114, 115, 116, 117, 119, 120, 121, 121, 123, 125, 126, 127, 128, 130, 131, 133, 134, 134, 136, 137, 138, 139, 141, 143, 144, 145, 147, 147, 148, 149, 150, 153, 154, 155, 156, 158, 159, 160, 160, 161, 164, 165, 166, 167, 169, 170, 171, 172, 172, 175, 176, 177, 178, 180, 181, 182, 184, 186, 186, 187, 188, 189, 191, 192, 194, 195, 197, 198, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 211, 211, 213, 215, 216, 217, 219, 220, 221, 222, 225, 225, 226, 227, 228, 230, 231, 232, 233, 236, 237, 237, 238, 239, 241, 242, 243, 245, 247, 248, 249, 249, 250, 252, 253, 255])))
eit_dark_green_b = numpy.concatenate((np.zeros(197).astype('int'), numpy.array([3, 10, 17, 17, 20, 24, 27, 34, 37, 44, 48, 55, 58, 58, 62, 65, 72, 79, 82, 86, 93, 96, 99, 103, 103, 110, 117, 120, 124, 130, 134, 137, 141, 151, 151, 155, 158, 161, 168, 172, 175, 179, 189, 192, 192, 196, 199, 206, 210, 213, 220, 227, 230, 234, 234, 237, 244, 248, 255])))
eit_dark_red_r = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 3, 4, 5, 6, 6, 8, 9, 11, 12, 14, 15, 16, 17, 19, 19, 21, 22, 23, 25, 26, 27, 28, 31, 32, 32, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 45, 47, 48, 49, 51, 53, 54, 55, 56, 58, 58, 59, 60, 62, 64, 65, 66, 67, 69, 70, 70, 72, 73, 75, 76, 77, 78, 80, 82, 83, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 97, 97, 98, 99, 100, 103, 104, 105, 106, 108, 109, 109, 110, 111, 114, 115, 116, 117, 119, 120, 121, 121, 123, 125, 126, 127, 128, 130, 131, 133, 134, 134, 136, 137, 138, 139, 141, 143, 144, 145, 147, 147, 148, 149, 150, 153, 154, 155, 156, 158, 159, 160, 160, 161, 164, 165, 166, 167, 169, 170, 171, 172, 172, 175, 176, 177, 178, 180, 181, 182, 184, 186, 186, 187, 188, 189, 191, 192, 194, 195, 197, 198, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 211, 211, 213, 215, 216, 217, 219, 220, 221, 222, 225, 225, 226, 227, 228, 230, 231, 232, 233, 236, 237, 237, 238, 239, 241, 242, 243, 245, 247, 248, 249, 249, 250, 252, 253, 255])))
eit_dark_red_g = numpy.concatenate((np.zeros(148).astype('int'), numpy.array([1, 1, 5, 9, 11, 13, 15, 18, 20, 24, 26, 26, 30, 32, 34, 35, 39, 43, 45, 47, 51, 51, 52, 54, 56, 62, 64, 66, 68, 71, 73, 75, 75, 77, 83, 85, 86, 88, 92, 94, 96, 98, 98, 103, 105, 107, 109, 113, 115, 117, 120, 124, 124, 126, 128, 130, 134, 136, 139, 141, 145, 147, 147, 149, 151, 154, 158, 160, 162, 166, 168, 170, 171, 171, 175, 179, 181, 183, 187, 188, 190, 192, 198, 198, 200, 202, 204, 207, 209, 211, 213, 219, 221, 221, 222, 224, 228, 230, 232, 236, 239, 241, 243, 243, 245, 249, 251, 255])))
eit_dark_red_b = numpy.concatenate((np.zeros(204).astype('int'), numpy.array([3, 7, 15, 19, 27, 31, 31, 35, 39, 47, 54, 58, 62, 70, 74, 78, 82, 82, 90, 98, 102, 105, 113, 117, 121, 125, 137, 137, 141, 145, 149, 156, 160, 164, 168, 180, 184, 184, 188, 192, 200, 204, 207, 215, 223, 227, 231, 231, 235, 243, 247, 255])))

def eit_color_table(wavelength):
    'Returns one of the fundamental color tables for SOHO EIT images.'
    try:
        (r, g, b) = {171: (eit_dark_blue_r, eit_dark_blue_g, eit_dark_blue_b), 195: (eit_dark_green_r, eit_dark_green_g, eit_dark_green_b), 284: (eit_yellow_r, eit_yellow_g, eit_yellow_b), 304: (eit_dark_red_r, eit_dark_red_g, eit_dark_red_b)}[wavelength]
    except KeyError:
        raise ValueError('Invalid EIT wavelength. Valid values are 171, 195, 284, 304.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SOHO EIT {:s}'.format(str(wavelength)), cdict)
lasco_c2_r = numpy.concatenate((numpy.array([0.0, 1.0, 2.0, 5.0, 8.0, 11.0, 14.0, 17.0, 20.0, 23.0, 26.0, 28.0, 31.0, 34.0, 37.0, 42.0, 44.0, 47.0, 50.0, 55.0, 57.0, 60.0, 65.0, 68.0, 70.0, 75.0, 78.0, 82.0, 85.0, 88.0, 92.0, 95.0, 99.0, 102.0, 107.0, 110.0, 114.0, 117.0, 121.0, 124.0, 128.0, 133.0, 136.0, 140.0, 143.0, 147.0, 152.0, 155.0, 159.0, 163.0, 166.0, 170.0, 175.0, 178.0, 182.0, 186.0, 189.0, 194.0, 198.0, 201.0, 205.0, 210.0, 214.0, 217.0, 221.0, 226.0, 230.0, 233.0, 237.0, 241.0, 246.0, 250.0, 253.0]), (255 * numpy.ones(183))))
lasco_c2_g = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1.0, 5.0, 11.0, 17.0, 20.0, 26.0, 32.0, 35.0, 41.0, 47.0, 52.0, 56.0, 62.0, 68.0, 73.0, 77.0, 83.0, 88.0, 94.0, 100.0, 103.0, 109.0, 115.0, 120.0, 126.0, 130.0, 136.0, 141.0, 147.0, 153.0, 158.0, 164.0, 168.0, 173.0, 179.0, 185.0, 190.0, 196.0, 202.0, 207.0, 213.0, 217.0, 222.0, 228.0, 234.0, 239.0, 245.0, 251.0]), (255 * numpy.ones(156))))
lasco_c2_b = numpy.concatenate((np.zeros(78).astype('int'), numpy.array([0.0, 7.0, 19.0, 31.0, 43.0, 54.0, 66.0, 74.0, 86.0, 98.0, 109.0, 121.0, 133.0, 145.0, 156.0, 168.0, 176.0, 188.0, 200.0, 211.0, 223.0, 235.0, 247.0]), (255 * numpy.ones(156))))
lasco_c3_r = numpy.concatenate((np.zeros(77).astype('int'), numpy.array([5.0, 13.0, 25.0, 33.0, 45.0, 53.0, 65.0, 73.0, 85.0, 94.0, 106.0, 114.0, 126.0, 134.0, 146.0, 154.0, 166.0, 175.0, 187.0, 195.0, 207.0, 215.0, 227.0, 235.0, 247.0]), (255 * numpy.ones(154))))
lasco_c3_g = numpy.concatenate((np.zeros(39).astype('int'), numpy.array([4.0, 7.0, 12.0, 15.0, 20.0, 23.0, 28.0, 31.0, 36.0, 39.0, 44.0, 47.0, 52.0, 55.0, 60.0, 63.0, 68.0, 71.0, 76.0, 79.0, 84.0, 87.0, 92.0, 95.0, 100.0, 103.0, 108.0, 111.0, 116.0, 119.0, 124.0, 127.0, 132.0, 135.0, 140.0, 143.0, 148.0, 151.0, 156.0, 159.0, 164.0, 167.0, 172.0, 175.0, 180.0, 183.0, 188.0, 191.0, 196.0, 199.0, 204.0, 207.0, 212.0, 215.0, 220.0, 223.0, 228.0, 231.0, 236.0, 239.0, 244.0, 247.0, 252.0, 255.0, 255.0]), (255 * numpy.ones(154))))
lasco_c3_b = numpy.concatenate((numpy.array([0.0, 4.0, 6.0, 10.0, 13.0, 17.0, 20.0, 24.0, 27.0, 31.0, 33.0, 37.0, 40.0, 44.0, 47.0, 51.0, 54.0, 58.0, 61.0, 65.0, 67.0, 71.0, 74.0, 78.0, 81.0, 85.0, 88.0, 92.0, 94.0, 99.0, 101.0, 105.0, 108.0, 112.0, 115.0, 119.0, 122.0, 126.0, 128.0, 132.0, 135.0, 139.0, 142.0, 146.0, 149.0, 153.0, 155.0, 160.0, 162.0, 166.0, 169.0, 173.0, 176.0, 180.0, 183.0, 187.0, 189.0, 193.0, 196.0, 200.0, 203.0, 207.0, 210.0, 214.0, 217.0, 221.0, 223.0, 227.0, 230.0, 234.0, 237.0, 241.0, 244.0, 248.0, 250.0]), (255 * numpy.ones(181))))

def lasco_color_table(number):
    'Returns one of the fundamental color tables for SOHO LASCO images.'
    try:
        (r, g, b) = {2: (lasco_c2_r, lasco_c2_g, lasco_c2_b), 3: (lasco_c3_r, lasco_c3_g, lasco_c3_b)}[number]
    except KeyError:
        raise ValueError('Invalid LASCO number. Valid values are 2, 3.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SOHO LASCO C{:s}'.format(str(number)), cdict)
sxt_gold_r = numpy.concatenate((numpy.linspace(0, 255, num=185, endpoint=False), (255 * numpy.ones(71))))
tempResult = arange(256)
	
===================================================================	
module: 83	
----------------------------	

'\nNothing here but dictionaries for generating LinearSegmentedColormaps,\nand a dictionary of these dictionaries.\n'
from __future__ import absolute_import, division, print_function
import numpy as np
import matplotlib.colors as colors
from sunpy.extern.six.moves import range, zip
__all__ = ['aia_color_table', 'lasco_color_table', 'eit_color_table', 'sxt_color_table', 'xrt_color_table', 'trace_color_table', 'sot_color_table', 'hmi_mag_color_table']

def _mkx(i, steps, n):
    ' Generate list according to pattern of g0 and b0. '
    x = []
    for step in steps:
        x.extend(list(range(i, (step + n), n)))
        i = (step + (n - 1))
    return x

def padfr(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the front. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (([pad] * diff) + lst)

def paden(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the end. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (lst + ([pad] * diff))
r0 = numpy.array(paden([0, 1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 212, 214, 215, 217, 218, 220, 221, 223, 224, 226, 227, 228, 230, 231, 233, 234, 236, 237, 239, 240, 241, 243, 244, 246, 247, 249, 250, 252, 253], 256, 255))
g0 = numpy.array(padfr(_mkx(1, range(17, 256, 17), 2), 256))
b0 = numpy.array(padfr(_mkx(3, range(51, 256, 51), 4), 256))
c0 = numpy.arange(256, dtype='f')
c1 = (np.sqrt(c0) * np.sqrt(255.0)).astype('f')
c2 = ((np.arange(256) ** 2) / 255.0).astype('f')
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (c1.max() + (c2.max() / 2.0))).astype('f')
aia_wave_dict = {1600: (c3, c3, c2), 1700: (c1, c0, c0), 4500: (c0, c0, (b0 / 2.0)), 94: (c2, c3, c0), 131: (g0, r0, r0), 171: (r0, c0, b0), 193: (c1, c0, c2), 211: (c1, c0, c3), 304: (r0, g0, b0), 335: (c2, c0, c1)}

def aia_color_table(wavelength):
    'Returns one of the fundamental color tables for SDO AIA images.\n       Based on aia_lct.pro part of SDO/AIA on SSWIDL written by\n       Karel Schrijver (2010/04/12).\n    '
    try:
        (r, g, b) = aia_wave_dict[wavelength]
    except KeyError:
        raise ValueError('Invalid AIA wavelength. Valid values are 1600,1700,4500,94,131,171,193,211,304,335.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SDO AIA {:s}'.format(str(wavelength)), cdict)
eit_yellow_r = numpy.array([0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 28, 29, 30, 31, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 47, 48, 49, 51, 52, 53, 54, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 89, 90, 91, 93, 94, 95, 96, 98, 99, 100, 102, 103, 104, 105, 107, 108, 109, 110, 112, 113, 114, 116, 117, 118, 119, 121, 122, 123, 124, 126, 127, 128, 130, 131, 132, 133, 135, 136, 137, 138, 140, 141, 142, 144, 145, 146, 147, 149, 150, 151, 153, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 167, 168, 169, 170, 172, 173, 174, 175, 177, 178, 179, 181, 182, 183, 184, 186, 187, 188, 189, 191, 192, 193, 195, 196, 197, 198, 200, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 214, 215, 216, 218, 219, 220, 221, 223, 224, 225, 226, 228, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 242, 243, 244, 246, 247, 248, 249, 251, 252, 253, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255])
eit_yellow_g = numpy.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 206, 207, 209, 209, 210, 210, 212, 213, 213, 215, 216, 216, 218, 219, 219, 221, 221, 222, 224, 224, 225, 227, 227, 228, 230, 230, 231, 231, 232, 234, 234, 235, 237, 237, 238, 240, 240, 241, 241, 243, 244, 244, 246, 247, 247, 249, 250, 250, 252, 252, 253, 255, 255, 255])
eit_yellow_b = numpy.concatenate((np.zeros(201).astype('int'), numpy.array([7, 7, 15, 22, 22, 30, 30, 37, 45, 45, 52, 60, 60, 67, 75, 75, 82, 82, 90, 97, 97, 105, 112, 112, 120, 127, 127, 135, 135, 142, 150, 150, 157, 165, 165, 172, 180, 180, 187, 187, 195, 202, 202, 210, 217, 217, 225, 232, 232, 240, 240, 247, 255, 255, 255])))
eit_dark_blue_r = numpy.concatenate((np.zeros(206).astype('int'), numpy.array([9, 13, 21, 25, 25, 29, 33, 41, 49, 53, 57, 65, 69, 73, 77, 77, 85, 94, 98, 102, 110, 114, 118, 122, 134, 134, 138, 142, 146, 154, 158, 162, 166, 179, 183, 183, 187, 191, 199, 203, 207, 215, 223, 227, 231, 231, 235, 243, 247, 255])))
eit_dark_blue_g = numpy.concatenate((np.zeros(128).astype('int'), numpy.array([2, 2, 4, 5, 7, 12, 13, 15, 17, 20, 21, 21, 23, 25, 29, 31, 33, 34, 37, 39, 41, 41, 44, 47, 49, 50, 52, 55, 57, 60, 61, 61, 65, 66, 68, 69, 73, 76, 77, 79, 82, 82, 84, 85, 87, 92, 94, 95, 97, 100, 102, 103, 103, 105, 110, 111, 113, 114, 118, 119, 121, 122, 122, 127, 129, 130, 132, 135, 137, 138, 142, 145, 145, 146, 148, 150, 153, 154, 158, 159, 162, 164, 164, 166, 167, 170, 174, 175, 177, 180, 182, 183, 185, 185, 188, 191, 193, 195, 198, 199, 201, 203, 207, 207, 209, 211, 212, 215, 217, 219, 220, 225, 227, 227, 228, 230, 233, 235, 236, 239, 243, 244, 246, 246, 247, 251, 252, 255])))
eit_dark_blue_b = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 4, 5, 6, 8, 8, 10, 12, 14, 16, 18, 20, 21, 23, 25, 25, 28, 29, 31, 33, 35, 36, 37, 42, 43, 43, 44, 46, 48, 50, 51, 52, 56, 58, 59, 61, 61, 63, 65, 66, 69, 71, 73, 74, 75, 78, 78, 80, 81, 84, 86, 88, 89, 90, 93, 94, 94, 97, 99, 101, 103, 104, 105, 108, 111, 112, 112, 113, 116, 118, 119, 120, 124, 126, 127, 128, 131, 131, 132, 134, 135, 139, 141, 142, 143, 146, 147, 147, 149, 150, 154, 155, 157, 158, 161, 162, 164, 164, 166, 169, 170, 172, 173, 176, 177, 180, 181, 181, 184, 185, 187, 188, 191, 193, 195, 196, 199, 199, 200, 202, 203, 207, 208, 210, 211, 214, 215, 217, 217, 218, 222, 223, 225, 226, 229, 230, 231, 233, 233, 237, 238, 240, 241, 244, 245, 246, 249, 252, 252, 253, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255])))
eit_dark_green_r = numpy.concatenate((np.zeros(130).astype('int'), numpy.array([1, 3, 4, 9, 11, 12, 14, 17, 19, 19, 20, 22, 27, 29, 30, 32, 35, 37, 38, 38, 41, 45, 46, 48, 50, 53, 54, 58, 59, 59, 62, 64, 66, 67, 71, 74, 75, 77, 80, 80, 82, 83, 85, 90, 91, 93, 95, 98, 100, 101, 101, 103, 108, 109, 111, 112, 116, 117, 119, 121, 121, 125, 127, 129, 130, 133, 135, 137, 140, 143, 143, 145, 146, 148, 151, 153, 156, 158, 161, 163, 163, 164, 166, 169, 172, 174, 175, 179, 180, 182, 183, 183, 187, 190, 192, 193, 196, 198, 200, 201, 206, 206, 208, 209, 211, 214, 216, 217, 219, 224, 225, 225, 227, 229, 232, 234, 235, 238, 242, 243, 245, 245, 246, 250, 251, 255])))
eit_dark_green_g = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 3, 4, 5, 6, 6, 8, 9, 11, 12, 14, 15, 16, 17, 19, 19, 21, 22, 23, 25, 26, 27, 28, 31, 32, 32, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 45, 47, 48, 49, 51, 53, 54, 55, 56, 58, 58, 59, 60, 62, 64, 65, 66, 67, 69, 70, 70, 72, 73, 75, 76, 77, 78, 80, 82, 83, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 97, 97, 98, 99, 100, 103, 104, 105, 106, 108, 109, 109, 110, 111, 114, 115, 116, 117, 119, 120, 121, 121, 123, 125, 126, 127, 128, 130, 131, 133, 134, 134, 136, 137, 138, 139, 141, 143, 144, 145, 147, 147, 148, 149, 150, 153, 154, 155, 156, 158, 159, 160, 160, 161, 164, 165, 166, 167, 169, 170, 171, 172, 172, 175, 176, 177, 178, 180, 181, 182, 184, 186, 186, 187, 188, 189, 191, 192, 194, 195, 197, 198, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 211, 211, 213, 215, 216, 217, 219, 220, 221, 222, 225, 225, 226, 227, 228, 230, 231, 232, 233, 236, 237, 237, 238, 239, 241, 242, 243, 245, 247, 248, 249, 249, 250, 252, 253, 255])))
eit_dark_green_b = numpy.concatenate((np.zeros(197).astype('int'), numpy.array([3, 10, 17, 17, 20, 24, 27, 34, 37, 44, 48, 55, 58, 58, 62, 65, 72, 79, 82, 86, 93, 96, 99, 103, 103, 110, 117, 120, 124, 130, 134, 137, 141, 151, 151, 155, 158, 161, 168, 172, 175, 179, 189, 192, 192, 196, 199, 206, 210, 213, 220, 227, 230, 234, 234, 237, 244, 248, 255])))
eit_dark_red_r = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 3, 4, 5, 6, 6, 8, 9, 11, 12, 14, 15, 16, 17, 19, 19, 21, 22, 23, 25, 26, 27, 28, 31, 32, 32, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 45, 47, 48, 49, 51, 53, 54, 55, 56, 58, 58, 59, 60, 62, 64, 65, 66, 67, 69, 70, 70, 72, 73, 75, 76, 77, 78, 80, 82, 83, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 97, 97, 98, 99, 100, 103, 104, 105, 106, 108, 109, 109, 110, 111, 114, 115, 116, 117, 119, 120, 121, 121, 123, 125, 126, 127, 128, 130, 131, 133, 134, 134, 136, 137, 138, 139, 141, 143, 144, 145, 147, 147, 148, 149, 150, 153, 154, 155, 156, 158, 159, 160, 160, 161, 164, 165, 166, 167, 169, 170, 171, 172, 172, 175, 176, 177, 178, 180, 181, 182, 184, 186, 186, 187, 188, 189, 191, 192, 194, 195, 197, 198, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 211, 211, 213, 215, 216, 217, 219, 220, 221, 222, 225, 225, 226, 227, 228, 230, 231, 232, 233, 236, 237, 237, 238, 239, 241, 242, 243, 245, 247, 248, 249, 249, 250, 252, 253, 255])))
eit_dark_red_g = numpy.concatenate((np.zeros(148).astype('int'), numpy.array([1, 1, 5, 9, 11, 13, 15, 18, 20, 24, 26, 26, 30, 32, 34, 35, 39, 43, 45, 47, 51, 51, 52, 54, 56, 62, 64, 66, 68, 71, 73, 75, 75, 77, 83, 85, 86, 88, 92, 94, 96, 98, 98, 103, 105, 107, 109, 113, 115, 117, 120, 124, 124, 126, 128, 130, 134, 136, 139, 141, 145, 147, 147, 149, 151, 154, 158, 160, 162, 166, 168, 170, 171, 171, 175, 179, 181, 183, 187, 188, 190, 192, 198, 198, 200, 202, 204, 207, 209, 211, 213, 219, 221, 221, 222, 224, 228, 230, 232, 236, 239, 241, 243, 243, 245, 249, 251, 255])))
eit_dark_red_b = numpy.concatenate((np.zeros(204).astype('int'), numpy.array([3, 7, 15, 19, 27, 31, 31, 35, 39, 47, 54, 58, 62, 70, 74, 78, 82, 82, 90, 98, 102, 105, 113, 117, 121, 125, 137, 137, 141, 145, 149, 156, 160, 164, 168, 180, 184, 184, 188, 192, 200, 204, 207, 215, 223, 227, 231, 231, 235, 243, 247, 255])))

def eit_color_table(wavelength):
    'Returns one of the fundamental color tables for SOHO EIT images.'
    try:
        (r, g, b) = {171: (eit_dark_blue_r, eit_dark_blue_g, eit_dark_blue_b), 195: (eit_dark_green_r, eit_dark_green_g, eit_dark_green_b), 284: (eit_yellow_r, eit_yellow_g, eit_yellow_b), 304: (eit_dark_red_r, eit_dark_red_g, eit_dark_red_b)}[wavelength]
    except KeyError:
        raise ValueError('Invalid EIT wavelength. Valid values are 171, 195, 284, 304.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SOHO EIT {:s}'.format(str(wavelength)), cdict)
lasco_c2_r = numpy.concatenate((numpy.array([0.0, 1.0, 2.0, 5.0, 8.0, 11.0, 14.0, 17.0, 20.0, 23.0, 26.0, 28.0, 31.0, 34.0, 37.0, 42.0, 44.0, 47.0, 50.0, 55.0, 57.0, 60.0, 65.0, 68.0, 70.0, 75.0, 78.0, 82.0, 85.0, 88.0, 92.0, 95.0, 99.0, 102.0, 107.0, 110.0, 114.0, 117.0, 121.0, 124.0, 128.0, 133.0, 136.0, 140.0, 143.0, 147.0, 152.0, 155.0, 159.0, 163.0, 166.0, 170.0, 175.0, 178.0, 182.0, 186.0, 189.0, 194.0, 198.0, 201.0, 205.0, 210.0, 214.0, 217.0, 221.0, 226.0, 230.0, 233.0, 237.0, 241.0, 246.0, 250.0, 253.0]), (255 * numpy.ones(183))))
lasco_c2_g = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1.0, 5.0, 11.0, 17.0, 20.0, 26.0, 32.0, 35.0, 41.0, 47.0, 52.0, 56.0, 62.0, 68.0, 73.0, 77.0, 83.0, 88.0, 94.0, 100.0, 103.0, 109.0, 115.0, 120.0, 126.0, 130.0, 136.0, 141.0, 147.0, 153.0, 158.0, 164.0, 168.0, 173.0, 179.0, 185.0, 190.0, 196.0, 202.0, 207.0, 213.0, 217.0, 222.0, 228.0, 234.0, 239.0, 245.0, 251.0]), (255 * numpy.ones(156))))
lasco_c2_b = numpy.concatenate((np.zeros(78).astype('int'), numpy.array([0.0, 7.0, 19.0, 31.0, 43.0, 54.0, 66.0, 74.0, 86.0, 98.0, 109.0, 121.0, 133.0, 145.0, 156.0, 168.0, 176.0, 188.0, 200.0, 211.0, 223.0, 235.0, 247.0]), (255 * numpy.ones(156))))
lasco_c3_r = numpy.concatenate((np.zeros(77).astype('int'), numpy.array([5.0, 13.0, 25.0, 33.0, 45.0, 53.0, 65.0, 73.0, 85.0, 94.0, 106.0, 114.0, 126.0, 134.0, 146.0, 154.0, 166.0, 175.0, 187.0, 195.0, 207.0, 215.0, 227.0, 235.0, 247.0]), (255 * numpy.ones(154))))
lasco_c3_g = numpy.concatenate((np.zeros(39).astype('int'), numpy.array([4.0, 7.0, 12.0, 15.0, 20.0, 23.0, 28.0, 31.0, 36.0, 39.0, 44.0, 47.0, 52.0, 55.0, 60.0, 63.0, 68.0, 71.0, 76.0, 79.0, 84.0, 87.0, 92.0, 95.0, 100.0, 103.0, 108.0, 111.0, 116.0, 119.0, 124.0, 127.0, 132.0, 135.0, 140.0, 143.0, 148.0, 151.0, 156.0, 159.0, 164.0, 167.0, 172.0, 175.0, 180.0, 183.0, 188.0, 191.0, 196.0, 199.0, 204.0, 207.0, 212.0, 215.0, 220.0, 223.0, 228.0, 231.0, 236.0, 239.0, 244.0, 247.0, 252.0, 255.0, 255.0]), (255 * numpy.ones(154))))
lasco_c3_b = numpy.concatenate((numpy.array([0.0, 4.0, 6.0, 10.0, 13.0, 17.0, 20.0, 24.0, 27.0, 31.0, 33.0, 37.0, 40.0, 44.0, 47.0, 51.0, 54.0, 58.0, 61.0, 65.0, 67.0, 71.0, 74.0, 78.0, 81.0, 85.0, 88.0, 92.0, 94.0, 99.0, 101.0, 105.0, 108.0, 112.0, 115.0, 119.0, 122.0, 126.0, 128.0, 132.0, 135.0, 139.0, 142.0, 146.0, 149.0, 153.0, 155.0, 160.0, 162.0, 166.0, 169.0, 173.0, 176.0, 180.0, 183.0, 187.0, 189.0, 193.0, 196.0, 200.0, 203.0, 207.0, 210.0, 214.0, 217.0, 221.0, 223.0, 227.0, 230.0, 234.0, 237.0, 241.0, 244.0, 248.0, 250.0]), (255 * numpy.ones(181))))

def lasco_color_table(number):
    'Returns one of the fundamental color tables for SOHO LASCO images.'
    try:
        (r, g, b) = {2: (lasco_c2_r, lasco_c2_g, lasco_c2_b), 3: (lasco_c3_r, lasco_c3_g, lasco_c3_b)}[number]
    except KeyError:
        raise ValueError('Invalid LASCO number. Valid values are 2, 3.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SOHO LASCO C{:s}'.format(str(number)), cdict)
sxt_gold_r = numpy.concatenate((numpy.linspace(0, 255, num=185, endpoint=False), (255 * numpy.ones(71))))
sxt_gold_g = ((255 * (numpy.arange(256) ** 1.25)) / (255.0 ** 1.25))
tempResult = arange(71)
	
===================================================================	
module: 84	
----------------------------	

'\nNothing here but dictionaries for generating LinearSegmentedColormaps,\nand a dictionary of these dictionaries.\n'
from __future__ import absolute_import, division, print_function
import numpy as np
import matplotlib.colors as colors
from sunpy.extern.six.moves import range, zip
__all__ = ['aia_color_table', 'lasco_color_table', 'eit_color_table', 'sxt_color_table', 'xrt_color_table', 'trace_color_table', 'sot_color_table', 'hmi_mag_color_table']

def _mkx(i, steps, n):
    ' Generate list according to pattern of g0 and b0. '
    x = []
    for step in steps:
        x.extend(list(range(i, (step + n), n)))
        i = (step + (n - 1))
    return x

def padfr(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the front. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (([pad] * diff) + lst)

def paden(lst, len_, pad=0):
    ' Pad lst to contain at least len_ items by adding pad to the end. '
    diff = (len_ - len(lst))
    diff = (0 if (diff < 0) else diff)
    return (lst + ([pad] * diff))
r0 = numpy.array(paden([0, 1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 15, 17, 18, 20, 21, 23, 24, 26, 27, 28, 30, 31, 33, 34, 36, 37, 39, 40, 42, 43, 44, 46, 47, 49, 50, 52, 53, 55, 56, 57, 59, 60, 62, 63, 65, 66, 68, 69, 70, 72, 73, 75, 76, 78, 79, 81, 82, 84, 85, 86, 88, 89, 91, 92, 94, 95, 97, 98, 99, 101, 102, 104, 105, 107, 108, 110, 111, 113, 114, 115, 117, 118, 120, 121, 123, 124, 126, 127, 128, 130, 131, 133, 134, 136, 137, 139, 140, 141, 143, 144, 146, 147, 149, 150, 152, 153, 155, 156, 157, 159, 160, 162, 163, 165, 166, 168, 169, 170, 172, 173, 175, 176, 178, 179, 181, 182, 184, 185, 186, 188, 189, 191, 192, 194, 195, 197, 198, 199, 201, 202, 204, 205, 207, 208, 210, 211, 212, 214, 215, 217, 218, 220, 221, 223, 224, 226, 227, 228, 230, 231, 233, 234, 236, 237, 239, 240, 241, 243, 244, 246, 247, 249, 250, 252, 253], 256, 255))
g0 = numpy.array(padfr(_mkx(1, range(17, 256, 17), 2), 256))
b0 = numpy.array(padfr(_mkx(3, range(51, 256, 51), 4), 256))
c0 = numpy.arange(256, dtype='f')
c1 = (np.sqrt(c0) * np.sqrt(255.0)).astype('f')
c2 = ((np.arange(256) ** 2) / 255.0).astype('f')
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (c1.max() + (c2.max() / 2.0))).astype('f')
aia_wave_dict = {1600: (c3, c3, c2), 1700: (c1, c0, c0), 4500: (c0, c0, (b0 / 2.0)), 94: (c2, c3, c0), 131: (g0, r0, r0), 171: (r0, c0, b0), 193: (c1, c0, c2), 211: (c1, c0, c3), 304: (r0, g0, b0), 335: (c2, c0, c1)}

def aia_color_table(wavelength):
    'Returns one of the fundamental color tables for SDO AIA images.\n       Based on aia_lct.pro part of SDO/AIA on SSWIDL written by\n       Karel Schrijver (2010/04/12).\n    '
    try:
        (r, g, b) = aia_wave_dict[wavelength]
    except KeyError:
        raise ValueError('Invalid AIA wavelength. Valid values are 1600,1700,4500,94,131,171,193,211,304,335.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SDO AIA {:s}'.format(str(wavelength)), cdict)
eit_yellow_r = numpy.array([0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 28, 29, 30, 31, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 47, 48, 49, 51, 52, 53, 54, 56, 57, 58, 59, 61, 62, 63, 65, 66, 67, 68, 70, 71, 72, 73, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 89, 90, 91, 93, 94, 95, 96, 98, 99, 100, 102, 103, 104, 105, 107, 108, 109, 110, 112, 113, 114, 116, 117, 118, 119, 121, 122, 123, 124, 126, 127, 128, 130, 131, 132, 133, 135, 136, 137, 138, 140, 141, 142, 144, 145, 146, 147, 149, 150, 151, 153, 154, 155, 156, 158, 159, 160, 161, 163, 164, 165, 167, 168, 169, 170, 172, 173, 174, 175, 177, 178, 179, 181, 182, 183, 184, 186, 187, 188, 189, 191, 192, 193, 195, 196, 197, 198, 200, 201, 202, 204, 205, 206, 207, 209, 210, 211, 212, 214, 215, 216, 218, 219, 220, 221, 223, 224, 225, 226, 228, 229, 230, 232, 233, 234, 235, 237, 238, 239, 240, 242, 243, 244, 246, 247, 248, 249, 251, 252, 253, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255])
eit_yellow_g = numpy.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 206, 207, 209, 209, 210, 210, 212, 213, 213, 215, 216, 216, 218, 219, 219, 221, 221, 222, 224, 224, 225, 227, 227, 228, 230, 230, 231, 231, 232, 234, 234, 235, 237, 237, 238, 240, 240, 241, 241, 243, 244, 244, 246, 247, 247, 249, 250, 250, 252, 252, 253, 255, 255, 255])
eit_yellow_b = numpy.concatenate((np.zeros(201).astype('int'), numpy.array([7, 7, 15, 22, 22, 30, 30, 37, 45, 45, 52, 60, 60, 67, 75, 75, 82, 82, 90, 97, 97, 105, 112, 112, 120, 127, 127, 135, 135, 142, 150, 150, 157, 165, 165, 172, 180, 180, 187, 187, 195, 202, 202, 210, 217, 217, 225, 232, 232, 240, 240, 247, 255, 255, 255])))
eit_dark_blue_r = numpy.concatenate((np.zeros(206).astype('int'), numpy.array([9, 13, 21, 25, 25, 29, 33, 41, 49, 53, 57, 65, 69, 73, 77, 77, 85, 94, 98, 102, 110, 114, 118, 122, 134, 134, 138, 142, 146, 154, 158, 162, 166, 179, 183, 183, 187, 191, 199, 203, 207, 215, 223, 227, 231, 231, 235, 243, 247, 255])))
eit_dark_blue_g = numpy.concatenate((np.zeros(128).astype('int'), numpy.array([2, 2, 4, 5, 7, 12, 13, 15, 17, 20, 21, 21, 23, 25, 29, 31, 33, 34, 37, 39, 41, 41, 44, 47, 49, 50, 52, 55, 57, 60, 61, 61, 65, 66, 68, 69, 73, 76, 77, 79, 82, 82, 84, 85, 87, 92, 94, 95, 97, 100, 102, 103, 103, 105, 110, 111, 113, 114, 118, 119, 121, 122, 122, 127, 129, 130, 132, 135, 137, 138, 142, 145, 145, 146, 148, 150, 153, 154, 158, 159, 162, 164, 164, 166, 167, 170, 174, 175, 177, 180, 182, 183, 185, 185, 188, 191, 193, 195, 198, 199, 201, 203, 207, 207, 209, 211, 212, 215, 217, 219, 220, 225, 227, 227, 228, 230, 233, 235, 236, 239, 243, 244, 246, 246, 247, 251, 252, 255])))
eit_dark_blue_b = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 4, 5, 6, 8, 8, 10, 12, 14, 16, 18, 20, 21, 23, 25, 25, 28, 29, 31, 33, 35, 36, 37, 42, 43, 43, 44, 46, 48, 50, 51, 52, 56, 58, 59, 61, 61, 63, 65, 66, 69, 71, 73, 74, 75, 78, 78, 80, 81, 84, 86, 88, 89, 90, 93, 94, 94, 97, 99, 101, 103, 104, 105, 108, 111, 112, 112, 113, 116, 118, 119, 120, 124, 126, 127, 128, 131, 131, 132, 134, 135, 139, 141, 142, 143, 146, 147, 147, 149, 150, 154, 155, 157, 158, 161, 162, 164, 164, 166, 169, 170, 172, 173, 176, 177, 180, 181, 181, 184, 185, 187, 188, 191, 193, 195, 196, 199, 199, 200, 202, 203, 207, 208, 210, 211, 214, 215, 217, 217, 218, 222, 223, 225, 226, 229, 230, 231, 233, 233, 237, 238, 240, 241, 244, 245, 246, 249, 252, 252, 253, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255])))
eit_dark_green_r = numpy.concatenate((np.zeros(130).astype('int'), numpy.array([1, 3, 4, 9, 11, 12, 14, 17, 19, 19, 20, 22, 27, 29, 30, 32, 35, 37, 38, 38, 41, 45, 46, 48, 50, 53, 54, 58, 59, 59, 62, 64, 66, 67, 71, 74, 75, 77, 80, 80, 82, 83, 85, 90, 91, 93, 95, 98, 100, 101, 101, 103, 108, 109, 111, 112, 116, 117, 119, 121, 121, 125, 127, 129, 130, 133, 135, 137, 140, 143, 143, 145, 146, 148, 151, 153, 156, 158, 161, 163, 163, 164, 166, 169, 172, 174, 175, 179, 180, 182, 183, 183, 187, 190, 192, 193, 196, 198, 200, 201, 206, 206, 208, 209, 211, 214, 216, 217, 219, 224, 225, 225, 227, 229, 232, 234, 235, 238, 242, 243, 245, 245, 246, 250, 251, 255])))
eit_dark_green_g = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 3, 4, 5, 6, 6, 8, 9, 11, 12, 14, 15, 16, 17, 19, 19, 21, 22, 23, 25, 26, 27, 28, 31, 32, 32, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 45, 47, 48, 49, 51, 53, 54, 55, 56, 58, 58, 59, 60, 62, 64, 65, 66, 67, 69, 70, 70, 72, 73, 75, 76, 77, 78, 80, 82, 83, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 97, 97, 98, 99, 100, 103, 104, 105, 106, 108, 109, 109, 110, 111, 114, 115, 116, 117, 119, 120, 121, 121, 123, 125, 126, 127, 128, 130, 131, 133, 134, 134, 136, 137, 138, 139, 141, 143, 144, 145, 147, 147, 148, 149, 150, 153, 154, 155, 156, 158, 159, 160, 160, 161, 164, 165, 166, 167, 169, 170, 171, 172, 172, 175, 176, 177, 178, 180, 181, 182, 184, 186, 186, 187, 188, 189, 191, 192, 194, 195, 197, 198, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 211, 211, 213, 215, 216, 217, 219, 220, 221, 222, 225, 225, 226, 227, 228, 230, 231, 232, 233, 236, 237, 237, 238, 239, 241, 242, 243, 245, 247, 248, 249, 249, 250, 252, 253, 255])))
eit_dark_green_b = numpy.concatenate((np.zeros(197).astype('int'), numpy.array([3, 10, 17, 17, 20, 24, 27, 34, 37, 44, 48, 55, 58, 58, 62, 65, 72, 79, 82, 86, 93, 96, 99, 103, 103, 110, 117, 120, 124, 130, 134, 137, 141, 151, 151, 155, 158, 161, 168, 172, 175, 179, 189, 192, 192, 196, 199, 206, 210, 213, 220, 227, 230, 234, 234, 237, 244, 248, 255])))
eit_dark_red_r = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1, 3, 4, 5, 6, 6, 8, 9, 11, 12, 14, 15, 16, 17, 19, 19, 21, 22, 23, 25, 26, 27, 28, 31, 32, 32, 33, 34, 36, 37, 38, 39, 42, 43, 44, 45, 45, 47, 48, 49, 51, 53, 54, 55, 56, 58, 58, 59, 60, 62, 64, 65, 66, 67, 69, 70, 70, 72, 73, 75, 76, 77, 78, 80, 82, 83, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 97, 97, 98, 99, 100, 103, 104, 105, 106, 108, 109, 109, 110, 111, 114, 115, 116, 117, 119, 120, 121, 121, 123, 125, 126, 127, 128, 130, 131, 133, 134, 134, 136, 137, 138, 139, 141, 143, 144, 145, 147, 147, 148, 149, 150, 153, 154, 155, 156, 158, 159, 160, 160, 161, 164, 165, 166, 167, 169, 170, 171, 172, 172, 175, 176, 177, 178, 180, 181, 182, 184, 186, 186, 187, 188, 189, 191, 192, 194, 195, 197, 198, 198, 199, 200, 202, 204, 205, 206, 208, 209, 210, 211, 211, 213, 215, 216, 217, 219, 220, 221, 222, 225, 225, 226, 227, 228, 230, 231, 232, 233, 236, 237, 237, 238, 239, 241, 242, 243, 245, 247, 248, 249, 249, 250, 252, 253, 255])))
eit_dark_red_g = numpy.concatenate((np.zeros(148).astype('int'), numpy.array([1, 1, 5, 9, 11, 13, 15, 18, 20, 24, 26, 26, 30, 32, 34, 35, 39, 43, 45, 47, 51, 51, 52, 54, 56, 62, 64, 66, 68, 71, 73, 75, 75, 77, 83, 85, 86, 88, 92, 94, 96, 98, 98, 103, 105, 107, 109, 113, 115, 117, 120, 124, 124, 126, 128, 130, 134, 136, 139, 141, 145, 147, 147, 149, 151, 154, 158, 160, 162, 166, 168, 170, 171, 171, 175, 179, 181, 183, 187, 188, 190, 192, 198, 198, 200, 202, 204, 207, 209, 211, 213, 219, 221, 221, 222, 224, 228, 230, 232, 236, 239, 241, 243, 243, 245, 249, 251, 255])))
eit_dark_red_b = numpy.concatenate((np.zeros(204).astype('int'), numpy.array([3, 7, 15, 19, 27, 31, 31, 35, 39, 47, 54, 58, 62, 70, 74, 78, 82, 82, 90, 98, 102, 105, 113, 117, 121, 125, 137, 137, 141, 145, 149, 156, 160, 164, 168, 180, 184, 184, 188, 192, 200, 204, 207, 215, 223, 227, 231, 231, 235, 243, 247, 255])))

def eit_color_table(wavelength):
    'Returns one of the fundamental color tables for SOHO EIT images.'
    try:
        (r, g, b) = {171: (eit_dark_blue_r, eit_dark_blue_g, eit_dark_blue_b), 195: (eit_dark_green_r, eit_dark_green_g, eit_dark_green_b), 284: (eit_yellow_r, eit_yellow_g, eit_yellow_b), 304: (eit_dark_red_r, eit_dark_red_g, eit_dark_red_b)}[wavelength]
    except KeyError:
        raise ValueError('Invalid EIT wavelength. Valid values are 171, 195, 284, 304.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SOHO EIT {:s}'.format(str(wavelength)), cdict)
lasco_c2_r = numpy.concatenate((numpy.array([0.0, 1.0, 2.0, 5.0, 8.0, 11.0, 14.0, 17.0, 20.0, 23.0, 26.0, 28.0, 31.0, 34.0, 37.0, 42.0, 44.0, 47.0, 50.0, 55.0, 57.0, 60.0, 65.0, 68.0, 70.0, 75.0, 78.0, 82.0, 85.0, 88.0, 92.0, 95.0, 99.0, 102.0, 107.0, 110.0, 114.0, 117.0, 121.0, 124.0, 128.0, 133.0, 136.0, 140.0, 143.0, 147.0, 152.0, 155.0, 159.0, 163.0, 166.0, 170.0, 175.0, 178.0, 182.0, 186.0, 189.0, 194.0, 198.0, 201.0, 205.0, 210.0, 214.0, 217.0, 221.0, 226.0, 230.0, 233.0, 237.0, 241.0, 246.0, 250.0, 253.0]), (255 * numpy.ones(183))))
lasco_c2_g = numpy.concatenate((np.zeros(52).astype('int'), numpy.array([1.0, 5.0, 11.0, 17.0, 20.0, 26.0, 32.0, 35.0, 41.0, 47.0, 52.0, 56.0, 62.0, 68.0, 73.0, 77.0, 83.0, 88.0, 94.0, 100.0, 103.0, 109.0, 115.0, 120.0, 126.0, 130.0, 136.0, 141.0, 147.0, 153.0, 158.0, 164.0, 168.0, 173.0, 179.0, 185.0, 190.0, 196.0, 202.0, 207.0, 213.0, 217.0, 222.0, 228.0, 234.0, 239.0, 245.0, 251.0]), (255 * numpy.ones(156))))
lasco_c2_b = numpy.concatenate((np.zeros(78).astype('int'), numpy.array([0.0, 7.0, 19.0, 31.0, 43.0, 54.0, 66.0, 74.0, 86.0, 98.0, 109.0, 121.0, 133.0, 145.0, 156.0, 168.0, 176.0, 188.0, 200.0, 211.0, 223.0, 235.0, 247.0]), (255 * numpy.ones(156))))
lasco_c3_r = numpy.concatenate((np.zeros(77).astype('int'), numpy.array([5.0, 13.0, 25.0, 33.0, 45.0, 53.0, 65.0, 73.0, 85.0, 94.0, 106.0, 114.0, 126.0, 134.0, 146.0, 154.0, 166.0, 175.0, 187.0, 195.0, 207.0, 215.0, 227.0, 235.0, 247.0]), (255 * numpy.ones(154))))
lasco_c3_g = numpy.concatenate((np.zeros(39).astype('int'), numpy.array([4.0, 7.0, 12.0, 15.0, 20.0, 23.0, 28.0, 31.0, 36.0, 39.0, 44.0, 47.0, 52.0, 55.0, 60.0, 63.0, 68.0, 71.0, 76.0, 79.0, 84.0, 87.0, 92.0, 95.0, 100.0, 103.0, 108.0, 111.0, 116.0, 119.0, 124.0, 127.0, 132.0, 135.0, 140.0, 143.0, 148.0, 151.0, 156.0, 159.0, 164.0, 167.0, 172.0, 175.0, 180.0, 183.0, 188.0, 191.0, 196.0, 199.0, 204.0, 207.0, 212.0, 215.0, 220.0, 223.0, 228.0, 231.0, 236.0, 239.0, 244.0, 247.0, 252.0, 255.0, 255.0]), (255 * numpy.ones(154))))
lasco_c3_b = numpy.concatenate((numpy.array([0.0, 4.0, 6.0, 10.0, 13.0, 17.0, 20.0, 24.0, 27.0, 31.0, 33.0, 37.0, 40.0, 44.0, 47.0, 51.0, 54.0, 58.0, 61.0, 65.0, 67.0, 71.0, 74.0, 78.0, 81.0, 85.0, 88.0, 92.0, 94.0, 99.0, 101.0, 105.0, 108.0, 112.0, 115.0, 119.0, 122.0, 126.0, 128.0, 132.0, 135.0, 139.0, 142.0, 146.0, 149.0, 153.0, 155.0, 160.0, 162.0, 166.0, 169.0, 173.0, 176.0, 180.0, 183.0, 187.0, 189.0, 193.0, 196.0, 200.0, 203.0, 207.0, 210.0, 214.0, 217.0, 221.0, 223.0, 227.0, 230.0, 234.0, 237.0, 241.0, 244.0, 248.0, 250.0]), (255 * numpy.ones(181))))

def lasco_color_table(number):
    'Returns one of the fundamental color tables for SOHO LASCO images.'
    try:
        (r, g, b) = {2: (lasco_c2_r, lasco_c2_g, lasco_c2_b), 3: (lasco_c3_r, lasco_c3_g, lasco_c3_b)}[number]
    except KeyError:
        raise ValueError('Invalid LASCO number. Valid values are 2, 3.')
    cdict = create_cdict(r, g, b)
    return matplotlib.colors.LinearSegmentedColormap('SOHO LASCO C{:s}'.format(str(number)), cdict)
sxt_gold_r = numpy.concatenate((numpy.linspace(0, 255, num=185, endpoint=False), (255 * numpy.ones(71))))
sxt_gold_g = ((255 * (numpy.arange(256) ** 1.25)) / (255.0 ** 1.25))
sxt_gold_b = numpy.concatenate((numpy.zeros(185), ((255.0 * numpy.arange(71)) / 71.0)))
tempResult = arange(256)
	
===================================================================	
iris_sji_color_table: 156	
----------------------------	

'Return the standard color table for IRIS SJI files'
tempResult = arange(0, 256)
	
===================================================================	
iris_sji_color_table: 161	
----------------------------	

'Return the standard color table for IRIS SJI files'
c0 = numpy.arange(0, 256)
c1 = (np.sqrt(c0) * np.sqrt(255)).astype(numpy.uint8)
c2 = ((c0 ** 2) / 255.0).astype(numpy.uint8)
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (np.max(c1) + (np.max(c2) / 2.0))).astype(numpy.uint8)
c4 = np.zeros(256).astype(numpy.uint8)
tempResult = arange(0, 206)
	
===================================================================	
iris_sji_color_table: 164	
----------------------------	

'Return the standard color table for IRIS SJI files'
c0 = numpy.arange(0, 256)
c1 = (np.sqrt(c0) * np.sqrt(255)).astype(numpy.uint8)
c2 = ((c0 ** 2) / 255.0).astype(numpy.uint8)
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (np.max(c1) + (np.max(c2) / 2.0))).astype(numpy.uint8)
c4 = np.zeros(256).astype(numpy.uint8)
c4[50:256] = ((1 / 165.0) * (np.arange(0, 206) ** 2)).astype(numpy.uint8)
c5 = (((1 + c1) + c3.astype(np.uint)) / 2.0).astype(numpy.uint8)
rr = (numpy.ones(256, dtype=numpy.uint8) * 255)
tempResult = arange(0, 176)
	
===================================================================	
iris_sji_color_table: 166	
----------------------------	

'Return the standard color table for IRIS SJI files'
c0 = numpy.arange(0, 256)
c1 = (np.sqrt(c0) * np.sqrt(255)).astype(numpy.uint8)
c2 = ((c0 ** 2) / 255.0).astype(numpy.uint8)
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (np.max(c1) + (np.max(c2) / 2.0))).astype(numpy.uint8)
c4 = np.zeros(256).astype(numpy.uint8)
c4[50:256] = ((1 / 165.0) * (np.arange(0, 206) ** 2)).astype(numpy.uint8)
c5 = (((1 + c1) + c3.astype(np.uint)) / 2.0).astype(numpy.uint8)
rr = (numpy.ones(256, dtype=numpy.uint8) * 255)
rr[0:176] = ((numpy.arange(0, 176) / 175.0) * 255.0)
gg = numpy.zeros(256, dtype=numpy.uint8)
tempResult = arange(0, 156)
	
===================================================================	
iris_sji_color_table: 168	
----------------------------	

'Return the standard color table for IRIS SJI files'
c0 = numpy.arange(0, 256)
c1 = (np.sqrt(c0) * np.sqrt(255)).astype(numpy.uint8)
c2 = ((c0 ** 2) / 255.0).astype(numpy.uint8)
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (np.max(c1) + (np.max(c2) / 2.0))).astype(numpy.uint8)
c4 = np.zeros(256).astype(numpy.uint8)
c4[50:256] = ((1 / 165.0) * (np.arange(0, 206) ** 2)).astype(numpy.uint8)
c5 = (((1 + c1) + c3.astype(np.uint)) / 2.0).astype(numpy.uint8)
rr = (numpy.ones(256, dtype=numpy.uint8) * 255)
rr[0:176] = ((numpy.arange(0, 176) / 175.0) * 255.0)
gg = numpy.zeros(256, dtype=numpy.uint8)
gg[100:256] = ((numpy.arange(0, 156) / 155.0) * 255.0)
bb = numpy.zeros(256, dtype=numpy.uint8)
tempResult = arange(0, 106)
	
===================================================================	
iris_sji_color_table: 170	
----------------------------	

'Return the standard color table for IRIS SJI files'
c0 = numpy.arange(0, 256)
c1 = (np.sqrt(c0) * np.sqrt(255)).astype(numpy.uint8)
c2 = ((c0 ** 2) / 255.0).astype(numpy.uint8)
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (np.max(c1) + (np.max(c2) / 2.0))).astype(numpy.uint8)
c4 = np.zeros(256).astype(numpy.uint8)
c4[50:256] = ((1 / 165.0) * (np.arange(0, 206) ** 2)).astype(numpy.uint8)
c5 = (((1 + c1) + c3.astype(np.uint)) / 2.0).astype(numpy.uint8)
rr = (numpy.ones(256, dtype=numpy.uint8) * 255)
rr[0:176] = ((numpy.arange(0, 176) / 175.0) * 255.0)
gg = numpy.zeros(256, dtype=numpy.uint8)
gg[100:256] = ((numpy.arange(0, 156) / 155.0) * 255.0)
bb = numpy.zeros(256, dtype=numpy.uint8)
bb[150:256] = ((numpy.arange(0, 106) / 105.0) * 255.0)
agg = numpy.zeros(256, dtype=numpy.uint8)
tempResult = arange(0, 136)
	
===================================================================	
iris_sji_color_table: 172	
----------------------------	

'Return the standard color table for IRIS SJI files'
c0 = numpy.arange(0, 256)
c1 = (np.sqrt(c0) * np.sqrt(255)).astype(numpy.uint8)
c2 = ((c0 ** 2) / 255.0).astype(numpy.uint8)
c3 = (((c1 + (c2 / 2.0)) * 255.0) / (np.max(c1) + (np.max(c2) / 2.0))).astype(numpy.uint8)
c4 = np.zeros(256).astype(numpy.uint8)
c4[50:256] = ((1 / 165.0) * (np.arange(0, 206) ** 2)).astype(numpy.uint8)
c5 = (((1 + c1) + c3.astype(np.uint)) / 2.0).astype(numpy.uint8)
rr = (numpy.ones(256, dtype=numpy.uint8) * 255)
rr[0:176] = ((numpy.arange(0, 176) / 175.0) * 255.0)
gg = numpy.zeros(256, dtype=numpy.uint8)
gg[100:256] = ((numpy.arange(0, 156) / 155.0) * 255.0)
bb = numpy.zeros(256, dtype=numpy.uint8)
bb[150:256] = ((numpy.arange(0, 106) / 105.0) * 255.0)
agg = numpy.zeros(256, dtype=numpy.uint8)
agg[120:256] = ((numpy.arange(0, 136) / 135.0) * 255.0)
abb = numpy.zeros(256, dtype=numpy.uint8)
tempResult = arange(0, 66)
	
===================================================================	
_resample_nearest_linear: 33	
----------------------------	

'Resample Map using either linear or nearest interpolation.'
dimlist = []
for i in range(orig.ndim):
    tempResult = arange(dimensions[i])
	
===================================================================	
_resample_nearest_linear: 35	
----------------------------	

'Resample Map using either linear or nearest interpolation.'
dimlist = []
for i in range(orig.ndim):
    base = numpy.arange(dimensions[i])
    dimlist.append(((((orig.shape[i] - m1) / (dimensions[i] - m1)) * (base + offset)) - offset))
tempResult = arange(i, dtype=numpy.float)
	
===================================================================	
_remove_lytaf_events: 71	
----------------------------	

'\n    Removes periods of LYRA artifacts from a time series.\n\n    This functions removes periods corresponding to certain artifacts recorded\n    in the LYRA annotation file from an array of times given by the time input.\n    If a list of arrays of other properties is supplied through the channels\n    kwarg, then the relevant values from these arrays are also removed.  This\n    is done by assuming that each element in each array supplied corresponds to\n    the time in the same index in time array.  The artifacts to be removed are\n    given via the artifacts kwarg.  The default is "all", meaning that all\n    artifacts will be removed.  However, a subset of artifacts can be removed\n    by supplying a list of strings of the desired artifact types.\n\n    Parameters\n    ----------\n    time : `numpy.ndarray` of `datetime.datetime`\n        Gives the times of the timeseries.\n\n    channels : `list` of `numpy.array` convertible to float64.\n        Contains arrays of the irradiances taken at the times in the time\n        variable.  Each element in the list must have the same number of\n        elements as time.\n\n    artifacts : `list` of strings\n        Contain the artifact types to be removed.  For list of artifact types\n        see reference [1].  For example, if user wants to remove only large\n        angle rotations, listed at reference [1] as LAR, let artifacts=["LAR"].\n        Default=[], i.e. no artifacts will be removed.\n\n    return_artifacts : `bool`\n        Set to True to return a numpy recarray containing the start time, end\n        time and type of all artifacts removed.\n        Default=False\n\n    fitsfile : `str`\n        file name (including file path and suffix, .fits) of output fits file\n        which is generated if this kwarg is not None.\n        Default=None, i.e. no fits file is output.\n\n    csvfile : `str`\n        file name (including file path and suffix, .csv) of output csv file\n        which is generated if this kwarg is not None.\n        Default=None, i.e. no csv file is output.\n\n    filecolumns : `list` of strings\n        Gives names of columns of any output files produced.  Although\n        initially set to None above, the default is in fact\n        ["time", "channel0", "channel1",..."channelN"]\n        where N is the number of irradiance arrays in the channels input\n        (assuming 0-indexed counting).\n\n    lytaf_path : `str`\n        directory path where the LYRA annotation files are stored.\n\n    force_use_local_lytaf : `bool`\n        Ensures current local version of lytaf files are not replaced by\n        up-to-date online versions even if current local lytaf files do not\n        cover entire input time range etc.\n        Default=False\n\n    Returns\n    -------\n    clean_time : `numpy.ndarray` of `datetime.datetime`\n        time array with artifact periods removed.\n\n    clean_channels : `list` ndarrays/array-likes convertible to float64\n        list of irradiance arrays with artifact periods removed.\n\n    artifact_status : `dict`\n        List of 4 variables containing information on what artifacts were\n        found, removed, etc. from the time series.\n        artifact_status["lytaf"] = artifacts found : `numpy.recarray`\n            The full LYRA annotation file for the time series time range\n            output by get_lytaf_events().\n        artifact_status["removed"] = artifacts removed : `numpy.recarray`\n            Artifacts which were found and removed from from time series.\n        artifact_status["not_removed"] = artifacts found but not removed :\n              `numpy.recarray`\n            Artifacts which were found but not removed as they were not\n            included when user defined artifacts kwarg.\n        artifact_status["not_found"] = artifacts not found : `list` of strings\n            Artifacts listed to be removed by user when defining artifacts\n            kwarg which were not found in time series time range.\n\n    References\n    ----------\n    [1] http://proba2.oma.be/data/TARDIS\n\n    Example\n    -------\n    Sample data for example\n        >>> time = np.array([datetime(2013, 2, 1)+timedelta(minutes=i)\n                             for i in range(120)])\n        >>> channel_1 = np.zeros(len(TIME))+0.4\n        >>> channel_2 = np.zeros(len(TIME))+0.1\n    Remove LARs (Large Angle Rotations) from time series.\n        >>> time_clean, channels_clean = remove_lyra_artifacts(\n              time, channels=[channel_1, channel2], artifacts=[\'LAR\'])\n\n    '
if (not lytaf_path):
    lytaf_path = get_and_create_download_dir()
if (channels and (type(channels) is not list)):
    raise TypeError("channels must be None or a list of numpy arrays of dtype 'float64'.")
if (not artifacts):
    raise ValueError('User has supplied no artifacts to remove.')
if (type(artifacts) is str):
    artifacts = [artifacts]
if (not all((isinstance(artifact_type, str) for artifact_type in artifacts))):
    raise TypeError('All elements in artifacts must in strings.')
all_lytaf_event_types = get_lytaf_event_types(lytaf_path=lytaf_path, print_event_types=False)
for artifact in artifacts:
    if (artifact not in all_lytaf_event_types):
        print(all_lytaf_event_types)
        raise ValueError('{0} is not a valid artifact type. See above.'.format(artifact))
clean_time = numpy.array([parse_time(t) for t in time])
clean_channels = copy.deepcopy(channels)
artifacts_not_found = []
lytaf = get_lytaf_events(time[0], time[(- 1)], lytaf_path=lytaf_path, force_use_local_lytaf=force_use_local_lytaf)
artifact_indices = numpy.empty(0, dtype='int64')
for artifact_type in artifacts:
    indices = numpy.where((lytaf['event_type'] == artifact_type))[0]
    if (len(indices) == 0):
        artifacts_not_found.append(artifact_type)
    else:
        artifact_indices = numpy.concatenate((artifact_indices, indices))
artifact_indices.sort()
if (not len(artifact_indices)):
    warn('None of user supplied artifacts were found.')
    artifacts_not_found = artifacts
else:
    bad_indices = numpy.empty(0, dtype='int64')
    tempResult = arange(len(time))
	
===================================================================	
uncompress_countrate: 119	
----------------------------	

'Convert the compressed count rate inside of observing summary file from\n    a compressed byte to a true count rate\n\n    Parameters\n    ----------\n    compressed_countrate : byte array\n        A compressed count rate returned from an observing summary file.\n\n    References\n    ----------\n    Hsi_obs_summ_decompress.pro `<http://hesperia.gsfc.nasa.gov/ssw/hessi/idl/qlook_archive/hsi_obs_summ_decompress.pro>`_\n    '
if ((compressed_countrate.min() < 0) or (compressed_countrate.max() > 255)):
    raise ValueError('Exepected uncompressed counts {} to in range 0-255'.format(compressed_countrate))
tempResult = arange(0, 16, 1)
	
===================================================================	
parse_obssumm_hdulist: 111	
----------------------------	

'\n    Parse a RHESSI observation summary file.\n\n    Parameters\n    ----------\n    hdulist : list\n        The HDU list from the fits file.\n\n    Returns\n    -------\n    out : `dict`\n        Returns a dictionary.\n\n    '
header = hdulist[0].header
reference_time_ut = parse_time(hdulist[5].data.field('UT_REF')[0])
time_interval_sec = hdulist[5].data.field('TIME_INTV')[0]
labels = ['3 - 6 keV', '6 - 12 keV', '12 - 25 keV', '25 - 50 keV', '50 - 100 keV', '100 - 300 keV', '300 - 800 keV', '800 - 7000 keV', '7000 - 20000 keV']
compressed_countrate = numpy.array(hdulist[6].data.field('countrate'))
countrate = uncompress_countrate(compressed_countrate)
dim = np.array(countrate[:, 0]).size
tempResult = arange(dim)
	
===================================================================	
parse_obssumm_file: 98	
----------------------------	

"\n    Parse a RHESSI observation summary file.\n    Note: this is for the Lightcurve datatype only, the TimSeries uses the\n    parse_obssumm_hdulist(hdulist) method to enable implicit source detection.\n\n    Parameters\n    ----------\n    filename : str\n        The filename of a RHESSI fits file.\n\n    Returns\n    -------\n    out : `dict`\n        Returns a dictionary.\n\n    Examples\n    --------\n    >>> import sunpy.instr.rhessi as rhessi\n    >>> f = rhessi.get_obssumm_file(('2011/04/04', '2011/04/05'))   # doctest: +SKIP\n    >>> data = rhessi.parse_obssumm_file(f[0])   # doctest: +SKIP\n\n    "
afits = astropy.io.fits.open(filename)
header = afits[0].header
reference_time_ut = parse_time(afits[5].data.field('UT_REF')[0])
time_interval_sec = afits[5].data.field('TIME_INTV')[0]
labels = ['3 - 6 keV', '6 - 12 keV', '12 - 25 keV', '25 - 50 keV', '50 - 100 keV', '100 - 300 keV', '300 - 800 keV', '800 - 7000 keV', '7000 - 20000 keV']
compressed_countrate = numpy.array(afits[6].data.field('countrate'))
countrate = uncompress_countrate(compressed_countrate)
dim = np.array(countrate[:, 0]).size
tempResult = arange(dim)
	
===================================================================	
backprojection: 165	
----------------------------	

'\n    Given a stacked calibrated event list fits file create a back\n    projection image.\n\n    .. warning:: The image is not in the right orientation!\n\n    Parameters\n    ----------\n    calibrated_event_list : string\n        filename of a RHESSI calibrated event list\n    pixel_size : `~astropy.units.Quantity` instance\n        the size of the pixels in arcseconds. Default is (1,1).\n    image_dim : `~astropy.units.Quantity` instance\n        the size of the output image in number of pixels\n\n    Returns\n    -------\n    out : RHESSImap\n        Return a backprojection map.\n\n    Examples\n    --------\n    >>> import sunpy.data\n    >>> import sunpy.data.sample\n    >>> import sunpy.instr.rhessi as rhessi\n    >>> sunpy.data.download_sample_data(overwrite=False)   # doctest: +SKIP\n    >>> map = rhessi.backprojection(sunpy.data.sample.RHESSI_EVENT_LIST)   # doctest: +SKIP\n    >>> map.peek()   # doctest: +SKIP\n\n    '
pixel_size = pixel_size.to(astropy.units.arcsec)
image_dim = numpy.array(image_dim.to(u.pix).value, dtype=int)
afits = astropy.io.fits.open(calibrated_event_list)
info_parameters = afits[2]
xyoffset = info_parameters.data.field('USED_XYOFFSET')[0]
time_range = TimeRange(info_parameters.data.field('ABSOLUTE_TIME_RANGE')[0])
image = numpy.zeros(image_dim)
det_index_mask = afits[1].data.field('det_index_mask')[0]
tempResult = arange(9)
	
===================================================================	
_backproject: 144	
----------------------------	

'\n    Given a stacked calibrated event list fits file create a back\n    projection image for an individual detectors. This function is used by\n    backprojection.\n\n    Parameters\n    ----------\n    calibrated_event_list : string\n        filename of a RHESSI calibrated event list\n    detector : int\n        the detector number\n    pixel_size : 2-tuple\n        the size of the pixels in arcseconds. Default is (1,1).\n    image_dim : 2-tuple\n        the size of the output image in number of pixels\n\n    Returns\n    -------\n    out : ndarray\n        Return a backprojection image.\n\n    Examples\n    --------\n    >>> import sunpy.instr.rhessi as rhessi\n\n    '
afits = astropy.io.fits.open(calibrated_event_list)
fits_detector_index = (detector + 2)
detector_index = (detector - 1)
grid_angle = ((numpy.pi / 2.0) - grid_orientation[detector_index])
harm_ang_pitch = (grid_pitch[detector_index] / 1)
phase_map_center = afits[fits_detector_index].data.field('phase_map_ctr')
this_roll_angle = afits[fits_detector_index].data.field('roll_angle')
modamp = afits[fits_detector_index].data.field('modamp')
grid_transmission = afits[fits_detector_index].data.field('gridtran')
count = afits[fits_detector_index].data.field('count')
tempResult = arange((image_dim[0] * image_dim[1]))
	
===================================================================	
test_flux_to_classletter: 279	
----------------------------	

'Test converting fluxes into a class letter'
tempResult = arange(9, 2.0, (- 1))
	
===================================================================	
test_remove_lytaf_events_1: 80	
----------------------------	

'Test _remove_lytaf_events() with some artifacts found and others not.'
(time_test, channels_test, artifacts_status_test) = sunpy.instr.lyra._remove_lytaf_events(TIME, channels=CHANNELS, artifacts=['LAR', 'Offpoint'], return_artifacts=True, lytaf_path=TEST_DATA_PATH, force_use_local_lytaf=True)
bad_indices = numpy.logical_and((TIME >= LYTAF_TEST['begin_time'][0]), (TIME <= LYTAF_TEST['end_time'][0]))
tempResult = arange(len(TIME))
	
===================================================================	
module: 9	
----------------------------	

'\nGeneral ANA Tests\n'
import tempfile
import numpy as np
import pytest
from sunpy.io import ana
from sunpy.tests.helpers import skip_ana
img_size = (456, 345)
tempResult = arange(numpy.product(img_size))
	
===================================================================	
TestFiletools.test_read_file_fits_gzip: 39	
----------------------------	

for fits_extension in ['.fts', '.fit', '.fits']:
    pair = sunpy.io.read_file(os.path.join(sunpy.data.test.rootdir, 'gzip_test{ext}.gz'.format(ext=fits_extension)))
    assert isinstance(pair, list)
    assert (len(pair) == 1)
    assert (len(pair[0]) == 2)
    assert isinstance(pair[0][0], numpy.ndarray)
    assert isinstance(pair[0][1], sunpy.io.header.FileHeader)
    tempResult = arange(32)
	
===================================================================	
test_array_elements_values: 22	
----------------------------	

tempResult = arange(3.0)
	
===================================================================	
test_array_elements_values: 23	
----------------------------	

numpy.testing.assert_allclose(TESTING['MYSTRUCTURE']['MYFARRAY'], numpy.arange(3.0))
tempResult = arange(6.0, step=2)
	
===================================================================	
test_array_elements_values: 26	
----------------------------	

numpy.testing.assert_allclose(TESTING['MYSTRUCTURE']['MYFARRAY'], numpy.arange(3.0))
numpy.testing.assert_allclose(TESTING['MYSTRUCTURE']['MYFARRAYD'][:, 0], numpy.arange(6.0, step=2))
assert (TESTING['MYSTRUCTURE']['MYDARRAYD'][(1, 2)] == 5.0)
assert (TESTING['MYSTRUCTURE']['NESTEDSTRUCT']['MYLARRAYD'][(3, 0, 1)] == 19)
tempResult = arange(12, 17, step=2)
	
===================================================================	
module: 14	
----------------------------	

'\nGeneric LightCurve Tests\n'
from __future__ import absolute_import
import numpy as np
import pandas
import pytest
import datetime
import sunpy
import sunpy.lightcurve
from sunpy.data.test import get_test_filepath
EVE_AVERAGES_CSV = get_test_filepath('EVE_He_II_304_averages.csv')
base = datetime.datetime.today()
dates = [(base + datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange((24 * 60))
	
===================================================================	
TestMap.test_patterns: 59	
----------------------------	

eitmap = sunpy.map.Map(a_fname)
assert isinstance(eitmap, sunpy.map.GenericMap)
maps = sunpy.map.Map(os.path.join(filepath, 'EIT'))
assert isinstance(maps, list)
assert [isinstance(amap, sunpy.map.GenericMap) for amap in maps]
maps = sunpy.map.Map(os.path.join(filepath, 'EIT', '*'))
assert isinstance(maps, list)
assert [isinstance(amap, sunpy.map.GenericMap) for amap in maps]
amap = sunpy.map.Map(maps[0])
assert isinstance(amap, sunpy.map.GenericMap)
maps = sunpy.map.Map(a_list_of_many)
assert isinstance(maps, list)
assert [isinstance(amap, sunpy.map.GenericMap) for amap in maps]
pair_map = sunpy.map.Map((amap.data, amap.meta))
assert isinstance(pair_map, sunpy.map.GenericMap)
pair_map = sunpy.map.Map(amap.data, amap.meta)
assert isinstance(pair_map, sunpy.map.GenericMap)
with astropy.io.fits.open(a_fname) as hdul:
    data = hdul[0].data
    header = hdul[0].header
pair_map = sunpy.map.Map((data, header))
assert isinstance(pair_map, sunpy.map.GenericMap)
pair_map = sunpy.map.Map(data, header)
assert isinstance(pair_map, sunpy.map.GenericMap)
tempResult = arange(0, 100)
	
===================================================================	
test_draw_contours_aia: 109	
----------------------------	

aia171_test_map.plot()
tempResult = arange(1, 100, 10)
	
===================================================================	
_warp_sun_coordinates: 44	
----------------------------	

'\n    Function that returns a new list of coordinates for each input coord.\n    This is an inverse function needed by the scikit-image `transform.warp`\n    function.\n\n    Parameters\n    ----------\n    xy : `numpy.ndarray`\n        Array from `transform.warp`\n    smap : `~sunpy.map`\n        Original map that we want to transform\n    dt : `~astropy.units.Quantity`\n        Desired interval to rotate the input map by solar differential rotation.\n\n    Returns\n    -------\n    xy2 : `~numpy.ndarray`\n        Array with the inverse transformation\n    '
rotated_time = (smap.date - datetime.timedelta(seconds=dt.to(u.s).value))
tempResult = arange(0, smap.dimensions.x.value)
	
===================================================================	
_warp_sun_coordinates: 45	
----------------------------	

'\n    Function that returns a new list of coordinates for each input coord.\n    This is an inverse function needed by the scikit-image `transform.warp`\n    function.\n\n    Parameters\n    ----------\n    xy : `numpy.ndarray`\n        Array from `transform.warp`\n    smap : `~sunpy.map`\n        Original map that we want to transform\n    dt : `~astropy.units.Quantity`\n        Desired interval to rotate the input map by solar differential rotation.\n\n    Returns\n    -------\n    xy2 : `~numpy.ndarray`\n        Array with the inverse transformation\n    '
rotated_time = (smap.date - datetime.timedelta(seconds=dt.to(u.s).value))
x = numpy.arange(0, smap.dimensions.x.value)
tempResult = arange(0, smap.dimensions.y.value)
	
===================================================================	
Spectrogram.plot: 250	
----------------------------	

'\n        Plot spectrogram onto figure.\n\n        Parameters\n        ----------\n        figure : `~matplotlib.Figure`\n            Figure to plot the spectrogram on. If None, new Figure is created.\n        overlays : list\n            List of overlays (functions that receive figure and axes and return\n            new ones) to be applied after drawing.\n        colorbar : bool\n            Flag that determines whether or not to draw a colorbar. If existing\n            figure is passed, it is attempted to overdraw old colorbar.\n        vmin : float\n            Clip intensities lower than vmin before drawing.\n        vmax : float\n            Clip intensities higher than vmax before drawing.\n        linear : bool\n            If set to True, "stretch" image to make frequency axis linear.\n        showz : bool\n            If set to True, the value of the pixel that is hovered with the\n            mouse is shown in the bottom right corner.\n        yres : int or None\n            To be used in combination with linear=True. If None, sample the\n            image with half the minimum frequency delta. Else, sample the\n            image to be at most yres pixels in vertical dimension. Defaults\n            to 1080 because that\'s a common screen size.\n        max_dist : float or None\n            If not None, mask elements that are further than max_dist away\n            from actual data points (ie, frequencies that actually have data\n            from the receiver and are not just nearest-neighbour interpolated).\n        '
if linear:
    delt = yres
    if (delt is not None):
        delt = max(((self.freq_axis[0] - self.freq_axis[(- 1)]) / (yres - 1)), (_min_delt(self.freq_axis) / 2.0))
        delt = float(delt)
    data = _LinearView(self.clip_values(vmin, vmax), delt)
    tempResult = arange(self.freq_axis[0], self.freq_axis[(- 1)], (- data.delt))
	
===================================================================	
_LinearView.__init__: 70	
----------------------------	

self.arr = arr
if (delt is None):
    delt = (_min_delt(arr.freq_axis) / 2.0)
self.delt = delt
midpoints = ((self.arr.freq_axis[:(- 1)] + self.arr.freq_axis[1:]) / 2)
self.midpoints = numpy.concatenate([midpoints, arr.freq_axis[(- 1):]])
self.max_mp_delt = numpy.min((self.midpoints[1:] - self.midpoints[:(- 1)]))
tempResult = arange(self.arr.freq_axis[0], self.arr.freq_axis[(- 1)], (- self.delt))
	
===================================================================	
test_homogenize_constant: 137	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
tempResult = arange(3600)
	
===================================================================	
test_homogenize_constant: 139	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
c1 = CallistoSpectrogram(a, numpy.arange(3600), numpy.array([1]), datetime(2011, 1, 1), datetime(2011, 1, 1, 1), 0, 1, 'Time', 'Frequency', 'Test', None, None, None, False)
b = (a + 10)
tempResult = arange(3600)
	
===================================================================	
test_homogenize_factor: 126	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
tempResult = arange(3600)
	
===================================================================	
test_homogenize_factor: 128	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
c1 = CallistoSpectrogram(a, numpy.arange(3600), numpy.array([1]), datetime(2011, 1, 1), datetime(2011, 1, 1, 1), 0, 1, 'Time', 'Frequency', 'Test', None, None, None, False)
b = (2 * a)
tempResult = arange(3600)
	
===================================================================	
test_homogenize_rightfq: 159	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
tempResult = arange(3600)
	
===================================================================	
test_homogenize_rightfq: 161	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
c1 = CallistoSpectrogram(a, numpy.arange(3600), numpy.array([1]), datetime(2011, 1, 1), datetime(2011, 1, 1, 1), 0, 1, 'Time', 'Frequency', 'Test', None, None, None, False)
b = ((2 * a) + 1)
tempResult = arange(3600)
	
===================================================================	
test_homogenize_rightfq: 161	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
c1 = CallistoSpectrogram(a, numpy.arange(3600), numpy.array([1]), datetime(2011, 1, 1), datetime(2011, 1, 1, 1), 0, 1, 'Time', 'Frequency', 'Test', None, None, None, False)
b = ((2 * a) + 1)
tempResult = arange(3600)
	
===================================================================	
test_homogenize_rightfq: 161	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
c1 = CallistoSpectrogram(a, numpy.arange(3600), numpy.array([1]), datetime(2011, 1, 1), datetime(2011, 1, 1, 1), 0, 1, 'Time', 'Frequency', 'Test', None, None, None, False)
b = ((2 * a) + 1)
tempResult = arange(3600)
	
===================================================================	
test_homogenize_both: 148	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
tempResult = arange(3600)
	
===================================================================	
test_homogenize_both: 150	
----------------------------	

a = numpy.float64(numpy.random.randint(0, 255, 3600))[numpy.newaxis, :]
c1 = CallistoSpectrogram(a, numpy.arange(3600), numpy.array([1]), datetime(2011, 1, 1), datetime(2011, 1, 1, 1), 0, 1, 'Time', 'Frequency', 'Test', None, None, None, False)
b = ((2 * a) + 1)
tempResult = arange(3600)
	
===================================================================	
test_flatten: 315	
----------------------------	

tempResult = arange((5 * 3600))
	
===================================================================	
test_parse_time_numpy_date: 44	
----------------------------	

tempResult = arange('2005-02', '2005-03', dtype='datetime64[D]')
	
===================================================================	
test_parse_time_numpy_datetime: 50	
----------------------------	

tempResult = arange('2005-02-01T00', '2005-02-01T10', dtype='datetime64')
	
===================================================================	
test_generic_construction_concatenation: 299	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
test_generic_construction_concatenation: 300	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
intensity1 = numpy.sin(numpy.arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60))))
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
generic_ts: 73	
----------------------------	

base = parse_time('2016/10/01T05:00:00')
dates = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
table_ts: 87	
----------------------------	

base = datetime.datetime.today()
times = Time([(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))])
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_generic_construction_ts_list: 182	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_generic_construction_ts_list: 183	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
intensity1 = numpy.sin(numpy.arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60))))
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_generic_construction_basic: 133	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_generic_construction_concatenation: 203	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_generic_construction_concatenation: 204	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
intensity1 = numpy.sin(numpy.arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60))))
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_generic_construction_basic_omitted_details: 149	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_meta_from_fits_header: 114	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_table_to_ts: 223	
----------------------------	

base = datetime.datetime.today()
times = Time([(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))])
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
TestTimeSeries.test_generic_construction_basic_different_meta_types: 167	
----------------------------	

base = datetime.datetime.today()
times = [(base - datetime.timedelta(minutes=x)) for x in range(0, (24 * 60))]
tempResult = arange(0, (12 * numpy.pi), ((12 * numpy.pi) / (24 * 60)))
	
===================================================================	
LineAnimator.__init__: 385	
----------------------------	

plot_axis_index = int(plot_axis_index)
if (plot_axis_index not in range((- 2), 2)):
    raise ValueError('plot_axis_index must be either 0 or 1 (or equivalent negative indices) referring to the axis of data to be used for a single plot.')
if (data.ndim < 2):
    raise ValueError('data must have at least two dimensions.  One for data for each single plot and at least one for time/iteration.')
if (axis_ranges[plot_axis_index] is not None):
    if (len(axis_ranges[plot_axis_index]) != data.shape[plot_axis_index]):
        raise ValueError('The plot_axis_index axis range must be specified as None or an array. Not [min, max]')
self.naxis = data.ndim
self.num_sliders = (self.naxis - 1)
if ((axis_ranges is not None) and all(((axis_range is None) for axis_range in axis_ranges))):
    axis_ranges = None
if (axis_ranges[plot_axis_index] is None):
    tempResult = arange(data.shape[plot_axis_index])
	
===================================================================	
convert_pixel_to_data: 30	
----------------------------	

'\n    Calculate the data coordinate for particular pixel indices.\n\n    Parameters\n    ----------\n    size : 2d ndarray\n        Number of pixels in width and height.\n    scale : 2d ndarray\n        The size of a pixel (dx,dy) in data coordinates (equivalent to WCS/CDELT)\n    reference_pixel : 2d ndarray\n        The reference pixel (x,y) at which the reference coordinate is given (equivalent to WCS/CRPIX)\n    reference_coordinate : 2d ndarray\n        The data coordinate (x, y) as measured at the reference pixel (equivalent to WCS/CRVAL)\n    x,y : int or ndarray\n        The pixel values at which data coordinates are requested. If none are given,\n        returns coordinates for every pixel.\n\n    Returns\n    -------\n    out : ndarray\n        The data coordinates at pixel (x,y).\n\n    Notes\n    -----\n    This function assumes a gnomic projection which is correct for a detector at the focus\n    of an optic observing the Sun.\n    '
cdelt = numpy.array(scale)
crpix = numpy.array(reference_pixel)
crval = numpy.array(reference_coordinate)
if ((x is None) and (y is None)):
    tempResult = arange(size[0])
	
===================================================================	
convert_pixel_to_data: 30	
----------------------------	

'\n    Calculate the data coordinate for particular pixel indices.\n\n    Parameters\n    ----------\n    size : 2d ndarray\n        Number of pixels in width and height.\n    scale : 2d ndarray\n        The size of a pixel (dx,dy) in data coordinates (equivalent to WCS/CDELT)\n    reference_pixel : 2d ndarray\n        The reference pixel (x,y) at which the reference coordinate is given (equivalent to WCS/CRPIX)\n    reference_coordinate : 2d ndarray\n        The data coordinate (x, y) as measured at the reference pixel (equivalent to WCS/CRVAL)\n    x,y : int or ndarray\n        The pixel values at which data coordinates are requested. If none are given,\n        returns coordinates for every pixel.\n\n    Returns\n    -------\n    out : ndarray\n        The data coordinates at pixel (x,y).\n\n    Notes\n    -----\n    This function assumes a gnomic projection which is correct for a detector at the focus\n    of an optic observing the Sun.\n    '
cdelt = numpy.array(scale)
crpix = numpy.array(reference_pixel)
crval = numpy.array(reference_coordinate)
if ((x is None) and (y is None)):
    tempResult = arange(size[1])
	
***************************************************	
spacetelescope_synphot-0.1: 40	
===================================================================	
BaseGaussian1D.sampleset: 206	
----------------------------	

'Return ``x`` array that samples the feature.\n\n        Parameters\n        ----------\n        factor_step : float\n            Factor for sample step calculation. The step is calculated\n            using ``factor_step * self.stddev``.\n\n        kwargs : dict\n            Keyword(s) for ``bounding_box`` calculation.\n            Default ``factor`` is set to 5 to be compatible with\n            ASTROLIB PYSYNPHOT.\n\n        '
if ('factor' not in kwargs):
    kwargs['factor'] = 5.0
(w1, w2) = self.bounding_box(**kwargs)
dw = (factor_step * self.stddev)
if (self._n_models == 1):
    tempResult = arange(w1, w2, dw)
	
===================================================================	
MexicanHat1D.sampleset: 268	
----------------------------	

'Return ``x`` array that samples the feature.\n\n        Parameters\n        ----------\n        factor_step : float\n            Factor for sample step calculation. The step is calculated\n            using ``factor_step * self.sigma``.\n\n        kwargs : dict\n            Keyword(s) for ``bounding_box`` calculation.\n\n        '
(w1, w2) = self.bounding_box(**kwargs)
dw = (factor_step * self.sigma)
if (self._n_models == 1):
    tempResult = arange(w1, w2, dw)
	
===================================================================	
Lorentz1D.sampleset: 255	
----------------------------	

'Return ``x`` array that samples the feature.\n\n        Parameters\n        ----------\n        factor_step : float\n            Factor for sample step calculation. The step is calculated\n            using ``factor_step * self.fwhm``.\n\n        kwargs : dict\n            Keyword(s) for ``bounding_box`` calculation.\n\n        '
(w1, w2) = self.bounding_box(**kwargs)
dw = (factor_step * self.fwhm)
if (self._n_models == 1):
    tempResult = arange(w1, w2, dw)
	
===================================================================	
Box1D._calc_sampleset: 87	
----------------------------	

'Calculate sampleset for each model.'
if minimal:
    arr = [(w1 - step), w1, w2, (w2 + step)]
else:
    tempResult = arange((w1 - step), ((w2 + step) + step), step)
	
===================================================================	
generate_wavelengths: 65	
----------------------------	

'Generate wavelength array to be used for spectrum sampling.\n\n    .. math::\n\n        minwave \\le \\lambda < maxwave\n\n    Parameters\n    ----------\n    minwave, maxwave : float\n        Lower and upper limits of the wavelengths.\n        These must be values in linear space regardless of ``log``.\n\n    num : int\n        The number of wavelength values.\n        This is only used when ``delta=None``.\n\n    delta : float or `None`\n        Delta between wavelength values.\n        When ``log=True``, this is the spacing in log space.\n\n    log : bool\n        If `True`, the wavelength values are evenly spaced in log scale.\n        Otherwise, spacing is linear.\n\n    wave_unit : str or `~astropy.units.core.Unit`\n        Wavelength unit. Default is Angstrom.\n\n    Returns\n    -------\n    waveset : `~astropy.units.quantity.Quantity`\n        Generated wavelength set.\n\n    waveset_str : str\n        Info string associated with the result.\n\n    '
wave_unit = units.validate_unit(wave_unit)
if (delta is not None):
    num = None
waveset_str = 'Min: {0}, Max: {1}, Num: {2}, Delta: {3}, Log: {4}'.format(minwave, maxwave, num, delta, log)
if log:
    logmin = numpy.log10(minwave)
    logmax = numpy.log10(maxwave)
    if (delta is None):
        waveset = numpy.logspace(logmin, logmax, num, endpoint=False)
    else:
        tempResult = arange(logmin, logmax, delta)
	
===================================================================	
generate_wavelengths: 69	
----------------------------	

'Generate wavelength array to be used for spectrum sampling.\n\n    .. math::\n\n        minwave \\le \\lambda < maxwave\n\n    Parameters\n    ----------\n    minwave, maxwave : float\n        Lower and upper limits of the wavelengths.\n        These must be values in linear space regardless of ``log``.\n\n    num : int\n        The number of wavelength values.\n        This is only used when ``delta=None``.\n\n    delta : float or `None`\n        Delta between wavelength values.\n        When ``log=True``, this is the spacing in log space.\n\n    log : bool\n        If `True`, the wavelength values are evenly spaced in log scale.\n        Otherwise, spacing is linear.\n\n    wave_unit : str or `~astropy.units.core.Unit`\n        Wavelength unit. Default is Angstrom.\n\n    Returns\n    -------\n    waveset : `~astropy.units.quantity.Quantity`\n        Generated wavelength set.\n\n    waveset_str : str\n        Info string associated with the result.\n\n    '
wave_unit = units.validate_unit(wave_unit)
if (delta is not None):
    num = None
waveset_str = 'Min: {0}, Max: {1}, Num: {2}, Delta: {3}, Log: {4}'.format(minwave, maxwave, num, delta, log)
if log:
    logmin = numpy.log10(minwave)
    logmax = numpy.log10(maxwave)
    if (delta is None):
        waveset = numpy.logspace(logmin, logmax, num, endpoint=False)
    else:
        waveset = (10 ** numpy.arange(logmin, logmax, delta))
elif (delta is None):
    waveset = numpy.linspace(minwave, maxwave, num, endpoint=False)
else:
    tempResult = arange(minwave, maxwave, delta)
	
===================================================================	
test_calculate_bin_edges: 14	
----------------------------	

	
===================================================================	
test_calculate_bin_edges: 14	
----------------------------	

	
===================================================================	
test_calculate_bin_edges: 14	
----------------------------	

	
===================================================================	
TestConstFlux1D.setup_class: 64	
----------------------------	

tempResult = arange(1, 21000, 5000)
	
===================================================================	
TestPowerLawFlux1D.setup_class: 154	
----------------------------	

tempResult = arange(3000, 3100, 10)
	
===================================================================	
TestBlackBody1D.test_eval: 53	
----------------------------	

tempResult = arange(3000, 3100, 10)
	
===================================================================	
TestObservation.test_as_spectrum: 110	
----------------------------	

tempResult = arange(6000, 6005)
	
===================================================================	
TestMathOperators.setup_class: 143	
----------------------------	

sp = SourceSpectrum(ConstFlux1D, amplitude=1)
bp = SpectralElement(Box1D, amplitude=1, x_0=5000, width=100)
tempResult = arange(1000, 10000)
	
===================================================================	
TestInitWithForce.setup_class: 120	
----------------------------	

tempResult = arange(3000, 4000)
	
===================================================================	
TestObservation.setup_class: 36	
----------------------------	

sp = SourceSpectrum(ConstFlux1D, amplitude=(1 * units.FLAM), meta={'warnings': {'w1': 'spec warn', 'w2': 'foo'}})
bp = spectrum.SpectralElement.from_file(_bandfile)
bp.warnings = {'w1': 'band warn'}
tempResult = arange(1000, 11001, dtype=numpy.float64)
	
===================================================================	
TestCountRate.setup_class: 240	
----------------------------	

tempResult = arange(1000, 1100, 0.5)
	
===================================================================	
TestCountRate.setup_class: 244	
----------------------------	

x = numpy.arange(1000, 1100, 0.5)
y = units.convert_flux(x, ((x - 1000) * u.count), units.PHOTLAM, area=_area).value
sp = SourceSpectrum(Empirical1D, points=x, lookup_table=y, meta={'expr': 'slope1'})
bp = SpectralElement(Empirical1D, points=[1009.95, 1010, 1030, 1030.05], lookup_table=[0, 1, 1, 0], meta={'expr': 'handmade_box'})
tempResult = arange(1000, 1020)
	
===================================================================	
TestReadWriteFITS.test_exceptions: 75	
----------------------------	

'Test for appropriate exceptions.'
outfile = os.path.join(self.outdir, 'outspec3.fits')
with pytest.raises(exceptions.SynphotError):
    tempResult = arange(3, dtype=numpy.float64)
	
===================================================================	
TestReadWriteFITS.test_exceptions: 79	
----------------------------	

'Test for appropriate exceptions.'
outfile = os.path.join(self.outdir, 'outspec3.fits')
with pytest.raises(exceptions.SynphotError):
    specio.write_fits_spec(outfile, self.wave, numpy.arange(3, dtype=numpy.float64))
with pytest.raises(exceptions.SynphotError):
    specio.write_fits_spec(outfile, self.wave, self.flux, precision='foo', overwrite=True)
with pytest.raises(exceptions.SynphotError):
    tempResult = arange(6)
	
===================================================================	
TestReadWriteFITS.test_exceptions: 81	
----------------------------	

'Test for appropriate exceptions.'
outfile = os.path.join(self.outdir, 'outspec3.fits')
with pytest.raises(exceptions.SynphotError):
    specio.write_fits_spec(outfile, self.wave, numpy.arange(3, dtype=numpy.float64))
with pytest.raises(exceptions.SynphotError):
    specio.write_fits_spec(outfile, self.wave, self.flux, precision='foo', overwrite=True)
with pytest.raises(exceptions.SynphotError):
    specio.write_fits_spec(outfile, numpy.arange(6), self.flux, overwrite=True)
with pytest.raises(exceptions.SynphotError):
    tempResult = arange(6)
	
===================================================================	
TestPowerLawSource.test_eval: 321	
----------------------------	

tempResult = arange(3000, 3100, 10)
	
===================================================================	
TestBoxBandpass.test_taper: 260	
----------------------------	

tempResult = arange(499, 501.01, 0.01)
	
===================================================================	
TestBlackBodySource.test_eval: 276	
----------------------------	

tempResult = arange(3000, 3100, 10)
	
===================================================================	
test_genwave: 44	
----------------------------	

	
===================================================================	
test_genwave: 44	
----------------------------	

	
===================================================================	
test_validate_wavelengths: 29	
----------------------------	

'Test wavelengths validation.'
tempResult = arange(1, 11)
	
===================================================================	
test_validate_wavelengths: 36	
----------------------------	

'Test wavelengths validation.'
a = numpy.arange(1, 11)
utils.validate_wavelengths(a)
a = a[::(- 1)]
utils.validate_wavelengths((a * astropy.units.micron))
with pytest.raises(exceptions.SynphotError):
    utils.validate_wavelengths((1.0 * astropy.units.K))
with pytest.raises(exceptions.ZeroWavelength):
    tempResult = arange(10)
	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
===================================================================	
test_overlap_status: 10	
----------------------------	

	
***************************************************	
librosa_librosa-0.5.1: 38	
===================================================================	
__beat_local_score: 98	
----------------------------	

'Construct the local score for an onset envlope and given period'
tempResult = arange((- period), (period + 1))
	
===================================================================	
__beat_track_dp: 105	
----------------------------	

'Core dynamic program for beat tracking'
backlink = numpy.zeros_like(localscore, dtype=int)
cumscore = numpy.zeros_like(localscore)
tempResult = arange(((- 2) * period), ((- numpy.round((period / 2))) + 1), dtype=int)
	
===================================================================	
__coord_time: 290	
----------------------------	

'Get time coordinates from frames'
tempResult = arange((n + 1))
	
===================================================================	
waveplot: 129	
----------------------------	

"Plot the amplitude envelope of a waveform.\n\n    If `y` is monophonic, a filled curve is drawn between `[-abs(y), abs(y)]`.\n\n    If `y` is stereo, the curve is drawn between `[-abs(y[1]), abs(y[0])]`,\n    so that the left and right channels are drawn above and below the axis,\n    respectively.\n\n    Long signals (`duration >= max_points`) are down-sampled to at\n    most `max_sr` before plotting.\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,) or (2,n)]\n        audio time series (mono or stereo)\n\n    sr : number > 0 [scalar]\n        sampling rate of `y`\n\n    max_points : postive number or None\n        Maximum number of time-points to plot: if `max_points` exceeds\n        the duration of `y`, then `y` is downsampled.\n\n        If `None`, no downsampling is performed.\n\n    x_axis : str {'time', 'off', 'none'} or None\n        If 'time', the x-axis is given time tick-marks.\n\n    offset : float\n        Horizontal offset (in time) to start the waveform plot\n\n    max_sr : number > 0 [scalar]\n        Maximum sampling rate for the visualization\n\n    kwargs\n        Additional keyword arguments to `matplotlib.pyplot.fill_between`\n\n    Returns\n    -------\n    pc : matplotlib.collections.PolyCollection\n        The PolyCollection created by `fill_between`.\n\n    See also\n    --------\n    librosa.core.resample\n    matplotlib.pyplot.fill_between\n\n\n    Examples\n    --------\n    Plot a monophonic waveform\n\n    >>> import matplotlib.pyplot as plt\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> plt.figure()\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Monophonic')\n\n    Or a stereo waveform\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(),\n    ...                      mono=False, duration=10)\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.waveplot(y, sr=sr)\n    >>> plt.title('Stereo')\n\n    Or harmonic and percussive components with transparency\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=10)\n    >>> y_harm, y_perc = librosa.effects.hpss(y)\n    >>> plt.subplot(3, 1, 3)\n    >>> librosa.display.waveplot(y_harm, sr=sr, alpha=0.25)\n    >>> librosa.display.waveplot(y_perc, sr=sr, color='r', alpha=0.5)\n    >>> plt.title('Harmonic + Percussive')\n    >>> plt.tight_layout()\n    "
util.valid_audio(y, mono=False)
if (not (isinstance(max_sr, int) and (max_sr > 0))):
    raise ParameterError('max_sr must be a non-negative integer')
target_sr = sr
hop_length = 1
if (max_points is not None):
    if (max_points <= 0):
        raise ParameterError('max_points must be strictly positive')
    if (max_points < y.shape[(- 1)]):
        target_sr = min(max_sr, ((sr * y.shape[(- 1)]) // max_points))
    hop_length = (sr // target_sr)
    if (y.ndim == 1):
        y = __envelope(y, hop_length)
    else:
        y = numpy.vstack([__envelope(_, hop_length) for _ in y])
if (y.ndim > 1):
    y_top = y[0]
    y_bottom = (- y[1])
else:
    y_top = y
    y_bottom = (- y)
axes = matplotlib.pyplot.gca()
kwargs.setdefault('color', next(axes._get_lines.prop_cycler)['color'])
tempResult = arange(len(y_top))
	
===================================================================	
__decorate_axis: 214	
----------------------------	

'Configure axis tickers, locators, and labels'
if (ax_type == 'tonnetz'):
    axis.set_major_formatter(TonnetzFormatter())
    tempResult = arange(6)
	
===================================================================	
__decorate_axis: 218	
----------------------------	

'Configure axis tickers, locators, and labels'
if (ax_type == 'tonnetz'):
    axis.set_major_formatter(TonnetzFormatter())
    axis.set_major_locator(FixedLocator((0.5 + numpy.arange(6))))
    axis.set_label_text('Tonnetz')
elif (ax_type == 'chroma'):
    axis.set_major_formatter(ChromaFormatter())
    tempResult = arange(10)
	
===================================================================	
__decorate_axis: 236	
----------------------------	

'Configure axis tickers, locators, and labels'
if (ax_type == 'tonnetz'):
    axis.set_major_formatter(TonnetzFormatter())
    axis.set_major_locator(FixedLocator((0.5 + numpy.arange(6))))
    axis.set_label_text('Tonnetz')
elif (ax_type == 'chroma'):
    axis.set_major_formatter(ChromaFormatter())
    axis.set_major_locator(FixedLocator((0.5 + np.add.outer((12 * np.arange(10)), [0, 2, 4, 5, 7, 9, 11]).ravel())))
    axis.set_label_text('Pitch class')
elif (ax_type == 'tempo'):
    axis.set_major_formatter(ScalarFormatter())
    axis.set_major_locator(LogLocator(base=2.0))
    axis.set_label_text('BPM')
elif (ax_type == 'time'):
    axis.set_major_formatter(TimeFormatter(lag=False))
    axis.set_major_locator(MaxNLocator(prune=None, steps=[1, 1.5, 5, 6, 10]))
    axis.set_label_text('Time')
elif (ax_type == 'lag'):
    axis.set_major_formatter(TimeFormatter(lag=True))
    axis.set_major_locator(MaxNLocator(prune=None, steps=[1, 1.5, 5, 6, 10]))
    axis.set_label_text('Lag')
elif (ax_type == 'cqt_note'):
    axis.set_major_formatter(NoteFormatter())
    axis.set_major_locator(LogLocator(base=2.0))
    axis.set_minor_formatter(NoteFormatter(major=False))
    tempResult = arange(1, 12)
	
===================================================================	
__decorate_axis: 242	
----------------------------	

'Configure axis tickers, locators, and labels'
if (ax_type == 'tonnetz'):
    axis.set_major_formatter(TonnetzFormatter())
    axis.set_major_locator(FixedLocator((0.5 + numpy.arange(6))))
    axis.set_label_text('Tonnetz')
elif (ax_type == 'chroma'):
    axis.set_major_formatter(ChromaFormatter())
    axis.set_major_locator(FixedLocator((0.5 + np.add.outer((12 * np.arange(10)), [0, 2, 4, 5, 7, 9, 11]).ravel())))
    axis.set_label_text('Pitch class')
elif (ax_type == 'tempo'):
    axis.set_major_formatter(ScalarFormatter())
    axis.set_major_locator(LogLocator(base=2.0))
    axis.set_label_text('BPM')
elif (ax_type == 'time'):
    axis.set_major_formatter(TimeFormatter(lag=False))
    axis.set_major_locator(MaxNLocator(prune=None, steps=[1, 1.5, 5, 6, 10]))
    axis.set_label_text('Time')
elif (ax_type == 'lag'):
    axis.set_major_formatter(TimeFormatter(lag=True))
    axis.set_major_locator(MaxNLocator(prune=None, steps=[1, 1.5, 5, 6, 10]))
    axis.set_label_text('Lag')
elif (ax_type == 'cqt_note'):
    axis.set_major_formatter(NoteFormatter())
    axis.set_major_locator(LogLocator(base=2.0))
    axis.set_minor_formatter(NoteFormatter(major=False))
    axis.set_minor_locator(LogLocator(base=2.0, subs=(2.0 ** (numpy.arange(1, 12) / 12.0))))
    axis.set_label_text('Note')
elif (ax_type in ['cqt_hz']):
    axis.set_major_formatter(LogHzFormatter())
    axis.set_major_locator(LogLocator(base=2.0))
    axis.set_minor_formatter(LogHzFormatter(major=False))
    tempResult = arange(1, 12)
	
===================================================================	
__coord_n: 286	
----------------------------	

'Get bare positions'
tempResult = arange((n + 1))
	
===================================================================	
chroma: 58	
----------------------------	

"Create a Filterbank matrix to convert STFT to chroma\n\n\n    Parameters\n    ----------\n    sr        : number > 0 [scalar]\n        audio sampling rate\n\n    n_fft     : int > 0 [scalar]\n        number of FFT bins\n\n    n_chroma  : int > 0 [scalar]\n        number of chroma bins\n\n    A440      : float > 0 [scalar]\n        Reference frequency for A440\n\n    ctroct    : float > 0 [scalar]\n\n    octwidth  : float > 0 or None [scalar]\n        `ctroct` and `octwidth` specify a dominance window -\n        a Gaussian weighting centered on `ctroct` (in octs, A0 = 27.5Hz)\n        and with a gaussian half-width of `octwidth`.\n        Set `octwidth` to `None` to use a flat weighting.\n\n    norm : float > 0 or np.inf\n        Normalization factor for each filter\n\n    base_c : bool\n        If True, the filter bank will start at 'C'.\n        If False, the filter bank will start at 'A'.\n\n    Returns\n    -------\n    wts : ndarray [shape=(n_chroma, 1 + n_fft / 2)]\n        Chroma filter matrix\n\n    See Also\n    --------\n    util.normalize\n    feature.chroma_stft\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    Examples\n    --------\n    Build a simple chroma filter bank\n\n    >>> chromafb = librosa.filters.chroma(22050, 4096)\n    array([[  1.689e-05,   3.024e-04, ...,   4.639e-17,   5.327e-17],\n           [  1.716e-05,   2.652e-04, ...,   2.674e-25,   3.176e-25],\n    ...,\n           [  1.578e-05,   3.619e-04, ...,   8.577e-06,   9.205e-06],\n           [  1.643e-05,   3.355e-04, ...,   1.474e-10,   1.636e-10]])\n\n    Use quarter-tones instead of semitones\n\n    >>> librosa.filters.chroma(22050, 4096, n_chroma=24)\n    array([[  1.194e-05,   2.138e-04, ...,   6.297e-64,   1.115e-63],\n           [  1.206e-05,   2.009e-04, ...,   1.546e-79,   2.929e-79],\n    ...,\n           [  1.162e-05,   2.372e-04, ...,   6.417e-38,   9.923e-38],\n           [  1.180e-05,   2.260e-04, ...,   4.697e-50,   7.772e-50]])\n\n\n    Equally weight all octaves\n\n    >>> librosa.filters.chroma(22050, 4096, octwidth=None)\n    array([[  3.036e-01,   2.604e-01, ...,   2.445e-16,   2.809e-16],\n           [  3.084e-01,   2.283e-01, ...,   1.409e-24,   1.675e-24],\n    ...,\n           [  2.836e-01,   3.116e-01, ...,   4.520e-05,   4.854e-05],\n           [  2.953e-01,   2.888e-01, ...,   7.768e-10,   8.629e-10]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> librosa.display.specshow(chromafb, x_axis='linear')\n    >>> plt.ylabel('Chroma filter')\n    >>> plt.title('Chroma filter bank')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n    "
wts = numpy.zeros((n_chroma, n_fft))
frequencies = numpy.linspace(0, sr, n_fft, endpoint=False)[1:]
frqbins = (n_chroma * hz_to_octs(frequencies, A440))
frqbins = numpy.concatenate(([(frqbins[0] - (1.5 * n_chroma))], frqbins))
binwidthbins = numpy.concatenate((numpy.maximum((frqbins[1:] - frqbins[:(- 1)]), 1.0), [1]))
tempResult = arange(0, n_chroma, dtype='d')
	
===================================================================	
constant_q: 94	
----------------------------	

'Construct a constant-Q basis.\n\n    This uses the filter bank described by [1]_.\n\n    .. [1] McVicar, Matthew.\n            "A machine learning approach to automatic chord extraction."\n            Dissertation, University of Bristol. 2013.\n\n\n    Parameters\n    ----------\n    sr : number > 0 [scalar]\n        Audio sampling rate\n\n    fmin : float > 0 [scalar]\n        Minimum frequency bin. Defaults to `C1 ~= 32.70`\n\n    n_bins : int > 0 [scalar]\n        Number of frequencies.  Defaults to 7 octaves (84 bins).\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)` [scalar]\n        Tuning deviation from A440 in fractions of a bin\n\n    window : string, tuple, number, or function\n        Windowing function to apply to filters.\n\n    filter_scale : float > 0 [scalar]\n        Scale of filter windows.\n        Small values (<1) use shorter windows for higher temporal resolution.\n\n    pad_fft : boolean\n        Center-pad all filters up to the nearest integral power of 2.\n\n        By default, padding is done with zeros, but this can be overridden\n        by setting the `mode=` field in *kwargs*.\n\n    norm : {inf, -inf, 0, float > 0}\n        Type of norm to use for basis function normalization.\n        See librosa.util.normalize\n\n    kwargs : additional keyword arguments\n        Arguments to `np.pad()` when `pad==True`.\n\n    Returns\n    -------\n    filters : np.ndarray, `len(filters) == n_bins`\n        `filters[i]` is `i`\\ th time-domain CQT basis filter\n\n    lengths : np.ndarray, `len(lengths) == n_bins`\n        The (fractional) length of each filter\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    See Also\n    --------\n    constant_q_lengths\n    librosa.core.cqt\n    librosa.util.normalize\n\n\n    Examples\n    --------\n    Use a shorter window for each filter\n\n    >>> basis, lengths = librosa.filters.constant_q(22050, filter_scale=0.5)\n\n    Plot one octave of filters in time and frequency\n\n    >>> import matplotlib.pyplot as plt\n    >>> basis, lengths = librosa.filters.constant_q(22050)\n    >>> plt.figure(figsize=(10, 6))\n    >>> plt.subplot(2, 1, 1)\n    >>> notes = librosa.midi_to_note(np.arange(24, 24 + len(basis)))\n    >>> for i, (f, n) in enumerate(zip(basis, notes[:12])):\n    ...     f_scale = librosa.util.normalize(f) / 2\n    ...     plt.plot(i + f_scale.real)\n    ...     plt.plot(i + f_scale.imag, linestyle=\':\')\n    >>> plt.axis(\'tight\')\n    >>> plt.yticks(np.arange(len(notes[:12])), notes[:12])\n    >>> plt.ylabel(\'CQ filters\')\n    >>> plt.title(\'CQ filters (one octave, time domain)\')\n    >>> plt.xlabel(\'Time (samples at 22050 Hz)\')\n    >>> plt.legend([\'Real\', \'Imaginary\'], frameon=True, framealpha=0.8)\n    >>> plt.subplot(2, 1, 2)\n    >>> F = np.abs(np.fft.fftn(basis, axes=[-1]))\n    >>> # Keep only the positive frequencies\n    >>> F = F[:, :(1 + F.shape[1] // 2)]\n    >>> librosa.display.specshow(F, x_axis=\'linear\')\n    >>> plt.yticks(np.arange(len(notes))[::12], notes[::12])\n    >>> plt.ylabel(\'CQ filters\')\n    >>> plt.title(\'CQ filter magnitudes (frequency domain)\')\n    >>> plt.tight_layout()\n    '
if (fmin is None):
    fmin = note_to_hz('C1')
lengths = constant_q_lengths(sr, fmin, n_bins=n_bins, bins_per_octave=bins_per_octave, tuning=tuning, window=window, filter_scale=filter_scale)
correction = (2.0 ** (float(tuning) / bins_per_octave))
fmin = (correction * fmin)
Q = (float(filter_scale) / ((2.0 ** (1.0 / bins_per_octave)) - 1))
freqs = ((Q * sr) / lengths)
filters = []
for (ilen, freq) in zip(lengths, freqs):
    tempResult = arange(ilen, dtype=float)
	
===================================================================	
dct: 21	
----------------------------	

"Discrete cosine transform (DCT type-III) basis.\n\n    .. [1] http://en.wikipedia.org/wiki/Discrete_cosine_transform\n\n    Parameters\n    ----------\n    n_filters : int > 0 [scalar]\n        number of output components (DCT filters)\n\n    n_input : int > 0 [scalar]\n        number of input components (frequency bins)\n\n    Returns\n    -------\n    dct_basis: np.ndarray [shape=(n_filters, n_input)]\n        DCT (type-III) basis vectors [1]_\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    Examples\n    --------\n    >>> n_fft = 2048\n    >>> dct_filters = librosa.filters.dct(13, 1 + n_fft // 2)\n    >>> dct_filters\n    array([[ 0.031,  0.031, ...,  0.031,  0.031],\n           [ 0.044,  0.044, ..., -0.044, -0.044],\n           ...,\n           [ 0.044,  0.044, ..., -0.044, -0.044],\n           [ 0.044,  0.044, ...,  0.044,  0.044]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> librosa.display.specshow(dct_filters, x_axis='linear')\n    >>> plt.ylabel('DCT function')\n    >>> plt.title('DCT filter bank')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n    "
basis = numpy.empty((n_filters, n_input))
basis[0, :] = (1.0 / numpy.sqrt(n_input))
tempResult = arange(1, (2 * n_input), 2)
	
===================================================================	
constant_q_lengths: 120	
----------------------------	

'Return length of each filter in a constant-Q basis.\n\n    Parameters\n    ----------\n    sr : number > 0 [scalar]\n        Audio sampling rate\n\n    fmin : float > 0 [scalar]\n        Minimum frequency bin.\n\n    n_bins : int > 0 [scalar]\n        Number of frequencies.  Defaults to 7 octaves (84 bins).\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)` [scalar]\n        Tuning deviation from A440 in fractions of a bin\n\n    window : str or callable\n        Window function to use on filters\n\n    filter_scale : float > 0 [scalar]\n        Resolution of filter windows. Larger values use longer windows.\n\n    Returns\n    -------\n    lengths : np.ndarray\n        The length of each filter.\n\n    Notes\n    -----\n    This function caches at level 10.\n\n    See Also\n    --------\n    constant_q\n    librosa.core.cqt\n    '
if (fmin <= 0):
    raise ParameterError('fmin must be positive')
if (bins_per_octave <= 0):
    raise ParameterError('bins_per_octave must be positive')
if (filter_scale <= 0):
    raise ParameterError('filter_scale must be positive')
if ((n_bins <= 0) or (not isinstance(n_bins, int))):
    raise ParameterError('n_bins must be a positive integer')
correction = (2.0 ** (float(tuning) / bins_per_octave))
fmin = (correction * fmin)
Q = (float(filter_scale) / ((2.0 ** (1.0 / bins_per_octave)) - 1))
tempResult = arange(n_bins, dtype=float)
	
===================================================================	
clicks: 157	
----------------------------	

"Returns a signal with the signal `click` placed at each specified time\n\n    Parameters\n    ----------\n    times : np.ndarray or None\n        times to place clicks, in seconds\n\n    frames : np.ndarray or None\n        frame indices to place clicks\n\n    sr : number > 0\n        desired sampling rate of the output signal\n\n    hop_length : int > 0\n        if positions are specified by `frames`, the number of samples between frames.\n\n    click_freq : float > 0\n        frequency (in Hz) of the default click signal.  Default is 1KHz.\n\n    click_duration : float > 0\n        duration (in seconds) of the default click signal.  Default is 100ms.\n\n    click : np.ndarray or None\n        optional click signal sample to use instead of the default blip.\n\n    length : int > 0\n        desired number of samples in the output signal\n\n\n    Returns\n    -------\n    click_signal : np.ndarray\n        Synthesized click signal\n\n\n    Raises\n    ------\n    ParameterError\n        - If neither `times` nor `frames` are provided.\n        - If any of `click_freq`, `click_duration`, or `length` are out of range.\n\n\n    Examples\n    --------\n    >>> # Sonify detected beat events\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n    >>> y_beats = librosa.clicks(frames=beats, sr=sr)\n\n    >>> # Or generate a signal of the same length as y\n    >>> y_beats = librosa.clicks(frames=beats, sr=sr, length=len(y))\n\n    >>> # Or use timing instead of frame indices\n    >>> times = librosa.frames_to_time(beats, sr=sr)\n    >>> y_beat_times = librosa.clicks(times=times, sr=sr)\n\n    >>> # Or with a click frequency of 880Hz and a 500ms sample\n    >>> y_beat_times880 = librosa.clicks(times=times, sr=sr,\n    ...                                  click_freq=880, click_duration=0.5)\n\n    Display click waveform next to the spectrogram\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> S = librosa.feature.melspectrogram(y=y, sr=sr)\n    >>> ax = plt.subplot(2,1,2)\n    >>> librosa.display.specshow(librosa.power_to_db(S, ref=np.max),\n    ...                          x_axis='time', y_axis='mel')\n    >>> plt.subplot(2,1,1, sharex=ax)\n    >>> librosa.display.waveplot(y_beat_times, sr=sr, label='Beat clicks')\n    >>> plt.legend()\n    >>> plt.xlim(15, 30)\n    >>> plt.tight_layout()\n    "
if (times is None):
    if (frames is None):
        raise ParameterError('either "times" or "frames" must be provided')
    positions = frames_to_samples(frames, hop_length=hop_length)
else:
    positions = time_to_samples(times, sr=sr)
if (click is not None):
    util.valid_audio(click, mono=True)
else:
    if (click_duration <= 0):
        raise ParameterError('click_duration must be strictly positive')
    if (click_freq <= 0):
        raise ParameterError('click_freq must be strictly positive')
    angular_freq = (((2 * numpy.pi) * click_freq) / float(sr))
    click = numpy.logspace(0, (- 10), num=int(numpy.round((sr * click_duration))), base=2.0)
    tempResult = arange(len(click))
	
===================================================================	
phase_vocoder: 112	
----------------------------	

'Phase vocoder.  Given an STFT matrix D, speed up by a factor of `rate`\n\n    Based on the implementation provided by [1]_.\n\n    .. [1] Ellis, D. P. W. "A phase vocoder in Matlab."\n        Columbia University, 2002.\n        http://www.ee.columbia.edu/~dpwe/resources/matlab/pvoc/\n\n    Examples\n    --------\n    >>> # Play at double speed\n    >>> y, sr   = librosa.load(librosa.util.example_audio_file())\n    >>> D       = librosa.stft(y, n_fft=2048, hop_length=512)\n    >>> D_fast  = librosa.phase_vocoder(D, 2.0, hop_length=512)\n    >>> y_fast  = librosa.istft(D_fast, hop_length=512)\n\n    >>> # Or play at 1/3 speed\n    >>> y, sr   = librosa.load(librosa.util.example_audio_file())\n    >>> D       = librosa.stft(y, n_fft=2048, hop_length=512)\n    >>> D_slow  = librosa.phase_vocoder(D, 1./3, hop_length=512)\n    >>> y_slow  = librosa.istft(D_slow, hop_length=512)\n\n    Parameters\n    ----------\n    D : np.ndarray [shape=(d, t), dtype=complex]\n        STFT matrix\n\n    rate :  float > 0 [scalar]\n        Speed-up factor: `rate > 1` is faster, `rate < 1` is slower.\n\n    hop_length : int > 0 [scalar] or None\n        The number of samples between successive columns of `D`.\n\n        If None, defaults to `n_fft/4 = (D.shape[0]-1)/2`\n\n    Returns\n    -------\n    D_stretched  : np.ndarray [shape=(d, t / rate), dtype=complex]\n        time-stretched STFT\n    '
n_fft = (2 * (D.shape[0] - 1))
if (hop_length is None):
    hop_length = int((n_fft // 4))
tempResult = arange(0, D.shape[1], rate, dtype=numpy.float)
	
===================================================================	
cqt_frequencies: 145	
----------------------------	

"Compute the center frequencies of Constant-Q bins.\n\n    Examples\n    --------\n    >>> # Get the CQT frequencies for 24 notes, starting at C2\n    >>> librosa.cqt_frequencies(24, fmin=librosa.note_to_hz('C2'))\n    array([  65.406,   69.296,   73.416,   77.782,   82.407,   87.307,\n             92.499,   97.999,  103.826,  110.   ,  116.541,  123.471,\n            130.813,  138.591,  146.832,  155.563,  164.814,  174.614,\n            184.997,  195.998,  207.652,  220.   ,  233.082,  246.942])\n\n    Parameters\n    ----------\n    n_bins  : int > 0 [scalar]\n        Number of constant-Q bins\n\n    fmin    : float > 0 [scalar]\n        Minimum frequency\n\n    bins_per_octave : int > 0 [scalar]\n        Number of bins per octave\n\n    tuning : float in `[-0.5, +0.5)`\n        Deviation from A440 tuning in fractional bins (cents)\n\n    Returns\n    -------\n    frequencies : np.ndarray [shape=(n_bins,)]\n        Center frequency for each CQT bin\n    "
correction = (2.0 ** (float(tuning) / bins_per_octave))
tempResult = arange(0, n_bins, dtype=float)
	
===================================================================	
tempo_frequencies: 159	
----------------------------	

'Compute the frequencies (in beats-per-minute) corresponding\n    to an onset auto-correlation or tempogram matrix.\n\n    Parameters\n    ----------\n    n_bins : int > 0\n        The number of lag bins\n\n    hop_length : int > 0\n        The number of samples between each bin\n\n    sr : number > 0\n        The audio sampling rate\n\n    Returns\n    -------\n    bin_frequencies : ndarray [shape=(n_bins,)]\n        vector of bin frequencies measured in BPM.\n\n        .. note:: `bin_frequencies[0] = +np.inf` corresponds to 0-lag\n\n    Examples\n    --------\n    Get the tempo frequencies corresponding to a 384-bin (8-second) tempogram\n\n    >>> librosa.tempo_frequencies(384)\n    array([      inf,  2583.984,  1291.992, ...,     6.782,\n               6.764,     6.747])\n    '
bin_frequencies = numpy.zeros(int(n_bins), dtype=numpy.float)
bin_frequencies[0] = numpy.inf
tempResult = arange(1.0, n_bins)
	
===================================================================	
spectral_contrast: 64	
----------------------------	

'Compute spectral contrast [1]_\n\n    .. [1] Jiang, Dan-Ning, Lie Lu, Hong-Jiang Zhang, Jian-Hua Tao,\n           and Lian-Hong Cai.\n           "Music type classification by spectral contrast feature."\n           In Multimedia and Expo, 2002. ICME\'02. Proceedings.\n           2002 IEEE International Conference on, vol. 1, pp. 113-116.\n           IEEE, 2002.\n\n    Parameters\n    ----------\n    y : np.ndarray [shape=(n,)] or None\n        audio time series\n\n    sr : number  > 0 [scalar]\n        audio sampling rate of `y`\n\n    S : np.ndarray [shape=(d, t)] or None\n        (optional) spectrogram magnitude\n\n    n_fft : int > 0 [scalar]\n        FFT window size\n\n    hop_length : int > 0 [scalar]\n        hop length for STFT. See `librosa.core.stft` for details.\n\n    freq : None or np.ndarray [shape=(d,)]\n        Center frequencies for spectrogram bins.\n        If `None`, then FFT bin center frequencies are used.\n        Otherwise, it can be a single array of `d` center frequencies.\n\n    fmin : float > 0\n        Frequency cutoff for the first bin `[0, fmin]`\n        Subsequent bins will cover `[fmin, 2*fmin]`, `[2*fmin, 4*fmin]`, etc.\n\n    n_bands : int > 1\n        number of frequency bands\n\n    quantile : float in (0, 1)\n        quantile for determining peaks and valleys\n\n    linear : bool\n        If `True`, return the linear difference of magnitudes:\n        `peaks - valleys`.\n\n        If `False`, return the logarithmic difference:\n        `log(peaks) - log(valleys)`.\n\n\n    Returns\n    -------\n    contrast : np.ndarray [shape=(n_bands + 1, t)]\n        each row of spectral contrast values corresponds to a given\n        octave-based frequency\n\n\n    Examples\n    --------\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> S = np.abs(librosa.stft(y))\n    >>> contrast = librosa.feature.spectral_contrast(S=S, sr=sr)\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.figure()\n    >>> plt.subplot(2, 1, 1)\n    >>> librosa.display.specshow(librosa.amplitude_to_db(S,\n    ...                                                  ref=np.max),\n    ...                          y_axis=\'log\')\n    >>> plt.colorbar(format=\'%+2.0f dB\')\n    >>> plt.title(\'Power spectrogram\')\n    >>> plt.subplot(2, 1, 2)\n    >>> librosa.display.specshow(contrast, x_axis=\'time\')\n    >>> plt.colorbar()\n    >>> plt.ylabel(\'Frequency bands\')\n    >>> plt.title(\'Spectral contrast\')\n    >>> plt.tight_layout()\n    '
(S, n_fft) = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length)
if (freq is None):
    freq = fft_frequencies(sr=sr, n_fft=n_fft)
freq = numpy.atleast_1d(freq)
if ((freq.ndim != 1) or (len(freq) != S.shape[0])):
    raise ParameterError('freq.shape mismatch: expected ({:d},)'.format(S.shape[0]))
if ((n_bands < 1) or (not isinstance(n_bands, int))):
    raise ParameterError('n_bands must be a positive integer')
if (not (0.0 < quantile < 1.0)):
    raise ParameterError('quantile must lie in the range (0, 1)')
if (fmin <= 0):
    raise ParameterError('fmin must be a positive number')
octa = numpy.zeros((n_bands + 2))
tempResult = arange(0, (n_bands + 1))
	
===================================================================	
delta: 18	
----------------------------	

"Compute delta features: local estimate of the derivative\n    of the input data along the selected axis.\n\n\n    Parameters\n    ----------\n    data      : np.ndarray\n        the input data matrix (eg, spectrogram)\n\n    width     : int >= 3, odd [scalar]\n        Number of frames over which to compute the delta feature\n\n    order     : int > 0 [scalar]\n        the order of the difference operator.\n        1 for first derivative, 2 for second, etc.\n\n    axis      : int [scalar]\n        the axis along which to compute deltas.\n        Default is -1 (columns).\n\n    trim      : bool\n        set to `True` to trim the output matrix to the original size.\n\n    Returns\n    -------\n    delta_data   : np.ndarray [shape=(d, t) or (d, t + window)]\n        delta matrix of `data`.\n\n    Notes\n    -----\n    This function caches at level 40.\n\n    Examples\n    --------\n    Compute MFCC deltas, delta-deltas\n\n    >>> y, sr = librosa.load(librosa.util.example_audio_file())\n    >>> mfcc = librosa.feature.mfcc(y=y, sr=sr)\n    >>> mfcc_delta = librosa.feature.delta(mfcc)\n    >>> mfcc_delta\n    array([[  2.929e+01,   3.090e+01, ...,   0.000e+00,   0.000e+00],\n           [  2.226e+01,   2.553e+01, ...,   3.944e-31,   3.944e-31],\n           ...,\n           [ -1.192e+00,  -6.099e-01, ...,   9.861e-32,   9.861e-32],\n           [ -5.349e-01,  -2.077e-01, ...,   1.183e-30,   1.183e-30]])\n    >>> mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n    >>> mfcc_delta2\n    array([[  1.281e+01,   1.020e+01, ...,   0.000e+00,   0.000e+00],\n           [  2.726e+00,   3.558e+00, ...,   0.000e+00,   0.000e+00],\n           ...,\n           [ -1.702e-01,  -1.509e-01, ...,   0.000e+00,   0.000e+00],\n           [ -9.021e-02,  -7.007e-02, ...,  -2.190e-47,  -2.190e-47]])\n\n    >>> import matplotlib.pyplot as plt\n    >>> plt.subplot(3, 1, 1)\n    >>> librosa.display.specshow(mfcc)\n    >>> plt.title('MFCC')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 2)\n    >>> librosa.display.specshow(mfcc_delta)\n    >>> plt.title(r'MFCC-$\\Delta$')\n    >>> plt.colorbar()\n    >>> plt.subplot(3, 1, 3)\n    >>> librosa.display.specshow(mfcc_delta2, x_axis='time')\n    >>> plt.title(r'MFCC-$\\Delta^2$')\n    >>> plt.colorbar()\n    >>> plt.tight_layout()\n\n    "
data = numpy.atleast_1d(data)
if ((width < 3) or (numpy.mod(width, 2) != 1)):
    raise ParameterError('width must be an odd integer >= 3')
if ((order <= 0) or (not isinstance(order, int))):
    raise ParameterError('order must be a positive integer')
half_length = (1 + int((width // 2)))
tempResult = arange((half_length - 1.0), (- half_length), (- 1.0))
	
===================================================================	
func: 29	
----------------------------	

tempResult = arange(x)
	
===================================================================	
test_harmonics_2d_varying: 678	
----------------------------	

tempResult = arange(16)
	
===================================================================	
test_harmonics_1d: 631	
----------------------------	

tempResult = arange(16)
	
===================================================================	
Get no callers of function numpy.arange at line 376 col 55.	
===================================================================	
test_harmonics_2d: 648	
----------------------------	

tempResult = arange(16)
	
===================================================================	
test_bad_coords: 257	
----------------------------	

tempResult = arange((S.shape[1] // 2))
	
===================================================================	
test_trim: 107	
----------------------------	


def __test(y, top_db, ref, trim_duration):
    (yt, idx) = librosa.effects.trim(y, top_db=top_db, ref=ref)
    fidx = ([slice(None)] * y.ndim)
    fidx[(- 1)] = slice(*idx.tolist())
    assert numpy.allclose(yt, y[fidx])
    rms = librosa.feature.rmse(y=librosa.to_mono(yt), center=False)
    logamp = librosa.logamplitude((rms ** 2), ref=ref, top_db=None)
    assert numpy.all((logamp > (- top_db)))
    rms_all = librosa.feature.rmse(y=librosa.to_mono(y)).squeeze()
    logamp_all = librosa.logamplitude((rms_all ** 2), ref=ref, top_db=None)
    start = int(librosa.samples_to_frames(idx[0]))
    stop = int(librosa.samples_to_frames(idx[1]))
    assert numpy.all((logamp_all[:start] <= (- top_db)))
    assert numpy.all((logamp_all[stop:] <= (- top_db)))
    duration = librosa.get_duration(yt)
    assert numpy.allclose(duration, trim_duration, atol=0.1), duration
sr = float(22050)
trim_duration = 3.0
tempResult = arange(0, (trim_duration * sr))
	
===================================================================	
__test_peaks: 378	
----------------------------	

odf = numpy.zeros(((duration * sr) // hop_length))
spacing = ((sr * 60.0) // (hop_length * tempo))
odf[::int(spacing)] = 1
tempogram = librosa.feature.tempogram(onset_envelope=odf, sr=sr, hop_length=hop_length, win_length=win_length, window=window, norm=norm)
eq_(tempogram.shape[0], win_length)
eq_(tempogram.shape[1], len(odf))
idx = numpy.where(librosa.util.localmax(tempogram.max(axis=1)))[0]
tempResult = arange(1, (1 + len(idx)))
	
===================================================================	
test_delta: 32	
----------------------------	


def __test(width, order, axis, x):
    delta = librosa.feature.delta(x, width=width, order=order, axis=axis, trim=False)
    delta_t = librosa.feature.delta(x, width=width, order=order, axis=axis, trim=True)
    eq_(x.shape, delta_t.shape)
    _s = ([slice(None)] * delta.ndim)
    _s[axis] = slice((((- width) // 2) - x.shape[axis]), ((- (width // 2)) - 1))
    delta_retrim = delta[_s]
    assert numpy.allclose(delta_t, delta_retrim)
    slice_orig = ([slice(None)] * x.ndim)
    slice_out = ([slice(None)] * delta.ndim)
    slice_orig[axis] = slice(((width // 2) + 1), (((- width) // 2) + 1))
    slice_out[axis] = slice((width // 2), ((- width) // 2))
    assert numpy.allclose((x + delta_t)[slice_out], x[slice_orig])
tempResult = arange(100.0)
	
===================================================================	
test_poly_features_synthetic: 289	
----------------------------	

srand()
sr = 22050
n_fft = 2048

def __test(S, coeffs, freq):
    order = (coeffs.shape[0] - 1)
    p = librosa.feature.poly_features(S=S, sr=sr, n_fft=n_fft, order=order, freq=freq)
    for i in range(S.shape[(- 1)]):
        assert numpy.allclose(coeffs, p[::(- 1), i].squeeze())

def __make_data(coeffs, freq):
    S = numpy.zeros_like(freq)
    for (i, c) in enumerate(coeffs):
        S = (S + (c * (freq ** i)))
    S = S.reshape((freq.shape[0], (- 1)))
    return S
for order in range(1, 3):
    freq = librosa.fft_frequencies(sr=sr, n_fft=n_fft)
    tempResult = arange(1, (1 + order))
	
===================================================================	
Get no callers of function numpy.arange at line 194 col 34.	
===================================================================	
Get no callers of function numpy.arange at line 20 col 8.	
===================================================================	
__test11111: 71	
----------------------------	

tempResult = arange(3)
	
===================================================================	
Get no callers of function numpy.arange at line 33 col 8.	
===================================================================	
__test11: 44	
----------------------------	

tempResult = arange(3)
	
===================================================================	
Get no callers of function numpy.arange at line 516 col 20.	
===================================================================	
Get no callers of function numpy.arange at line 516 col 43.	
===================================================================	
Get no callers of function numpy.arange at line 554 col 22.	
===================================================================	
test_softmask_int: 609	
----------------------------	

X = (2 * numpy.ones((3, 3), dtype=numpy.int32))
tempResult = arange(3)
	
***************************************************	
mne_python-0.15.0: 367	
===================================================================	
_fwd_eeg_fit_berg_scherg: 427	
----------------------------	

'Fit the Berg-Scherg equivalent spherical model dipole parameters.'
from scipy.optimize import fmin_cobyla
assert (nfit >= 2)
u = dict(y=numpy.zeros((nterms - 1)), resi=numpy.zeros((nterms - 1)), nfit=nfit, nterms=nterms, M=numpy.zeros(((nterms - 1), (nfit - 1))))
u['fn'] = _fwd_eeg_get_multi_sphere_model_coeffs(m, (nterms + 1))
f = (min([layer['rad'] for layer in m['layers']]) / max([layer['rad'] for layer in m['layers']]))
tempResult = arange(1, (nterms + 1))
	
===================================================================	
_fwd_bem_lin_pot_coeff: 117	
----------------------------	

'Calculate the coefficients for linear collocation approach.'
nps = [surf['np'] for surf in surfs]
np_tot = sum(nps)
coeff = numpy.zeros((np_tot, np_tot))
offsets = numpy.cumsum(numpy.concatenate(([0], nps)))
for (si_1, surf1) in enumerate(surfs):
    tempResult = arange(nps[si_1])
	
===================================================================	
_fit_coil_order_dev_head_trans: 157	
----------------------------	

'Compute Device to Head transform allowing for permutiatons of points.'
id_quat = numpy.concatenate([rot_to_quat(numpy.eye(3)), [0.0, 0.0, 0.0]])
best_order = None
best_g = (- 999)
best_quat = id_quat
tempResult = arange(len(head_pnts))
	
===================================================================	
_calculate_chpi_coil_locs: 386	
----------------------------	

'Calculate locations of each cHPI coils over time.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        Raw data with cHPI information.\n    t_step_min : float\n        Minimum time step to use. If correlations are sufficiently high,\n        t_step_max will be used.\n    t_step_max : float\n        Maximum time step to use.\n    t_window : float\n        Time window to use to estimate the head positions.\n    dist_limit : float\n        Minimum distance (m) to accept for coil position fitting.\n    gof_limit : float\n        Minimum goodness of fit to accept.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    time : ndarray, shape (N, 1)\n        The start time of each fitting interval\n    chpi_digs :ndarray, shape (N, 1)\n        Array of dig structures containing the cHPI locations. Includes\n        goodness of fit for each cHPI.\n\n    Notes\n    -----\n    The number of time points ``N`` will depend on the velocity of head\n    movements as well as ``t_step_max`` and ``t_step_min``.\n\n    See Also\n    --------\n    read_head_pos\n    write_head_pos\n    '
hpi_dig_head_rrs = _get_hpi_initial_fit(raw.info)
hpi = _setup_hpi_struct(raw.info, int(round((t_window * raw.info['sfreq']))))
head_dev_t = invert_transform(raw.info['dev_head_t'])['trans']
hpi_dig_dev_rrs = apply_trans(head_dev_t, hpi_dig_head_rrs)
last = dict(sin_fit=None, fit_time=t_step_min, coil_dev_rrs=hpi_dig_dev_rrs)
t_begin = raw.times[0]
t_end = raw.times[(- 1)]
tempResult = arange((t_begin + (t_window / 2.0)), t_end, t_step_min)
	
===================================================================	
filter_chpi: 426	
----------------------------	

'Remove cHPI and line noise from data.\n\n    .. note:: This function will only work properly if cHPI was on\n              during the recording.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        Raw data with cHPI information. Must be preloaded. Operates in-place.\n    include_line : bool\n        If True, also filter line noise.\n    t_step : float\n        Time step to use for estimation, default is 0.01 (10 ms).\n    t_window : float\n        Time window to use to estimate the amplitudes, default is\n        0.2 (200 ms).\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    raw : instance of Raw\n        The raw data.\n\n    Notes\n    -----\n    cHPI signals are in general not stationary, because head movements act\n    like amplitude modulators on cHPI signals. Thus it is recommended to\n    to use this procedure, which uses an iterative fitting method, to\n    remove cHPI signals, as opposed to notch filtering.\n\n    .. versionadded:: 0.12\n    '
if (not raw.preload):
    raise RuntimeError('raw data must be preloaded')
t_step = float(t_step)
t_window = float(t_window)
if (not ((t_step > 0) and (t_window > 0))):
    raise ValueError(('t_step (%s) and t_window (%s) must both be > 0.' % (t_step, t_window)))
n_step = int(numpy.ceil((t_step * raw.info['sfreq'])))
hpi = _setup_hpi_struct(raw.info, int(round((t_window * raw.info['sfreq']))), exclude='bads', remove_aliased=True, verbose=False)
tempResult = arange(0, (len(raw.times) + (hpi['n_window'] // 2)), n_step)
	
===================================================================	
_setup_hpi_struct: 182	
----------------------------	

'Generate HPI structure for HPI localization.\n\n    Returns\n    -------\n    hpi : dict\n        Dictionary of parameters representing the cHPI system and needed to\n        perform head localization.\n    '
from .preprocessing.maxwell import _prep_mf_coils
(hpi_freqs, hpi_pick, hpi_ons) = _get_hpi_info(info)
highest = info.get('lowpass')
highest = ((info['sfreq'] / 2.0) if (highest is None) else highest)
keepers = numpy.array([(h <= highest) for h in hpi_freqs], bool)
if remove_aliased:
    hpi_freqs = hpi_freqs[keepers]
    hpi_ons = hpi_ons[keepers]
elif (not keepers.all()):
    raise RuntimeError(('Found HPI frequencies %s above the lowpass (or Nyquist) frequency %0.1f' % (hpi_freqs[(~ keepers)].tolist(), highest)))
if (info['line_freq'] is not None):
    tempResult = arange(info['line_freq'], (info['sfreq'] / 3.0), info['line_freq'])
	
===================================================================	
_setup_hpi_struct: 186	
----------------------------	

'Generate HPI structure for HPI localization.\n\n    Returns\n    -------\n    hpi : dict\n        Dictionary of parameters representing the cHPI system and needed to\n        perform head localization.\n    '
from .preprocessing.maxwell import _prep_mf_coils
(hpi_freqs, hpi_pick, hpi_ons) = _get_hpi_info(info)
highest = info.get('lowpass')
highest = ((info['sfreq'] / 2.0) if (highest is None) else highest)
keepers = numpy.array([(h <= highest) for h in hpi_freqs], bool)
if remove_aliased:
    hpi_freqs = hpi_freqs[keepers]
    hpi_ons = hpi_ons[keepers]
elif (not keepers.all()):
    raise RuntimeError(('Found HPI frequencies %s above the lowpass (or Nyquist) frequency %0.1f' % (hpi_freqs[(~ keepers)].tolist(), highest)))
if (info['line_freq'] is not None):
    line_freqs = numpy.arange(info['line_freq'], (info['sfreq'] / 3.0), info['line_freq'])
else:
    line_freqs = numpy.zeros([0])
utils.logger.info(('Line interference frequencies: %s Hz' % ' '.join([('%d' % l) for l in line_freqs])))
tempResult = arange(model_n_window)
	
===================================================================	
_calculate_chpi_positions: 291	
----------------------------	

"Calculate head positions using cHPI coils.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        Raw data with cHPI information.\n    t_step_min : float\n        Minimum time step to use. If correlations are sufficiently high,\n        t_step_max will be used.\n    t_step_max : float\n        Maximum time step to use.\n    t_window : float\n        Time window to use to estimate the head positions.\n    dist_limit : float\n        Minimum distance (m) to accept for coil position fitting.\n    gof_limit : float\n        Minimum goodness of fit to accept.\n    use_distances : bool\n        use dist_limit to choose 'good' coils based on pairwise distancs.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    quats : ndarray, shape (N, 10)\n        The ``[t, q1, q2, q3, x, y, z, gof, err, v]`` for each fit.\n\n    Notes\n    -----\n    The number of time points ``N`` will depend on the velocity of head\n    movements as well as ``t_step_max`` and ``t_step_min``.\n\n    See Also\n    --------\n    read_head_pos\n    write_head_pos\n    "
from scipy.spatial.distance import cdist
hpi_dig_head_rrs = _get_hpi_initial_fit(raw.info)
hpi = _setup_hpi_struct(raw.info, int(round((t_window * raw.info['sfreq']))))
dev_head_t = raw.info['dev_head_t']['trans']
head_dev_t = invert_transform(raw.info['dev_head_t'])['trans']
hpi_dig_dev_rrs = apply_trans(head_dev_t, hpi_dig_head_rrs)
hpi_coil_dists = cdist(hpi_dig_head_rrs, hpi_dig_head_rrs)
last = dict(sin_fit=None, fit_time=t_step_min, coil_dev_rrs=hpi_dig_dev_rrs, quat=numpy.concatenate([rot_to_quat(dev_head_t[:3, :3]), dev_head_t[:3, 3]]))
t_begin = raw.times[0]
t_end = raw.times[(- 1)]
tempResult = arange((t_begin + (t_window / 2.0)), t_end, t_step_min)
	
===================================================================	
_decimate_points: 116	
----------------------------	

'Decimate the number of points using a voxel grid.\n\n    Create a voxel grid with a specified resolution and retain at most one\n    point per voxel. For each voxel, the point closest to its center is\n    retained.\n\n    Parameters\n    ----------\n    pts : array, shape (n_points, 3)\n        The points making up the head shape.\n    res : scalar\n        The resolution of the voxel space (side length of each voxel).\n\n    Returns\n    -------\n    pts : array, shape = (n_points, 3)\n        The decimated points.\n    '
from scipy.spatial.distance import cdist
pts = numpy.asarray(pts)
(xmin, ymin, zmin) = (pts.min(0) - (res / 2.0))
(xmax, ymax, zmax) = (pts.max(0) + res)
tempResult = arange(xmin, xmax, res)
	
===================================================================	
_decimate_points: 117	
----------------------------	

'Decimate the number of points using a voxel grid.\n\n    Create a voxel grid with a specified resolution and retain at most one\n    point per voxel. For each voxel, the point closest to its center is\n    retained.\n\n    Parameters\n    ----------\n    pts : array, shape (n_points, 3)\n        The points making up the head shape.\n    res : scalar\n        The resolution of the voxel space (side length of each voxel).\n\n    Returns\n    -------\n    pts : array, shape = (n_points, 3)\n        The decimated points.\n    '
from scipy.spatial.distance import cdist
pts = numpy.asarray(pts)
(xmin, ymin, zmin) = (pts.min(0) - (res / 2.0))
(xmax, ymax, zmax) = (pts.max(0) + res)
xax = numpy.arange(xmin, xmax, res)
tempResult = arange(ymin, ymax, res)
	
===================================================================	
_decimate_points: 118	
----------------------------	

'Decimate the number of points using a voxel grid.\n\n    Create a voxel grid with a specified resolution and retain at most one\n    point per voxel. For each voxel, the point closest to its center is\n    retained.\n\n    Parameters\n    ----------\n    pts : array, shape (n_points, 3)\n        The points making up the head shape.\n    res : scalar\n        The resolution of the voxel space (side length of each voxel).\n\n    Returns\n    -------\n    pts : array, shape = (n_points, 3)\n        The decimated points.\n    '
from scipy.spatial.distance import cdist
pts = numpy.asarray(pts)
(xmin, ymin, zmin) = (pts.min(0) - (res / 2.0))
(xmax, ymax, zmax) = (pts.max(0) + res)
xax = numpy.arange(xmin, xmax, res)
yax = numpy.arange(ymin, ymax, res)
tempResult = arange(zmin, zmax, res)
	
===================================================================	
compute_raw_covariance: 170	
----------------------------	

"Estimate noise covariance matrix from a continuous segment of raw data.\n\n    It is typically useful to estimate a noise covariance from empty room\n    data or time intervals before starting the stimulation.\n\n    .. note:: This function will:\n\n                  1. Partition the data into evenly spaced, equal-length\n                     epochs.\n                  2. Load them into memory.\n                  3. Subtract the mean across all time points and epochs\n                     for each channel.\n                  4. Process the :class:`Epochs` by\n                     :func:`compute_covariance`.\n\n              This will produce a slightly different result compared to\n              using :func:`make_fixed_length_events`, :class:`Epochs`, and\n              :func:`compute_covariance` directly, since that would (with\n              the recommended baseline correction) subtract the mean across\n              time *for each epoch* (instead of across epochs) for each\n              channel.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        Raw data\n    tmin : float\n        Beginning of time interval in seconds. Defaults to 0.\n    tmax : float | None (default None)\n        End of time interval in seconds. If None (default), use the end of the\n        recording.\n    tstep : float (default 0.2)\n        Length of data chunks for artefact rejection in seconds.\n        Can also be None to use a single epoch of (tmax - tmin)\n        duration. This can use a lot of memory for large ``Raw``\n        instances.\n    reject : dict | None (default None)\n        Rejection parameters based on peak-to-peak amplitude.\n        Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg'.\n        If reject is None then no rejection is done. Example::\n\n            reject = dict(grad=4000e-13, # T / m (gradiometers)\n                          mag=4e-12, # T (magnetometers)\n                          eeg=40e-6, # V (EEG channels)\n                          eog=250e-6 # V (EOG channels)\n                          )\n\n    flat : dict | None (default None)\n        Rejection parameters based on flatness of signal.\n        Valid keys are 'grad' | 'mag' | 'eeg' | 'eog' | 'ecg', and values\n        are floats that set the minimum acceptable peak-to-peak amplitude.\n        If flat is None then no rejection is done.\n    picks : array-like of int | None (default None)\n        Indices of channels to include (if None, data channels are used).\n    method : str | list | None (default 'empirical')\n        The method used for covariance estimation.\n        See :func:`mne.compute_covariance`.\n\n        .. versionadded:: 0.12\n\n    method_params : dict | None (default None)\n        Additional parameters to the estimation procedure.\n        See :func:`mne.compute_covariance`.\n\n        .. versionadded:: 0.12\n\n    cv : int | sklearn model_selection object (default 3)\n        The cross validation method. Defaults to 3, which will\n        internally trigger a default 3-fold shuffle split.\n\n        .. versionadded:: 0.12\n\n    scalings : dict | None (default None)\n        Defaults to ``dict(mag=1e15, grad=1e13, eeg=1e6)``.\n        These defaults will scale magnetometers and gradiometers\n        at the same unit.\n\n        .. versionadded:: 0.12\n\n    n_jobs : int (default 1)\n        Number of jobs to run in parallel.\n\n        .. versionadded:: 0.12\n\n    return_estimators : bool (default False)\n        Whether to return all estimators or the best. Only considered if\n        method equals 'auto' or is a list of str. Defaults to False\n\n        .. versionadded:: 0.12\n\n    reject_by_annotation : bool\n        Whether to reject based on annotations. If True (default), epochs\n        overlapping with segments whose description begins with ``'bad'`` are\n        rejected. If False, no rejection based on annotations is performed.\n\n        .. versionadded:: 0.14.0\n\n    verbose : bool | str | int | None (default None)\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    cov : instance of Covariance | list\n        The computed covariance. If method equals 'auto' or is a list of str\n        and return_estimators equals True, a list of covariance estimators is\n        returned (sorted by log-likelihood, from high to low, i.e. from best\n        to worst).\n\n    See Also\n    --------\n    compute_covariance : Estimate noise covariance matrix from epochs\n    "
tmin = (0.0 if (tmin is None) else float(tmin))
tmax = (raw.times[(- 1)] if (tmax is None) else float(tmax))
tstep = ((tmax - tmin) if (tstep is None) else float(tstep))
tstep_m1 = (tstep - (1.0 / raw.info['sfreq']))
events = make_fixed_length_events(raw, 1, tmin, tmax, tstep)
utils.logger.info(('Using up to %s segment%s' % (len(events), _pl(events))))
if (picks is None):
    tempResult = arange(raw.info['nchan'])
	
===================================================================	
_ShrunkCovariance.fit: 545	
----------------------------	

'Fit covariance model with oracle shrinkage regularization.'
from sklearn.covariance import shrunk_covariance
from sklearn.covariance import EmpiricalCovariance
self.estimator_ = EmpiricalCovariance(store_precision=self.store_precision, assume_centered=self.assume_centered)
cov = self.estimator_.fit(X).covariance_
if (not isinstance(self.shrinkage, (list, tuple))):
    tempResult = arange(len(cov))
	
===================================================================	
compute_covariance: 294	
----------------------------	

'Estimate noise covariance matrix from epochs.\n\n    The noise covariance is typically estimated on pre-stim periods\n    when the stim onset is defined from events.\n\n    If the covariance is computed for multiple event types (events\n    with different IDs), the following two options can be used and combined:\n\n        1. either an Epochs object for each event type is created and\n           a list of Epochs is passed to this function.\n        2. an Epochs object is created for multiple events and passed\n           to this function.\n\n    .. note:: Baseline correction should be used when creating the Epochs.\n              Otherwise the computed covariance matrix will be inaccurate.\n\n    .. note:: For multiple event types, it is also possible to create a\n              single Epochs object with events obtained using\n              merge_events(). However, the resulting covariance matrix\n              will only be correct if keep_sample_mean is True.\n\n    .. note:: The covariance can be unstable if the number of samples is\n              not sufficient. In that case it is common to regularize a\n              covariance estimate. The ``method`` parameter of this\n              function allows to regularize the covariance in an\n              automated way. It also allows to select between different\n              alternative estimation algorithms which themselves achieve\n              regularization. Details are described in [1]_.\n\n    Parameters\n    ----------\n    epochs : instance of Epochs, or a list of Epochs objects\n        The epochs.\n    keep_sample_mean : bool (default True)\n        If False, the average response over epochs is computed for\n        each event type and subtracted during the covariance\n        computation. This is useful if the evoked response from a\n        previous stimulus extends into the baseline period of the next.\n        Note. This option is only implemented for method=\'empirical\'.\n    tmin : float | None (default None)\n        Start time for baseline. If None start at first sample.\n    tmax : float | None (default None)\n        End time for baseline. If None end at last sample.\n    projs : list of Projection | None (default None)\n        List of projectors to use in covariance calculation, or None\n        to indicate that the projectors from the epochs should be\n        inherited. If None, then projectors from all epochs must match.\n    method : str | list | None (default \'empirical\')\n        The method used for covariance estimation. If \'empirical\' (default),\n        the sample covariance will be computed. A list can be passed to run a\n        set of the different methods.\n        If \'auto\' or a list of methods, the best estimator will be determined\n        based on log-likelihood and cross-validation on unseen data as\n        described in [1]_. Valid methods are:\n\n            * ``\'empirical\'``: the empirical or sample covariance\n            * ``\'diagonal_fixed\'``: a diagonal regularization as in\n              mne.cov.regularize (see MNE manual)\n            * ``\'ledoit_wolf\'``: the Ledoit-Wolf estimator [2]_\n            * ``\'shrunk\'``: like \'ledoit_wolf\' with cross-validation for\n              optimal alpha (see scikit-learn documentation on covariance\n              estimation)\n            * ``\'pca\'``: probabilistic PCA with low rank [3]_\n            * ``\'factor_analysis\'``: Factor Analysis with low rank [4]_\n\n        If ``\'auto\'``, this expands to::\n\n             [\'shrunk\', \'diagonal_fixed\', \'empirical\', \'factor_analysis\']\n\n        .. note:: ``\'ledoit_wolf\'`` and ``\'pca\'`` are similar to\n           ``\'shrunk\'`` and ``\'factor_analysis\'``, respectively. They are not\n           included to avoid redundancy. In most cases ``\'shrunk\'`` and\n           ``\'factor_analysis\'`` represent more appropriate default\n           choices.\n\n        The ``\'auto\'`` mode is not recommended if there are many\n        segments of data, since computation can take a long time.\n\n        .. versionadded:: 0.9.0\n\n    method_params : dict | None (default None)\n        Additional parameters to the estimation procedure. Only considered if\n        method is not None. Keys must correspond to the value(s) of `method`.\n        If None (default), expands to::\n\n            \'empirical\': {\'store_precision\': False, \'assume_centered\': True},\n            \'diagonal_fixed\': {\'grad\': 0.01, \'mag\': 0.01, \'eeg\': 0.0,\n                               \'store_precision\': False,\n                               \'assume_centered\': True},\n            \'ledoit_wolf\': {\'store_precision\': False, \'assume_centered\': True},\n            \'shrunk\': {\'shrinkage\': np.logspace(-4, 0, 30),\n                       \'store_precision\': False, \'assume_centered\': True},\n            \'pca\': {\'iter_n_components\': None},\n            \'factor_analysis\': {\'iter_n_components\': None}\n\n    cv : int | sklearn model_selection object (default 3)\n        The cross validation method. Defaults to 3, which will\n        internally trigger a default 3-fold shuffle split.\n    scalings : dict | None (default None)\n        Defaults to ``dict(mag=1e15, grad=1e13, eeg=1e6)``.\n        These defaults will scale magnetometers and gradiometers\n        at the same unit.\n    n_jobs : int (default 1)\n        Number of jobs to run in parallel.\n    return_estimators : bool (default False)\n        Whether to return all estimators or the best. Only considered if\n        method equals \'auto\' or is a list of str. Defaults to False\n    on_mismatch : str\n        What to do when the MEG<->Head transformations do not match between\n        epochs. If "raise" (default) an error is raised, if "warn" then a\n        warning is emitted, if "ignore" then nothing is printed. Having\n        mismatched transforms can in some cases lead to unexpected or\n        unstable results in covariance calculation, e.g. when data\n        have been processed with Maxwell filtering but not transformed\n        to the same head position.\n    verbose : bool | str | int | or None (default None)\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    cov : instance of Covariance | list\n        The computed covariance. If method equals \'auto\' or is a list of str\n        and return_estimators equals True, a list of covariance estimators is\n        returned (sorted by log-likelihood, from high to low, i.e. from best\n        to worst).\n\n    See Also\n    --------\n    compute_raw_covariance : Estimate noise covariance from raw data\n\n    References\n    ----------\n    .. [1] Engemann D. and Gramfort A. (2015) Automated model selection in\n           covariance estimation and spatial whitening of MEG and EEG\n           signals, vol. 108, 328-342, NeuroImage.\n    .. [2] Ledoit, O., Wolf, M., (2004). A well-conditioned estimator for\n           large-dimensional covariance matrices. Journal of Multivariate\n           Analysis 88 (2), 365 - 411.\n    .. [3] Tipping, M. E., Bishop, C. M., (1999). Probabilistic principal\n           component analysis. Journal of the Royal Statistical Society:\n           Series B (Statistical Methodology) 61 (3), 611 - 622.\n    .. [4] Barber, D., (2012). Bayesian reasoning and machine learning.\n           Cambridge University Press., Algorithm 21.1\n    '
accepted_methods = ('auto', 'empirical', 'diagonal_fixed', 'ledoit_wolf', 'shrunk', 'pca', 'factor_analysis')
msg = ('Invalid method ({method}). Accepted values (individually or in a list) are "%s" or None.' % '" or "'.join(accepted_methods))
scalings = _check_scalings_user(scalings)
_method_params = {'empirical': {'store_precision': False, 'assume_centered': True}, 'diagonal_fixed': {'grad': 0.01, 'mag': 0.01, 'eeg': 0.0, 'store_precision': False, 'assume_centered': True}, 'ledoit_wolf': {'store_precision': False, 'assume_centered': True}, 'shrunk': {'shrinkage': numpy.logspace((- 4), 0, 30), 'store_precision': False, 'assume_centered': True}, 'pca': {'iter_n_components': None}, 'factor_analysis': {'iter_n_components': None}}
if isinstance(method_params, dict):
    for (key, values) in method_params.items():
        if (key not in _method_params):
            raise ValueError(('key (%s) must be "%s"' % (key, '" or "'.join(_method_params))))
        _method_params[key].update(method_params[key])

def _unpack_epochs(epochs):
    if (len(epochs.event_id) > 1):
        epochs = [epochs[k] for k in epochs.event_id]
    else:
        epochs = [epochs]
    return epochs
if (not isinstance(epochs, list)):
    epochs = _unpack_epochs(epochs)
else:
    epochs = sum([_unpack_epochs(epoch) for epoch in epochs], [])
if any((((epochs_t.baseline is None) and (epochs_t.info['highpass'] < 0.5) and keep_sample_mean) for epochs_t in epochs)):
    warn('Epochs are not baseline corrected, covariance matrix may be inaccurate')
orig = epochs[0].info['dev_head_t']
if ((not isinstance(on_mismatch, string_types)) or (on_mismatch not in ['raise', 'warn', 'ignore'])):
    raise ValueError(('on_mismatch must be "raise", "warn", or "ignore", got %s' % on_mismatch))
for (ei, epoch) in enumerate(epochs):
    epoch.info._check_consistency()
    if (((orig is None) != (epoch.info['dev_head_t'] is None)) or ((orig is not None) and (not numpy.allclose(orig['trans'], epoch.info['dev_head_t']['trans'])))):
        msg = ('MEG<->Head transform mismatch between epochs[0]:\n%s\n\nand epochs[%s]:\n%s' % (orig, ei, epoch.info['dev_head_t']))
        if (on_mismatch == 'raise'):
            raise ValueError(msg)
        elif (on_mismatch == 'warn'):
            warn(msg)
bads = epochs[0].info['bads']
if (projs is None):
    projs = epochs[0].info['projs']
    for epochs_t in epochs[1:]:
        if (epochs_t.proj != epochs[0].proj):
            raise ValueError('Epochs must agree on the use of projections')
        for (proj_a, proj_b) in zip(epochs_t.info['projs'], projs):
            if (not _proj_equal(proj_a, proj_b)):
                raise ValueError('Epochs must have same projectors')
projs = _check_projs(projs)
ch_names = epochs[0].ch_names
for epochs_t in epochs[1:]:
    if (epochs_t.info['bads'] != bads):
        raise ValueError('Epochs must have same bad channels')
    if (epochs_t.ch_names != ch_names):
        raise ValueError('Epochs must have same channel names')
picks_list = _picks_by_type(epochs[0].info)
picks_meeg = numpy.concatenate([b for (_, b) in picks_list])
picks_meeg = numpy.sort(picks_meeg)
ch_names = [epochs[0].ch_names[k] for k in picks_meeg]
info = epochs[0].info
if (method is None):
    method = ['empirical']
elif (method == 'auto'):
    method = ['shrunk', 'diagonal_fixed', 'empirical', 'factor_analysis']
if (not isinstance(method, (list, tuple))):
    method = [method]
ok_sklearn = (check_version('sklearn', '0.15') is True)
if ((not ok_sklearn) and ((len(method) != 1) or (method[0] != 'empirical'))):
    raise ValueError('scikit-learn is not installed, `method` must be `empirical`')
if (keep_sample_mean is False):
    if ((len(method) != 1) or ('empirical' not in method)):
        raise ValueError('`keep_sample_mean=False` is only supportedwith `method="empirical"`')
    for (p, v) in _method_params.items():
        if (v.get('assume_centered', None) is False):
            raise ValueError('`assume_centered` must be True if `keep_sample_mean` is False')
    n_epoch_types = len(epochs)
    data_mean = ([0] * n_epoch_types)
    n_samples = numpy.zeros(n_epoch_types, dtype=numpy.int)
    n_epochs = numpy.zeros(n_epoch_types, dtype=numpy.int)
    for (ii, epochs_t) in enumerate(epochs):
        tslice = _get_tslice(epochs_t, tmin, tmax)
        for e in epochs_t:
            e = e[(picks_meeg, tslice)]
            if (not keep_sample_mean):
                data_mean[ii] += e
            n_samples[ii] += e.shape[1]
            n_epochs[ii] += 1
    n_samples_epoch = (n_samples // n_epochs)
    norm_const = numpy.sum((n_samples_epoch * (n_epochs - 1)))
    data_mean = [((1.0 / n_epoch) * numpy.dot(mean, mean.T)) for (n_epoch, mean) in zip(n_epochs, data_mean)]
if (not all(((k in accepted_methods) for k in method))):
    raise ValueError(msg.format(method=method))
info = pick_info(info, picks_meeg)
tslice = _get_tslice(epochs[0], tmin, tmax)
epochs = [ee.get_data()[:, picks_meeg, tslice] for ee in epochs]
tempResult = arange(len(picks_meeg))
	
===================================================================	
_auto_low_rank_model: 462	
----------------------------	

'Compute latent variable models.'
method_params = deepcopy(method_params)
iter_n_components = method_params.pop('iter_n_components')
if (iter_n_components is None):
    tempResult = arange(5, data.shape[1], 5)
	
===================================================================	
fft_resample: 131	
----------------------------	

'Do FFT resampling with a filter function (possibly using CUDA).\n\n    Parameters\n    ----------\n    x : 1-d array\n        The array to resample. Will be converted to float64 if necessary.\n    W : 1-d array or gpuarray\n        The filtering function to apply.\n    new_len : int\n        The size of the output array (before removing padding).\n    npads : tuple of int\n        Amount of padding to apply to the start and end of the\n        signal before resampling.\n    to_removes : tuple of int\n        Number of samples to remove after resampling.\n    cuda_dict : dict\n        Dictionary constructed using setup_cuda_multiply_repeated().\n    pad : str\n        The type of padding to use. Supports all :func:`np.pad` ``mode``\n        options. Can also be "reflect_limited" (default), which pads with a\n        reflected version of each vector mirrored on the first and last values\n        of the vector, followed by zeros.\n\n        .. versionadded:: 0.15\n\n    Returns\n    -------\n    x : 1-d array\n        Filtered version of x.\n    '
cuda_dict = (dict(use_cuda=False) if (cuda_dict is None) else cuda_dict)
if (x.dtype != numpy.float64):
    x = x.astype(numpy.float64)
x = _smart_pad(x, npads, pad)
old_len = len(x)
shorter = (new_len < old_len)
if (not cuda_dict['use_cuda']):
    N = int(min(new_len, old_len))
    x_fft = rfft(x).ravel()
    tempResult = arange(1, (len(x) + 1))
	
===================================================================	
_make_guesses: 265	
----------------------------	

'Make a guess space inside a sphere or BEM surface.'
if ('rr' in surf):
    utils.logger.info(('Guess surface (%s) is in %s coordinates' % (_bem_explain_surface(surf['id']), _coord_frame_name(surf['coord_frame']))))
else:
    utils.logger.info(('Making a spherical guess space with radius %7.1f mm...' % (1000 * surf['R'])))
utils.logger.info(('Filtering (grid = %6.f mm)...' % (1000 * grid)))
src = _make_volume_source_space(surf, grid, exclude, (1000 * mindist), do_neighbors=False, n_jobs=n_jobs)
assert ('vertno' in src)
tempResult = arange(src['nuse'])
	
===================================================================	
get_phantom_dipoles: 616	
----------------------------	

'Get standard phantom dipole locations and orientations.\n\n    Parameters\n    ----------\n    kind : str\n        Get the information for the given system:\n\n            ``vectorview`` (default)\n              The Neuromag VectorView phantom.\n            ``otaniemi``\n              The older Neuromag phantom used at Otaniemi.\n\n    Returns\n    -------\n    pos : ndarray, shape (n_dipoles, 3)\n        The dipole positions.\n    ori : ndarray, shape (n_dipoles, 3)\n        The dipole orientations.\n\n    Notes\n    -----\n    The Elekta phantoms have a radius of 79.5mm, and HPI coil locations\n    in the XY-plane at the axis extrema (e.g., (79.5, 0), (0, -79.5), ...).\n    '
_valid_types = ('vectorview', 'otaniemi')
if ((not isinstance(kind, string_types)) or (kind not in _valid_types)):
    raise ValueError(('kind must be one of %s, got %s' % (_valid_types, kind)))
if (kind == 'vectorview'):
    a = numpy.array([59.7, 48.6, 35.8, 24.8, 37.2, 27.5, 15.8, 7.9])
    b = numpy.array([46.1, 41.9, 38.3, 31.5, 13.9, 16.2, 20.0, 19.3])
    x = numpy.concatenate((a, ([0] * 8), (- b), ([0] * 8)))
    y = numpy.concatenate((([0] * 8), (- a), ([0] * 8), b))
    c = [22.9, 23.5, 25.5, 23.1, 52.0, 46.4, 41.0, 33.0]
    d = [44.4, 34.0, 21.6, 12.7, 62.4, 51.5, 39.1, 27.9]
    z = numpy.concatenate((c, c, d, d))
elif (kind == 'otaniemi'):
    a = numpy.array([56.3, 47.6, 39.0, 30.3])
    b = numpy.array([32.5, 27.5, 22.5, 17.5])
    c = numpy.zeros(4)
    x = numpy.concatenate((a, b, c, c, (- a), (- b), c, c))
    y = numpy.concatenate((c, c, (- a), (- b), c, c, b, a))
    z = numpy.concatenate((b, a, b, a, b, a, a, b))
pos = (np.vstack((x, y, z)).T / 1000.0)
ori = list()
for this_pos in pos:
    this_ori = numpy.zeros(3)
    idx = numpy.where((this_pos == 0))[0]
    tempResult = arange(3)
	
===================================================================	
EpochsArray.__init__: 876	
----------------------------	

dtype = (numpy.complex128 if numpy.any(numpy.iscomplex(data)) else numpy.float64)
data = numpy.asanyarray(data, dtype=dtype)
if (data.ndim != 3):
    raise ValueError('Data must be a 3D array of shape (n_epochs, n_channels, n_samples)')
if (len(info['ch_names']) != data.shape[1]):
    raise ValueError('Info and data must have same number of channels.')
if (events is None):
    n_epochs = len(data)
    tempResult = arange(n_epochs)
	
===================================================================	
average_movements: 1304	
----------------------------	

'Average data using Maxwell filtering, transforming using head positions.\n\n    Parameters\n    ----------\n    epochs : instance of Epochs\n        The epochs to operate on.\n    head_pos : array | tuple | None\n        The array should be of shape ``(N, 10)``, holding the position\n        parameters as returned by e.g. `read_head_pos`. For backward\n        compatibility, this can also be a tuple of ``(trans, rot t)``\n        as returned by `head_pos_to_trans_rot_t`.\n    orig_sfreq : float | None\n        The original sample frequency of the data (that matches the\n        event sample numbers in ``epochs.events``). Can be ``None``\n        if data have not been decimated or resampled.\n    picks : array-like of int | None\n        If None only MEG, EEG, SEEG, ECoG, and fNIRS channels are kept\n        otherwise the channels indices in picks are kept.\n    origin : array-like, shape (3,) | str\n        Origin of internal and external multipolar moment space in head\n        coords and in meters. The default is ``\'auto\'``, which means\n        a head-digitization-based origin fit.\n    weight_all : bool\n        If True, all channels are weighted by the SSS basis weights.\n        If False, only MEG channels are weighted, other channels\n        receive uniform weight per epoch.\n    int_order : int\n        Order of internal component of spherical expansion.\n    ext_order : int\n        Order of external component of spherical expansion.\n    regularize : str | None\n        Basis regularization type, must be "in" or None.\n        See :func:`mne.preprocessing.maxwell_filter` for details.\n        Regularization is chosen based only on the destination position.\n    destination : str | array-like, shape (3,) | None\n        The destination location for the head. Can be ``None``, which\n        will not change the head position, or a string path to a FIF file\n        containing a MEG device<->head transformation, or a 3-element array\n        giving the coordinates to translate to (with no rotations).\n        For example, ``destination=(0, 0, 0.04)`` would translate the bases\n        as ``--trans default`` would in MaxFilter™ (i.e., to the default\n        head location).\n\n        .. versionadded:: 0.12\n\n    ignore_ref : bool\n        If True, do not include reference channels in compensation. This\n        option should be True for KIT files, since Maxwell filtering\n        with reference channels is not currently supported.\n    return_mapping : bool\n        If True, return the mapping matrix.\n    mag_scale : float | str\n        The magenetometer scale-factor used to bring the magnetometers\n        to approximately the same order of magnitude as the gradiometers\n        (default 100.), as they have different units (T vs T/m).\n        Can be ``\'auto\'`` to use the reciprocal of the physical distance\n        between the gradiometer pickup loops (e.g., 0.0168 m yields\n        59.5 for VectorView).\n\n        .. versionadded:: 0.13\n\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    evoked : instance of Evoked\n        The averaged epochs.\n\n    See Also\n    --------\n    mne.preprocessing.maxwell_filter\n    mne.chpi.read_head_pos\n\n    Notes\n    -----\n    The Maxwell filtering version of this algorithm is described in [1]_,\n    in section V.B "Virtual signals and movement correction", equations\n    40-44. For additional validation, see [2]_.\n\n    Regularization has not been added because in testing it appears to\n    decrease dipole localization accuracy relative to using all components.\n    Fine calibration and cross-talk cancellation, however, could be added\n    to this algorithm based on user demand.\n\n    .. versionadded:: 0.11\n\n    References\n    ----------\n    .. [1] Taulu S. and Kajola M. "Presentation of electromagnetic\n           multichannel data: The signal space separation method,"\n           Journal of Applied Physics, vol. 97, pp. 124905 1-10, 2005.\n\n    .. [2] Wehner DT, Hämäläinen MS, Mody M, Ahlfors SP. "Head movements\n           of children in MEG: Quantification, effects on source\n           estimation, and compensation. NeuroImage 40:541–550, 2008.\n    '
from .preprocessing.maxwell import _trans_sss_basis, _reset_meg_bads, _check_usable, _col_norm_pinv, _get_n_moments, _get_mf_picks, _prep_mf_coils, _check_destination, _remove_meg_projs, _get_coil_scale
if (head_pos is None):
    raise TypeError('head_pos must be provided and cannot be None')
from .chpi import head_pos_to_trans_rot_t
if (not isinstance(epochs, BaseEpochs)):
    raise TypeError(('epochs must be an instance of Epochs, not %s' % (type(epochs),)))
orig_sfreq = (epochs.info['sfreq'] if (orig_sfreq is None) else orig_sfreq)
orig_sfreq = float(orig_sfreq)
if isinstance(head_pos, numpy.ndarray):
    head_pos = head_pos_to_trans_rot_t(head_pos)
(trn, rot, t) = head_pos
del head_pos
_check_usable(epochs)
origin = _check_origin(origin, epochs.info, 'head')
recon_trans = _check_destination(destination, epochs.info, True)
utils.logger.info(('Aligning and averaging up to %s epochs' % len(epochs.events)))
if (not numpy.array_equal(epochs.events[:, 0], numpy.unique(epochs.events[:, 0]))):
    raise RuntimeError('Epochs must have monotonically increasing events')
(meg_picks, mag_picks, grad_picks, good_picks, _) = _get_mf_picks(epochs.info, int_order, ext_order, ignore_ref)
(coil_scale, mag_scale) = _get_coil_scale(meg_picks, mag_picks, grad_picks, mag_scale, epochs.info)
(n_channels, n_times) = (len(epochs.ch_names), len(epochs.times))
tempResult = arange(n_channels)
	
===================================================================	
_minimize_time_diff: 956	
----------------------------	

'Find a boolean mask to minimize timing differences.'
from scipy.interpolate import interp1d
keep = numpy.ones(len(t_longer), dtype=bool)
if (len(t_shorter) == 0):
    keep.fill(False)
    return keep
scores = numpy.ones(len(t_longer))
tempResult = arange(len(t_shorter))
	
===================================================================	
_minimize_time_diff: 965	
----------------------------	

'Find a boolean mask to minimize timing differences.'
from scipy.interpolate import interp1d
keep = numpy.ones(len(t_longer), dtype=bool)
if (len(t_shorter) == 0):
    keep.fill(False)
    return keep
scores = numpy.ones(len(t_longer))
x1 = numpy.arange(len(t_shorter))
kwargs = dict(copy=False, bounds_error=False)
if ('assume_sorted' in _get_args(scipy.interpolate.interp1d.__init__)):
    kwargs['assume_sorted'] = True
shorter_interp = interp1d(x1, t_shorter, fill_value=t_shorter[(- 1)], **kwargs)
for ii in range((len(t_longer) - len(t_shorter))):
    scores.fill(numpy.inf)
    keep_mask = (~ numpy.eye(len(t_longer), dtype=bool)[keep])
    keep_mask[:, (~ keep)] = False
    tempResult = arange(((len(t_longer) - ii) - 1))
	
===================================================================	
BaseEpochs.__init__: 189	
----------------------------	

self.verbose = verbose
if (on_missing not in ['error', 'warning', 'ignore']):
    raise ValueError(('on_missing must be one of: error, warning, ignore. Got: %s' % on_missing))
if (event_id is None):
    event_id = list(numpy.unique(events[:, 2]))
if isinstance(event_id, dict):
    for key in event_id.keys():
        if (not isinstance(key, string_types)):
            raise TypeError(('Event names must be of type str, got %s (%s)' % (key, type(key))))
    event_id = dict(((key, _ensure_int(val, ('event_id[%s]' % key))) for (key, val) in event_id.items()))
elif isinstance(event_id, list):
    event_id = [_ensure_int(v, ('event_id[%s]' % vi)) for (vi, v) in enumerate(event_id)]
    event_id = dict(zip((str(i) for i in event_id), event_id))
else:
    event_id = _ensure_int(event_id, 'event_id')
    event_id = {str(event_id): event_id}
self.event_id = event_id
del event_id
if (events is not None):
    if (events.dtype.kind not in ['i', 'u']):
        raise ValueError('events must be an array of type int')
    if ((events.ndim != 2) or (events.shape[1] != 3)):
        raise ValueError('events must be 2D with 3 columns')
    for (key, val) in self.event_id.items():
        if (val not in events[:, 2]):
            msg = ('No matching events found for %s (event id %i)' % (key, val))
            if (on_missing == 'error'):
                raise ValueError(msg)
            elif (on_missing == 'warning'):
                warn(msg)
            else:
                pass
    values = list(self.event_id.values())
    selected = numpy.in1d(events[:, 2], values)
    if (selection is None):
        self.selection = numpy.where(selected)[0]
    else:
        self.selection = selection
    if (drop_log is None):
        self.drop_log = [(list() if (k in self.selection) else ['IGNORED']) for k in range(len(events))]
    else:
        self.drop_log = drop_log
    events = events[selected]
    if (len(numpy.unique(events[:, 0])) != len(events)):
        raise RuntimeError('Event time samples were not unique')
    n_events = len(events)
    if (n_events > 1):
        if (np.diff(events.astype(np.int64)[:, 0]).min() <= 0):
            warn('The events passed to the Epochs constructor are not chronologically ordered.', RuntimeWarning)
    if (n_events > 0):
        utils.logger.info(('%d matching events found' % n_events))
    else:
        raise ValueError('No desired events found.')
    self.events = events
    del events
else:
    self.drop_log = list()
    self.selection = numpy.array([], int)
if ((reject_tmin is not None) and (reject_tmin < tmin)):
    raise ValueError('reject_tmin needs to be None or >= tmin')
if ((reject_tmax is not None) and (reject_tmax > tmax)):
    raise ValueError('reject_tmax needs to be None or <= tmax')
if ((reject_tmin is not None) and (reject_tmax is not None)):
    if (reject_tmin >= reject_tmax):
        raise ValueError('reject_tmin needs to be < reject_tmax')
if ((detrend not in [None, 0, 1]) or isinstance(detrend, bool)):
    raise ValueError('detrend must be None, 0, or 1')
if (tmin > tmax):
    raise ValueError('tmin has to be less than or equal to tmax')
_check_baseline(baseline, tmin, tmax, info['sfreq'])
_log_rescale(baseline)
self.baseline = baseline
self.reject_tmin = reject_tmin
self.reject_tmax = reject_tmax
self.detrend = detrend
self._raw = raw
self.info = info
del info
if (picks is None):
    picks = list(range(len(self.info['ch_names'])))
else:
    self.info = pick_info(self.info, picks)
self.picks = _check_type_picks(picks)
if (len(picks) == 0):
    raise ValueError('Picks cannot be empty.')
if (data is None):
    self.preload = False
    self._data = None
else:
    assert (decim == 1)
    if ((data.ndim != 3) or (data.shape[2] != (round(((tmax - tmin) * self.info['sfreq'])) + 1))):
        raise RuntimeError('bad data shape')
    self.preload = True
    self._data = data
self._offset = None
sfreq = float(self.info['sfreq'])
start_idx = int(round((tmin * sfreq)))
tempResult = arange(start_idx, (int(round((tmax * sfreq))) + 1))
	
===================================================================	
_read_one_epoch_file: 1075	
----------------------------	

'Read a single FIF file.'
with f as fid:
    (info, meas) = read_meas_info(fid, tree, clean_bads=True)
    (events, mappings) = _read_events_fif(fid, tree)
    processed = dir_tree_find(meas, io.constants.FIFF.FIFFB_PROCESSED_DATA)
    if (len(processed) == 0):
        raise ValueError('Could not find processed data')
    epochs_node = dir_tree_find(tree, io.constants.FIFF.FIFFB_MNE_EPOCHS)
    if (len(epochs_node) == 0):
        epochs_node = dir_tree_find(tree, io.constants.FIFF.FIFFB_MNE_EPOCHS)
        if (len(epochs_node) == 0):
            epochs_node = dir_tree_find(tree, 122)
            if (len(epochs_node) == 0):
                raise ValueError('Could not find epochs data')
    my_epochs = epochs_node[0]
    data = None
    data_tag = None
    (bmin, bmax) = (None, None)
    baseline = None
    selection = None
    drop_log = None
    for k in range(my_epochs['nent']):
        kind = my_epochs['directory'][k].kind
        pos = my_epochs['directory'][k].pos
        if (kind == io.constants.FIFF.FIFF_FIRST_SAMPLE):
            tag = read_tag(fid, pos)
            first = int(tag.data)
        elif (kind == io.constants.FIFF.FIFF_LAST_SAMPLE):
            tag = read_tag(fid, pos)
            last = int(tag.data)
        elif (kind == io.constants.FIFF.FIFF_EPOCH):
            fid.seek(pos, 0)
            data_tag = read_tag_info(fid)
            data_tag.pos = pos
        elif (kind in [io.constants.FIFF.FIFF_MNE_BASELINE_MIN, 304]):
            tag = read_tag(fid, pos)
            bmin = float(tag.data)
        elif (kind in [io.constants.FIFF.FIFF_MNE_BASELINE_MAX, 305]):
            tag = read_tag(fid, pos)
            bmax = float(tag.data)
        elif (kind == io.constants.FIFF.FIFFB_MNE_EPOCHS_SELECTION):
            tag = read_tag(fid, pos)
            selection = numpy.array(tag.data)
        elif (kind == io.constants.FIFF.FIFFB_MNE_EPOCHS_DROP_LOG):
            tag = read_tag(fid, pos)
            drop_log = json.loads(tag.data)
    if ((bmin is not None) or (bmax is not None)):
        baseline = (bmin, bmax)
    n_samp = ((last - first) + 1)
    utils.logger.info('    Found the data of interest:')
    utils.logger.info(('        t = %10.2f ... %10.2f ms' % (((1000 * first) / info['sfreq']), ((1000 * last) / info['sfreq']))))
    if (info['comps'] is not None):
        utils.logger.info(('        %d CTF compensation matrices available' % len(info['comps'])))
    if (data_tag is None):
        raise ValueError('Epochs data not found')
    epoch_shape = (len(info['ch_names']), n_samp)
    expected = (len(events) * numpy.prod(epoch_shape))
    if (((data_tag.size // 4) - 4) != expected):
        raise ValueError(('Incorrect number of samples (%d instead of %d)' % ((data_tag.size // 4), expected)))
    cals = numpy.array([[(info['chs'][k]['cal'] * info['chs'][k].get('scale', 1.0))] for k in range(info['nchan'])], numpy.float64)
    if preload:
        data = read_tag(fid, data_tag.pos).data.astype(numpy.float64)
        data *= cals[numpy.newaxis, :, :]
    tmin = (first / info['sfreq'])
    tmax = (last / info['sfreq'])
    event_id = (dict(((str(e), e) for e in numpy.unique(events[:, 2]))) if (mappings is None) else mappings)
    if (selection is None):
        tempResult = arange(len(events))
	
===================================================================	
BaseEpochs.save: 755	
----------------------------	

'Save epochs in a fif file.\n\n        Parameters\n        ----------\n        fname : str\n            The name of the file, which should end with -epo.fif or\n            -epo.fif.gz.\n        split_size : string | int\n            Large raw files are automatically split into multiple pieces. This\n            parameter specifies the maximum size of each piece. If the\n            parameter is an integer, it specifies the size in Bytes. It is\n            also possible to pass a human-readable string, e.g., 100MB.\n            Note: Due to FIFF file limitations, the maximum split size is 2GB.\n\n            .. versionadded:: 0.10.0\n\n        Notes\n        -----\n        Bad epochs will be dropped before saving the epochs to disk.\n        '
check_fname(fname, 'epochs', ('-epo.fif', '-epo.fif.gz'))
split_size = _get_split_size(split_size)
self.drop_bad()
total_size = (self[0].get_data().nbytes * len(self))
total_size /= 2
n_parts = int(numpy.ceil((total_size / float(split_size))))
tempResult = arange(len(self))
	
===================================================================	
EpochsFIF.__init__: 1124	
----------------------------	

check_fname(fname, 'epochs', ('-epo.fif', '-epo.fif.gz'))
fnames = [fname]
ep_list = list()
raw = list()
for fname in fnames:
    utils.logger.info(('Reading %s ...' % fname))
    (fid, tree, _) = fiff_open(fname)
    next_fname = _get_next_fname(fid, fname, tree)
    (info, data, data_tag, events, event_id, tmin, tmax, baseline, selection, drop_log, epoch_shape, cals) = _read_one_epoch_file(fid, tree, preload)
    epoch = BaseEpochs(info, data, events, event_id, tmin, tmax, baseline, on_missing='ignore', selection=selection, drop_log=drop_log, proj=False, verbose=False)
    ep_list.append(epoch)
    if (not preload):
        raw.append(_RawContainer(fiff_open(fname)[0], data_tag, events[:, 0].copy(), epoch_shape, cals))
    if (next_fname is not None):
        fnames.append(next_fname)
(info, data, events, event_id, tmin, tmax, baseline, selection, drop_log, _) = _concatenate_epochs(ep_list, with_data=preload)
if (len(numpy.unique(events[:, 0])) != len(events)):
    raise RuntimeError('Event time samples were not unique')
assert ((len(drop_log) % len(fnames)) == 0)
step = (len(drop_log) // len(fnames))
tempResult = arange(step, (len(drop_log) + 1), step)
	
===================================================================	
make_fixed_length_events: 354	
----------------------------	

'Make a set of events separated by a fixed duration.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        A raw object to use the data from.\n    id : int\n        The id to use.\n    start : float\n        Time of first event.\n    stop : float | None\n        Maximum time of last event. If None, events extend to the end\n        of the recording.\n    duration: float\n        The duration to separate events by.\n    first_samp: bool\n        If True (default), times will have raw.first_samp added to them, as\n        in :func:`mne.find_events`. This behavior is not desirable if the\n        returned events will be combined with event times that already\n        have ``raw.first_samp`` added to them, e.g. event times that come\n        from :func:`mne.find_events`.\n\n    Returns\n    -------\n    new_events : array\n        The new events.\n    '
from .io.base import BaseRaw
if (not isinstance(raw, BaseRaw)):
    raise ValueError(('Input data must be an instance of Raw, got %s instead.' % type(raw)))
if (not isinstance(id, int)):
    raise ValueError('id must be an integer')
if (not isinstance(duration, (int, float))):
    raise ValueError(('duration must be an integer of a float, got %s instead.' % type(duration)))
start = raw.time_as_index(start)[0]
if (stop is not None):
    stop = raw.time_as_index(stop)[0]
else:
    stop = (raw.last_samp + 1)
if first_samp:
    start = (start + raw.first_samp)
    stop = min([(stop + raw.first_samp), (raw.last_samp + 1)])
else:
    stop = min([stop, len(raw.times)])
stop -= int(numpy.ceil((raw.info['sfreq'] * duration)))
tempResult = arange(start, (stop + 1), (raw.info['sfreq'] * duration))
	
===================================================================	
_read_evoked: 469	
----------------------------	

'Read evoked data from a FIF file.'
if (fname is None):
    raise ValueError('No evoked filename specified')
(f, tree, _) = fiff_open(fname)
with f as fid:
    (info, meas) = read_meas_info(fid, tree, clean_bads=True)
    processed = dir_tree_find(meas, io.constants.FIFF.FIFFB_PROCESSED_DATA)
    if (len(processed) == 0):
        raise ValueError('Could not find processed data')
    evoked_node = dir_tree_find(meas, io.constants.FIFF.FIFFB_EVOKED)
    if (len(evoked_node) == 0):
        raise ValueError('Could not find evoked data')
    if isinstance(condition, string_types):
        if (kind not in _aspect_dict.keys()):
            raise ValueError('kind must be "average" or "standard_error"')
        (comments, aspect_kinds, t) = _get_entries(fid, evoked_node, allow_maxshield)
        goods = (numpy.in1d(comments, [condition]) & numpy.in1d(aspect_kinds, [_aspect_dict[kind]]))
        found_cond = numpy.where(goods)[0]
        if (len(found_cond) != 1):
            raise ValueError(('condition "%s" (%s) not found, out of found datasets:\n  %s' % (condition, kind, t)))
        condition = found_cond[0]
    elif (condition is None):
        if (len(evoked_node) > 1):
            (_, _, conditions) = _get_entries(fid, evoked_node, allow_maxshield)
            raise TypeError(('Evoked file has more than one conditions, the condition parameters must be specified from:\n%s' % conditions))
        else:
            condition = 0
    if ((condition >= len(evoked_node)) or (condition < 0)):
        raise ValueError('Data set selector out of range')
    my_evoked = evoked_node[condition]
    (my_aspect, info['maxshield']) = _get_aspect(my_evoked, allow_maxshield)
    nchan = 0
    sfreq = (- 1)
    chs = []
    comment = last = first = first_time = nsamp = None
    for k in range(my_evoked['nent']):
        my_kind = my_evoked['directory'][k].kind
        pos = my_evoked['directory'][k].pos
        if (my_kind == io.constants.FIFF.FIFF_COMMENT):
            tag = read_tag(fid, pos)
            comment = tag.data
        elif (my_kind == io.constants.FIFF.FIFF_FIRST_SAMPLE):
            tag = read_tag(fid, pos)
            first = int(tag.data)
        elif (my_kind == io.constants.FIFF.FIFF_LAST_SAMPLE):
            tag = read_tag(fid, pos)
            last = int(tag.data)
        elif (my_kind == io.constants.FIFF.FIFF_NCHAN):
            tag = read_tag(fid, pos)
            nchan = int(tag.data)
        elif (my_kind == io.constants.FIFF.FIFF_SFREQ):
            tag = read_tag(fid, pos)
            sfreq = float(tag.data)
        elif (my_kind == io.constants.FIFF.FIFF_CH_INFO):
            tag = read_tag(fid, pos)
            chs.append(tag.data)
        elif (my_kind == io.constants.FIFF.FIFF_FIRST_TIME):
            tag = read_tag(fid, pos)
            first_time = float(tag.data)
        elif (my_kind == io.constants.FIFF.FIFF_NO_SAMPLES):
            tag = read_tag(fid, pos)
            nsamp = int(tag.data)
    if (comment is None):
        comment = 'No comment'
    if (nchan > 0):
        if (chs is None):
            raise ValueError('Local channel information was not found when it was expected.')
        if (len(chs) != nchan):
            raise ValueError('Number of channels and number of channel definitions are different')
        info['chs'] = chs
        utils.logger.info(('    Found channel information in evoked data. nchan = %d' % nchan))
        if (sfreq > 0):
            info['sfreq'] = sfreq
    nave = 1
    epoch = []
    for k in range(my_aspect['nent']):
        kind = my_aspect['directory'][k].kind
        pos = my_aspect['directory'][k].pos
        if (kind == io.constants.FIFF.FIFF_COMMENT):
            tag = read_tag(fid, pos)
            comment = tag.data
        elif (kind == io.constants.FIFF.FIFF_ASPECT_KIND):
            tag = read_tag(fid, pos)
            aspect_kind = int(tag.data)
        elif (kind == io.constants.FIFF.FIFF_NAVE):
            tag = read_tag(fid, pos)
            nave = int(tag.data)
        elif (kind == io.constants.FIFF.FIFF_EPOCH):
            tag = read_tag(fid, pos)
            epoch.append(tag)
    nepoch = len(epoch)
    if ((nepoch != 1) and (nepoch != info['nchan'])):
        raise ValueError(('Number of epoch tags is unreasonable (nepoch = %d nchan = %d)' % (nepoch, info['nchan'])))
    if (nepoch == 1):
        data = epoch[0].data
        if ((data.shape[1] == 1) and (info['nchan'] == 1)):
            data = data.T
    else:
        data = numpy.concatenate([e.data[None, :] for e in epoch], axis=0)
    data = data.astype(numpy.float)
    if (first is not None):
        nsamp = ((last - first) + 1)
    elif (first_time is not None):
        first = int(round((first_time * info['sfreq'])))
        last = (first + nsamp)
    else:
        raise RuntimeError('Could not read time parameters')
    if ((nsamp is not None) and (data.shape[1] != nsamp)):
        raise ValueError(('Incorrect number of samples (%d instead of  %d)' % (data.shape[1], nsamp)))
    nsamp = data.shape[1]
    last = ((first + nsamp) - 1)
    utils.logger.info('    Found the data of interest:')
    utils.logger.info(('        t = %10.2f ... %10.2f ms (%s)' % (((1000 * first) / info['sfreq']), ((1000 * last) / info['sfreq']), comment)))
    if (info['comps'] is not None):
        utils.logger.info(('        %d CTF compensation matrices available' % len(info['comps'])))
    utils.logger.info(('        nave = %d - aspect type = %d' % (nave, aspect_kind)))
cals = numpy.array([(info['chs'][k]['cal'] * info['chs'][k].get('scale', 1.0)) for k in range(info['nchan'])])
data *= cals[:, numpy.newaxis]
tempResult = arange(first, (last + 1), dtype=numpy.float)
	
===================================================================	
EvokedArray.__init__: 234	
----------------------------	

dtype = (numpy.complex128 if numpy.any(numpy.iscomplex(data)) else numpy.float64)
data = numpy.asanyarray(data, dtype=dtype)
if (data.ndim != 2):
    raise ValueError('Data must be a 2D array of shape (n_channels, n_samples)')
if (len(info['ch_names']) != numpy.shape(data)[0]):
    raise ValueError(('Info (%s) and data (%s) must have same number of channels.' % (len(info['ch_names']), numpy.shape(data)[0])))
self.data = data
self.first = int((tmin * info['sfreq']))
self.last = ((self.first + numpy.shape(data)[(- 1)]) - 1)
tempResult = arange(self.first, (self.last + 1), dtype=numpy.float)
	
===================================================================	
Evoked.shift_time: 99	
----------------------------	

"Shift time scale in evoked data.\n\n        Parameters\n        ----------\n        tshift : float\n            The amount of time shift to be applied if relative is True\n            else the first time point. When relative is True, positive value\n            of tshift moves the data forward while negative tshift moves it\n            backward.\n        relative : bool\n            If true, move the time backwards or forwards by specified amount.\n            Else, set the starting time point to the value of tshift.\n\n        Notes\n        -----\n        Maximum accuracy of time shift is 1 / evoked.info['sfreq']\n        "
times = self.times
sfreq = self.info['sfreq']
offset = (self.first if relative else 0)
self.first = (int((tshift * sfreq)) + offset)
self.last = ((self.first + len(times)) - 1)
tempResult = arange(self.first, (self.last + 1), dtype=numpy.float)
	
===================================================================	
FilterMixin.resample: 802	
----------------------------	

'Resample data.\n\n        .. note:: Data must be loaded.\n\n        Parameters\n        ----------\n        sfreq : float\n            New sample rate to use\n        npad : int | str\n            Amount to pad the start and end of the data.\n            Can also be "auto" to use a padding that will result in\n            a power-of-two size (can be much faster).\n        window : string or tuple\n            Window to use in resampling. See :func:`scipy.signal.resample`.\n        n_jobs : int\n            Number of jobs to run in parallel.\n        pad : str\n            The type of padding to use. Supports all :func:`numpy.pad` ``mode``\n            options. Can also be "reflect_limited", which pads with a\n            reflected version of each vector mirrored on the first and last\n            values of the vector, followed by zeros. The default is "edge",\n            which pads with the edge values of each vector.\n\n            .. versionadded:: 0.15\n        verbose : bool, str, int, or None\n            If not None, override default verbose level (see\n            :func:`mne.verbose` :ref:`Logging documentation <tut_logging>` for\n            more). Defaults to self.verbose.\n\n        Returns\n        -------\n        inst : instance of Epochs | instance of Evoked\n            The resampled epochs object.\n\n        See Also\n        --------\n        mne.io.Raw.resample\n\n        Notes\n        -----\n        For some data, it may be more accurate to use npad=0 to reduce\n        artifacts. This is dataset dependent -- check your data!\n        '
from .epochs import BaseEpochs
_check_preload(self, 'inst.resample')
sfreq = float(sfreq)
o_sfreq = self.info['sfreq']
self._data = resample(self._data, sfreq, o_sfreq, npad, window=window, n_jobs=n_jobs, pad=pad)
self.info['sfreq'] = float(sfreq)
tempResult = arange(self._data.shape[(- 1)], dtype=numpy.float)
	
===================================================================	
_mt_spectrum_remove: 526	
----------------------------	

'Use MT-spectrum to remove line frequencies.\n\n    Based on Chronux. If line_freqs is specified, all freqs within notch_width\n    of each line_freq is set to zero.\n    '
n_tapers = len(window_fun)
tempResult = arange(0, n_tapers, 2)
	
===================================================================	
_mt_spectrum_remove: 527	
----------------------------	

'Use MT-spectrum to remove line frequencies.\n\n    Based on Chronux. If line_freqs is specified, all freqs within notch_width\n    of each line_freq is set to zero.\n    '
n_tapers = len(window_fun)
tapers_odd = numpy.arange(0, n_tapers, 2)
tempResult = arange(1, n_tapers, 2)
	
===================================================================	
_mt_spectrum_remove: 531	
----------------------------	

'Use MT-spectrum to remove line frequencies.\n\n    Based on Chronux. If line_freqs is specified, all freqs within notch_width\n    of each line_freq is set to zero.\n    '
n_tapers = len(window_fun)
tapers_odd = numpy.arange(0, n_tapers, 2)
tapers_even = numpy.arange(1, n_tapers, 2)
tapers_use = window_fun[tapers_odd]
H0 = numpy.sum(tapers_use, axis=1)
H0_sq = sum_squared(H0)
tempResult = arange(x.size)
	
===================================================================	
_overlap_add_filter: 70	
----------------------------	

"Filter the signal x using h with overlap-add FFTs.\n\n    Parameters\n    ----------\n    x : array, shape (n_signals, n_times)\n        Signals to filter.\n    h : 1d array\n        Filter impulse response (FIR filter coefficients). Must be odd length\n        if phase == 'linear'.\n    n_fft : int\n        Length of the FFT. If None, the best size is determined automatically.\n    phase : str\n        If 'zero', the delay for the filter is compensated (and it must be\n        an odd-length symmetric filter). If 'linear', the response is\n        uncompensated. If 'zero-double', the filter is applied in the\n        forward and reverse directions. If 'minimum', a minimum-phase\n        filter will be used.\n    picks : array-like of int | None\n        Indices of channels to filter. If None all channels will be\n        filtered. Only supported for 2D (n_channels, n_times) and 3D\n        (n_epochs, n_channels, n_times) data.\n    n_jobs : int | str\n        Number of jobs to run in parallel. Can be 'cuda' if scikits.cuda\n        is installed properly and CUDA is initialized.\n    copy : bool\n        If True, a copy of x, filtered, is returned. Otherwise, it operates\n        on x in place.\n    pad : str\n        Padding type for ``_smart_pad``.\n\n    Returns\n    -------\n    xf : array, shape (n_signals, n_times)\n        x filtered.\n    "
n_jobs = check_n_jobs(n_jobs, allow_cuda=True)
(x, orig_shape, picks) = _prep_for_filtering(x, copy, picks)
_check_zero_phase_length(len(h), phase)
if (len(h) == 1):
    return ((x * (h ** 2)) if (phase == 'zero-double') else (x * h))
n_edge = max((min(len(h), x.shape[1]) - 1), 0)
utils.logger.debug(('Smart-padding with:  %s samples on each edge' % n_edge))
n_x = (x.shape[1] + (2 * n_edge))
if (phase == 'zero-double'):
    h = numpy.convolve(h, h[::(- 1)])
min_fft = ((2 * len(h)) - 1)
if (n_fft is None):
    max_fft = n_x
    if (max_fft >= min_fft):
        tempResult = arange(numpy.ceil(numpy.log2(min_fft)), (numpy.ceil(numpy.log2(max_fft)) + 1), dtype=int)
	
===================================================================	
_overlap_add_filter: 81	
----------------------------	

"Filter the signal x using h with overlap-add FFTs.\n\n    Parameters\n    ----------\n    x : array, shape (n_signals, n_times)\n        Signals to filter.\n    h : 1d array\n        Filter impulse response (FIR filter coefficients). Must be odd length\n        if phase == 'linear'.\n    n_fft : int\n        Length of the FFT. If None, the best size is determined automatically.\n    phase : str\n        If 'zero', the delay for the filter is compensated (and it must be\n        an odd-length symmetric filter). If 'linear', the response is\n        uncompensated. If 'zero-double', the filter is applied in the\n        forward and reverse directions. If 'minimum', a minimum-phase\n        filter will be used.\n    picks : array-like of int | None\n        Indices of channels to filter. If None all channels will be\n        filtered. Only supported for 2D (n_channels, n_times) and 3D\n        (n_epochs, n_channels, n_times) data.\n    n_jobs : int | str\n        Number of jobs to run in parallel. Can be 'cuda' if scikits.cuda\n        is installed properly and CUDA is initialized.\n    copy : bool\n        If True, a copy of x, filtered, is returned. Otherwise, it operates\n        on x in place.\n    pad : str\n        Padding type for ``_smart_pad``.\n\n    Returns\n    -------\n    xf : array, shape (n_signals, n_times)\n        x filtered.\n    "
n_jobs = check_n_jobs(n_jobs, allow_cuda=True)
(x, orig_shape, picks) = _prep_for_filtering(x, copy, picks)
_check_zero_phase_length(len(h), phase)
if (len(h) == 1):
    return ((x * (h ** 2)) if (phase == 'zero-double') else (x * h))
n_edge = max((min(len(h), x.shape[1]) - 1), 0)
utils.logger.debug(('Smart-padding with:  %s samples on each edge' % n_edge))
n_x = (x.shape[1] + (2 * n_edge))
if (phase == 'zero-double'):
    h = numpy.convolve(h, h[::(- 1)])
min_fft = ((2 * len(h)) - 1)
if (n_fft is None):
    max_fft = n_x
    if (max_fft >= min_fft):
        N = (2 ** numpy.arange(numpy.ceil(numpy.log2(min_fft)), (numpy.ceil(numpy.log2(max_fft)) + 1), dtype=int))
        cost = ((numpy.ceil((n_x / ((N - len(h)) + 1).astype(numpy.float))) * N) * (numpy.log2(N) + 1))
        cost += ((4e-05 * N) * n_x)
        n_fft = N[numpy.argmin(cost)]
    else:
        n_fft = next_fast_len(min_fft)
utils.logger.debug(('FFT block length:   %s' % n_fft))
if (n_fft < min_fft):
    raise ValueError(('n_fft is too short, has to be at least 2 * len(h) - 1 (%s), got %s' % (min_fft, n_fft)))
h_fft = fft(numpy.concatenate([h, numpy.zeros((n_fft - len(h)), dtype=h.dtype)]))
(n_jobs, cuda_dict, h_fft) = setup_cuda_fft_multiply_repeated(n_jobs, h_fft)
tempResult = arange(len(x))
	
===================================================================	
_Interp2.n_samp1: 898	
----------------------------	

assert (len(set(self._count.values())) == 1)
self._n_samp = n_samp
self.interp = self.interp
tempResult = arange(0, n_samp, 10000)
	
===================================================================	
_Interp2.interp1: 917	
----------------------------	

known_types = ('cos2', 'linear', 'zero', 'hann')
if (interp not in known_types):
    raise ValueError(('interp must be one of %s, got "%s"' % (known_types, interp)))
self._interp = interp
if (self.n_samp is not None):
    if (self._interp == 'zero'):
        self._interpolators = None
    elif (self._interp == 'linear'):
        interp = numpy.linspace(1, 0, self.n_samp, endpoint=False)
    elif (self._interp == 'cos2'):
        tempResult = arange(self.n_samp)
	
===================================================================	
_prep_for_filtering: 140	
----------------------------	

'Set up array as 2D for filtering ease.'
if (x.dtype != numpy.float64):
    raise TypeError('Arrays passed for filtering must have a dtype of np.float64')
if (copy is True):
    x = x.copy()
orig_shape = x.shape
x = numpy.atleast_2d(x)
x.shape = (numpy.prod(x.shape[:(- 1)]), x.shape[(- 1)])
if (picks is None):
    tempResult = arange(x.shape[0])
	
===================================================================	
_prep_for_filtering: 143	
----------------------------	

'Set up array as 2D for filtering ease.'
if (x.dtype != numpy.float64):
    raise TypeError('Arrays passed for filtering must have a dtype of np.float64')
if (copy is True):
    x = x.copy()
orig_shape = x.shape
x = numpy.atleast_2d(x)
x.shape = (numpy.prod(x.shape[:(- 1)]), x.shape[(- 1)])
if (picks is None):
    picks = numpy.arange(x.shape[0])
elif (len(orig_shape) == 3):
    (n_epochs, n_channels, n_times) = orig_shape
    tempResult = arange(0, (n_channels * n_epochs), n_channels)
	
===================================================================	
design_mne_c_filter: 831	
----------------------------	

'Create a FIR filter like that used by MNE-C.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sample frequency.\n    l_freq : float | None\n        The low filter frequency in Hz, default None.\n        Can be None to avoid high-passing.\n    h_freq : float\n        The high filter frequency in Hz, default 40.\n        Can be None to avoid low-passing.\n    l_trans_bandwidth : float | None\n        Low transition bandwidthin Hz. Can be None (default) to use 3 samples.\n    h_trans_bandwidth : float\n        High transition bandwidth in Hz.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more). Defaults to\n        self.verbose.\n\n    Returns\n    -------\n    h : ndarray, shape (8193,)\n        The linear-phase (symmetric) FIR filter coefficients.\n\n    Notes\n    -----\n    This function is provided mostly for reference purposes.\n\n    MNE-C uses a frequency-domain filter design technique by creating a\n    linear-phase filter of length 8193. In the frequency domain, the\n    4197 frequencies are directly constructed, with zeroes in the stop-band\n    and ones in the pass-band, with squared cosine ramps in between.\n    '
n_freqs = (((4096 + (2 * 2048)) // 2) + 1)
freq_resp = numpy.ones(n_freqs)
l_freq = (0 if (l_freq is None) else float(l_freq))
if (l_trans_bandwidth is None):
    l_width = 3
else:
    l_width = ((int((((n_freqs - 1) * l_trans_bandwidth) / (0.5 * sfreq))) + 1) // 2)
l_start = int((((n_freqs - 1) * l_freq) / (0.5 * sfreq)))
h_freq = ((sfreq / 2.0) if (h_freq is None) else float(h_freq))
h_width = ((int((((n_freqs - 1) * h_trans_bandwidth) / (0.5 * sfreq))) + 1) // 2)
h_start = int((((n_freqs - 1) * h_freq) / (0.5 * sfreq)))
utils.logger.info(('filter : %7.3f ... %6.1f Hz   bins : %d ... %d of %d hpw : %d lpw : %d' % (l_freq, h_freq, l_start, h_start, n_freqs, l_width, h_width)))
if (l_freq > 0):
    start = ((l_start - l_width) + 1)
    stop = ((start + (2 * l_width)) - 1)
    if ((start < 0) or (stop >= n_freqs)):
        raise RuntimeError('l_freq too low or l_trans_bandwidth too large')
    freq_resp[:start] = 0.0
    tempResult = arange(((- l_width) + 1), l_width)
	
===================================================================	
design_mne_c_filter: 838	
----------------------------	

'Create a FIR filter like that used by MNE-C.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sample frequency.\n    l_freq : float | None\n        The low filter frequency in Hz, default None.\n        Can be None to avoid high-passing.\n    h_freq : float\n        The high filter frequency in Hz, default 40.\n        Can be None to avoid low-passing.\n    l_trans_bandwidth : float | None\n        Low transition bandwidthin Hz. Can be None (default) to use 3 samples.\n    h_trans_bandwidth : float\n        High transition bandwidth in Hz.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more). Defaults to\n        self.verbose.\n\n    Returns\n    -------\n    h : ndarray, shape (8193,)\n        The linear-phase (symmetric) FIR filter coefficients.\n\n    Notes\n    -----\n    This function is provided mostly for reference purposes.\n\n    MNE-C uses a frequency-domain filter design technique by creating a\n    linear-phase filter of length 8193. In the frequency domain, the\n    4197 frequencies are directly constructed, with zeroes in the stop-band\n    and ones in the pass-band, with squared cosine ramps in between.\n    '
n_freqs = (((4096 + (2 * 2048)) // 2) + 1)
freq_resp = numpy.ones(n_freqs)
l_freq = (0 if (l_freq is None) else float(l_freq))
if (l_trans_bandwidth is None):
    l_width = 3
else:
    l_width = ((int((((n_freqs - 1) * l_trans_bandwidth) / (0.5 * sfreq))) + 1) // 2)
l_start = int((((n_freqs - 1) * l_freq) / (0.5 * sfreq)))
h_freq = ((sfreq / 2.0) if (h_freq is None) else float(h_freq))
h_width = ((int((((n_freqs - 1) * h_trans_bandwidth) / (0.5 * sfreq))) + 1) // 2)
h_start = int((((n_freqs - 1) * h_freq) / (0.5 * sfreq)))
utils.logger.info(('filter : %7.3f ... %6.1f Hz   bins : %d ... %d of %d hpw : %d lpw : %d' % (l_freq, h_freq, l_start, h_start, n_freqs, l_width, h_width)))
if (l_freq > 0):
    start = ((l_start - l_width) + 1)
    stop = ((start + (2 * l_width)) - 1)
    if ((start < 0) or (stop >= n_freqs)):
        raise RuntimeError('l_freq too low or l_trans_bandwidth too large')
    freq_resp[:start] = 0.0
    k = ((numpy.arange(((- l_width) + 1), l_width) / float(l_width)) + 3.0)
    freq_resp[start:stop] = (numpy.cos(((numpy.pi / 4.0) * k)) ** 2)
if (h_freq < (sfreq / 2.0)):
    start = ((h_start - h_width) + 1)
    stop = ((start + (2 * h_width)) - 1)
    if ((start < 0) or (stop >= n_freqs)):
        raise RuntimeError('h_freq too high or h_trans_bandwidth too large')
    tempResult = arange(((- h_width) + 1), h_width)
	
===================================================================	
_resample_stim_channels: 632	
----------------------------	

'Resample stim channels, carefully.\n\n    Parameters\n    ----------\n    stim_data : array, shape (n_samples,) or (n_stim_channels, n_samples)\n        Stim channels to resample.\n    up : float\n        Factor to upsample by.\n    down : float\n        Factor to downsample by.\n\n    Returns\n    -------\n    stim_resampled : array, shape (n_stim_channels, n_samples_resampled)\n        The resampled stim channels.\n\n    Note\n    ----\n    The approach taken here is equivalent to the approach in the C-code.\n    See the decimate_stimch function in MNE/mne_browse_raw/save.c\n    '
stim_data = numpy.atleast_2d(stim_data)
(n_stim_channels, n_samples) = stim_data.shape
ratio = (float(up) / down)
resampled_n_samples = int(round((n_samples * ratio)))
stim_resampled = numpy.zeros((n_stim_channels, resampled_n_samples))
tempResult = arange(resampled_n_samples)
	
===================================================================	
_spectral_helper: 469	
----------------------------	

'\n    Calculate various forms of windowed FFTs for PSD, CSD, etc.\n    This is a helper function that implements the commonality between the\n    psd, csd, and spectrogram functions. It is not designed to be called\n    externally. The windows are not averaged over; the result from each window\n    is returned.\n\n    Parameters\n    ---------\n    x : array_like\n        Array or sequence containing the data to be analyzed.\n    y : array_like\n        Array or sequence containing the data to be analyzed. If this is\n        the same object in memoery as x (i.e. _spectral_helper(x, x, ...)),\n        the extra computations are spared.\n    fs : float, optional\n        Sampling frequency of the time series. Defaults to 1.0.\n    window : str or tuple or array_like, optional\n        Desired window to use. See `get_window` for a list of windows and\n        required parameters. If `window` is array_like it will be used\n        directly as the window and its length will be used for nperseg.\n        Defaults to \'hann\'.\n    nperseg : int, optional\n        Length of each segment.  Defaults to 256.\n    noverlap : int, optional\n        Number of points to overlap between segments. If None,\n        ``noverlap = nperseg // 2``.  Defaults to None.\n    nfft : int, optional\n        Length of the FFT used, if a zero padded FFT is desired.  If None,\n        the FFT length is `nperseg`. Defaults to None.\n    detrend : str or function or False, optional\n        Specifies how to detrend each segment. If `detrend` is a string,\n        it is passed as the ``type`` argument to `detrend`.  If it is a\n        function, it takes a segment and returns a detrended segment.\n        If `detrend` is False, no detrending is done.  Defaults to \'constant\'.\n    return_onesided : bool, optional\n        If True, return a one-sided spectrum for real data. If False return\n        a two-sided spectrum. Note that for complex data, a two-sided\n        spectrum is always returned.\n    scaling : { \'density\', \'spectrum\' }, optional\n        Selects between computing the cross spectral density (\'density\')\n        where `Pxy` has units of V**2/Hz and computing the cross spectrum\n        (\'spectrum\') where `Pxy` has units of V**2, if `x` and `y` are\n        measured in V and fs is measured in Hz.  Defaults to \'density\'\n    axis : int, optional\n        Axis along which the periodogram is computed; the default is over\n        the last axis (i.e. ``axis=-1``).\n    mode : str, optional\n        Defines what kind of return values are expected. Options are [\'psd\',\n        \'complex\', \'magnitude\', \'angle\', \'phase\'].\n\n    Returns\n    -------\n    freqs : ndarray\n        Array of sample frequencies.\n    t : ndarray\n        Array of times corresponding to each data segment\n    result : ndarray\n        Array of output data, contents dependent on *mode* kwarg.\n\n    References\n    ----------\n    .. [1] Stack Overflow, "Rolling window for 1D arrays in Numpy?",\n        http://stackoverflow.com/a/6811241\n    .. [2] Stack Overflow, "Using strides for an efficient moving average\n        filter", http://stackoverflow.com/a/4947453\n\n    Notes\n    -----\n    Adapted from matplotlib.mlab\n    .. versionadded:: 0.16.0\n    '
from scipy import fftpack
from scipy.signal import signaltools
from scipy.signal.windows import get_window
if (mode not in ['psd', 'complex', 'magnitude', 'angle', 'phase']):
    raise ValueError(("Unknown value for mode %s, must be one of: 'default', 'psd', 'complex', 'magnitude', 'angle', 'phase'" % mode))
same_data = (y is x)
if ((not same_data) and (mode != 'psd')):
    raise ValueError("x and y must be equal if mode is not 'psd'")
axis = int(axis)
x = numpy.asarray(x)
if (not same_data):
    y = numpy.asarray(y)
    outdtype = numpy.result_type(x, y, numpy.complex64)
else:
    outdtype = numpy.result_type(x, numpy.complex64)
if (not same_data):
    xouter = list(x.shape)
    youter = list(y.shape)
    xouter.pop(axis)
    youter.pop(axis)
    try:
        outershape = np.broadcast(np.empty(xouter), np.empty(youter)).shape
    except ValueError:
        raise ValueError('x and y cannot be broadcast together.')
if same_data:
    if (x.size == 0):
        return (numpy.empty(x.shape), numpy.empty(x.shape), numpy.empty(x.shape))
elif ((x.size == 0) or (y.size == 0)):
    outshape = (outershape + (min([x.shape[axis], y.shape[axis]]),))
    emptyout = numpy.rollaxis(numpy.empty(outshape), (- 1), axis)
    return (emptyout, emptyout, emptyout)
if (x.ndim > 1):
    if (axis != (- 1)):
        x = numpy.rollaxis(x, axis, len(x.shape))
        if ((not same_data) and (y.ndim > 1)):
            y = numpy.rollaxis(y, axis, len(y.shape))
if (not same_data):
    if (x.shape[(- 1)] != y.shape[(- 1)]):
        if (x.shape[(- 1)] < y.shape[(- 1)]):
            pad_shape = list(x.shape)
            pad_shape[(- 1)] = (y.shape[(- 1)] - x.shape[(- 1)])
            x = numpy.concatenate((x, numpy.zeros(pad_shape)), (- 1))
        else:
            pad_shape = list(y.shape)
            pad_shape[(- 1)] = (x.shape[(- 1)] - y.shape[(- 1)])
            y = numpy.concatenate((y, numpy.zeros(pad_shape)), (- 1))
if (x.shape[(- 1)] < nperseg):
    warnings.warn('nperseg = {0:d}, is greater than input length = {1:d}, using nperseg = {1:d}'.format(nperseg, x.shape[(- 1)]))
    nperseg = x.shape[(- 1)]
nperseg = int(nperseg)
if (nperseg < 1):
    raise ValueError('nperseg must be a positive integer')
if (nfft is None):
    nfft = nperseg
elif (nfft < nperseg):
    raise ValueError('nfft must be greater than or equal to nperseg.')
else:
    nfft = int(nfft)
if (noverlap is None):
    noverlap = (nperseg // 2)
elif (noverlap >= nperseg):
    raise ValueError('noverlap must be less than nperseg.')
else:
    noverlap = int(noverlap)
if (not detrend):

    def detrend_func(d):
        return d
elif (not hasattr(detrend, '__call__')):

    def detrend_func(d):
        return scipy.signal.signaltools.detrend(d, type=detrend, axis=(- 1))
elif (axis != (- 1)):

    def detrend_func(d):
        d = numpy.rollaxis(d, (- 1), axis)
        d = detrend(d)
        return numpy.rollaxis(d, axis, len(d.shape))
else:
    detrend_func = detrend
if (isinstance(window, string_types) or (type(window) is tuple)):
    win = get_window(window, nperseg)
else:
    win = numpy.asarray(window)
    if (len(win.shape) != 1):
        raise ValueError('window must be 1-D')
    if (win.shape[0] != nperseg):
        raise ValueError('window must have length of nperseg')
if (numpy.result_type(win, numpy.complex64) != outdtype):
    win = win.astype(outdtype)
if (mode == 'psd'):
    if (scaling == 'density'):
        scale = (1.0 / (fs * (win * win).sum()))
    elif (scaling == 'spectrum'):
        scale = (1.0 / (win.sum() ** 2))
    else:
        raise ValueError(('Unknown scaling: %r' % scaling))
else:
    scale = 1
if (return_onesided is True):
    if numpy.iscomplexobj(x):
        sides = 'twosided'
    else:
        sides = 'onesided'
        if (not same_data):
            if numpy.iscomplexobj(y):
                sides = 'twosided'
else:
    sides = 'twosided'
if (sides == 'twosided'):
    num_freqs = nfft
elif (sides == 'onesided'):
    if (nfft % 2):
        num_freqs = ((nfft + 1) // 2)
    else:
        num_freqs = ((nfft // 2) + 1)
result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft)
result = result[..., :num_freqs]
freqs = scipy.fftpack.fftfreq(nfft, (1 / fs))[:num_freqs]
if (not same_data):
    result_y = _fft_helper(y, win, detrend_func, nperseg, noverlap, nfft)
    result_y = result_y[..., :num_freqs]
    result = (numpy.conjugate(result) * result_y)
elif (mode == 'psd'):
    result = (numpy.conjugate(result) * result)
elif (mode == 'magnitude'):
    result = numpy.absolute(result)
elif ((mode == 'angle') or (mode == 'phase')):
    result = numpy.angle(result)
elif (mode == 'complex'):
    pass
result *= scale
if (sides == 'onesided'):
    if (nfft % 2):
        result[..., 1:] *= 2
    else:
        result[..., 1:(- 1)] *= 2
tempResult = arange((nperseg / 2), ((x.shape[(- 1)] - (nperseg / 2)) + 1), (nperseg - noverlap))
	
===================================================================	
_write_annot: 928	
----------------------------	

'Write a Freesurfer annotation to a .annot file.\n\n    Parameters\n    ----------\n    fname : str\n        Path to annotation file\n    annot : numpy array, shape=(n_verts)\n        Annotation id at each vertex. Note: IDs must be computed from\n        RGBA colors, otherwise the mapping will be invalid.\n    ctab : numpy array, shape=(n_entries, 4)\n        RGBA colortable array.\n    names : list of str\n        List of region names to be stored in the annot file\n    '
with open(fname, 'wb') as fid:
    n_verts = len(annot)
    np.array(n_verts, dtype='>i4').tofile(fid)
    data = numpy.zeros((n_verts, 2), dtype='>i4')
    tempResult = arange(n_verts)
	
===================================================================	
Label.get_vertices_used: 291	
----------------------------	

"Get the source space's vertices inside the label.\n\n        Parameters\n        ----------\n        vertices : ndarray of int, shape (n_vertices,) | None\n            The set of vertices to compare the label to. If None, equals to\n            ``np.arange(10242)``. Defaults to None.\n\n        Returns\n        -------\n        label_verts : ndarray of in, shape (n_label_vertices,)\n            The vertices of the label corresponding used by the data.\n        "
if (vertices is None):
    tempResult = arange(10242)
	
===================================================================	
Label.__sub__: 203	
----------------------------	

'Subtract BiHemiLabels.'
if isinstance(other, BiHemiLabel):
    if (self.hemi == 'lh'):
        return (self - other.lh)
    else:
        return (self - other.rh)
elif isinstance(other, Label):
    if (self.subject != other.subject):
        raise ValueError(('Label subject parameters must match, got "%s" and "%s". Consider setting the subject parameter on initialization, or setting label.subject manually before combining labels.' % (self.subject, other.subject)))
else:
    raise TypeError(('Need: Label or BiHemiLabel. Got: %r' % other))
if (self.hemi == other.hemi):
    keep = numpy.in1d(self.vertices, other.vertices, True, invert=True)
else:
    tempResult = arange(len(self.vertices))
	
===================================================================	
Report._render_one_bem_axis: 704	
----------------------------	

'Render one axis of bem contours (only PNG).'
orientation_name2axis = dict(sagittal=0, axial=1, coronal=2)
orientation_axis = orientation_name2axis[orientation]
n_slices = shape[orientation_axis]
orig_size = numpy.roll(shape, orientation_axis)[[1, 2]]
name = orientation
html = []
html.append('<div class="col-xs-6 col-md-4">')
slides_klass = ('%s-%s' % (name, global_id))
tempResult = arange(0, n_slices, decim)
	
===================================================================	
Report.add_slider_to_section: 494	
----------------------------	

"Render a slider of figs to the report.\n\n        Parameters\n        ----------\n        figs : list of figures.\n            Each figure in the list can be an instance of\n            matplotlib.pyplot.Figure, mayavi.core.scene.Scene,\n            or np.ndarray (images read in using scipy.imread).\n        captions : list of str | list of float | None\n            A list of captions to the figures. If float, a str will be\n            constructed as `%f s`. If None, it will default to\n            `Data slice %d`.\n        section : str\n            Name of the section. If section already exists, the figures\n            will be appended to the end of the section.\n        title : str\n            The title of the slider.\n        scale : float | None | callable\n            Scale the images maintaining the aspect ratio.\n            If None, no scaling is applied. If float, scale will determine\n            the relative scaling (might not work for scale <= 1 depending on\n            font sizes). If function, should take a figure object as input\n            parameter. Defaults to None.\n        image_format : str | None\n            The image format to be used for the report, can be 'png' or 'svd'.\n            None (default) will use the default specified during Report\n            class construction.\n\n        Notes\n        -----\n        .. versionadded:: 0.10.0\n        "
_check_scale(scale)
image_format = _check_image_format(self, image_format)
if (not isinstance(figs[0], list)):
    figs = [figs]
else:
    raise NotImplementedError('`add_slider_to_section` can only add one slider at a time.')
(figs, _, _) = self._validate_input(figs, section, section)
sectionvar = self._sectionvars[section]
self._sectionlabels.append(sectionvar)
global_id = self._get_id()
img_klass = self._sectionvars[section]
name = 'slider'
html = []
slides_klass = ('%s-%s' % (name, global_id))
div_klass = ('span12 %s' % slides_klass)
if isinstance(figs[0], list):
    figs = figs[0]
tempResult = arange(0, len(figs))
	
===================================================================	
_morph_mult: 938	
----------------------------	

'Help morphing.\n\n    Equivalent to "data = (e[:, idx_use_data] * data)[idx_use_out]"\n    but faster.\n    '
if (len(idx_use_data) < e.shape[1]):
    if use_sparse:
        data = (e[:, idx_use_data] * data)
    else:
        tempResult = arange(data.shape[1])
	
===================================================================	
_BaseSourceEstimate._update_times: 361	
----------------------------	

'Update the times attribute after changing tmin, tmax, or tstep.'
tempResult = arange(self.shape[(- 1)])
	
===================================================================	
_center_of_mass: 572	
----------------------------	

'Find the center of mass on a surface.'
if ((values == 0).all() or (values < 0).any()):
    raise ValueError('All values must be non-negative and at least one must be non-zero, cannot compute COM')
subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
surf = read_surface(os.path.join(subjects_dir, subject, 'surf', ((hemi + '.') + surf)))
if (restrict_vertices is True):
    restrict_vertices = vertices
elif (restrict_vertices is False):
    tempResult = arange(surf[0].shape[0])
	
===================================================================	
_BaseSourceEstimate.bin: 492	
----------------------------	

'Return a source estimate object with data summarized over time bins.\n\n        Time bins of ``width`` seconds. This method is intended for\n        visualization only. No filter is applied to the data before binning,\n        making the method inappropriate as a tool for downsampling data.\n\n        Parameters\n        ----------\n        width : scalar\n            Width of the individual bins in seconds.\n        tstart : scalar | None\n            Time point where the first bin starts. The default is the first\n            time point of the stc.\n        tstop : scalar | None\n            Last possible time point contained in a bin (if the last bin would\n            be shorter than width it is dropped). The default is the last time\n            point of the stc.\n        func : callable\n            Function that is applied to summarize the data. Needs to accept a\n            numpy.array as first input and an ``axis`` keyword argument.\n\n        Returns\n        -------\n        stc : SourceEstimate | VectorSourceEstimate\n            The binned source estimate.\n        '
if (tstart is None):
    tstart = self.tmin
if (tstop is None):
    tstop = self.times[(- 1)]
tempResult = arange(tstart, (tstop + self.tstep), width)
	
===================================================================	
SourceEstimate.center_of_mass: 761	
----------------------------	

'Compute the center of mass of activity.\n\n        This function computes the spatial center of mass on the surface\n        as well as the temporal center of mass as in [1]_.\n\n        .. note:: All activity must occur in a single hemisphere, otherwise\n                  an error is raised. The "mass" of each point in space for\n                  computing the spatial center of mass is computed by summing\n                  across time, and vice-versa for each point in time in\n                  computing the temporal center of mass. This is useful for\n                  quantifying spatio-temporal cluster locations, especially\n                  when combined with :func:`mne.vertex_to_mni`.\n\n        Parameters\n        ----------\n        subject : string | None\n            The subject the stc is defined for.\n        hemi : int, or None\n            Calculate the center of mass for the left (0) or right (1)\n            hemisphere. If None, one of the hemispheres must be all zeroes,\n            and the center of mass will be calculated for the other\n            hemisphere (useful for getting COM for clusters).\n        restrict_vertices : bool | array of int | instance of SourceSpaces\n            If True, returned vertex will be one from stc. Otherwise, it could\n            be any vertex from surf. If an array of int, the returned vertex\n            will come from that array. If instance of SourceSpaces (as of\n            0.13), the returned vertex will be from the given source space.\n            For most accuruate estimates, do not restrict vertices.\n        subjects_dir : str, or None\n            Path to the SUBJECTS_DIR. If None, the path is obtained by using\n            the environment variable SUBJECTS_DIR.\n        surf : str\n            The surface to use for Euclidean distance center of mass\n            finding. The default here is "sphere", which finds the center\n            of mass on the spherical surface to help avoid potential issues\n            with cortical folding.\n\n        See Also\n        --------\n        mne.Label.center_of_mass\n        mne.vertex_to_mni\n\n        Returns\n        -------\n        vertex : int\n            Vertex of the spatial center of mass for the inferred hemisphere,\n            with each vertex weighted by the sum of the stc across time. For a\n            boolean stc, then, this would be weighted purely by the duration\n            each vertex was active.\n        hemi : int\n            Hemisphere the vertex was taken from.\n        t : float\n            Time of the temporal center of mass (weighted by the sum across\n            source vertices).\n\n        References\n        ----------\n        .. [1] Larson and Lee, "The cortical dynamics underlying effective\n               switching of auditory spatial attention", NeuroImage 2012.\n        '
if (not isinstance(surf, string_types)):
    raise TypeError(('surf must be a string, got %s' % (type(surf),)))
subject = _check_subject(self.subject, subject)
if numpy.any((self.data < 0)):
    raise ValueError('Cannot compute COM with negative values')
values = numpy.sum(self.data, axis=1)
tempResult = arange(len(self.vertices[0]))
	
===================================================================	
SourceEstimate.center_of_mass: 761	
----------------------------	

'Compute the center of mass of activity.\n\n        This function computes the spatial center of mass on the surface\n        as well as the temporal center of mass as in [1]_.\n\n        .. note:: All activity must occur in a single hemisphere, otherwise\n                  an error is raised. The "mass" of each point in space for\n                  computing the spatial center of mass is computed by summing\n                  across time, and vice-versa for each point in time in\n                  computing the temporal center of mass. This is useful for\n                  quantifying spatio-temporal cluster locations, especially\n                  when combined with :func:`mne.vertex_to_mni`.\n\n        Parameters\n        ----------\n        subject : string | None\n            The subject the stc is defined for.\n        hemi : int, or None\n            Calculate the center of mass for the left (0) or right (1)\n            hemisphere. If None, one of the hemispheres must be all zeroes,\n            and the center of mass will be calculated for the other\n            hemisphere (useful for getting COM for clusters).\n        restrict_vertices : bool | array of int | instance of SourceSpaces\n            If True, returned vertex will be one from stc. Otherwise, it could\n            be any vertex from surf. If an array of int, the returned vertex\n            will come from that array. If instance of SourceSpaces (as of\n            0.13), the returned vertex will be from the given source space.\n            For most accuruate estimates, do not restrict vertices.\n        subjects_dir : str, or None\n            Path to the SUBJECTS_DIR. If None, the path is obtained by using\n            the environment variable SUBJECTS_DIR.\n        surf : str\n            The surface to use for Euclidean distance center of mass\n            finding. The default here is "sphere", which finds the center\n            of mass on the spherical surface to help avoid potential issues\n            with cortical folding.\n\n        See Also\n        --------\n        mne.Label.center_of_mass\n        mne.vertex_to_mni\n\n        Returns\n        -------\n        vertex : int\n            Vertex of the spatial center of mass for the inferred hemisphere,\n            with each vertex weighted by the sum of the stc across time. For a\n            boolean stc, then, this would be weighted purely by the duration\n            each vertex was active.\n        hemi : int\n            Hemisphere the vertex was taken from.\n        t : float\n            Time of the temporal center of mass (weighted by the sum across\n            source vertices).\n\n        References\n        ----------\n        .. [1] Larson and Lee, "The cortical dynamics underlying effective\n               switching of auditory spatial attention", NeuroImage 2012.\n        '
if (not isinstance(surf, string_types)):
    raise TypeError(('surf must be a string, got %s' % (type(surf),)))
subject = _check_subject(self.subject, subject)
if numpy.any((self.data < 0)):
    raise ValueError('Cannot compute COM with negative values')
values = numpy.sum(self.data, axis=1)
tempResult = arange(len(self.vertices[1]))
	
===================================================================	
SourceEstimate.center_of_mass: 774	
----------------------------	

'Compute the center of mass of activity.\n\n        This function computes the spatial center of mass on the surface\n        as well as the temporal center of mass as in [1]_.\n\n        .. note:: All activity must occur in a single hemisphere, otherwise\n                  an error is raised. The "mass" of each point in space for\n                  computing the spatial center of mass is computed by summing\n                  across time, and vice-versa for each point in time in\n                  computing the temporal center of mass. This is useful for\n                  quantifying spatio-temporal cluster locations, especially\n                  when combined with :func:`mne.vertex_to_mni`.\n\n        Parameters\n        ----------\n        subject : string | None\n            The subject the stc is defined for.\n        hemi : int, or None\n            Calculate the center of mass for the left (0) or right (1)\n            hemisphere. If None, one of the hemispheres must be all zeroes,\n            and the center of mass will be calculated for the other\n            hemisphere (useful for getting COM for clusters).\n        restrict_vertices : bool | array of int | instance of SourceSpaces\n            If True, returned vertex will be one from stc. Otherwise, it could\n            be any vertex from surf. If an array of int, the returned vertex\n            will come from that array. If instance of SourceSpaces (as of\n            0.13), the returned vertex will be from the given source space.\n            For most accuruate estimates, do not restrict vertices.\n        subjects_dir : str, or None\n            Path to the SUBJECTS_DIR. If None, the path is obtained by using\n            the environment variable SUBJECTS_DIR.\n        surf : str\n            The surface to use for Euclidean distance center of mass\n            finding. The default here is "sphere", which finds the center\n            of mass on the spherical surface to help avoid potential issues\n            with cortical folding.\n\n        See Also\n        --------\n        mne.Label.center_of_mass\n        mne.vertex_to_mni\n\n        Returns\n        -------\n        vertex : int\n            Vertex of the spatial center of mass for the inferred hemisphere,\n            with each vertex weighted by the sum of the stc across time. For a\n            boolean stc, then, this would be weighted purely by the duration\n            each vertex was active.\n        hemi : int\n            Hemisphere the vertex was taken from.\n        t : float\n            Time of the temporal center of mass (weighted by the sum across\n            source vertices).\n\n        References\n        ----------\n        .. [1] Larson and Lee, "The cortical dynamics underlying effective\n               switching of auditory spatial attention", NeuroImage 2012.\n        '
if (not isinstance(surf, string_types)):
    raise TypeError(('surf must be a string, got %s' % (type(surf),)))
subject = _check_subject(self.subject, subject)
if numpy.any((self.data < 0)):
    raise ValueError('Cannot compute COM with negative values')
values = numpy.sum(self.data, axis=1)
vert_inds = [numpy.arange(len(self.vertices[0])), (numpy.arange(len(self.vertices[1])) + len(self.vertices[0]))]
if (hemi is None):
    hemi = numpy.where(numpy.array([numpy.sum(values[vi]) for vi in vert_inds]))[0]
    if (not (len(hemi) == 1)):
        raise ValueError('Could not infer hemisphere')
    hemi = hemi[0]
if (hemi not in [0, 1]):
    raise ValueError('hemi must be 0 or 1')
vertices = self.vertices[hemi]
values = values[vert_inds[hemi]]
del vert_inds
vertex = _center_of_mass(vertices, values, hemi=['lh', 'rh'][hemi], surf=surf, subject=subject, subjects_dir=subjects_dir, restrict_vertices=restrict_vertices)
masses = np.sum(self.data, axis=0).astype(float)
tempResult = arange(self.shape[1])
	
===================================================================	
_get_connectivity_from_edges: 1218	
----------------------------	

'Given edges sparse matrix, create connectivity matrix.'
n_vertices = edges.shape[0]
utils.logger.info(('-- number of connected vertices : %d' % n_vertices))
nnz = edges.col.size
tempResult = arange(n_times)
	
===================================================================	
_get_connectivity_from_edges: 1222	
----------------------------	

'Given edges sparse matrix, create connectivity matrix.'
n_vertices = edges.shape[0]
utils.logger.info(('-- number of connected vertices : %d' % n_vertices))
nnz = edges.col.size
aux = ((n_vertices * numpy.arange(n_times)[:, None]) * numpy.ones((1, nnz), numpy.int))
col = (edges.col[None, :] + aux).ravel()
row = (edges.row[None, :] + aux).ravel()
if (n_times > 1):
    tempResult = arange((n_times - 1))
	
===================================================================	
_get_connectivity_from_edges: 1222	
----------------------------	

'Given edges sparse matrix, create connectivity matrix.'
n_vertices = edges.shape[0]
utils.logger.info(('-- number of connected vertices : %d' % n_vertices))
nnz = edges.col.size
aux = ((n_vertices * numpy.arange(n_times)[:, None]) * numpy.ones((1, nnz), numpy.int))
col = (edges.col[None, :] + aux).ravel()
row = (edges.row[None, :] + aux).ravel()
if (n_times > 1):
    tempResult = arange(n_vertices)
	
===================================================================	
_get_connectivity_from_edges: 1223	
----------------------------	

'Given edges sparse matrix, create connectivity matrix.'
n_vertices = edges.shape[0]
utils.logger.info(('-- number of connected vertices : %d' % n_vertices))
nnz = edges.col.size
aux = ((n_vertices * numpy.arange(n_times)[:, None]) * numpy.ones((1, nnz), numpy.int))
col = (edges.col[None, :] + aux).ravel()
row = (edges.row[None, :] + aux).ravel()
if (n_times > 1):
    o = ((n_vertices * np.arange((n_times - 1))[:, None]) + np.arange(n_vertices)[None, :]).ravel()
    tempResult = arange(1, n_times)
	
===================================================================	
_get_connectivity_from_edges: 1223	
----------------------------	

'Given edges sparse matrix, create connectivity matrix.'
n_vertices = edges.shape[0]
utils.logger.info(('-- number of connected vertices : %d' % n_vertices))
nnz = edges.col.size
aux = ((n_vertices * numpy.arange(n_times)[:, None]) * numpy.ones((1, nnz), numpy.int))
col = (edges.col[None, :] + aux).ravel()
row = (edges.row[None, :] + aux).ravel()
if (n_times > 1):
    o = ((n_vertices * np.arange((n_times - 1))[:, None]) + np.arange(n_vertices)[None, :]).ravel()
    tempResult = arange(n_vertices)
	
===================================================================	
grade_to_vertices: 1061	
----------------------------	

"Convert a grade to source space vertices for a given subject.\n\n    Parameters\n    ----------\n    subject : str\n        Name of the subject\n    grade : int | list\n        Resolution of the icosahedral mesh (typically 5). If None, all\n        vertices will be used (potentially filling the surface). If a list,\n        then values will be morphed to the set of vertices specified in\n        in grade[0] and grade[1]. Note that specifying the vertices (e.g.,\n        grade=[np.arange(10242), np.arange(10242)] for fsaverage on a\n        standard grade 5 source space) can be substantially faster than\n        computing vertex locations. Note that if subject='fsaverage'\n        and 'grade=5', this set of vertices will automatically be used\n        (instead of computed) for speed, since this is a common morph.\n    subjects_dir : string, or None\n        Path to SUBJECTS_DIR if it is not set in the environment\n    n_jobs : int\n        Number of jobs to run in parallel\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    vertices : list of arrays of int\n        Vertex numbers for LH and RH\n    "
if ((subject == 'fsaverage') and (grade == 5)):
    tempResult = arange(10242)
	
===================================================================	
grade_to_vertices: 1061	
----------------------------	

"Convert a grade to source space vertices for a given subject.\n\n    Parameters\n    ----------\n    subject : str\n        Name of the subject\n    grade : int | list\n        Resolution of the icosahedral mesh (typically 5). If None, all\n        vertices will be used (potentially filling the surface). If a list,\n        then values will be morphed to the set of vertices specified in\n        in grade[0] and grade[1]. Note that specifying the vertices (e.g.,\n        grade=[np.arange(10242), np.arange(10242)] for fsaverage on a\n        standard grade 5 source space) can be substantially faster than\n        computing vertex locations. Note that if subject='fsaverage'\n        and 'grade=5', this set of vertices will automatically be used\n        (instead of computed) for speed, since this is a common morph.\n    subjects_dir : string, or None\n        Path to SUBJECTS_DIR if it is not set in the environment\n    n_jobs : int\n        Number of jobs to run in parallel\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    vertices : list of arrays of int\n        Vertex numbers for LH and RH\n    "
if ((subject == 'fsaverage') and (grade == 5)):
    tempResult = arange(10242)
	
===================================================================	
grade_to_vertices: 1082	
----------------------------	

"Convert a grade to source space vertices for a given subject.\n\n    Parameters\n    ----------\n    subject : str\n        Name of the subject\n    grade : int | list\n        Resolution of the icosahedral mesh (typically 5). If None, all\n        vertices will be used (potentially filling the surface). If a list,\n        then values will be morphed to the set of vertices specified in\n        in grade[0] and grade[1]. Note that specifying the vertices (e.g.,\n        grade=[np.arange(10242), np.arange(10242)] for fsaverage on a\n        standard grade 5 source space) can be substantially faster than\n        computing vertex locations. Note that if subject='fsaverage'\n        and 'grade=5', this set of vertices will automatically be used\n        (instead of computed) for speed, since this is a common morph.\n    subjects_dir : string, or None\n        Path to SUBJECTS_DIR if it is not set in the environment\n    n_jobs : int\n        Number of jobs to run in parallel\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    vertices : list of arrays of int\n        Vertex numbers for LH and RH\n    "
if ((subject == 'fsaverage') and (grade == 5)):
    return [numpy.arange(10242), numpy.arange(10242)]
subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
spheres_to = [os.path.join(subjects_dir, subject, 'surf', (xh + '.sphere.reg')) for xh in ['lh', 'rh']]
(lhs, rhs) = [read_surface(s)[0] for s in spheres_to]
if (grade is not None):
    if isinstance(grade, list):
        if (not (len(grade) == 2)):
            raise ValueError('grade as a list must have two elements (arrays of output vertices)')
        vertices = grade
    else:
        ico = _get_ico_tris(grade, return_surf=True)
        lhs /= numpy.sqrt(numpy.sum((lhs ** 2), axis=1))[:, None]
        rhs /= numpy.sqrt(numpy.sum((rhs ** 2), axis=1))[:, None]
        (parallel, my_compute_nearest, _) = parallel_func(_compute_nearest, n_jobs)
        (lhs, rhs, rr) = [a.astype(numpy.float32) for a in [lhs, rhs, ico['rr']]]
        vertices = parallel((my_compute_nearest(xhs, rr) for xhs in [lhs, rhs]))
        vertices = [numpy.sort(verts) for verts in vertices]
        for verts in vertices:
            if (np.diff(verts) == 0).any():
                raise ValueError(('Cannot use icosahedral grade %s with subject %s, mapping %s vertices onto the high-resolution mesh yields repeated vertices, use a lower grade or a list of vertices from an existing source space' % (grade, subject, len(verts))))
else:
    tempResult = arange(lhs.shape[0])
	
===================================================================	
grade_to_vertices: 1082	
----------------------------	

"Convert a grade to source space vertices for a given subject.\n\n    Parameters\n    ----------\n    subject : str\n        Name of the subject\n    grade : int | list\n        Resolution of the icosahedral mesh (typically 5). If None, all\n        vertices will be used (potentially filling the surface). If a list,\n        then values will be morphed to the set of vertices specified in\n        in grade[0] and grade[1]. Note that specifying the vertices (e.g.,\n        grade=[np.arange(10242), np.arange(10242)] for fsaverage on a\n        standard grade 5 source space) can be substantially faster than\n        computing vertex locations. Note that if subject='fsaverage'\n        and 'grade=5', this set of vertices will automatically be used\n        (instead of computed) for speed, since this is a common morph.\n    subjects_dir : string, or None\n        Path to SUBJECTS_DIR if it is not set in the environment\n    n_jobs : int\n        Number of jobs to run in parallel\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    vertices : list of arrays of int\n        Vertex numbers for LH and RH\n    "
if ((subject == 'fsaverage') and (grade == 5)):
    return [numpy.arange(10242), numpy.arange(10242)]
subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
spheres_to = [os.path.join(subjects_dir, subject, 'surf', (xh + '.sphere.reg')) for xh in ['lh', 'rh']]
(lhs, rhs) = [read_surface(s)[0] for s in spheres_to]
if (grade is not None):
    if isinstance(grade, list):
        if (not (len(grade) == 2)):
            raise ValueError('grade as a list must have two elements (arrays of output vertices)')
        vertices = grade
    else:
        ico = _get_ico_tris(grade, return_surf=True)
        lhs /= numpy.sqrt(numpy.sum((lhs ** 2), axis=1))[:, None]
        rhs /= numpy.sqrt(numpy.sum((rhs ** 2), axis=1))[:, None]
        (parallel, my_compute_nearest, _) = parallel_func(_compute_nearest, n_jobs)
        (lhs, rhs, rr) = [a.astype(numpy.float32) for a in [lhs, rhs, ico['rr']]]
        vertices = parallel((my_compute_nearest(xhs, rr) for xhs in [lhs, rhs]))
        vertices = [numpy.sort(verts) for verts in vertices]
        for verts in vertices:
            if (np.diff(verts) == 0).any():
                raise ValueError(('Cannot use icosahedral grade %s with subject %s, mapping %s vertices onto the high-resolution mesh yields repeated vertices, use a lower grade or a list of vertices from an existing source space' % (grade, subject, len(verts))))
else:
    tempResult = arange(rhs.shape[0])
	
===================================================================	
_make_discrete_source_space: 904	
----------------------------	

'Use a discrete set of source locs/oris to make src space.\n\n    Parameters\n    ----------\n    pos : dict\n        Must have entries "rr" and "nn". Data should be in meters.\n    coord_frame : str\n        The coordinate frame in which the positions are given; default: \'mri\'.\n        The frame must be one defined in transforms.py:_str_to_frame\n\n    Returns\n    -------\n    src : dict\n        The source space.\n    '
if (coord_frame not in _str_to_frame):
    raise KeyError(('coord_frame must be one of %s, not "%s"' % (list(transforms._str_to_frame.keys()), coord_frame)))
coord_frame = _str_to_frame[coord_frame]
rr = numpy.array(pos['rr'], float)
nn = numpy.array(pos['nn'], float)
if (not ((rr.ndim == nn.ndim == 2) and (nn.shape[0] == nn.shape[0]) and (rr.shape[1] == nn.shape[1]))):
    raise RuntimeError('"rr" and "nn" must both be 2D arrays with the same number of rows and 3 columns')
npts = rr.shape[0]
_normalize_vectors(nn)
nz = numpy.sum((numpy.sum((nn * nn), axis=1) == 0))
if (nz != 0):
    raise RuntimeError(('%d sources have zero length normal' % nz))
utils.logger.info('Positions (in meters) and orientations')
utils.logger.info(('%d sources' % npts))
tempResult = arange(npts)
	
===================================================================	
add_source_space_distances: 1286	
----------------------------	

"Compute inter-source distances along the cortical surface.\n\n    This function will also try to add patch info for the source space.\n    It will only occur if the ``dist_limit`` is sufficiently high that all\n    points on the surface are within ``dist_limit`` of a point in the\n    source space.\n\n    Parameters\n    ----------\n    src : instance of SourceSpaces\n        The source spaces to compute distances for.\n    dist_limit : float\n        The upper limit of distances to include (in meters).\n        Note: if limit < np.inf, scipy > 0.13 (bleeding edge as of\n        10/2013) must be installed.\n    n_jobs : int\n        Number of jobs to run in parallel. Will only use (up to) as many\n        cores as there are source spaces.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    src : instance of SourceSpaces\n        The original source spaces, with distance information added.\n        The distances are stored in src[n]['dist'].\n        Note: this function operates in-place.\n\n    Notes\n    -----\n    Requires scipy >= 0.11 (> 0.13 for `dist_limit < np.inf`).\n\n    This function can be memory- and CPU-intensive. On a high-end machine\n    (2012) running 6 jobs in parallel, an ico-5 (10242 per hemi) source space\n    takes about 10 minutes to compute all distances (`dist_limit = np.inf`).\n    With `dist_limit = 0.007`, computing distances takes about 1 minute.\n\n    We recommend computing distances once per source space and then saving\n    the source space to disk, as the computed distances will automatically be\n    stored along with the source space data for future use.\n    "
from scipy.sparse.csgraph import dijkstra
n_jobs = check_n_jobs(n_jobs)
src = _ensure_src(src)
if (not numpy.isscalar(dist_limit)):
    raise ValueError(('limit must be a scalar, got %s' % repr(dist_limit)))
if (not check_version('scipy', '0.11')):
    raise RuntimeError('scipy >= 0.11 must be installed (or > 0.13 if dist_limit < np.inf')
if (not all(((s['type'] == 'surf') for s in src))):
    raise RuntimeError('Currently all source spaces must be of surface type')
if (dist_limit < numpy.inf):
    try:
        dijkstra(scipy.sparse.csr_matrix(numpy.zeros((2, 2))), limit=1.0)
    except TypeError:
        raise RuntimeError('Cannot use "limit < np.inf" unless scipy > 0.13 is installed')
(parallel, p_fun, _) = parallel_func(_do_src_distances, n_jobs)
min_dists = list()
min_idxs = list()
utils.logger.info(('Calculating source space distances (limit=%s mm)...' % (1000 * dist_limit)))
for s in src:
    connectivity = mesh_dist(s['tris'], s['rr'])
    tempResult = arange(len(s['vertno']))
	
===================================================================	
add_source_space_distances: 1290	
----------------------------	

"Compute inter-source distances along the cortical surface.\n\n    This function will also try to add patch info for the source space.\n    It will only occur if the ``dist_limit`` is sufficiently high that all\n    points on the surface are within ``dist_limit`` of a point in the\n    source space.\n\n    Parameters\n    ----------\n    src : instance of SourceSpaces\n        The source spaces to compute distances for.\n    dist_limit : float\n        The upper limit of distances to include (in meters).\n        Note: if limit < np.inf, scipy > 0.13 (bleeding edge as of\n        10/2013) must be installed.\n    n_jobs : int\n        Number of jobs to run in parallel. Will only use (up to) as many\n        cores as there are source spaces.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    src : instance of SourceSpaces\n        The original source spaces, with distance information added.\n        The distances are stored in src[n]['dist'].\n        Note: this function operates in-place.\n\n    Notes\n    -----\n    Requires scipy >= 0.11 (> 0.13 for `dist_limit < np.inf`).\n\n    This function can be memory- and CPU-intensive. On a high-end machine\n    (2012) running 6 jobs in parallel, an ico-5 (10242 per hemi) source space\n    takes about 10 minutes to compute all distances (`dist_limit = np.inf`).\n    With `dist_limit = 0.007`, computing distances takes about 1 minute.\n\n    We recommend computing distances once per source space and then saving\n    the source space to disk, as the computed distances will automatically be\n    stored along with the source space data for future use.\n    "
from scipy.sparse.csgraph import dijkstra
n_jobs = check_n_jobs(n_jobs)
src = _ensure_src(src)
if (not numpy.isscalar(dist_limit)):
    raise ValueError(('limit must be a scalar, got %s' % repr(dist_limit)))
if (not check_version('scipy', '0.11')):
    raise RuntimeError('scipy >= 0.11 must be installed (or > 0.13 if dist_limit < np.inf')
if (not all(((s['type'] == 'surf') for s in src))):
    raise RuntimeError('Currently all source spaces must be of surface type')
if (dist_limit < numpy.inf):
    try:
        dijkstra(scipy.sparse.csr_matrix(numpy.zeros((2, 2))), limit=1.0)
    except TypeError:
        raise RuntimeError('Cannot use "limit < np.inf" unless scipy > 0.13 is installed')
(parallel, p_fun, _) = parallel_func(_do_src_distances, n_jobs)
min_dists = list()
min_idxs = list()
utils.logger.info(('Calculating source space distances (limit=%s mm)...' % (1000 * dist_limit)))
for s in src:
    connectivity = mesh_dist(s['tris'], s['rr'])
    d = parallel((p_fun(connectivity, s['vertno'], r, dist_limit) for r in numpy.array_split(numpy.arange(len(s['vertno'])), n_jobs)))
    min_idx = numpy.array([dd[1] for dd in d])
    min_dist = numpy.array([dd[2] for dd in d])
    midx = numpy.argmin(min_dist, axis=0)
    tempResult = arange(len(s['rr']))
	
===================================================================	
_make_volume_source_space: 934	
----------------------------	

'Make a source space which covers the volume bounded by surf.'
if ('rr' in surf):
    mins = numpy.min(surf['rr'], axis=0)
    maxs = numpy.max(surf['rr'], axis=0)
    cm = numpy.mean(surf['rr'], axis=0)
    maxdist = np.linalg.norm((surf['rr'] - cm), axis=1).max()
else:
    mins = (surf['r0'] - surf['R'])
    maxs = (surf['r0'] + surf['R'])
    cm = surf['r0'].copy()
    maxdist = surf['R']
utils.logger.info(('Surface CM = (%6.1f %6.1f %6.1f) mm' % ((1000 * cm[0]), (1000 * cm[1]), (1000 * cm[2]))))
utils.logger.info(('Surface fits inside a sphere with radius %6.1f mm' % (1000 * maxdist)))
utils.logger.info('Surface extent:')
for (c, mi, ma) in zip('xyz', mins, maxs):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, (1000 * mi), (1000 * ma))))
maxn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in maxs], int)
minn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in mins], int)
utils.logger.info('Grid extent:')
for (c, mi, ma) in zip('xyz', minn, maxn):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, ((1000 * mi) * grid), ((1000 * ma) * grid))))
ns = ((maxn - minn) + 1)
npts = numpy.prod(ns)
nrow = ns[0]
ncol = ns[1]
nplane = (nrow * ncol)
tempResult = arange(minn[2], (maxn[2] + 1))
	
===================================================================	
_make_volume_source_space: 934	
----------------------------	

'Make a source space which covers the volume bounded by surf.'
if ('rr' in surf):
    mins = numpy.min(surf['rr'], axis=0)
    maxs = numpy.max(surf['rr'], axis=0)
    cm = numpy.mean(surf['rr'], axis=0)
    maxdist = np.linalg.norm((surf['rr'] - cm), axis=1).max()
else:
    mins = (surf['r0'] - surf['R'])
    maxs = (surf['r0'] + surf['R'])
    cm = surf['r0'].copy()
    maxdist = surf['R']
utils.logger.info(('Surface CM = (%6.1f %6.1f %6.1f) mm' % ((1000 * cm[0]), (1000 * cm[1]), (1000 * cm[2]))))
utils.logger.info(('Surface fits inside a sphere with radius %6.1f mm' % (1000 * maxdist)))
utils.logger.info('Surface extent:')
for (c, mi, ma) in zip('xyz', mins, maxs):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, (1000 * mi), (1000 * ma))))
maxn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in maxs], int)
minn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in mins], int)
utils.logger.info('Grid extent:')
for (c, mi, ma) in zip('xyz', minn, maxn):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, ((1000 * mi) * grid), ((1000 * ma) * grid))))
ns = ((maxn - minn) + 1)
npts = numpy.prod(ns)
nrow = ns[0]
ncol = ns[1]
nplane = (nrow * ncol)
tempResult = arange(minn[1], (maxn[1] + 1))
	
===================================================================	
_make_volume_source_space: 934	
----------------------------	

'Make a source space which covers the volume bounded by surf.'
if ('rr' in surf):
    mins = numpy.min(surf['rr'], axis=0)
    maxs = numpy.max(surf['rr'], axis=0)
    cm = numpy.mean(surf['rr'], axis=0)
    maxdist = np.linalg.norm((surf['rr'] - cm), axis=1).max()
else:
    mins = (surf['r0'] - surf['R'])
    maxs = (surf['r0'] + surf['R'])
    cm = surf['r0'].copy()
    maxdist = surf['R']
utils.logger.info(('Surface CM = (%6.1f %6.1f %6.1f) mm' % ((1000 * cm[0]), (1000 * cm[1]), (1000 * cm[2]))))
utils.logger.info(('Surface fits inside a sphere with radius %6.1f mm' % (1000 * maxdist)))
utils.logger.info('Surface extent:')
for (c, mi, ma) in zip('xyz', mins, maxs):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, (1000 * mi), (1000 * ma))))
maxn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in maxs], int)
minn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in mins], int)
utils.logger.info('Grid extent:')
for (c, mi, ma) in zip('xyz', minn, maxn):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, ((1000 * mi) * grid), ((1000 * ma) * grid))))
ns = ((maxn - minn) + 1)
npts = numpy.prod(ns)
nrow = ns[0]
ncol = ns[1]
nplane = (nrow * ncol)
tempResult = arange(minn[0], (maxn[0] + 1))
	
===================================================================	
_make_volume_source_space: 961	
----------------------------	

'Make a source space which covers the volume bounded by surf.'
if ('rr' in surf):
    mins = numpy.min(surf['rr'], axis=0)
    maxs = numpy.max(surf['rr'], axis=0)
    cm = numpy.mean(surf['rr'], axis=0)
    maxdist = np.linalg.norm((surf['rr'] - cm), axis=1).max()
else:
    mins = (surf['r0'] - surf['R'])
    maxs = (surf['r0'] + surf['R'])
    cm = surf['r0'].copy()
    maxdist = surf['R']
utils.logger.info(('Surface CM = (%6.1f %6.1f %6.1f) mm' % ((1000 * cm[0]), (1000 * cm[1]), (1000 * cm[2]))))
utils.logger.info(('Surface fits inside a sphere with radius %6.1f mm' % (1000 * maxdist)))
utils.logger.info('Surface extent:')
for (c, mi, ma) in zip('xyz', mins, maxs):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, (1000 * mi), (1000 * ma))))
maxn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in maxs], int)
minn = numpy.array([((numpy.floor((numpy.abs(m) / grid)) + 1) if (m > 0) else ((- numpy.floor((numpy.abs(m) / grid))) - 1)) for m in mins], int)
utils.logger.info('Grid extent:')
for (c, mi, ma) in zip('xyz', minn, maxn):
    utils.logger.info(('    %s = %6.1f ... %6.1f mm' % (c, ((1000 * mi) * grid), ((1000 * ma) * grid))))
ns = ((maxn - minn) + 1)
npts = numpy.prod(ns)
nrow = ns[0]
ncol = ns[1]
nplane = (nrow * ncol)
rr = numpy.meshgrid(numpy.arange(minn[2], (maxn[2] + 1)), numpy.arange(minn[1], (maxn[1] + 1)), numpy.arange(minn[0], (maxn[0] + 1)), indexing='ij')
(x, y, z) = (rr[2].ravel(), rr[1].ravel(), rr[0].ravel())
rr = np.array([(x * grid), (y * grid), (z * grid)]).T
sp = dict(np=npts, nn=numpy.zeros((npts, 3)), rr=rr, inuse=numpy.ones(npts, int), type='vol', nuse=npts, coord_frame=io.constants.FIFF.FIFFV_COORD_MRI, id=(- 1), shape=ns)
sp['nn'][:, 2] = 1.0
assert (sp['rr'].shape[0] == npts)
utils.logger.info('%d sources before omitting any.', sp['nuse'])
dists = numpy.linalg.norm((sp['rr'] - cm), axis=1)
bads = numpy.where(numpy.logical_or((dists < exclude), (dists > maxdist)))[0]
sp['inuse'][bads] = False
sp['nuse'] -= len(bads)
utils.logger.info('%d sources after omitting infeasible sources.', sp['nuse'])
if ('rr' in surf):
    _filter_source_spaces(surf, mindist, None, [sp], n_jobs)
else:
    vertno = numpy.where(sp['inuse'])[0]
    bads = (numpy.linalg.norm((sp['rr'][vertno] - surf['r0']), axis=(- 1)) >= (surf['R'] - (mindist / 1000.0)))
    sp['nuse'] -= bads.sum()
    sp['inuse'][vertno[bads]] = False
    sp['vertno'] = numpy.where(sp['inuse'])[0]
    del vertno
del surf
utils.logger.info(('%d sources remaining after excluding the sources outside the surface and less than %6.1f mm inside.' % (sp['nuse'], mindist)))
if (not do_neighbors):
    if (volume_label is not None):
        raise RuntimeError('volume_label cannot be None unless do_neighbors is True')
    return sp
tempResult = arange(npts)
	
===================================================================	
_get_morph_src_reordering: 1474	
----------------------------	

'Get the reordering indices for a morphed source space.\n\n    Parameters\n    ----------\n    vertices : list\n        The vertices for the left and right hemispheres.\n    src_from : instance of SourceSpaces\n        The original source space.\n    subject_from : str\n        The source subject.\n    subject_to : str\n        The destination subject.\n    subjects_dir : string, or None\n        Path to SUBJECTS_DIR if it is not set in the environment.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    data_idx : ndarray, shape (n_vertices,)\n        The array used to reshape the data.\n    from_vertices : list\n        The right and left hemisphere vertex numbers for the "from" subject.\n    '
subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
from_vertices = list()
data_idxs = list()
offset = 0
for (ii, hemi) in enumerate(('lh', 'rh')):
    best = _get_vertex_map_nn(src_from[ii], subject_from, subject_to, hemi, subjects_dir)
    full_mapping = best[src_from[ii]['vertno']]
    used_vertices = numpy.in1d(full_mapping, vertices[ii])
    from_vertices.append(src_from[ii]['vertno'][used_vertices])
    remaining_mapping = full_mapping[used_vertices]
    if ((not numpy.array_equal(numpy.sort(remaining_mapping), vertices[ii])) or (not np.in1d(vertices[ii], full_mapping).all())):
        raise RuntimeError(('Could not map vertices, perhaps the wrong subject "%s" was provided?' % subject_from))
    implicit_mapping = numpy.argsort(remaining_mapping)
    data_idx = numpy.argsort(implicit_mapping)
    data_idx += offset
    data_idxs.append(data_idx)
    offset += len(implicit_mapping)
data_idx = numpy.concatenate(data_idxs)
tempResult = arange(sum((len(v) for v in vertices)))
	
===================================================================	
_add_interpolator: 1120	
----------------------------	

'Compute a sparse matrix to interpolate the data into an MRI volume.'
utils.logger.info(('Reading %s...' % mri_name))
header = _get_mgz_header(mri_name)
(mri_width, mri_height, mri_depth) = header['dims']
s.update(dict(mri_width=mri_width, mri_height=mri_height, mri_depth=mri_depth))
trans = header['vox2ras_tkr'].copy()
trans[:3, :] /= 1000.0
s['vox_mri_t'] = Transform('mri_voxel', 'mri', trans)
trans = scipy.linalg.inv(numpy.dot(header['vox2ras_tkr'], header['ras2vox']))
trans[:3, 3] /= 1000.0
s['mri_ras_t'] = Transform('mri', 'ras', trans)
s['mri_volume_name'] = mri_name
nvox = ((mri_width * mri_height) * mri_depth)
if (not add_interpolator):
    s['interpolator'] = scipy.sparse.csr_matrix((nvox, s['np']))
    return
_print_coord_trans(s['src_mri_t'], 'Source space : ')
_print_coord_trans(s['vox_mri_t'], 'MRI volume : ')
_print_coord_trans(s['mri_ras_t'], 'MRI volume : ')
combo_trans = combine_transforms(s['vox_mri_t'], invert_transform(s['src_mri_t']), 'mri_voxel', 'mri_voxel')
combo_trans['trans'] = combo_trans['trans'].astype(numpy.float32)
utils.logger.info('Setting up interpolation...')
data = []
indices = []
indptr = numpy.zeros((nvox + 1), numpy.int32)
for p in range(mri_depth):
    tempResult = arange(mri_width, dtype=numpy.float32)
	
===================================================================	
_add_interpolator: 1122	
----------------------------	

'Compute a sparse matrix to interpolate the data into an MRI volume.'
utils.logger.info(('Reading %s...' % mri_name))
header = _get_mgz_header(mri_name)
(mri_width, mri_height, mri_depth) = header['dims']
s.update(dict(mri_width=mri_width, mri_height=mri_height, mri_depth=mri_depth))
trans = header['vox2ras_tkr'].copy()
trans[:3, :] /= 1000.0
s['vox_mri_t'] = Transform('mri_voxel', 'mri', trans)
trans = scipy.linalg.inv(numpy.dot(header['vox2ras_tkr'], header['ras2vox']))
trans[:3, 3] /= 1000.0
s['mri_ras_t'] = Transform('mri', 'ras', trans)
s['mri_volume_name'] = mri_name
nvox = ((mri_width * mri_height) * mri_depth)
if (not add_interpolator):
    s['interpolator'] = scipy.sparse.csr_matrix((nvox, s['np']))
    return
_print_coord_trans(s['src_mri_t'], 'Source space : ')
_print_coord_trans(s['vox_mri_t'], 'MRI volume : ')
_print_coord_trans(s['mri_ras_t'], 'MRI volume : ')
combo_trans = combine_transforms(s['vox_mri_t'], invert_transform(s['src_mri_t']), 'mri_voxel', 'mri_voxel')
combo_trans['trans'] = combo_trans['trans'].astype(numpy.float32)
utils.logger.info('Setting up interpolation...')
data = []
indices = []
indptr = numpy.zeros((nvox + 1), numpy.int32)
for p in range(mri_depth):
    js = numpy.arange(mri_width, dtype=numpy.float32)
    js = np.tile(js[np.newaxis, :], (mri_height, 1)).ravel()
    tempResult = arange(mri_height, dtype=numpy.float32)
	
===================================================================	
_do_src_distances: 1321	
----------------------------	

'Compute source space distances in chunks.'
from scipy.sparse.csgraph import dijkstra
if (limit < numpy.inf):
    func = partial(dijkstra, limit=limit)
else:
    func = dijkstra
chunk_size = 20
tempResult = arange(0, len(run_inds), chunk_size)
	
===================================================================	
_do_src_distances: 1326	
----------------------------	

'Compute source space distances in chunks.'
from scipy.sparse.csgraph import dijkstra
if (limit < numpy.inf):
    func = partial(dijkstra, limit=limit)
else:
    func = dijkstra
chunk_size = 20
lims = numpy.r_[(numpy.arange(0, len(run_inds), chunk_size), len(run_inds))]
n_chunks = (len(lims) - 1)
d = numpy.empty((len(run_inds), len(vertno)), numpy.float32)
min_dist = numpy.empty((n_chunks, con.shape[0]))
min_idx = numpy.empty((n_chunks, con.shape[0]), numpy.int32)
tempResult = arange(con.shape[0])
	
===================================================================	
_tessellate_sphere: 302	
----------------------------	

'Create a tessellation of a unit sphere.'
rr = numpy.array([[1, 0, 0], [(- 1), 0, 0], [0, 1, 0], [0, (- 1), 0], [0, 0, 1], [0, 0, (- 1)]], float)
tris = numpy.array([[0, 4, 2], [2, 4, 1], [1, 4, 3], [3, 4, 0], [0, 2, 5], [2, 1, 5], [1, 3, 5], [3, 0, 5]], int)
if (mylevel < 1):
    raise ValueError('# of levels must be >= 1')
tris = tris[:, [2, 1, 0]]
for _ in range(1, mylevel):
    '\n        Subdivide each triangle in the old approximation and normalize\n        the new points thus generated to lie on the surface of the unit\n        sphere.\n\n        Each input triangle with vertices labelled [0,1,2] as shown\n        below will be turned into four new triangles:\n\n                             Make new points\n                             a = (0+2)/2\n                             b = (0+1)/2\n                             c = (1+2)/2\n                 1\n                /\\           Normalize a, b, c\n               /  \\\n             b/____\\c        Construct new triangles\n             /\\    /\\        [0,b,a]\n            /  \\  /  \\       [b,1,c]\n           /____\\/____\\      [a,b,c]\n          0     a      2     [a,c,2]\n\n        '
    a = _norm_midpt(tris[:, 0], tris[:, 2], rr)
    b = _norm_midpt(tris[:, 0], tris[:, 1], rr)
    c = _norm_midpt(tris[:, 1], tris[:, 2], rr)
    lims = numpy.cumsum([len(rr), len(a), len(b), len(c)])
    tempResult = arange(lims[0], lims[1])
	
===================================================================	
_tessellate_sphere: 303	
----------------------------	

'Create a tessellation of a unit sphere.'
rr = numpy.array([[1, 0, 0], [(- 1), 0, 0], [0, 1, 0], [0, (- 1), 0], [0, 0, 1], [0, 0, (- 1)]], float)
tris = numpy.array([[0, 4, 2], [2, 4, 1], [1, 4, 3], [3, 4, 0], [0, 2, 5], [2, 1, 5], [1, 3, 5], [3, 0, 5]], int)
if (mylevel < 1):
    raise ValueError('# of levels must be >= 1')
tris = tris[:, [2, 1, 0]]
for _ in range(1, mylevel):
    '\n        Subdivide each triangle in the old approximation and normalize\n        the new points thus generated to lie on the surface of the unit\n        sphere.\n\n        Each input triangle with vertices labelled [0,1,2] as shown\n        below will be turned into four new triangles:\n\n                             Make new points\n                             a = (0+2)/2\n                             b = (0+1)/2\n                             c = (1+2)/2\n                 1\n                /\\           Normalize a, b, c\n               /  \\\n             b/____\\c        Construct new triangles\n             /\\    /\\        [0,b,a]\n            /  \\  /  \\       [b,1,c]\n           /____\\/____\\      [a,b,c]\n          0     a      2     [a,c,2]\n\n        '
    a = _norm_midpt(tris[:, 0], tris[:, 2], rr)
    b = _norm_midpt(tris[:, 0], tris[:, 1], rr)
    c = _norm_midpt(tris[:, 1], tris[:, 2], rr)
    lims = numpy.cumsum([len(rr), len(a), len(b), len(c)])
    aidx = numpy.arange(lims[0], lims[1])
    tempResult = arange(lims[1], lims[2])
	
===================================================================	
_tessellate_sphere: 304	
----------------------------	

'Create a tessellation of a unit sphere.'
rr = numpy.array([[1, 0, 0], [(- 1), 0, 0], [0, 1, 0], [0, (- 1), 0], [0, 0, 1], [0, 0, (- 1)]], float)
tris = numpy.array([[0, 4, 2], [2, 4, 1], [1, 4, 3], [3, 4, 0], [0, 2, 5], [2, 1, 5], [1, 3, 5], [3, 0, 5]], int)
if (mylevel < 1):
    raise ValueError('# of levels must be >= 1')
tris = tris[:, [2, 1, 0]]
for _ in range(1, mylevel):
    '\n        Subdivide each triangle in the old approximation and normalize\n        the new points thus generated to lie on the surface of the unit\n        sphere.\n\n        Each input triangle with vertices labelled [0,1,2] as shown\n        below will be turned into four new triangles:\n\n                             Make new points\n                             a = (0+2)/2\n                             b = (0+1)/2\n                             c = (1+2)/2\n                 1\n                /\\           Normalize a, b, c\n               /  \\\n             b/____\\c        Construct new triangles\n             /\\    /\\        [0,b,a]\n            /  \\  /  \\       [b,1,c]\n           /____\\/____\\      [a,b,c]\n          0     a      2     [a,c,2]\n\n        '
    a = _norm_midpt(tris[:, 0], tris[:, 2], rr)
    b = _norm_midpt(tris[:, 0], tris[:, 1], rr)
    c = _norm_midpt(tris[:, 1], tris[:, 2], rr)
    lims = numpy.cumsum([len(rr), len(a), len(b), len(c)])
    aidx = numpy.arange(lims[0], lims[1])
    bidx = numpy.arange(lims[1], lims[2])
    tempResult = arange(lims[2], lims[3])
	
===================================================================	
_create_surf_spacing: 365	
----------------------------	

'Load a surf and use the subdivided icosahedron to get points.'
surf = read_surface(surf, return_dict=True)[(- 1)]
complete_surface_info(surf, copy=False)
if (stype == 'all'):
    surf['inuse'] = numpy.ones(surf['np'], int)
    surf['use_tris'] = None
else:
    surf_name = os.path.join(subjects_dir, subject, 'surf', (hemi + '.sphere'))
    utils.logger.info(('Loading geometry from %s...' % surf_name))
    from_surf = read_surface(surf_name, return_dict=True)[(- 1)]
    complete_surface_info(from_surf, copy=False)
    _normalize_vectors(from_surf['rr'])
    if (from_surf['np'] != surf['np']):
        raise RuntimeError('Mismatch between number of surface vertices, possible parcellation error?')
    _normalize_vectors(ico_surf['rr'])
    mmap = _compute_nearest(from_surf['rr'], ico_surf['rr'])
    nmap = len(mmap)
    surf['inuse'] = numpy.zeros(surf['np'], int)
    for k in range(nmap):
        if surf['inuse'][mmap[k]]:
            neigh = _get_surf_neighbors(surf, mmap[k])
            was = mmap[k]
            inds = numpy.where(numpy.logical_not(surf['inuse'][neigh]))[0]
            if (len(inds) == 0):
                raise RuntimeError(('Could not find neighbor for vertex %d / %d' % (k, nmap)))
            else:
                mmap[k] = neigh[inds[(- 1)]]
            utils.logger.info('    Source space vertex moved from %d to %d because of double occupation', was, mmap[k])
        elif ((mmap[k] < 0) or (mmap[k] > surf['np'])):
            raise RuntimeError(('Map number out of range (%d), this is probably due to inconsistent surfaces. Parts of the FreeSurfer reconstruction need to be redone.' % mmap[k]))
        surf['inuse'][mmap[k]] = True
    utils.logger.info('Setting up the triangulation for the decimated surface...')
    surf['use_tris'] = numpy.array([mmap[ist] for ist in ico_surf['tris']], numpy.int32)
if (surf['use_tris'] is not None):
    surf['nuse_tri'] = len(surf['use_tris'])
else:
    surf['nuse_tri'] = 0
surf['nuse'] = numpy.sum(surf['inuse'])
surf['vertno'] = numpy.where(surf['inuse'])[0]
tempResult = arange(surf['np'])
	
===================================================================	
_find_nearest_tri_pt: 596	
----------------------------	

'Find nearest point mapping to a set of triangles.\n\n    If run_all is False, if the point lies within a triangle, it stops.\n    If run_all is True, edges of other triangles are checked in case\n    those (somehow) are closer.\n    '
if (pt_tris is None):
    pt_tris = slice(len(tri_geom['r1']))
rrs = (rr - tri_geom['r1'][pt_tris])
tri_nn = tri_geom['nn'][pt_tris]
vect = numpy.einsum('ijk,ik->ij', tri_geom['r1213'][pt_tris], rrs)
mats = tri_geom['mat'][pt_tris]
pqs = numpy.einsum('ijk,ik->ji', mats, vect)
found = False
dists = numpy.sum((rrs * tri_nn), axis=1)
idx = numpy.where(numpy.all((pqs >= 0.0), axis=0))[0]
idx = idx[numpy.where(numpy.all((pqs[:, idx] <= 1.0), axis=0))[0]]
idx = idx[numpy.where((numpy.sum(pqs[:, idx], axis=0) < 1.0))[0]]
dist = numpy.inf
if (len(idx) > 0):
    found = True
    pt = idx[numpy.argmin(numpy.abs(dists[idx]))]
    (p, q) = pqs[:, pt]
    dist = dists[pt]
    if (not isinstance(pt_tris, slice)):
        pt = pt_tris[pt]
if ((found is False) or (run_all is True)):
    tempResult = arange(dists.shape[0])
	
===================================================================	
_get_solids: 678	
----------------------------	

'Compute _sum_solids_div total angle in chunks.'
tot_angle = numpy.zeros(len(fros))
tempResult = arange(0, len(fros), 100)
	
===================================================================	
_make_morph_map_hemi: 569	
----------------------------	

'Construct morph map for one hemisphere.'
if ((subject_from == subject_to) and (reg_from == reg_to)):
    fname = os.path.join(subjects_dir, subject_from, 'surf', reg_from)
    n_pts = len(read_surface(fname, verbose=False)[0])
    return speye(n_pts, n_pts, format='csr')
fname = os.path.join(subjects_dir, subject_from, 'surf', reg_from)
(from_rr, from_tri) = read_surface(fname, verbose=False)
fname = os.path.join(subjects_dir, subject_to, 'surf', reg_to)
to_rr = read_surface(fname, verbose=False)[0]
_normalize_vectors(from_rr)
_normalize_vectors(to_rr)
nn_pts_idx = _compute_nearest(from_rr, to_rr)
from_pt_tris = _triangle_neighbors(from_tri, len(from_rr))
from_pt_tris = [from_pt_tris[pt_idx] for pt_idx in nn_pts_idx]
tri_inds = []
weights = []
tri_geom = _get_tri_supp_geom(dict(rr=from_rr, tris=from_tri))
for (pt_tris, to_pt) in zip(from_pt_tris, to_rr):
    (p, q, idx, dist) = _find_nearest_tri_pt(to_pt, tri_geom, pt_tris, run_all=False)
    tri_inds.append(idx)
    weights.append([(1.0 - (p + q)), p, q])
nn_idx = from_tri[tri_inds]
weights = numpy.array(weights)
tempResult = arange(len(to_rr))
	
===================================================================	
UpdateChannelsMixin.drop_channels: 243	
----------------------------	

'Drop some channels.\n\n        Parameters\n        ----------\n        ch_names : list\n            List of the names of the channels to remove.\n\n        Returns\n        -------\n        inst : instance of Raw, Epochs, or Evoked\n            The modified instance.\n\n        See Also\n        --------\n        pick_channels\n\n        Notes\n        -----\n        .. versionadded:: 0.9.0\n        '
msg = "'ch_names' should be a list of strings (the name[s] of the channel to be dropped), not a {0}."
if isinstance(ch_names, string_types):
    raise ValueError(msg.format('string'))
elif (not all([isinstance(ch_name, string_types) for ch_name in ch_names])):
    raise ValueError(msg.format(type(ch_names[0])))
missing = [ch_name for ch_name in ch_names if (ch_name not in self.ch_names)]
if (len(missing) > 0):
    msg = 'Channel(s) {0} not found, nothing dropped.'
    raise ValueError(msg.format(', '.join(missing)))
bad_idx = [self.ch_names.index(ch_name) for ch_name in ch_names if (ch_name in self.ch_names)]
tempResult = arange(len(self.ch_names))
	
===================================================================	
generate_2d_layout: 403	
----------------------------	

"Generate a custom 2D layout from xy points.\n\n    Generates a 2-D layout for plotting with plot_topo methods and\n    functions. XY points will be normalized between 0 and 1, where\n    normalization extremes will be either the min/max of xy, or\n    the width/height of bg_image.\n\n    Parameters\n    ----------\n    xy : ndarray (N x 2)\n        The xy coordinates of sensor locations.\n    w : float\n        The width of each sensor's axis (between 0 and 1)\n    h : float\n        The height of each sensor's axis (between 0 and 1)\n    pad : float\n        Portion of the box to reserve for padding. The value can range between\n        0.0 (boxes will touch, default) to 1.0 (boxes consist of only padding).\n    ch_names : list\n        The names of each channel. Must be a list of strings, with one\n        string per channel.\n    ch_indices : list\n        Index of each channel - must be a collection of unique integers,\n        one index per channel.\n    name : string\n        The name of this layout type.\n    bg_image : str | ndarray\n        The image over which sensor axes will be plotted. Either a path to an\n        image file, or an array that can be plotted with plt.imshow. If\n        provided, xy points will be normalized by the width/height of this\n        image. If not, xy points will be normalized by their own min/max.\n    normalize : bool\n        Whether to normalize the coordinates to run from 0 to 1. Defaults to\n        True.\n\n    Returns\n    -------\n    layout : Layout\n        A Layout object that can be plotted with plot_topo\n        functions and methods.\n\n    See Also\n    --------\n    make_eeg_layout, make_grid_layout\n\n    Notes\n    -----\n    .. versionadded:: 0.9.0\n    "
from scipy.ndimage import imread
if (ch_indices is None):
    tempResult = arange(xy.shape[0])
	
===================================================================	
make_eeg_layout: 144	
----------------------------	

"Create .lout file from EEG electrode digitization.\n\n    Parameters\n    ----------\n    info : instance of Info\n        Measurement info (e.g., raw.info).\n    radius : float\n        Viewport radius as a fraction of main figure height. Defaults to 0.5.\n    width : float | None\n        Width of sensor axes as a fraction of main figure height. By default,\n        this will be the maximum width possible without axes overlapping.\n    height : float | None\n        Height of sensor axes as a fraction of main figure height. By default,\n        this will be the maximum height possible withough axes overlapping.\n    exclude : list of string | str\n        List of channels to exclude. If empty do not exclude any.\n        If 'bads', exclude channels in info['bads'] (default).\n\n    Returns\n    -------\n    layout : Layout\n        The generated Layout.\n\n    See Also\n    --------\n    make_grid_layout, generate_2d_layout\n    "
if (not (0 <= radius <= 0.5)):
    raise ValueError('The radius parameter should be between 0 and 0.5.')
if ((width is not None) and (not (0 <= width <= 1.0))):
    raise ValueError('The width parameter should be between 0 and 1.')
if ((height is not None) and (not (0 <= height <= 1.0))):
    raise ValueError('The height parameter should be between 0 and 1.')
picks = pick_types(info, meg=False, eeg=True, ref_meg=False, exclude=exclude)
loc2d = _auto_topomap_coords(info, picks)
names = [info['chs'][i]['ch_name'] for i in picks]
loc2d_min = numpy.min(loc2d, axis=0)
loc2d_max = numpy.max(loc2d, axis=0)
loc2d = ((loc2d - ((loc2d_max + loc2d_min) / 2.0)) / (loc2d_max - loc2d_min))
if ((width is None) or (height is None)):
    (width, height) = _box_size(loc2d, width, height, padding=0.1)
loc2d *= (2 * radius)
scaling = min((1 / (1.0 + width)), (1 / (1.0 + height)))
loc2d *= scaling
width *= scaling
height *= scaling
loc2d += 0.5
n_channels = loc2d.shape[0]
pos = numpy.c_[((loc2d[:, 0] - (0.5 * width)), (loc2d[:, 1] - (0.5 * height)), (width * numpy.ones(n_channels)), (height * numpy.ones(n_channels)))]
box = (0, 1, 0, 1)
tempResult = arange(n_channels)
	
===================================================================	
read_montage: 157	
----------------------------	

"Read a generic (built-in) montage.\n\n    Individualized (digitized) electrode positions should be read in using\n    :func:`read_dig_montage`.\n\n    In most cases, you should only need to set the `kind` parameter to load one\n    of the built-in montages (see Notes).\n\n    Parameters\n    ----------\n    kind : str\n        The name of the montage file without the file extension (e.g.\n        kind='easycap-M10' for 'easycap-M10.txt'). Files with extensions\n        '.elc', '.txt', '.csd', '.elp', '.hpts', '.sfp', '.loc' ('.locs' and\n        '.eloc') or .bvef are supported.\n    ch_names : list of str | None\n        If not all electrodes defined in the montage are present in the EEG\n        data, use this parameter to select a subset of electrode positions to\n        load. If None (default), all defined electrode positions are returned.\n\n        .. note:: ``ch_names`` are compared to channel names in the montage\n                  file after converting them both to upper case. If a match is\n                  found, the letter case in the original ``ch_names`` is used\n                  in the returned montage.\n\n    path : str | None\n        The path of the folder containing the montage file. Defaults to the\n        mne/channels/data/montages folder in your mne-python installation.\n    unit : 'm' | 'cm' | 'mm'\n        Unit of the input file. If not 'm' (default), coordinates will be\n        rescaled to 'm'.\n    transform : bool\n        If True, points will be transformed to Neuromag space. The fidicuals,\n        'nasion', 'lpa', 'rpa' must be specified in the montage file. Useful\n        for points captured using Polhemus FastSCAN. Default is False.\n\n    Returns\n    -------\n    montage : instance of Montage\n        The montage.\n\n    See Also\n    --------\n    DigMontage\n    Montage\n    read_dig_montage\n\n    Notes\n    -----\n    Built-in montages are not scaled or transformed by default.\n\n    Montages can contain fiducial points in addition to electrode channels,\n    e.g. ``biosemi64`` contains 67 locations. In the following table, the\n    number of channels and fiducials is given in parentheses in the description\n    column (e.g. 64+3 means 64 channels and 3 fiducials).\n\n    Valid ``kind`` arguments are:\n\n    ===================   =====================================================\n    Kind                  Description\n    ===================   =====================================================\n    standard_1005         Electrodes are named and positioned according to the\n                          international 10-05 system (343+3 locations)\n    standard_1020         Electrodes are named and positioned according to the\n                          international 10-20 system (94+3 locations)\n    standard_alphabetic   Electrodes are named with LETTER-NUMBER combinations\n                          (A1, B2, F4, ...) (65+3 locations)\n    standard_postfixed    Electrodes are named according to the international\n                          10-20 system using postfixes for intermediate\n                          positions (100+3 locations)\n    standard_prefixed     Electrodes are named according to the international\n                          10-20 system using prefixes for intermediate\n                          positions (74+3 locations)\n    standard_primed       Electrodes are named according to the international\n                          10-20 system using prime marks (' and '') for\n                          intermediate positions (100+3 locations)\n\n    biosemi16             BioSemi cap with 16 electrodes (16+3 locations)\n    biosemi32             BioSemi cap with 32 electrodes (32+3 locations)\n    biosemi64             BioSemi cap with 64 electrodes (64+3 locations)\n    biosemi128            BioSemi cap with 128 electrodes (128+3 locations)\n    biosemi160            BioSemi cap with 160 electrodes (160+3 locations)\n    biosemi256            BioSemi cap with 256 electrodes (256+3 locations)\n\n    easycap-M1            EasyCap with 10-05 electrode names (74 locations)\n    easycap-M10           EasyCap with numbered electrodes (61 locations)\n\n    EGI_256               Geodesic Sensor Net (256 locations)\n\n    GSN-HydroCel-32       HydroCel Geodesic Sensor Net and Cz (33+3 locations)\n    GSN-HydroCel-64_1.0   HydroCel Geodesic Sensor Net (64+3 locations)\n    GSN-HydroCel-65_1.0   HydroCel Geodesic Sensor Net and Cz (65+3 locations)\n    GSN-HydroCel-128      HydroCel Geodesic Sensor Net (128+3 locations)\n    GSN-HydroCel-129      HydroCel Geodesic Sensor Net and Cz (129+3 locations)\n    GSN-HydroCel-256      HydroCel Geodesic Sensor Net (256+3 locations)\n    GSN-HydroCel-257      HydroCel Geodesic Sensor Net and Cz (257+3 locations)\n\n    mgh60                 The (older) 60-channel cap used at\n                          MGH (60+3 locations)\n    mgh70                 The (newer) 70-channel BrainVision cap used at\n                          MGH (70+3 locations)\n    ===================   =====================================================\n\n    .. versionadded:: 0.9.0\n    "
if (path is None):
    path = os.path.join(os.path.dirname(__file__), 'data', 'montages')
if (not os.path.isabs(kind)):
    supported = ('.elc', '.txt', '.csd', '.sfp', '.elp', '.hpts', '.loc', '.locs', '.eloc', '.bvef')
    montages = [os.path.splitext(f) for f in os.listdir(path)]
    montages = [m for m in montages if ((m[1] in supported) and (kind == m[0]))]
    if (len(montages) != 1):
        raise ValueError('Could not find the montage. Please provide the full path.')
    (kind, ext) = montages[0]
else:
    (kind, ext) = os.path.splitext(kind)
fname = os.path.join(path, (kind + ext))
fid_names = ['lpa', 'nz', 'rpa']
if (ext == '.sfp'):
    fid_names = ['fidt9', 'fidnz', 'fidt10']
    with open(fname, 'r') as f:
        lines = f.read().replace('\t', ' ').splitlines()
    (ch_names_, pos) = ([], [])
    for (ii, line) in enumerate(lines):
        line = line.strip().split()
        if (len(line) > 0):
            if (len(line) != 4):
                raise ValueError(('Malformed .sfp file in line ' + str(ii)))
            (this_name, x, y, z) = line
            ch_names_.append(this_name)
            pos.append([float(cord) for cord in (x, y, z)])
    pos = numpy.asarray(pos)
elif (ext == '.elc'):
    ch_names_ = []
    pos = []
    with open(fname) as fid:
        for line in fid:
            if ('UnitPosition' in line):
                units = line.split()[1]
                scale_factor = dict(m=1.0, mm=0.001)[units]
                break
        else:
            raise RuntimeError(('Could not detect units in file %s' % fname))
        for line in fid:
            if ('Positions\n' in line):
                break
        pos = []
        for line in fid:
            if ('Labels\n' in line):
                break
            pos.append(list(map(float, line.split())))
        for line in fid:
            if ((not line) or (not (set(line) - set([' '])))):
                break
            ch_names_.append(line.strip(' ').strip('\n'))
    pos = (numpy.array(pos) * scale_factor)
elif (ext == '.txt'):
    try:
        data = numpy.genfromtxt(fname, dtype='str', skip_header=1)
    except TypeError:
        data = numpy.genfromtxt(fname, dtype='str', skiprows=1)
    ch_names_ = data[:, 0].tolist()
    az = numpy.deg2rad(data[:, 2].astype(float))
    pol = numpy.deg2rad(data[:, 1].astype(float))
    pos = _sph_to_cart(np.array([(np.ones(len(az)) * 85.0), az, pol]).T)
elif (ext == '.csd'):
    try:
        data = numpy.genfromtxt(fname, dtype='str', skip_header=2)
    except TypeError:
        data = numpy.genfromtxt(fname, dtype='str', skiprows=2)
    ch_names_ = data[:, 0].tolist()
    az = numpy.deg2rad(data[:, 1].astype(float))
    pol = numpy.deg2rad((90.0 - data[:, 2].astype(float)))
    pos = _sph_to_cart(np.array([np.ones(len(az)), az, pol]).T)
elif (ext == '.elp'):
    dtype = numpy.dtype('S8, S8, f8, f8, f8')
    try:
        data = numpy.loadtxt(fname, dtype=dtype, skip_header=1)
    except TypeError:
        data = numpy.loadtxt(fname, dtype=dtype, skiprows=1)
    ch_names_ = data['f1'].astype(str).tolist()
    az = data['f2']
    horiz = data['f3']
    radius = numpy.abs((az / 180.0))
    az = numpy.deg2rad(numpy.array([(h if (a >= 0.0) else (180 + h)) for (h, a) in zip(horiz, az)]))
    pol = (radius * numpy.pi)
    pos = _sph_to_cart(np.array([(np.ones(len(az)) * 85.0), az, pol]).T)
elif (ext == '.hpts'):
    fid_names = ['1', '2', '3']
    dtype = [('type', 'S8'), ('name', 'S8'), ('x', 'f8'), ('y', 'f8'), ('z', 'f8')]
    data = numpy.loadtxt(fname, dtype=dtype)
    ch_names_ = data['name'].astype(str).tolist()
    pos = np.vstack((data['x'], data['y'], data['z'])).T
elif (ext in ('.loc', '.locs', '.eloc')):
    ch_names_ = np.loadtxt(fname, dtype='S4', usecols=[3]).astype(str).tolist()
    topo = numpy.loadtxt(fname, dtype=float, usecols=[1, 2])
    sph = _topo_to_sph(topo)
    pos = _sph_to_cart(sph)
    pos[:, [0, 1]] = (pos[:, [1, 0]] * [(- 1), 1])
elif (ext == '.bvef'):
    root = ElementTree.parse(fname).getroot()
    ch_names_ = [s.text for s in root.findall('./Electrode/Name')]
    theta = [float(s.text) for s in root.findall('./Electrode/Theta')]
    pol = numpy.deg2rad(numpy.array(theta))
    phi = [float(s.text) for s in root.findall('./Electrode/Phi')]
    az = numpy.deg2rad(numpy.array(phi))
    pos = _sph_to_cart(np.array([(np.ones(len(az)) * 85.0), az, pol]).T)
else:
    raise ValueError(('Currently the "%s" template is not supported.' % kind))
tempResult = arange(len(pos))
	
===================================================================	
test_find_layout: 184	
----------------------------	

'Test finding layout'
import matplotlib.pyplot as plt
assert_raises(ValueError, find_layout, _get_test_info(), ch_type='meep')
sample_info = read_info(fif_fname)
grads = pick_types(sample_info, meg='grad')
sample_info2 = pick_info(sample_info, grads)
mags = pick_types(sample_info, meg='mag')
sample_info3 = pick_info(sample_info, mags)
sample_info4 = copy.deepcopy(sample_info)
for (ii, name) in enumerate(sample_info4['ch_names']):
    new = name.replace(' ', '')
    sample_info4['chs'][ii]['ch_name'] = new
eegs = pick_types(sample_info, meg=False, eeg=True)
sample_info5 = pick_info(sample_info, eegs)
lout = find_layout(sample_info, ch_type=None)
assert_equal(lout.kind, 'Vectorview-all')
assert_true(all(((' ' in k) for k in lout.names)))
lout = find_layout(sample_info2, ch_type='meg')
assert_equal(lout.kind, 'Vectorview-all')
lout = find_layout(sample_info4, ch_type=None)
assert_equal(lout.kind, 'Vectorview-all')
assert_true(all(((' ' not in k) for k in lout.names)))
lout = find_layout(sample_info, ch_type='grad')
assert_equal(lout.kind, 'Vectorview-grad')
lout = find_layout(sample_info2)
assert_equal(lout.kind, 'Vectorview-grad')
lout = find_layout(sample_info2, ch_type='grad')
assert_equal(lout.kind, 'Vectorview-grad')
lout = find_layout(sample_info2, ch_type='meg')
assert_equal(lout.kind, 'Vectorview-all')
lout = find_layout(sample_info, ch_type='mag')
assert_equal(lout.kind, 'Vectorview-mag')
lout = find_layout(sample_info3)
assert_equal(lout.kind, 'Vectorview-mag')
lout = find_layout(sample_info3, ch_type='mag')
assert_equal(lout.kind, 'Vectorview-mag')
lout = find_layout(sample_info3, ch_type='meg')
assert_equal(lout.kind, 'Vectorview-all')
lout = find_layout(sample_info, ch_type='eeg')
assert_equal(lout.kind, 'EEG')
lout = find_layout(sample_info5)
assert_equal(lout.kind, 'EEG')
lout = find_layout(sample_info5, ch_type='eeg')
assert_equal(lout.kind, 'EEG')
lout = find_layout(read_info(fname_ctf_raw))
assert_equal(lout.kind, 'CTF-275')
fname_bti_raw = os.path.join(bti_dir, 'exported4D_linux_raw.fif')
lout = find_layout(read_info(fname_bti_raw))
assert_equal(lout.kind, 'magnesWH3600')
raw_kit = read_raw_kit(fname_kit_157)
lout = find_layout(raw_kit.info)
assert_equal(lout.kind, 'KIT-157')
raw_kit.info['bads'] = ['MEG  13', 'MEG  14', 'MEG  15', 'MEG  16']
lout = find_layout(raw_kit.info)
assert_equal(lout.kind, 'KIT-157')
raw_umd = read_raw_kit(fname_kit_umd)
lout = find_layout(raw_umd.info)
assert_equal(lout.kind, 'KIT-UMD-3')
lout.plot()
tempResult = arange(10)
	
===================================================================	
test_psi: 24	
----------------------------	

'Test Phase Slope Index (PSI) estimation'
sfreq = 50.0
n_signals = 3
n_epochs = 10
n_times = 500
rng = numpy.random.RandomState(42)
data = rng.randn(n_epochs, n_signals, n_times)
for i in range(n_epochs):
    data[i, 1, 10:] = data[i, 0, :(- 10)]
    data[i, 2, :(- 10)] = data[i, 0, 10:]
(psi, freqs, times, n_epochs, n_tapers) = phase_slope_index(data, mode='fourier', sfreq=sfreq)
assert_true((psi[(1, 0, 0)] < 0))
assert_true((psi[(2, 0, 0)] > 0))
indices = (numpy.array([0]), numpy.array([1]))
(psi_2, freqs, times, n_epochs, n_tapers) = phase_slope_index(data, mode='fourier', sfreq=sfreq, indices=indices)
assert_array_almost_equal(psi_2[(0, 0)], (- psi[(1, 0, 0)]))
tempResult = arange(5.0, 20, 0.5)
	
===================================================================	
test_spectral_connectivity: 52	
----------------------------	

'Test frequency-domain connectivity methods'
rng = numpy.random.RandomState(0)
trans_bandwidth = 2.0
sfreq = 50.0
n_signals = 3
n_epochs = 8
n_times = 256
tmin = 0.0
tmax = ((n_times - 1) / sfreq)
data = rng.randn(n_signals, (n_epochs * n_times))
times_data = numpy.linspace(tmin, tmax, n_times)
(fstart, fend) = (5.0, 15.0)
data[1, :] = filter_data(data[0, :], sfreq, fstart, fend, filter_length='auto', fir_design='firwin2', l_trans_bandwidth=trans_bandwidth, h_trans_bandwidth=trans_bandwidth)
data[1, :] += (0.01 * rng.randn((n_times * n_epochs)))
data = data.reshape(n_signals, n_epochs, n_times)
data = numpy.transpose(data, [1, 0, 2])
assert_raises(ValueError, spectral_connectivity, data, method='notamethod')
assert_raises(ValueError, spectral_connectivity, data, mode='notamode')
assert_raises(ValueError, spectral_connectivity, data, fmin=10, fmax=(10 + (0.5 * (sfreq / float(n_times)))))
assert_raises(ValueError, spectral_connectivity, data, fmin=10, fmax=5)
assert_raises(ValueError, spectral_connectivity, data, fmin=(0, 11), fmax=(5, 10))
assert_raises(ValueError, spectral_connectivity, data, fmin=(11,), fmax=(12, 15))
methods = ['coh', 'cohy', 'imcoh', ['plv', 'ppc', 'pli', 'pli2_unbiased', 'wpli', 'wpli2_debiased', 'coh']]
modes = ['multitaper', 'fourier', 'cwt_morlet']
tempResult = arange(3, 24.5, 1)
	
===================================================================	
_stc_gen: 16	
----------------------------	

'Simulate a SourceEstimate generator'
tempResult = arange(data.shape[1])
	
===================================================================	
test_indices: 13	
----------------------------	

'Test connectivity indexing methods'
n_seeds_test = [1, 3, 4]
n_targets_test = [2, 3, 200]
rng = numpy.random.RandomState(42)
for n_seeds in n_seeds_test:
    for n_targets in n_targets_test:
        tempResult = arange((n_seeds + n_targets))
	
===================================================================	
CSP.plot_patterns: 130	
----------------------------	

'Plot topographic patterns of components.\n\n        The patterns explain how the measured data was generated from the\n        neural sources (a.k.a. the forward model).\n\n        Parameters\n        ----------\n        info : instance of Info\n            Info dictionary of the epochs used for fitting.\n            If not possible, consider using ``create_info``.\n        components : float | array of floats | None.\n           The patterns to plot. If None, n_components will be shown.\n        ch_type : \'mag\' | \'grad\' | \'planar1\' | \'planar2\' | \'eeg\' | None\n            The channel type to plot. For \'grad\', the gradiometers are\n            collected in pairs and the RMS for each pair is plotted.\n            If None, then first available channel type from order given\n            above is used. Defaults to None.\n        layout : None | Layout\n            Layout instance specifying sensor positions (does not need to be\n            specified for Neuromag data). If possible, the correct layout file\n            is inferred from the data; if no appropriate layout file was found\n            the layout is automatically generated from the sensor locations.\n        vmin : float | callable\n            The value specfying the lower bound of the color range.\n            If None, and vmax is None, -vmax is used. Else np.min(data).\n            If callable, the output equals vmin(data).\n        vmax : float | callable\n            The value specfying the upper bound of the color range.\n            If None, the maximum absolute value is used. If vmin is None,\n            but vmax is not, defaults to np.min(data).\n            If callable, the output equals vmax(data).\n        cmap : matplotlib colormap | (colormap, bool) | \'interactive\' | None\n            Colormap to use. If tuple, the first value indicates the colormap\n            to use and the second value is a boolean defining interactivity. In\n            interactive mode the colors are adjustable by clicking and dragging\n            the colorbar with left and right mouse button. Left mouse button\n            moves the scale up and down and right mouse button adjusts the\n            range. Hitting space bar resets the range. Up and down arrows can\n            be used to change the colormap. If None, \'Reds\' is used for all\n            positive data, otherwise defaults to \'RdBu_r\'. If \'interactive\',\n            translates to (None, True). Defaults to \'RdBu_r\'.\n\n            .. warning::  Interactive mode works smoothly only for a small\n                amount of topomaps.\n\n        sensors : bool | str\n            Add markers for sensor locations to the plot. Accepts matplotlib\n            plot format string (e.g., \'r+\' for red plusses). If True,\n            a circle will be used (via .add_artist). Defaults to True.\n        colorbar : bool\n            Plot a colorbar.\n        scalings : dict | float | None\n            The scalings of the channel types to be applied for plotting.\n            If None, defaults to ``dict(eeg=1e6, grad=1e13, mag=1e15)``.\n        units : dict | str | None\n            The unit of the channel type used for colorbar label. If\n            scale is None the unit is automatically determined.\n        res : int\n            The resolution of the topomap image (n pixels along each side).\n        size : float\n            Side length per topomap in inches.\n        cbar_fmt : str\n            String format for colorbar values.\n        name_format : str\n            String format for topomap values. Defaults to "CSP%01d"\n        show : bool\n            Show figure if True.\n        show_names : bool | callable\n            If True, show channel names on top of the map. If a callable is\n            passed, channel names will be formatted using the callable; e.g.,\n            to delete the prefix \'MEG \' from all channel names, pass the\n            function lambda x: x.replace(\'MEG \', \'\'). If `mask` is not None,\n            only significant sensors will be shown.\n        title : str | None\n            Title. If None (default), no title is displayed.\n        mask : ndarray of bool, shape (n_channels, n_times) | None\n            The channels to be marked as significant at a given time point.\n            Indices set to `True` will be considered. Defaults to None.\n        mask_params : dict | None\n            Additional plotting parameters for plotting significant sensors.\n            Default (None) equals::\n\n                dict(marker=\'o\', markerfacecolor=\'w\', markeredgecolor=\'k\',\n                     linewidth=0, markersize=4)\n\n        outlines : \'head\' | \'skirt\' | dict | None\n            The outlines to be drawn. If \'head\', the default head scheme will\n            be drawn. If \'skirt\' the head scheme will be drawn, but sensors are\n            allowed to be plotted outside of the head circle. If dict, each key\n            refers to a tuple of x and y positions, the values in \'mask_pos\'\n            will serve as image mask, and the \'autoshrink\' (bool) field will\n            trigger automated shrinking of the positions due to points outside\n            the outline. Alternatively, a matplotlib patch object can be passed\n            for advanced masking options, either directly or as a function that\n            returns patches (required for multi-axis plots). If None, nothing\n            will be drawn. Defaults to \'head\'.\n        contours : int | array of float\n            The number of contour lines to draw. If 0, no contours will be\n            drawn. When an integer, matplotlib ticker locator is used to find\n            suitable values for the contour thresholds (may sometimes be\n            inaccurate, use array for accuracy). If an array, the values\n            represent the levels for the contours. Defaults to 6.\n        image_interp : str\n            The image interpolation to be used.\n            All matplotlib options are accepted.\n        average : float | None\n            The time window around a given time to be used for averaging\n            (seconds). For example, 0.01 would translate into window that\n            starts 5 ms before and ends 5 ms after a given time point.\n            Defaults to None, which means no averaging.\n        head_pos : dict | None\n            If None (default), the sensors are positioned such that they span\n            the head circle. If dict, can have entries \'center\' (tuple) and\n            \'scale\' (tuple) for what the center and scale of the head\n            should be relative to the electrode locations.\n\n        Returns\n        -------\n        fig : instance of matplotlib.figure.Figure\n           The figure.\n        '
from .. import EvokedArray
if (components is None):
    tempResult = arange(self.n_components)
	
===================================================================	
CSP.plot_filters: 140	
----------------------------	

'Plot topographic filters of components.\n\n        The filters are used to extract discriminant neural sources from\n        the measured data (a.k.a. the backward model).\n\n        Parameters\n        ----------\n        info : instance of Info\n            Info dictionary of the epochs used for fitting.\n            If not possible, consider using ``create_info``.\n        components : float | array of floats | None.\n           The patterns to plot. If None, n_components will be shown.\n        ch_type : \'mag\' | \'grad\' | \'planar1\' | \'planar2\' | \'eeg\' | None\n            The channel type to plot. For \'grad\', the gradiometers are\n            collected in pairs and the RMS for each pair is plotted.\n            If None, then first available channel type from order given\n            above is used. Defaults to None.\n        layout : None | Layout\n            Layout instance specifying sensor positions (does not need to be\n            specified for Neuromag data). If possible, the correct layout file\n            is inferred from the data; if no appropriate layout file was found\n            the layout is automatically generated from the sensor locations.\n        vmin : float | callable\n            The value specfying the lower bound of the color range.\n            If None, and vmax is None, -vmax is used. Else np.min(data).\n            If callable, the output equals vmin(data).\n        vmax : float | callable\n            The value specfying the upper bound of the color range.\n            If None, the maximum absolute value is used. If vmin is None,\n            but vmax is not, defaults to np.min(data).\n            If callable, the output equals vmax(data).\n        cmap : matplotlib colormap | (colormap, bool) | \'interactive\' | None\n            Colormap to use. If tuple, the first value indicates the colormap\n            to use and the second value is a boolean defining interactivity. In\n            interactive mode the colors are adjustable by clicking and dragging\n            the colorbar with left and right mouse button. Left mouse button\n            moves the scale up and down and right mouse button adjusts the\n            range. Hitting space bar resets the range. Up and down arrows can\n            be used to change the colormap. If None, \'Reds\' is used for all\n            positive data, otherwise defaults to \'RdBu_r\'. If \'interactive\',\n            translates to (None, True). Defaults to \'RdBu_r\'.\n\n            .. warning::  Interactive mode works smoothly only for a small\n                amount of topomaps.\n\n        sensors : bool | str\n            Add markers for sensor locations to the plot. Accepts matplotlib\n            plot format string (e.g., \'r+\' for red plusses). If True,\n            a circle will be used (via .add_artist). Defaults to True.\n        colorbar : bool\n            Plot a colorbar.\n        scalings : dict | float | None\n            The scalings of the channel types to be applied for plotting.\n            If None, defaults to ``dict(eeg=1e6, grad=1e13, mag=1e15)``.\n        units : dict | str | None\n            The unit of the channel type used for colorbar label. If\n            scale is None the unit is automatically determined.\n        res : int\n            The resolution of the topomap image (n pixels along each side).\n        size : float\n            Side length per topomap in inches.\n        cbar_fmt : str\n            String format for colorbar values.\n        name_format : str\n            String format for topomap values. Defaults to "CSP%01d"\n        show : bool\n            Show figure if True.\n        show_names : bool | callable\n            If True, show channel names on top of the map. If a callable is\n            passed, channel names will be formatted using the callable; e.g.,\n            to delete the prefix \'MEG \' from all channel names, pass the\n            function lambda x: x.replace(\'MEG \', \'\'). If `mask` is not None,\n            only significant sensors will be shown.\n        title : str | None\n            Title. If None (default), no title is displayed.\n        mask : ndarray of bool, shape (n_channels, n_times) | None\n            The channels to be marked as significant at a given time point.\n            Indices set to `True` will be considered. Defaults to None.\n        mask_params : dict | None\n            Additional plotting parameters for plotting significant sensors.\n            Default (None) equals::\n\n                dict(marker=\'o\', markerfacecolor=\'w\', markeredgecolor=\'k\',\n                     linewidth=0, markersize=4)\n\n        outlines : \'head\' | \'skirt\' | dict | None\n            The outlines to be drawn. If \'head\', the default head scheme will\n            be drawn. If \'skirt\' the head scheme will be drawn, but sensors are\n            allowed to be plotted outside of the head circle. If dict, each key\n            refers to a tuple of x and y positions, the values in \'mask_pos\'\n            will serve as image mask, and the \'autoshrink\' (bool) field will\n            trigger automated shrinking of the positions due to points outside\n            the outline. Alternatively, a matplotlib patch object can be passed\n            for advanced masking options, either directly or as a function that\n            returns patches (required for multi-axis plots). If None, nothing\n            will be drawn. Defaults to \'head\'.\n        contours : int | array of float\n            The number of contour lines to draw. If 0, no contours will be\n            drawn. When an integer, matplotlib ticker locator is used to find\n            suitable values for the contour thresholds (may sometimes be\n            inaccurate, use array for accuracy). If an array, the values\n            represent the levels for the contours. Defaults to 6.\n        image_interp : str\n            The image interpolation to be used.\n            All matplotlib options are accepted.\n        average : float | None\n            The time window around a given time to be used for averaging\n            (seconds). For example, 0.01 would translate into window that\n            starts 5 ms before and ends 5 ms after a given time point.\n            Defaults to None, which means no averaging.\n        head_pos : dict | None\n            If None (default), the sensors are positioned such that they span\n            the head circle. If dict, can have entries \'center\' (tuple) and\n            \'scale\' (tuple) for what the center and scale of the head\n            should be relative to the electrode locations.\n\n        Returns\n        -------\n        fig : instance of matplotlib.figure.Figure\n           The figure.\n        '
from .. import EvokedArray
if (components is None):
    tempResult = arange(self.n_components)
	
===================================================================	
_ajd_pham: 157	
----------------------------	

'Approximate joint diagonalization based on Pham\'s algorithm.\n\n    This is a direct implementation of the PHAM\'s AJD algorithm [1].\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_epochs, n_channels, n_channels)\n        A set of covariance matrices to diagonalize.\n    eps : float, defaults to 1e-6\n        The tolerance for stoping criterion.\n    max_iter : int, defaults to 1000\n        The maximum number of iteration to reach convergence.\n\n    Returns\n    -------\n    V : ndarray, shape (n_channels, n_channels)\n        The diagonalizer.\n    D : ndarray, shape (n_epochs, n_channels, n_channels)\n        The set of quasi diagonal matrices.\n\n    References\n    ----------\n    .. [1] Pham, Dinh Tuan. "Joint approximate diagonalization of positive\n           definite Hermitian matrices." SIAM Journal on Matrix Analysis and\n           Applications 22, no. 4 (2001): 1136-1152.\n\n    '
n_epochs = X.shape[0]
A = np.concatenate(X, axis=0).T
(n_times, n_m) = A.shape
V = numpy.eye(n_times)
epsilon = ((n_times * (n_times - 1)) * eps)
for it in range(max_iter):
    decr = 0
    for ii in range(1, n_times):
        for jj in range(ii):
            tempResult = arange(ii, n_m, n_times)
	
===================================================================	
_ajd_pham: 158	
----------------------------	

'Approximate joint diagonalization based on Pham\'s algorithm.\n\n    This is a direct implementation of the PHAM\'s AJD algorithm [1].\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_epochs, n_channels, n_channels)\n        A set of covariance matrices to diagonalize.\n    eps : float, defaults to 1e-6\n        The tolerance for stoping criterion.\n    max_iter : int, defaults to 1000\n        The maximum number of iteration to reach convergence.\n\n    Returns\n    -------\n    V : ndarray, shape (n_channels, n_channels)\n        The diagonalizer.\n    D : ndarray, shape (n_epochs, n_channels, n_channels)\n        The set of quasi diagonal matrices.\n\n    References\n    ----------\n    .. [1] Pham, Dinh Tuan. "Joint approximate diagonalization of positive\n           definite Hermitian matrices." SIAM Journal on Matrix Analysis and\n           Applications 22, no. 4 (2001): 1136-1152.\n\n    '
n_epochs = X.shape[0]
A = np.concatenate(X, axis=0).T
(n_times, n_m) = A.shape
V = numpy.eye(n_times)
epsilon = ((n_times * (n_times - 1)) * eps)
for it in range(max_iter):
    decr = 0
    for ii in range(1, n_times):
        for jj in range(ii):
            Ii = numpy.arange(ii, n_m, n_times)
            tempResult = arange(jj, n_m, n_times)
	
===================================================================	
_times_to_delays: 200	
----------------------------	

'Convert a tmin/tmax in seconds to delays.'
tempResult = arange(int(numpy.round((tmin * sfreq))), int((numpy.round((tmax * sfreq)) + 1)))
	
===================================================================	
_predict_slices: 213	
----------------------------	

"Aux function of GeneralizationAcrossTime.\n\n    Run classifiers predictions loop across time samples.\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_epochs, n_features, n_times)\n        To-be-fitted data.\n    estimators : list of array-like, shape (n_times, n_folds)\n        List of array of scikit-learn classifiers fitted in cross-validation.\n    cv_splits : list of tuples\n        List of tuples of train and test array generated from cv.\n    train_times : list\n        List of list of slices selecting data from X from which is prediction\n        is generated.\n    predict_method : str\n        Specifies prediction method for the estimator.\n    predict_mode : {'cross-validation', 'mean-prediction'}\n        Indicates how predictions are achieved with regards to the cross-\n        validation procedure:\n            'cross-validation' : estimates a single prediction per sample based\n                on the unique independent classifier fitted in the cross-\n                validation.\n            'mean-prediction' : estimates k predictions per sample, based on\n                each of the k-fold cross-validation classifiers, and average\n                these predictions into a single estimate per sample.\n        Default: 'cross-validation'\n    n_orig_epochs : int\n        Original number of predicted epochs before slice definition. Note\n        that the number of epochs may have been cropped if the cross validation\n        is not deterministic (e.g. with ShuffleSplit, we may only predict a\n        subset of epochs).\n    test_epochs : list of slices\n        List of slices to select the tested epoched in the cv.\n    "
(n_epochs, _, n_times) = X.shape
n_train = len(estimators)
n_test = [len(test_t_idxs) for test_t_idxs in train_times]
y_pred = None
for (train_t_idx, (estimator_cv, test_t_idxs)) in enumerate(zip(estimators, train_times)):
    tempResult = arange(n_times)
	
===================================================================	
test_cross_val_multiscore: 128	
----------------------------	

'Test cross_val_multiscore for computing scores on decoding over time.'
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LogisticRegression
X = numpy.random.rand(20, 3)
tempResult = arange(20)
	
===================================================================	
test_cross_val_multiscore: 133	
----------------------------	

'Test cross_val_multiscore for computing scores on decoding over time.'
from sklearn.model_selection import KFold, cross_val_score
from sklearn.linear_model import LogisticRegression
X = numpy.random.rand(20, 3)
y = (numpy.arange(20) % 2)
clf = LogisticRegression()
cv = KFold(2, random_state=0)
assert_array_equal(cross_val_score(clf, X, y, cv=cv), cross_val_multiscore(clf, X, y, cv=cv))
X = numpy.random.rand(20, 4, 3)
tempResult = arange(20)
	
===================================================================	
test_linearmodel: 109	
----------------------------	

'Test LinearModel class for computing filters and patterns.'
from sklearn.linear_model import LinearRegression
numpy.random.seed(42)
clf = LinearModel()
(n, n_features) = (20, 3)
X = numpy.random.rand(n, n_features)
tempResult = arange(n)
	
===================================================================	
test_csp: 66	
----------------------------	

'Test Common Spatial Patterns algorithm on epochs\n    '
raw = mne.io.read_raw_fif(raw_fname, preload=False)
events = read_events(event_name)
picks = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=False, exclude='bads')
picks = picks[2:12:3]
raw.add_proj([], remove_existing=True)
epochs = Epochs(raw, events, event_id, tmin, tmax, picks=picks, baseline=(None, 0), preload=True, proj=False)
epochs_data = epochs.get_data()
n_channels = epochs_data.shape[1]
y = epochs.events[:, (- 1)]
assert_raises(ValueError, CSP, n_components='foo', norm_trace=False)
for reg in ['foo', (- 0.1), 1.1]:
    assert_raises(ValueError, CSP, reg=reg, norm_trace=False)
for reg in ['oas', 'ledoit_wolf', 0, 0.5, 1.0]:
    CSP(reg=reg, norm_trace=False)
for cov_est in ['foo', None]:
    assert_raises(ValueError, CSP, cov_est=cov_est, norm_trace=False)
assert_raises(ValueError, CSP, norm_trace='foo')
for cov_est in ['concat', 'epoch']:
    CSP(cov_est=cov_est, norm_trace=False)
n_components = 3
for norm_trace in [True, False]:
    csp = CSP(n_components=n_components, norm_trace=norm_trace)
    csp.fit(epochs_data, epochs.events[:, (- 1)])
assert_equal(len(csp.mean_), n_components)
assert_equal(len(csp.std_), n_components)
X = csp.fit_transform(epochs_data, y)
sources = csp.transform(epochs_data)
assert_true((sources.shape[1] == n_components))
assert_true((csp.filters_.shape == (n_channels, n_channels)))
assert_true((csp.patterns_.shape == (n_channels, n_channels)))
assert_array_almost_equal(sources, X)
assert_raises(ValueError, csp.fit, epochs_data, numpy.zeros_like(epochs.events))
assert_raises(ValueError, csp.fit, epochs, y)
assert_raises(ValueError, csp.transform, epochs)
epochs.pick_types(meg='mag')
cmap = ('RdBu', True)
tempResult = arange(n_components)
	
===================================================================	
test_receptive_field: 106	
----------------------------	

'Test model prep and fitting.'
from sklearn.linear_model import Ridge
mod = Ridge()
(tmin, tmax) = ((- 10.0), 0)
n_feats = 3
X = rng.randn(10000, n_feats)
w = rng.randn((int(((tmax - tmin) + 1)) * n_feats))
X_del = numpy.concatenate(_delay_time_series(X, tmin, tmax, 1.0).transpose(2, 0, 1), axis=1)
y = numpy.dot(X_del, w)
feature_names = [('feature_%i' % ii) for ii in [0, 1, 2]]
rf = ReceptiveField(tmin, tmax, 1, feature_names, estimator=mod, patterns=True)
rf.fit(X, y)
tempResult = arange(tmin, (tmax + 1))
	
===================================================================	
test_receptive_field_nd: 281	
----------------------------	

'Test multidimensional support.'
from sklearn.linear_model import Ridge
x = rng.randn(1000, 3)
y = numpy.zeros((1000, 2))
slim = [0, 5]
for ii in range(1, 5):
    y[ii:, (ii % 2)] += ((((- 1) ** ii) * ii) * x[:(- ii), (ii % 3)])
y -= numpy.mean(y, axis=0)
x -= numpy.mean(x, axis=0)
x_off = (x + 1000.0)
expected = [[[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 4, 0], [0, 0, 2, 0, 0, 0]], [[0, 0, 0, (- 3), 0, 0], [0, (- 1), 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]]
tdr = TimeDelayingRidge(slim[0], slim[1], 1.0, 0.1, 'laplacian')
for estimator in (Ridge(alpha=0.0), 0.0, 0.01, tdr):
    model = ReceptiveField(slim[0], slim[1], 1.0, estimator=estimator)
    model.fit(x, y)
    tempResult = arange(slim[0], (slim[1] + 1))
	
===================================================================	
test_time_delay: 71	
----------------------------	

'Test that time-delaying w/ times and samples works properly.'
X = np.random.RandomState(0).randn(1000, 2)
assert ((X == 0).sum() == 0)
test_tlims = [((1, 2), 1), ((1, 1), 1), ((0, 2), 1), ((0, 1), 1), ((0, 0), 1), (((- 1), 2), 1), (((- 1), 1), 1), (((- 1), 0), 1), (((- 1), (- 1)), 1), (((- 2), 2), 1), (((- 2), 1), 1), (((- 2), 0), 1), (((- 2), (- 1)), 1), (((- 2), (- 1)), 1), ((0, 0.2), 10), (((- 0.1), 0.1), 10)]
for ((tmin, tmax), isfreq) in test_tlims:
    assert_raises(ValueError, _delay_time_series, X, tmin, tmax, sfreq=[1])
    assert_raises(ValueError, _delay_time_series, X, numpy.complex(tmin), tmax, 1)
    (start, stop) = (int(round((tmin * isfreq))), (int(round((tmax * isfreq))) + 1))
    n_delays = (stop - start)
    X_delayed = _delay_time_series(X, tmin, tmax, isfreq)
    assert_equal(X_delayed.shape, (1000, 2, n_delays))
    delays = _times_to_delays(tmin, tmax, isfreq)
    tempResult = arange(start, stop)
	
===================================================================	
test_time_delay: 74	
----------------------------	

'Test that time-delaying w/ times and samples works properly.'
X = np.random.RandomState(0).randn(1000, 2)
assert ((X == 0).sum() == 0)
test_tlims = [((1, 2), 1), ((1, 1), 1), ((0, 2), 1), ((0, 1), 1), ((0, 0), 1), (((- 1), 2), 1), (((- 1), 1), 1), (((- 1), 0), 1), (((- 1), (- 1)), 1), (((- 2), 2), 1), (((- 2), 1), 1), (((- 2), 0), 1), (((- 2), (- 1)), 1), (((- 2), (- 1)), 1), ((0, 0.2), 10), (((- 0.1), 0.1), 10)]
for ((tmin, tmax), isfreq) in test_tlims:
    assert_raises(ValueError, _delay_time_series, X, tmin, tmax, sfreq=[1])
    assert_raises(ValueError, _delay_time_series, X, numpy.complex(tmin), tmax, 1)
    (start, stop) = (int(round((tmin * isfreq))), (int(round((tmax * isfreq))) + 1))
    n_delays = (stop - start)
    X_delayed = _delay_time_series(X, tmin, tmax, isfreq)
    assert_equal(X_delayed.shape, (1000, 2, n_delays))
    delays = _times_to_delays(tmin, tmax, isfreq)
    assert_array_equal(delays, numpy.arange(start, stop))
    keep = _delays_to_slice(delays)
    expected = numpy.where((X_delayed != 0).all((- 1)).all((- 1)))[0]
    tempResult = arange(len(X_delayed))
	
===================================================================	
test_receptive_field_1d: 250	
----------------------------	

'Test that the fast solving works like Ridge.'
from sklearn.linear_model import Ridge
rng = numpy.random.RandomState(0)
x = rng.randn(500, 1)
for delay in range((- 2), 3):
    y = numpy.zeros(500)
    slims = [((- 2), 4)]
    if (delay == 0):
        y[:] = x[:, 0]
    elif (delay < 0):
        y[:delay] = x[(- delay):, 0]
        slims += [((- 4), (- 1))]
    else:
        y[delay:] = x[:(- delay), 0]
        slims += [(1, 2)]
    for ndim in (1, 2):
        y.shape = ((y.shape[0],) + ((1,) * (ndim - 1)))
        for slim in slims:
            lap = TimeDelayingRidge(slim[0], slim[1], 1.0, 0.1, 'laplacian', fit_intercept=False)
            for estimator in (Ridge(alpha=0.0), Ridge(alpha=0.1), 0.0, 0.1, lap):
                for offset in ((- 100), 0, 100):
                    model = ReceptiveField(slim[0], slim[1], 1.0, estimator=estimator)
                    use_x = (x + offset)
                    model.fit(use_x, y)
                    if (estimator is lap):
                        continue
                    assert_allclose(model.estimator_.intercept_, (- offset), atol=0.1)
                    tempResult = arange(slim[0], (slim[1] + 1))
	
===================================================================	
make_data: 12	
----------------------------	

(n_epochs, n_chan, n_time) = (50, 32, 10)
X = numpy.random.rand(n_epochs, n_chan, n_time)
tempResult = arange(n_epochs)
	
===================================================================	
test_search_light: 55	
----------------------------	

'Test SlidingEstimator'
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.ensemble import BaggingClassifier
(X, y) = make_data()
(n_epochs, _, n_time) = X.shape
assert_raises(ValueError, SlidingEstimator, 'foo')
sl = SlidingEstimator(Ridge())
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.__repr__()[:18], '<SlidingEstimator(')
sl.fit(X, y)
assert_equal(sl.__repr__()[(- 28):], ', fitted with 10 estimators>')
assert_raises(ValueError, sl.fit, X[1:], y)
assert_raises(ValueError, sl.fit, X[:, :, 0], y)
sl.fit(X, y, sample_weight=numpy.ones_like(y))
assert_raises(ValueError, sl.predict, X[:, :, :2])
y_pred = sl.predict(X)
assert_true((y_pred.dtype == int))
assert_array_equal(y_pred.shape, [n_epochs, n_time])
y_proba = sl.predict_proba(X)
assert_true((y_proba.dtype == float))
assert_array_equal(y_proba.shape, [n_epochs, n_time, 2])
score = sl.score(X, y)
assert_array_equal(score.shape, [n_time])
assert_true((numpy.sum(numpy.abs(score)) != 0))
assert_true((score.dtype == float))
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.scoring, None)
for scoring in ['foo', 999]:
    sl = SlidingEstimator(LogisticRegression(), scoring=scoring)
    sl.fit(X, y)
    assert_raises((ValueError, TypeError), sl.score, X, y)
sl = SlidingEstimator(LogisticRegression(random_state=0), scoring='roc_auc')
tempResult = arange(len(X))
	
===================================================================	
test_search_light: 58	
----------------------------	

'Test SlidingEstimator'
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.ensemble import BaggingClassifier
(X, y) = make_data()
(n_epochs, _, n_time) = X.shape
assert_raises(ValueError, SlidingEstimator, 'foo')
sl = SlidingEstimator(Ridge())
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.__repr__()[:18], '<SlidingEstimator(')
sl.fit(X, y)
assert_equal(sl.__repr__()[(- 28):], ', fitted with 10 estimators>')
assert_raises(ValueError, sl.fit, X[1:], y)
assert_raises(ValueError, sl.fit, X[:, :, 0], y)
sl.fit(X, y, sample_weight=numpy.ones_like(y))
assert_raises(ValueError, sl.predict, X[:, :, :2])
y_pred = sl.predict(X)
assert_true((y_pred.dtype == int))
assert_array_equal(y_pred.shape, [n_epochs, n_time])
y_proba = sl.predict_proba(X)
assert_true((y_proba.dtype == float))
assert_array_equal(y_proba.shape, [n_epochs, n_time, 2])
score = sl.score(X, y)
assert_array_equal(score.shape, [n_time])
assert_true((numpy.sum(numpy.abs(score)) != 0))
assert_true((score.dtype == float))
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.scoring, None)
for scoring in ['foo', 999]:
    sl = SlidingEstimator(LogisticRegression(), scoring=scoring)
    sl.fit(X, y)
    assert_raises((ValueError, TypeError), sl.score, X, y)
sl = SlidingEstimator(LogisticRegression(random_state=0), scoring='roc_auc')
y = (numpy.arange(len(X)) % 3)
sl.fit(X, y)
assert_raises(ValueError, sl.score, X, y)
tempResult = arange(len(X))
	
===================================================================	
test_search_light: 62	
----------------------------	

'Test SlidingEstimator'
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.ensemble import BaggingClassifier
(X, y) = make_data()
(n_epochs, _, n_time) = X.shape
assert_raises(ValueError, SlidingEstimator, 'foo')
sl = SlidingEstimator(Ridge())
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.__repr__()[:18], '<SlidingEstimator(')
sl.fit(X, y)
assert_equal(sl.__repr__()[(- 28):], ', fitted with 10 estimators>')
assert_raises(ValueError, sl.fit, X[1:], y)
assert_raises(ValueError, sl.fit, X[:, :, 0], y)
sl.fit(X, y, sample_weight=numpy.ones_like(y))
assert_raises(ValueError, sl.predict, X[:, :, :2])
y_pred = sl.predict(X)
assert_true((y_pred.dtype == int))
assert_array_equal(y_pred.shape, [n_epochs, n_time])
y_proba = sl.predict_proba(X)
assert_true((y_proba.dtype == float))
assert_array_equal(y_proba.shape, [n_epochs, n_time, 2])
score = sl.score(X, y)
assert_array_equal(score.shape, [n_time])
assert_true((numpy.sum(numpy.abs(score)) != 0))
assert_true((score.dtype == float))
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.scoring, None)
for scoring in ['foo', 999]:
    sl = SlidingEstimator(LogisticRegression(), scoring=scoring)
    sl.fit(X, y)
    assert_raises((ValueError, TypeError), sl.score, X, y)
sl = SlidingEstimator(LogisticRegression(random_state=0), scoring='roc_auc')
y = (numpy.arange(len(X)) % 3)
sl.fit(X, y)
assert_raises(ValueError, sl.score, X, y)
y = ((numpy.arange(len(X)) % 2) + 1)
sl.fit(X, y)
score = sl.score(X, y)
assert_array_equal(score, [roc_auc_score((y - 1), (_y_pred - 1)) for _y_pred in sl.decision_function(X).T])
tempResult = arange(len(X))
	
===================================================================	
test_search_light: 93	
----------------------------	

'Test SlidingEstimator'
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score, make_scorer
from sklearn.ensemble import BaggingClassifier
(X, y) = make_data()
(n_epochs, _, n_time) = X.shape
assert_raises(ValueError, SlidingEstimator, 'foo')
sl = SlidingEstimator(Ridge())
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.__repr__()[:18], '<SlidingEstimator(')
sl.fit(X, y)
assert_equal(sl.__repr__()[(- 28):], ', fitted with 10 estimators>')
assert_raises(ValueError, sl.fit, X[1:], y)
assert_raises(ValueError, sl.fit, X[:, :, 0], y)
sl.fit(X, y, sample_weight=numpy.ones_like(y))
assert_raises(ValueError, sl.predict, X[:, :, :2])
y_pred = sl.predict(X)
assert_true((y_pred.dtype == int))
assert_array_equal(y_pred.shape, [n_epochs, n_time])
y_proba = sl.predict_proba(X)
assert_true((y_proba.dtype == float))
assert_array_equal(y_proba.shape, [n_epochs, n_time, 2])
score = sl.score(X, y)
assert_array_equal(score.shape, [n_time])
assert_true((numpy.sum(numpy.abs(score)) != 0))
assert_true((score.dtype == float))
sl = SlidingEstimator(LogisticRegression())
assert_equal(sl.scoring, None)
for scoring in ['foo', 999]:
    sl = SlidingEstimator(LogisticRegression(), scoring=scoring)
    sl.fit(X, y)
    assert_raises((ValueError, TypeError), sl.score, X, y)
sl = SlidingEstimator(LogisticRegression(random_state=0), scoring='roc_auc')
y = (numpy.arange(len(X)) % 3)
sl.fit(X, y)
assert_raises(ValueError, sl.score, X, y)
y = ((numpy.arange(len(X)) % 2) + 1)
sl.fit(X, y)
score = sl.score(X, y)
assert_array_equal(score, [roc_auc_score((y - 1), (_y_pred - 1)) for _y_pred in sl.decision_function(X).T])
y = (numpy.arange(len(X)) % 2)
sl1 = SlidingEstimator(LogisticRegression(), scoring=roc_auc_score)
sl1.fit(X, y)
assert_raises(ValueError, sl1.score, X, y)
sl1 = SlidingEstimator(LogisticRegression(), scoring='roc_auc')
sl1.fit(X, y)
rng = numpy.random.RandomState(0)
X = rng.randn(*X.shape)
score_sl = sl1.score(X, y)
assert_array_equal(score_sl.shape, [n_time])
assert_true((score_sl.dtype == float))
scoring = make_scorer(roc_auc_score, needs_threshold=True)
score_manual = [scoring(est, x, y) for (est, x) in zip(sl1.estimators_, X.transpose(2, 0, 1))]
assert_array_equal(score_manual, score_sl)
sl = SlidingEstimator(LogisticRegression(random_state=0), n_jobs=1, scoring='roc_auc')
score_1job = sl.fit(X, y).score(X, y)
sl.n_jobs = 2
score_njobs = sl.fit(X, y).score(X, y)
assert_array_equal(score_1job, score_njobs)
sl.predict(X)
sl.fit(X[(..., [0])], y)
sl.predict(X[(..., [0])])

class _LogRegTransformer(LogisticRegression):

    def transform(self, X):
        return super(_LogRegTransformer, self).predict_proba(X)[(..., 1)]
pipe = make_pipeline(SlidingEstimator(_LogRegTransformer()), LogisticRegression())
pipe.fit(X, y)
pipe.predict(X)
X = numpy.random.rand(10, 3, 4, 2)
tempResult = arange(10)
	
===================================================================	
test_generalization_light: 143	
----------------------------	

'Test GeneralizingEstimator'
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
(X, y) = make_data()
(n_epochs, _, n_time) = X.shape
gl = GeneralizingEstimator(LogisticRegression())
assert_equal(repr(gl)[:23], '<GeneralizingEstimator(')
gl.fit(X, y)
gl.fit(X, y, sample_weight=numpy.ones_like(y))
assert_equal(gl.__repr__()[(- 28):], ', fitted with 10 estimators>')
y_pred = gl.predict(X)
assert_array_equal(y_pred.shape, [n_epochs, n_time, n_time])
assert_true((y_pred.dtype == int))
y_proba = gl.predict_proba(X)
assert_true((y_proba.dtype == float))
assert_array_equal(y_proba.shape, [n_epochs, n_time, n_time, 2])
y_pred = gl.predict(X[:, :, :2])
assert_array_equal(y_pred.shape, [n_epochs, n_time, 2])
score = gl.score(X[:, :, :3], y)
assert_array_equal(score.shape, [n_time, 3])
assert_true((numpy.sum(numpy.abs(score)) != 0))
assert_true((score.dtype == float))
gl = GeneralizingEstimator(LogisticRegression(), scoring='roc_auc')
gl.fit(X, y)
score = gl.score(X, y)
auc = roc_auc_score(y, gl.estimators_[0].predict_proba(X[(..., 0)])[(..., 1)])
assert_equal(score[(0, 0)], auc)
for scoring in ['foo', 999]:
    gl = GeneralizingEstimator(LogisticRegression(), scoring=scoring)
    gl.fit(X, y)
    assert_raises((ValueError, TypeError), gl.score, X, y)
gl = GeneralizingEstimator(LogisticRegression(), scoring='roc_auc')
tempResult = arange(len(X))
	
===================================================================	
test_generalization_light: 146	
----------------------------	

'Test GeneralizingEstimator'
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
(X, y) = make_data()
(n_epochs, _, n_time) = X.shape
gl = GeneralizingEstimator(LogisticRegression())
assert_equal(repr(gl)[:23], '<GeneralizingEstimator(')
gl.fit(X, y)
gl.fit(X, y, sample_weight=numpy.ones_like(y))
assert_equal(gl.__repr__()[(- 28):], ', fitted with 10 estimators>')
y_pred = gl.predict(X)
assert_array_equal(y_pred.shape, [n_epochs, n_time, n_time])
assert_true((y_pred.dtype == int))
y_proba = gl.predict_proba(X)
assert_true((y_proba.dtype == float))
assert_array_equal(y_proba.shape, [n_epochs, n_time, n_time, 2])
y_pred = gl.predict(X[:, :, :2])
assert_array_equal(y_pred.shape, [n_epochs, n_time, 2])
score = gl.score(X[:, :, :3], y)
assert_array_equal(score.shape, [n_time, 3])
assert_true((numpy.sum(numpy.abs(score)) != 0))
assert_true((score.dtype == float))
gl = GeneralizingEstimator(LogisticRegression(), scoring='roc_auc')
gl.fit(X, y)
score = gl.score(X, y)
auc = roc_auc_score(y, gl.estimators_[0].predict_proba(X[(..., 0)])[(..., 1)])
assert_equal(score[(0, 0)], auc)
for scoring in ['foo', 999]:
    gl = GeneralizingEstimator(LogisticRegression(), scoring=scoring)
    gl.fit(X, y)
    assert_raises((ValueError, TypeError), gl.score, X, y)
gl = GeneralizingEstimator(LogisticRegression(), scoring='roc_auc')
y = (numpy.arange(len(X)) % 3)
gl.fit(X, y)
assert_raises(ValueError, gl.score, X, y)
tempResult = arange(len(X))
	
===================================================================	
test_generalization_light: 160	
----------------------------	

'Test GeneralizingEstimator'
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
(X, y) = make_data()
(n_epochs, _, n_time) = X.shape
gl = GeneralizingEstimator(LogisticRegression())
assert_equal(repr(gl)[:23], '<GeneralizingEstimator(')
gl.fit(X, y)
gl.fit(X, y, sample_weight=numpy.ones_like(y))
assert_equal(gl.__repr__()[(- 28):], ', fitted with 10 estimators>')
y_pred = gl.predict(X)
assert_array_equal(y_pred.shape, [n_epochs, n_time, n_time])
assert_true((y_pred.dtype == int))
y_proba = gl.predict_proba(X)
assert_true((y_proba.dtype == float))
assert_array_equal(y_proba.shape, [n_epochs, n_time, n_time, 2])
y_pred = gl.predict(X[:, :, :2])
assert_array_equal(y_pred.shape, [n_epochs, n_time, 2])
score = gl.score(X[:, :, :3], y)
assert_array_equal(score.shape, [n_time, 3])
assert_true((numpy.sum(numpy.abs(score)) != 0))
assert_true((score.dtype == float))
gl = GeneralizingEstimator(LogisticRegression(), scoring='roc_auc')
gl.fit(X, y)
score = gl.score(X, y)
auc = roc_auc_score(y, gl.estimators_[0].predict_proba(X[(..., 0)])[(..., 1)])
assert_equal(score[(0, 0)], auc)
for scoring in ['foo', 999]:
    gl = GeneralizingEstimator(LogisticRegression(), scoring=scoring)
    gl.fit(X, y)
    assert_raises((ValueError, TypeError), gl.score, X, y)
gl = GeneralizingEstimator(LogisticRegression(), scoring='roc_auc')
y = (numpy.arange(len(X)) % 3)
gl.fit(X, y)
assert_raises(ValueError, gl.score, X, y)
y = ((numpy.arange(len(X)) % 2) + 1)
gl.fit(X, y)
score = gl.score(X, y)
manual_score = [[roc_auc_score((y - 1), _y_pred) for _y_pred in _y_preds] for _y_preds in gl.decision_function(X).transpose(1, 2, 0)]
assert_array_equal(score, manual_score)
gl = GeneralizingEstimator(LogisticRegression(), n_jobs=2)
gl.fit(X, y)
y_pred = gl.predict(X)
assert_array_equal(y_pred.shape, [n_epochs, n_time, n_time])
score = gl.score(X, y)
assert_array_equal(score.shape, [n_time, n_time])
gl.fit(X[(..., [0])], y)
gl.predict(X[(..., [0])])
X = numpy.random.rand(10, 3, 4, 2)
tempResult = arange(10)
	
===================================================================	
test_scaler: 62	
----------------------------	

'Test methods of Scaler.'
raw = mne.io.read_raw_fif(raw_fname)
events = read_events(event_name)
picks = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=False, exclude='bads')
picks = picks[1:13:3]
epochs = Epochs(raw, events, event_id, tmin, tmax, picks=picks, baseline=(None, 0), preload=True)
epochs_data = epochs.get_data()
y = epochs.events[:, (- 1)]
methods = (None, dict(mag=5, grad=10, eeg=20), 'mean', 'median')
infos = (epochs.info, epochs.info, None, None)
epochs_data_t = epochs_data.transpose([1, 0, 2])
for (method, info) in zip(methods, infos):
    if ((method == 'median') and (not check_version('sklearn', '0.17'))):
        assert_raises(ValueError, Scaler, info, method)
        continue
    if ((method == 'mean') and (not check_version('sklearn', ''))):
        assert_raises(ImportError, Scaler, info, method)
        continue
    scaler = Scaler(info, method)
    X = scaler.fit_transform(epochs_data, y)
    assert_equal(X.shape, epochs_data.shape)
    if ((method is None) or isinstance(method, dict)):
        sd = (DEFAULTS['scalings'] if (method is None) else method)
        stds = numpy.zeros(len(picks))
        for key in ('mag', 'grad'):
            stds[pick_types(epochs.info, meg=key)] = (1.0 / sd[key])
        stds[pick_types(epochs.info, meg=False, eeg=True)] = (1.0 / sd['eeg'])
        means = numpy.zeros(len(epochs.ch_names))
    elif (method == 'mean'):
        stds = numpy.array([numpy.std(ch_data) for ch_data in epochs_data_t])
        means = numpy.array([numpy.mean(ch_data) for ch_data in epochs_data_t])
    else:
        percs = numpy.array([numpy.percentile(ch_data, [25, 50, 75]) for ch_data in epochs_data_t])
        stds = (percs[:, 2] - percs[:, 0])
        means = percs[:, 1]
    assert_allclose(((X * stds[:, numpy.newaxis]) + means[:, numpy.newaxis]), epochs_data, rtol=1e-12, atol=1e-20, err_msg=method)
    X2 = scaler.fit(epochs_data, y).transform(epochs_data)
    assert_array_equal(X, X2)
    Xi = scaler.inverse_transform(X)
    assert_array_almost_equal(epochs_data, Xi)
assert_raises(ValueError, scaler.fit, epochs, y)
assert_raises(ValueError, scaler.transform, epochs)
tempResult = arange(len(raw.ch_names))
	
===================================================================	
restrict_forward_to_stc: 721	
----------------------------	

'Restrict forward operator to active sources in a source estimate.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    stc : SourceEstimate\n        Source estimate.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_label\n    '
fwd_out = deepcopy(fwd)
src_sel = _stc_src_sel(fwd['src'], stc)
fwd_out['source_rr'] = fwd['source_rr'][src_sel]
fwd_out['nsource'] = len(src_sel)
if is_fixed_orient(fwd):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        tempResult = arange(3)
	
===================================================================	
restrict_forward_to_stc: 723	
----------------------------	

'Restrict forward operator to active sources in a source estimate.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    stc : SourceEstimate\n        Source estimate.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_label\n    '
fwd_out = deepcopy(fwd)
src_sel = _stc_src_sel(fwd['src'], stc)
fwd_out['source_rr'] = fwd['source_rr'][src_sel]
fwd_out['nsource'] = len(src_sel)
if is_fixed_orient(fwd):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
else:
    tempResult = arange(3)
	
===================================================================	
restrict_forward_to_stc: 725	
----------------------------	

'Restrict forward operator to active sources in a source estimate.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    stc : SourceEstimate\n        Source estimate.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_label\n    '
fwd_out = deepcopy(fwd)
src_sel = _stc_src_sel(fwd['src'], stc)
fwd_out['source_rr'] = fwd['source_rr'][src_sel]
fwd_out['nsource'] = len(src_sel)
if is_fixed_orient(fwd):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
else:
    idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    if (fwd['sol_grad'] is not None):
        tempResult = arange(9)
	
===================================================================	
restrict_forward_to_stc: 734	
----------------------------	

'Restrict forward operator to active sources in a source estimate.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    stc : SourceEstimate\n        Source estimate.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_label\n    '
fwd_out = deepcopy(fwd)
src_sel = _stc_src_sel(fwd['src'], stc)
fwd_out['source_rr'] = fwd['source_rr'][src_sel]
fwd_out['nsource'] = len(src_sel)
if is_fixed_orient(fwd):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
else:
    idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    if (fwd['sol_grad'] is not None):
        idx_grad = ((9 * src_sel[:, None]) + np.arange(9)).ravel()
fwd_out['source_nn'] = fwd['source_nn'][idx]
fwd_out['sol']['data'] = fwd['sol']['data'][:, idx]
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = fwd['sol_grad']['data'][:, idx_grad]
fwd_out['sol']['ncol'] = len(idx)
if is_fixed_orient(fwd, orig=True):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        tempResult = arange(3)
	
===================================================================	
restrict_forward_to_stc: 736	
----------------------------	

'Restrict forward operator to active sources in a source estimate.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    stc : SourceEstimate\n        Source estimate.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_label\n    '
fwd_out = deepcopy(fwd)
src_sel = _stc_src_sel(fwd['src'], stc)
fwd_out['source_rr'] = fwd['source_rr'][src_sel]
fwd_out['nsource'] = len(src_sel)
if is_fixed_orient(fwd):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
else:
    idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    if (fwd['sol_grad'] is not None):
        idx_grad = ((9 * src_sel[:, None]) + np.arange(9)).ravel()
fwd_out['source_nn'] = fwd['source_nn'][idx]
fwd_out['sol']['data'] = fwd['sol']['data'][:, idx]
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = fwd['sol_grad']['data'][:, idx_grad]
fwd_out['sol']['ncol'] = len(idx)
if is_fixed_orient(fwd, orig=True):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
else:
    tempResult = arange(3)
	
===================================================================	
restrict_forward_to_stc: 738	
----------------------------	

'Restrict forward operator to active sources in a source estimate.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    stc : SourceEstimate\n        Source estimate.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_label\n    '
fwd_out = deepcopy(fwd)
src_sel = _stc_src_sel(fwd['src'], stc)
fwd_out['source_rr'] = fwd['source_rr'][src_sel]
fwd_out['nsource'] = len(src_sel)
if is_fixed_orient(fwd):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
else:
    idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    if (fwd['sol_grad'] is not None):
        idx_grad = ((9 * src_sel[:, None]) + np.arange(9)).ravel()
fwd_out['source_nn'] = fwd['source_nn'][idx]
fwd_out['sol']['data'] = fwd['sol']['data'][:, idx]
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = fwd['sol_grad']['data'][:, idx_grad]
fwd_out['sol']['ncol'] = len(idx)
if is_fixed_orient(fwd, orig=True):
    idx = src_sel
    if (fwd['sol_grad'] is not None):
        idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
else:
    idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    if (fwd['sol_grad'] is not None):
        tempResult = arange(9)
	
===================================================================	
restrict_forward_to_label: 794	
----------------------------	

'Restrict forward operator to labels.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    labels : label object | list\n        Label object or list of label objects.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_stc\n    '
message = 'labels must be instance of Label or a list of Label.'
vertices = [numpy.array([], int), numpy.array([], int)]
if (not isinstance(labels, list)):
    labels = [labels]
for label in labels:
    if (not isinstance(label, Label)):
        raise TypeError((message + (' Instead received %s' % type(label))))
    i = (0 if (label.hemi == 'lh') else 1)
    vertices[i] = numpy.append(vertices[i], label.vertices)
vertices = [numpy.unique(vert_hemi) for vert_hemi in vertices]
fwd_out = deepcopy(fwd)
fwd_out['source_rr'] = numpy.zeros((0, 3))
fwd_out['nsource'] = 0
fwd_out['source_nn'] = numpy.zeros((0, 3))
fwd_out['sol']['data'] = numpy.zeros((fwd['sol']['data'].shape[0], 0))
fwd_out['_orig_sol'] = numpy.zeros((fwd['_orig_sol'].shape[0], 0))
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = numpy.zeros((fwd['sol_grad']['data'].shape[0], 0))
    fwd_out['_orig_sol_grad'] = numpy.zeros((fwd['_orig_sol_grad'].shape[0], 0))
fwd_out['sol']['ncol'] = 0
nuse_lh = fwd['src'][0]['nuse']
for i in range(2):
    fwd_out['src'][i]['vertno'] = numpy.array([], int)
    fwd_out['src'][i]['nuse'] = 0
    fwd_out['src'][i]['inuse'] = fwd['src'][i]['inuse'].copy()
    fwd_out['src'][i]['inuse'].fill(0)
    fwd_out['src'][i]['use_tris'] = numpy.array([[]], int)
    fwd_out['src'][i]['nuse_tri'] = numpy.array([0])
    src_sel = numpy.intersect1d(fwd['src'][i]['vertno'], vertices[i])
    src_sel = numpy.searchsorted(fwd['src'][i]['vertno'], src_sel)
    vertno = fwd['src'][i]['vertno'][src_sel]
    fwd_out['src'][i]['inuse'][vertno] = 1
    fwd_out['src'][i]['nuse'] += len(vertno)
    fwd_out['src'][i]['vertno'] = numpy.where(fwd_out['src'][i]['inuse'])[0]
    src_sel += (i * nuse_lh)
    fwd_out['source_rr'] = numpy.vstack([fwd_out['source_rr'], fwd['source_rr'][src_sel]])
    fwd_out['nsource'] += len(src_sel)
    if is_fixed_orient(fwd):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            tempResult = arange(3)
	
===================================================================	
restrict_forward_to_label: 796	
----------------------------	

'Restrict forward operator to labels.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    labels : label object | list\n        Label object or list of label objects.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_stc\n    '
message = 'labels must be instance of Label or a list of Label.'
vertices = [numpy.array([], int), numpy.array([], int)]
if (not isinstance(labels, list)):
    labels = [labels]
for label in labels:
    if (not isinstance(label, Label)):
        raise TypeError((message + (' Instead received %s' % type(label))))
    i = (0 if (label.hemi == 'lh') else 1)
    vertices[i] = numpy.append(vertices[i], label.vertices)
vertices = [numpy.unique(vert_hemi) for vert_hemi in vertices]
fwd_out = deepcopy(fwd)
fwd_out['source_rr'] = numpy.zeros((0, 3))
fwd_out['nsource'] = 0
fwd_out['source_nn'] = numpy.zeros((0, 3))
fwd_out['sol']['data'] = numpy.zeros((fwd['sol']['data'].shape[0], 0))
fwd_out['_orig_sol'] = numpy.zeros((fwd['_orig_sol'].shape[0], 0))
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = numpy.zeros((fwd['sol_grad']['data'].shape[0], 0))
    fwd_out['_orig_sol_grad'] = numpy.zeros((fwd['_orig_sol_grad'].shape[0], 0))
fwd_out['sol']['ncol'] = 0
nuse_lh = fwd['src'][0]['nuse']
for i in range(2):
    fwd_out['src'][i]['vertno'] = numpy.array([], int)
    fwd_out['src'][i]['nuse'] = 0
    fwd_out['src'][i]['inuse'] = fwd['src'][i]['inuse'].copy()
    fwd_out['src'][i]['inuse'].fill(0)
    fwd_out['src'][i]['use_tris'] = numpy.array([[]], int)
    fwd_out['src'][i]['nuse_tri'] = numpy.array([0])
    src_sel = numpy.intersect1d(fwd['src'][i]['vertno'], vertices[i])
    src_sel = numpy.searchsorted(fwd['src'][i]['vertno'], src_sel)
    vertno = fwd['src'][i]['vertno'][src_sel]
    fwd_out['src'][i]['inuse'][vertno] = 1
    fwd_out['src'][i]['nuse'] += len(vertno)
    fwd_out['src'][i]['vertno'] = numpy.where(fwd_out['src'][i]['inuse'])[0]
    src_sel += (i * nuse_lh)
    fwd_out['source_rr'] = numpy.vstack([fwd_out['source_rr'], fwd['source_rr'][src_sel]])
    fwd_out['nsource'] += len(src_sel)
    if is_fixed_orient(fwd):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    else:
        tempResult = arange(3)
	
===================================================================	
restrict_forward_to_label: 798	
----------------------------	

'Restrict forward operator to labels.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    labels : label object | list\n        Label object or list of label objects.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_stc\n    '
message = 'labels must be instance of Label or a list of Label.'
vertices = [numpy.array([], int), numpy.array([], int)]
if (not isinstance(labels, list)):
    labels = [labels]
for label in labels:
    if (not isinstance(label, Label)):
        raise TypeError((message + (' Instead received %s' % type(label))))
    i = (0 if (label.hemi == 'lh') else 1)
    vertices[i] = numpy.append(vertices[i], label.vertices)
vertices = [numpy.unique(vert_hemi) for vert_hemi in vertices]
fwd_out = deepcopy(fwd)
fwd_out['source_rr'] = numpy.zeros((0, 3))
fwd_out['nsource'] = 0
fwd_out['source_nn'] = numpy.zeros((0, 3))
fwd_out['sol']['data'] = numpy.zeros((fwd['sol']['data'].shape[0], 0))
fwd_out['_orig_sol'] = numpy.zeros((fwd['_orig_sol'].shape[0], 0))
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = numpy.zeros((fwd['sol_grad']['data'].shape[0], 0))
    fwd_out['_orig_sol_grad'] = numpy.zeros((fwd['_orig_sol_grad'].shape[0], 0))
fwd_out['sol']['ncol'] = 0
nuse_lh = fwd['src'][0]['nuse']
for i in range(2):
    fwd_out['src'][i]['vertno'] = numpy.array([], int)
    fwd_out['src'][i]['nuse'] = 0
    fwd_out['src'][i]['inuse'] = fwd['src'][i]['inuse'].copy()
    fwd_out['src'][i]['inuse'].fill(0)
    fwd_out['src'][i]['use_tris'] = numpy.array([[]], int)
    fwd_out['src'][i]['nuse_tri'] = numpy.array([0])
    src_sel = numpy.intersect1d(fwd['src'][i]['vertno'], vertices[i])
    src_sel = numpy.searchsorted(fwd['src'][i]['vertno'], src_sel)
    vertno = fwd['src'][i]['vertno'][src_sel]
    fwd_out['src'][i]['inuse'][vertno] = 1
    fwd_out['src'][i]['nuse'] += len(vertno)
    fwd_out['src'][i]['vertno'] = numpy.where(fwd_out['src'][i]['inuse'])[0]
    src_sel += (i * nuse_lh)
    fwd_out['source_rr'] = numpy.vstack([fwd_out['source_rr'], fwd['source_rr'][src_sel]])
    fwd_out['nsource'] += len(src_sel)
    if is_fixed_orient(fwd):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    else:
        idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
        if (fwd['sol_grad'] is not None):
            tempResult = arange(9)
	
===================================================================	
restrict_forward_to_label: 807	
----------------------------	

'Restrict forward operator to labels.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    labels : label object | list\n        Label object or list of label objects.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_stc\n    '
message = 'labels must be instance of Label or a list of Label.'
vertices = [numpy.array([], int), numpy.array([], int)]
if (not isinstance(labels, list)):
    labels = [labels]
for label in labels:
    if (not isinstance(label, Label)):
        raise TypeError((message + (' Instead received %s' % type(label))))
    i = (0 if (label.hemi == 'lh') else 1)
    vertices[i] = numpy.append(vertices[i], label.vertices)
vertices = [numpy.unique(vert_hemi) for vert_hemi in vertices]
fwd_out = deepcopy(fwd)
fwd_out['source_rr'] = numpy.zeros((0, 3))
fwd_out['nsource'] = 0
fwd_out['source_nn'] = numpy.zeros((0, 3))
fwd_out['sol']['data'] = numpy.zeros((fwd['sol']['data'].shape[0], 0))
fwd_out['_orig_sol'] = numpy.zeros((fwd['_orig_sol'].shape[0], 0))
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = numpy.zeros((fwd['sol_grad']['data'].shape[0], 0))
    fwd_out['_orig_sol_grad'] = numpy.zeros((fwd['_orig_sol_grad'].shape[0], 0))
fwd_out['sol']['ncol'] = 0
nuse_lh = fwd['src'][0]['nuse']
for i in range(2):
    fwd_out['src'][i]['vertno'] = numpy.array([], int)
    fwd_out['src'][i]['nuse'] = 0
    fwd_out['src'][i]['inuse'] = fwd['src'][i]['inuse'].copy()
    fwd_out['src'][i]['inuse'].fill(0)
    fwd_out['src'][i]['use_tris'] = numpy.array([[]], int)
    fwd_out['src'][i]['nuse_tri'] = numpy.array([0])
    src_sel = numpy.intersect1d(fwd['src'][i]['vertno'], vertices[i])
    src_sel = numpy.searchsorted(fwd['src'][i]['vertno'], src_sel)
    vertno = fwd['src'][i]['vertno'][src_sel]
    fwd_out['src'][i]['inuse'][vertno] = 1
    fwd_out['src'][i]['nuse'] += len(vertno)
    fwd_out['src'][i]['vertno'] = numpy.where(fwd_out['src'][i]['inuse'])[0]
    src_sel += (i * nuse_lh)
    fwd_out['source_rr'] = numpy.vstack([fwd_out['source_rr'], fwd['source_rr'][src_sel]])
    fwd_out['nsource'] += len(src_sel)
    if is_fixed_orient(fwd):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    else:
        idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
        if (fwd['sol_grad'] is not None):
            idx_grad = ((9 * src_sel[:, None]) + np.arange(9)).ravel()
    fwd_out['source_nn'] = numpy.vstack([fwd_out['source_nn'], fwd['source_nn'][idx]])
    fwd_out['sol']['data'] = numpy.hstack([fwd_out['sol']['data'], fwd['sol']['data'][:, idx]])
    if (fwd['sol_grad'] is not None):
        fwd_out['sol_grad']['data'] = numpy.hstack([fwd_out['sol_grad']['data'], fwd['sol_rad']['data'][:, idx_grad]])
    fwd_out['sol']['ncol'] += len(idx)
    if is_fixed_orient(fwd, orig=True):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            tempResult = arange(3)
	
===================================================================	
restrict_forward_to_label: 809	
----------------------------	

'Restrict forward operator to labels.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    labels : label object | list\n        Label object or list of label objects.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_stc\n    '
message = 'labels must be instance of Label or a list of Label.'
vertices = [numpy.array([], int), numpy.array([], int)]
if (not isinstance(labels, list)):
    labels = [labels]
for label in labels:
    if (not isinstance(label, Label)):
        raise TypeError((message + (' Instead received %s' % type(label))))
    i = (0 if (label.hemi == 'lh') else 1)
    vertices[i] = numpy.append(vertices[i], label.vertices)
vertices = [numpy.unique(vert_hemi) for vert_hemi in vertices]
fwd_out = deepcopy(fwd)
fwd_out['source_rr'] = numpy.zeros((0, 3))
fwd_out['nsource'] = 0
fwd_out['source_nn'] = numpy.zeros((0, 3))
fwd_out['sol']['data'] = numpy.zeros((fwd['sol']['data'].shape[0], 0))
fwd_out['_orig_sol'] = numpy.zeros((fwd['_orig_sol'].shape[0], 0))
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = numpy.zeros((fwd['sol_grad']['data'].shape[0], 0))
    fwd_out['_orig_sol_grad'] = numpy.zeros((fwd['_orig_sol_grad'].shape[0], 0))
fwd_out['sol']['ncol'] = 0
nuse_lh = fwd['src'][0]['nuse']
for i in range(2):
    fwd_out['src'][i]['vertno'] = numpy.array([], int)
    fwd_out['src'][i]['nuse'] = 0
    fwd_out['src'][i]['inuse'] = fwd['src'][i]['inuse'].copy()
    fwd_out['src'][i]['inuse'].fill(0)
    fwd_out['src'][i]['use_tris'] = numpy.array([[]], int)
    fwd_out['src'][i]['nuse_tri'] = numpy.array([0])
    src_sel = numpy.intersect1d(fwd['src'][i]['vertno'], vertices[i])
    src_sel = numpy.searchsorted(fwd['src'][i]['vertno'], src_sel)
    vertno = fwd['src'][i]['vertno'][src_sel]
    fwd_out['src'][i]['inuse'][vertno] = 1
    fwd_out['src'][i]['nuse'] += len(vertno)
    fwd_out['src'][i]['vertno'] = numpy.where(fwd_out['src'][i]['inuse'])[0]
    src_sel += (i * nuse_lh)
    fwd_out['source_rr'] = numpy.vstack([fwd_out['source_rr'], fwd['source_rr'][src_sel]])
    fwd_out['nsource'] += len(src_sel)
    if is_fixed_orient(fwd):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    else:
        idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
        if (fwd['sol_grad'] is not None):
            idx_grad = ((9 * src_sel[:, None]) + np.arange(9)).ravel()
    fwd_out['source_nn'] = numpy.vstack([fwd_out['source_nn'], fwd['source_nn'][idx]])
    fwd_out['sol']['data'] = numpy.hstack([fwd_out['sol']['data'], fwd['sol']['data'][:, idx]])
    if (fwd['sol_grad'] is not None):
        fwd_out['sol_grad']['data'] = numpy.hstack([fwd_out['sol_grad']['data'], fwd['sol_rad']['data'][:, idx_grad]])
    fwd_out['sol']['ncol'] += len(idx)
    if is_fixed_orient(fwd, orig=True):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    else:
        tempResult = arange(3)
	
===================================================================	
restrict_forward_to_label: 811	
----------------------------	

'Restrict forward operator to labels.\n\n    Parameters\n    ----------\n    fwd : Forward\n        Forward operator.\n    labels : label object | list\n        Label object or list of label objects.\n\n    Returns\n    -------\n    fwd_out : dict\n        Restricted forward operator.\n\n    See Also\n    --------\n    restrict_forward_to_stc\n    '
message = 'labels must be instance of Label or a list of Label.'
vertices = [numpy.array([], int), numpy.array([], int)]
if (not isinstance(labels, list)):
    labels = [labels]
for label in labels:
    if (not isinstance(label, Label)):
        raise TypeError((message + (' Instead received %s' % type(label))))
    i = (0 if (label.hemi == 'lh') else 1)
    vertices[i] = numpy.append(vertices[i], label.vertices)
vertices = [numpy.unique(vert_hemi) for vert_hemi in vertices]
fwd_out = deepcopy(fwd)
fwd_out['source_rr'] = numpy.zeros((0, 3))
fwd_out['nsource'] = 0
fwd_out['source_nn'] = numpy.zeros((0, 3))
fwd_out['sol']['data'] = numpy.zeros((fwd['sol']['data'].shape[0], 0))
fwd_out['_orig_sol'] = numpy.zeros((fwd['_orig_sol'].shape[0], 0))
if (fwd['sol_grad'] is not None):
    fwd_out['sol_grad']['data'] = numpy.zeros((fwd['sol_grad']['data'].shape[0], 0))
    fwd_out['_orig_sol_grad'] = numpy.zeros((fwd['_orig_sol_grad'].shape[0], 0))
fwd_out['sol']['ncol'] = 0
nuse_lh = fwd['src'][0]['nuse']
for i in range(2):
    fwd_out['src'][i]['vertno'] = numpy.array([], int)
    fwd_out['src'][i]['nuse'] = 0
    fwd_out['src'][i]['inuse'] = fwd['src'][i]['inuse'].copy()
    fwd_out['src'][i]['inuse'].fill(0)
    fwd_out['src'][i]['use_tris'] = numpy.array([[]], int)
    fwd_out['src'][i]['nuse_tri'] = numpy.array([0])
    src_sel = numpy.intersect1d(fwd['src'][i]['vertno'], vertices[i])
    src_sel = numpy.searchsorted(fwd['src'][i]['vertno'], src_sel)
    vertno = fwd['src'][i]['vertno'][src_sel]
    fwd_out['src'][i]['inuse'][vertno] = 1
    fwd_out['src'][i]['nuse'] += len(vertno)
    fwd_out['src'][i]['vertno'] = numpy.where(fwd_out['src'][i]['inuse'])[0]
    src_sel += (i * nuse_lh)
    fwd_out['source_rr'] = numpy.vstack([fwd_out['source_rr'], fwd['source_rr'][src_sel]])
    fwd_out['nsource'] += len(src_sel)
    if is_fixed_orient(fwd):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    else:
        idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
        if (fwd['sol_grad'] is not None):
            idx_grad = ((9 * src_sel[:, None]) + np.arange(9)).ravel()
    fwd_out['source_nn'] = numpy.vstack([fwd_out['source_nn'], fwd['source_nn'][idx]])
    fwd_out['sol']['data'] = numpy.hstack([fwd_out['sol']['data'], fwd['sol']['data'][:, idx]])
    if (fwd['sol_grad'] is not None):
        fwd_out['sol_grad']['data'] = numpy.hstack([fwd_out['sol_grad']['data'], fwd['sol_rad']['data'][:, idx_grad]])
    fwd_out['sol']['ncol'] += len(idx)
    if is_fixed_orient(fwd, orig=True):
        idx = src_sel
        if (fwd['sol_grad'] is not None):
            idx_grad = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
    else:
        idx = ((3 * src_sel[:, None]) + np.arange(3)).ravel()
        if (fwd['sol_grad'] is not None):
            tempResult = arange(9)
	
===================================================================	
_block_diag: 78	
----------------------------	

'Construct a block diagonal from a packed structure.\n\n    You have to try it on a matrix to see what it\'s doing.\n\n    If A is not sparse, then returns a sparse block diagonal "bd",\n    diagonalized from the\n    elements in "A".\n    "A" is ma x na, comprising bdn=(na/"n") blocks of submatrices.\n    Each submatrix is ma x "n", and these submatrices are\n    placed down the diagonal of the matrix.\n\n    If A is already sparse, then the operation is reversed, yielding\n    a block\n    row matrix, where each set of n columns corresponds to a block element\n    from the block diagonal.\n\n    Parameters\n    ----------\n    A : array\n        The matrix\n    n : int\n        The block size\n    Returns\n    -------\n    bd : sparse matrix\n        The block diagonal matrix\n    '
if scipy.sparse.issparse(A):
    raise NotImplemented('sparse reversal not implemented yet')
(ma, na) = A.shape
bdn = (na // int(n))
if ((na % n) > 0):
    raise ValueError('Width of matrix must be a multiple of n')
tempResult = arange((ma * bdn), dtype=numpy.int)
	
===================================================================	
_block_diag: 81	
----------------------------	

'Construct a block diagonal from a packed structure.\n\n    You have to try it on a matrix to see what it\'s doing.\n\n    If A is not sparse, then returns a sparse block diagonal "bd",\n    diagonalized from the\n    elements in "A".\n    "A" is ma x na, comprising bdn=(na/"n") blocks of submatrices.\n    Each submatrix is ma x "n", and these submatrices are\n    placed down the diagonal of the matrix.\n\n    If A is already sparse, then the operation is reversed, yielding\n    a block\n    row matrix, where each set of n columns corresponds to a block element\n    from the block diagonal.\n\n    Parameters\n    ----------\n    A : array\n        The matrix\n    n : int\n        The block size\n    Returns\n    -------\n    bd : sparse matrix\n        The block diagonal matrix\n    '
if scipy.sparse.issparse(A):
    raise NotImplemented('sparse reversal not implemented yet')
(ma, na) = A.shape
bdn = (na // int(n))
if ((na % n) > 0):
    raise ValueError('Width of matrix must be a multiple of n')
tmp = np.arange((ma * bdn), dtype=np.int).reshape(bdn, ma)
tmp = numpy.tile(tmp, (1, n))
ii = tmp.ravel()
tempResult = arange(na, dtype=numpy.int)
	
===================================================================	
compute_orient_prior: 555	
----------------------------	

'Compute orientation prior.\n\n    Parameters\n    ----------\n    forward : dict\n        Forward operator.\n    loose : float in [0, 1]\n        The loose orientation parameter.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    orient_prior : array\n        Orientation priors.\n    '
is_fixed_ori = is_fixed_orient(forward)
n_sources = forward['sol']['data'].shape[1]
if (loose is None):
    warn('loose=None is deprecated and will be removed in 0.16, use loose=0. for fixed constraint and loose=1. for free orientations', DeprecationWarning)
    loose = (0.0 if is_fixed_ori else 1.0)
loose = float(loose)
if (not (0 <= loose <= 1)):
    raise ValueError(('loose value should be smaller than 1 and bigger than 0, got %s.' % (loose,)))
if ((loose < 1) and (not forward['surf_ori'])):
    raise ValueError(('Forward operator is not oriented in surface coordinates. loose parameter should be 1 not %s.' % loose))
if (is_fixed_ori and (loose != 0)):
    raise ValueError('loose must be 0. with forward operator with fixed orientation.')
orient_prior = numpy.ones(n_sources, dtype=numpy.float)
if ((not is_fixed_ori) and (loose < 1)):
    utils.logger.info(('Applying loose dipole orientations. Loose value of %s.' % loose))
    tempResult = arange(n_sources)
	
===================================================================	
_inv_block_diag: 96	
----------------------------	

'Construct an inverse block diagonal from a packed structure.\n\n    You have to try it on a matrix to see what it\'s doing.\n\n    "A" is ma x na, comprising bdn=(na/"n") blocks of submatrices.\n    Each submatrix is ma x "n", and the inverses of these submatrices\n    are placed down the diagonal of the matrix.\n\n    Parameters\n    ----------\n    A : array\n        The matrix.\n    n : int\n        The block size.\n\n    Returns\n    -------\n    bd : sparse matrix\n        The block diagonal matrix.\n    '
(ma, na) = A.shape
bdn = (na // int(n))
if ((na % n) > 0):
    raise ValueError('Width of matrix must be a multiple of n')
A = A.copy()
for start in range(0, na, 3):
    A[:, start:(start + 3)] = scipy.linalg.inv(A[:, start:(start + 3)])
tempResult = arange((ma * bdn), dtype=numpy.int)
	
===================================================================	
_inv_block_diag: 99	
----------------------------	

'Construct an inverse block diagonal from a packed structure.\n\n    You have to try it on a matrix to see what it\'s doing.\n\n    "A" is ma x na, comprising bdn=(na/"n") blocks of submatrices.\n    Each submatrix is ma x "n", and the inverses of these submatrices\n    are placed down the diagonal of the matrix.\n\n    Parameters\n    ----------\n    A : array\n        The matrix.\n    n : int\n        The block size.\n\n    Returns\n    -------\n    bd : sparse matrix\n        The block diagonal matrix.\n    '
(ma, na) = A.shape
bdn = (na // int(n))
if ((na % n) > 0):
    raise ValueError('Width of matrix must be a multiple of n')
A = A.copy()
for start in range(0, na, 3):
    A[:, start:(start + 3)] = scipy.linalg.inv(A[:, start:(start + 3)])
tmp = np.arange((ma * bdn), dtype=np.int).reshape(bdn, ma)
tmp = numpy.tile(tmp, (1, n))
ii = tmp.ravel()
tempResult = arange(na, dtype=numpy.int)
	
===================================================================	
_bem_specify_coils: 77	
----------------------------	

'Set up for computing the solution at a set of MEG coils.\n\n    Parameters\n    ----------\n    bem : dict\n        BEM information\n    coils : list of dict, len(n_MEG_sensors)\n        MEG sensor information dicts\n    coord_frame : int\n        Class constant identifying coordinate frame\n    mults : ndarray, shape (1, n_BEM_vertices)\n        Multiplier for every vertex in BEM\n    n_jobs : int\n        Number of jobs to run in parallel\n\n    Returns\n    -------\n    sol: ndarray, shape (n_MEG_sensors, n_BEM_vertices)\n        MEG solution\n    '
(coils, coord_frame) = _check_coil_frame(coils, coord_frame, bem)
(rmags, cosmags, ws, bins) = _concatenate_coils(coils)
lens = numpy.cumsum(numpy.r_[(0, [len(s['rr']) for s in bem['surfs']])])
sol = numpy.zeros(((bins[(- 1)] + 1), bem['solution'].shape[1]))
tempResult = arange(0, sol.shape[0], 100)
	
===================================================================	
_concatenate_coils: 68	
----------------------------	

'Concatenate MEG coil parameters.'
rmags = numpy.concatenate([coil['rmag'] for coil in coils])
cosmags = numpy.concatenate([coil['cosmag'] for coil in coils])
ws = numpy.concatenate([coil['w'] for coil in coils])
n_int = numpy.array([len(coil['rmag']) for coil in coils])
if (n_int[(- 1)] == 0):
    raise RuntimeError('not supported')
tempResult = arange(len(n_int))
	
===================================================================	
_do_inf_pots: 159	
----------------------------	

'Calculate infinite potentials for MEG or EEG sensors using chunks.\n\n    Parameters\n    ----------\n    mri_rr : ndarray, shape (n_dipoles, 3)\n        3D dipole source positions in MRI coordinates\n    bem_rr : ndarray, shape (n_BEM_vertices, 3)\n        3D vertex positions for all surfaces in the BEM\n    mri_Q :\n        3x3 head -> MRI transform. I.e., head_mri_t.dot(np.eye(3))\n    sol : ndarray, shape (n_sensors_subset, n_BEM_vertices_subset)\n        Comes from _bem_specify_coils\n\n    Returns\n    -------\n    B : ndarray, (n_dipoles * 3, n_sensors)\n        Forward solution for sensors due to volume currents\n    '
tempResult = arange(0, len(mri_rr), 200)
	
===================================================================	
_make_surface_mapping: 145	
----------------------------	

"Re-map M/EEG data to a surface.\n\n    Parameters\n    ----------\n    info : instance of Info\n        Measurement info.\n    surf : dict\n        The surface to map the data to. The required fields are `'rr'`,\n        `'nn'`, and `'coord_frame'`. Must be in head coordinates.\n    ch_type : str\n        Must be either `'meg'` or `'eeg'`, determines the type of field.\n    trans : None | dict\n        If None, no transformation applied. Should be a Head<->MRI\n        transformation.\n    mode : str\n        Either `'accurate'` or `'fast'`, determines the quality of the\n        Legendre polynomial expansion used. `'fast'` should be sufficient\n        for most applications.\n    n_jobs : int\n        Number of permutations to run in parallel (requires joblib package).\n    origin : array-like, shape (3,) | str\n        Origin of internal and external multipolar moment space in head\n        coords and in meters. The default is ``'auto'``, which means\n        a head-digitization-based origin fit.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    mapping : array\n        A n_vertices x n_sensors array that remaps the MEG or EEG data,\n        as `new_data = np.dot(mapping, data)`.\n    "
if (not all(((key in surf) for key in ['rr', 'nn']))):
    raise KeyError('surf must have both "rr" and "nn"')
if ('coord_frame' not in surf):
    raise KeyError('The surface coordinate frame must be specified in surf["coord_frame"]')
if (mode not in ['accurate', 'fast']):
    raise ValueError(('mode must be "accurate" or "fast", not "%s"' % mode))
orig_surf = surf
surf = transform_surface_to(deepcopy(surf), 'head', trans)
n_jobs = check_n_jobs(n_jobs)
origin = _check_origin(origin, info)
if (ch_type not in ('meg', 'eeg')):
    raise ValueError(('unknown coil type "%s"' % ch_type))
if (ch_type == 'meg'):
    picks = pick_types(info, meg=True, eeg=False, ref_meg=False)
    utils.logger.info('Prepare MEG mapping...')
else:
    picks = pick_types(info, meg=False, eeg=True, ref_meg=False)
    utils.logger.info('Prepare EEG mapping...')
if (len(picks) == 0):
    raise RuntimeError('cannot map, no channels found')
chs = pick_info(info, picks)['chs']
if (ch_type == 'meg'):
    coils = _create_meg_coils(chs, 'normal', info['dev_head_t'])
    type_str = 'coils'
    miss = 0.0001
else:
    coils = _create_eeg_els(chs)
    type_str = 'electrodes'
    miss = 0.001
(int_rad, noise, lut_fun, n_fact) = _setup_dots(mode, coils, ch_type)
utils.logger.info(('Computing dot products for %i %s...' % (len(coils), type_str)))
self_dots = _do_self_dots(int_rad, False, coils, origin, ch_type, lut_fun, n_fact, n_jobs)
tempResult = arange(len(surf['rr']))
	
===================================================================	
_comp_sum_eeg: 86	
----------------------------	

'Lead field dot products using Legendre polynomial (P_n) series.'
n_chunk = (50000000 // ((8 * max(n_fact.shape)) * 2))
tempResult = arange(0, beta.size, n_chunk)
	
===================================================================	
_do_surface_dots: 214	
----------------------------	

"Compute the map construction products.\n\n    Parameters\n    ----------\n    intrad : float\n        The integration radius. It is used to calculate beta as:\n        beta = (intrad * intrad) / (r1 * r2)\n    volume : bool\n        If True, compute a volume integral.\n    coils : list of dict\n        The coils.\n    surf : dict\n        The surface on which the field is interpolated.\n    sel : array\n        Indices of the surface vertices to select.\n    r0 : array, shape (3 x 1)\n        The origin of the sphere.\n    ch_type : str\n        The channel type. It can be 'meg' or 'eeg'.\n    lut : callable\n        Look-up table for Legendre polynomials.\n    n_fact : array\n        Coefficients in the integration sum.\n    n_jobs : int\n        Number of jobs to run in parallel.\n\n    Returns\n    -------\n    products : array, shape (n_coils, n_coils)\n        The integration products.\n    "
rmags = [(coil['rmag'] - r0[numpy.newaxis, :]) for coil in coils]
rlens = [numpy.sqrt(numpy.sum((r * r), axis=1)) for r in rmags]
rmags = [(r / rl[:, numpy.newaxis]) for (r, rl) in zip(rmags, rlens)]
cosmags = [coil['cosmag'] for coil in coils]
ws = [coil['w'] for coil in coils]
rref = None
refl = None
if (ch_type == 'eeg'):
    intrad *= 0.7
rsurf = (surf['rr'][sel] - r0[numpy.newaxis, :])
lsurf = numpy.sqrt(numpy.sum((rsurf * rsurf), axis=1))
rsurf /= lsurf[:, numpy.newaxis]
this_nn = surf['nn'][sel]
(parallel, p_fun, _) = parallel_func(_do_surface_dots_subset, n_jobs)
tempResult = arange(len(rmags))
	
===================================================================	
_comp_sums_meg: 100	
----------------------------	

'Lead field dot products using Legendre polynomial (P_n) series.\n\n    Parameters\n    ----------\n    beta : array, shape (n_points * n_points, 1)\n        Coefficients of the integration.\n    ctheta : array, shape (n_points * n_points, 1)\n        Cosine of the angle between the sensor integration points.\n    lut_fun : callable\n        Look-up table for evaluating Legendre polynomials.\n    n_fact : array\n        Coefficients in the integration sum.\n    volume_integral : bool\n        If True, compute volume integral.\n\n    Returns\n    -------\n    sums : array, shape (4, n_points * n_points)\n        The results.\n    '
sums = numpy.empty((n_fact.shape[1], len(beta)))
n_chunk = (50000000 // ((8 * max(n_fact.shape)) * 2))
tempResult = arange(0, beta.size, n_chunk)
	
===================================================================	
_get_legen_table: 65	
----------------------------	

'Return a (generated) LUT of Legendre (derivative) polynomial coeffs.'
if ((n_interp % 2) != 0):
    raise RuntimeError('n_interp must be even')
fname = os.path.join(_get_extra_data_path(), 'tables')
if (not os.path.isdir(fname)):
    os.makedirs(fname)
if (ch_type == 'meg'):
    fname = os.path.join(fname, ('legder_%s_%s.bin' % (n_coeff, n_interp)))
    leg_fun = _get_legen_der
    extra_str = ' derivative'
    lut_shape = ((n_interp + 1), n_coeff, 3)
else:
    fname = os.path.join(fname, ('legval_%s_%s.bin' % (n_coeff, n_interp)))
    leg_fun = _get_legen
    extra_str = ''
    lut_shape = ((n_interp + 1), n_coeff)
if ((not os.path.isfile(fname)) or force_calc):
    utils.logger.info(('Generating Legendre%s table...' % extra_str))
    x_interp = numpy.linspace((- 1), 1, (n_interp + 1))
    lut = leg_fun(x_interp, n_coeff).astype(numpy.float32)
    if (not force_calc):
        with open(fname, 'wb') as fid:
            fid.write(lut.tostring())
else:
    utils.logger.info(('Reading Legendre%s table...' % extra_str))
    with open(fname, 'rb', buffering=0) as fid:
        lut = numpy.fromfile(fid, numpy.float32)
lut.shape = lut_shape
tempResult = arange(1, n_coeff, dtype=float)
	
===================================================================	
_do_self_dots: 166	
----------------------------	

"Perform the lead field dot product integrations.\n\n    Parameters\n    ----------\n    intrad : float\n        The integration radius. It is used to calculate beta as:\n        beta = (intrad * intrad) / (r1 * r2).\n    volume : bool\n        If True, perform volume integral.\n    coils : list of dict\n        The coils.\n    r0 : array, shape (3 x 1)\n        The origin of the sphere.\n    ch_type : str\n        The channel type. It can be 'meg' or 'eeg'.\n    lut : callable\n        Look-up table for evaluating Legendre polynomials.\n    n_fact : array\n        Coefficients in the integration sum.\n    n_jobs : int\n        Number of jobs to run in parallel.\n\n    Returns\n    -------\n    products : array, shape (n_coils, n_coils)\n        The integration products.\n    "
if (ch_type == 'eeg'):
    intrad *= 0.7
rmags = [(coil['rmag'] - r0[numpy.newaxis, :]) for coil in coils]
rlens = [numpy.sqrt(numpy.sum((r * r), axis=1)) for r in rmags]
rmags = [(r / rl[:, numpy.newaxis]) for (r, rl) in zip(rmags, rlens)]
cosmags = [coil['cosmag'] for coil in coils]
ws = [coil['w'] for coil in coils]
(parallel, p_fun, _) = parallel_func(_do_self_dots_subset, n_jobs)
tempResult = arange(len(rmags))
	
===================================================================	
test_legendre_val: 39	
----------------------------	

'Test Legendre polynomial (derivative) equivalence.'
rng = numpy.random.RandomState(0)
xs = numpy.linspace((- 1.0), 1.0, 1000)
n_terms = 100
vals_np = numpy.polynomial.legendre.legvander(xs, (n_terms - 1))
for (nc, interp) in zip([100, 50], ['nearest', 'linear']):
    (lut, n_fact) = _get_legen_table('eeg', n_coeff=nc, force_calc=True)
    lut_fun = interp1d(numpy.linspace((- 1), 1, lut.shape[0]), lut, interp, axis=0)
    vals_i = lut_fun(xs)
    assert_allclose(vals_np[:, 1:(vals_i.shape[1] + 1)], vals_i, rtol=0.01, atol=0.005)
    ctheta = ((rng.rand(20, 30) * 2.0) - 1.0)
    beta = (rng.rand(20, 30) * 0.8)
    c1 = _comp_sum_eeg(beta.flatten(), ctheta.flatten(), lut_fun, n_fact)
    c1.shape = beta.shape
    tempResult = arange(1, n_terms, dtype=float)
	
===================================================================	
test_make_forward_dipole: 184	
----------------------------	

'Test forward-projecting dipoles.'
rng = numpy.random.RandomState(0)
evoked = read_evokeds(fname_evo)[0]
cov = read_cov(fname_cov)
dip_c = read_dipole(fname_dip)
picks = pick_types(evoked.info, meg='mag', eeg=False)
evoked.pick_channels([evoked.ch_names[p] for p in picks])
info = evoked.info
n_test_dipoles = 3
tempResult = arange(len(dip_c))
	
===================================================================	
test_make_forward_dipole: 220	
----------------------------	

'Test forward-projecting dipoles.'
rng = numpy.random.RandomState(0)
evoked = read_evokeds(fname_evo)[0]
cov = read_cov(fname_cov)
dip_c = read_dipole(fname_dip)
picks = pick_types(evoked.info, meg='mag', eeg=False)
evoked.pick_channels([evoked.ch_names[p] for p in picks])
info = evoked.info
n_test_dipoles = 3
dipsel = numpy.sort(rng.permutation(numpy.arange(len(dip_c)))[:n_test_dipoles])
dip_test = Dipole(times=dip_c.times[dipsel], pos=dip_c.pos[dipsel], amplitude=dip_c.amplitude[dipsel], ori=dip_c.ori[dipsel], gof=dip_c.gof[dipsel])
sphere = make_sphere_model(head_radius=0.1)
with warnings.catch_warnings(record=True) as w:
    (fwd, stc) = make_forward_dipole(dip_test, sphere, info, trans=fname_trans)
    assert_true(issubclass(w[(- 1)].category, RuntimeWarning))
assert_true(isinstance(stc, list))
for nd in range(n_test_dipoles):
    assert_true(isinstance(stc[nd], VolSourceEstimate))
(times, pos, amplitude, ori, gof) = ([], [], [], [], [])
nave = 100
for s in stc:
    evo_test = simulate_evoked(fwd, s, info, cov, nave=nave, random_state=rng)
    (dfit, resid) = fit_dipole(evo_test, cov, sphere, None)
    times += dfit.times.tolist()
    pos += dfit.pos.tolist()
    amplitude += dfit.amplitude.tolist()
    ori += dfit.ori.tolist()
    gof += dfit.gof.tolist()
dip_fit = Dipole(times, pos, amplitude, ori, gof)
diff = (dip_test.pos - dip_fit.pos)
corr = numpy.corrcoef(dip_test.pos.ravel(), dip_fit.pos.ravel())[(0, 1)]
dist = numpy.sqrt(numpy.mean(numpy.sum((diff * diff), axis=1)))
gc_dist = ((180 / numpy.pi) * numpy.mean(numpy.arccos(numpy.sum((dip_test.ori * dip_fit.ori), axis=1))))
amp_err = numpy.sqrt(numpy.mean(((dip_test.amplitude - dip_fit.amplitude) ** 2)))
assert_allclose(dip_fit.pos, dip_test.pos, rtol=0, atol=0.01, err_msg='position mismatch')
assert_true((dist < 0.01), ('dist: %s' % dist))
assert_true((corr > (1 - 0.01)), ('corr: %s' % corr))
assert_true((gc_dist < 20), ('gc_dist: %s' % gc_dist))
assert_true((amp_err < 1e-08), ('amp_err: %s' % amp_err))
dip_outside = Dipole(times=[0.0, 0.001], pos=[[0.0, 0.0, 1.0], [0.0, 0.0, 0.04]], amplitude=[1e-07, 1e-07], ori=[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0]], gof=1)
assert_raises(ValueError, make_forward_dipole, dip_outside, fname_bem, info, fname_trans)
times = [0.0, 0.0, 0.0, 0.001, 0.001, 0.002]
pos = ((numpy.random.rand(6, 3) * 0.02) + numpy.array([0.0, 0.0, 0.04])[numpy.newaxis, :])
amplitude = (numpy.random.rand(6) * 1e-07)
ori = (numpy.eye(6, 3) + numpy.eye(6, 3, (- 3)))
tempResult = arange(len(times))
	
===================================================================	
test_make_forward_dipole: 224	
----------------------------	

'Test forward-projecting dipoles.'
rng = numpy.random.RandomState(0)
evoked = read_evokeds(fname_evo)[0]
cov = read_cov(fname_cov)
dip_c = read_dipole(fname_dip)
picks = pick_types(evoked.info, meg='mag', eeg=False)
evoked.pick_channels([evoked.ch_names[p] for p in picks])
info = evoked.info
n_test_dipoles = 3
dipsel = numpy.sort(rng.permutation(numpy.arange(len(dip_c)))[:n_test_dipoles])
dip_test = Dipole(times=dip_c.times[dipsel], pos=dip_c.pos[dipsel], amplitude=dip_c.amplitude[dipsel], ori=dip_c.ori[dipsel], gof=dip_c.gof[dipsel])
sphere = make_sphere_model(head_radius=0.1)
with warnings.catch_warnings(record=True) as w:
    (fwd, stc) = make_forward_dipole(dip_test, sphere, info, trans=fname_trans)
    assert_true(issubclass(w[(- 1)].category, RuntimeWarning))
assert_true(isinstance(stc, list))
for nd in range(n_test_dipoles):
    assert_true(isinstance(stc[nd], VolSourceEstimate))
(times, pos, amplitude, ori, gof) = ([], [], [], [], [])
nave = 100
for s in stc:
    evo_test = simulate_evoked(fwd, s, info, cov, nave=nave, random_state=rng)
    (dfit, resid) = fit_dipole(evo_test, cov, sphere, None)
    times += dfit.times.tolist()
    pos += dfit.pos.tolist()
    amplitude += dfit.amplitude.tolist()
    ori += dfit.ori.tolist()
    gof += dfit.gof.tolist()
dip_fit = Dipole(times, pos, amplitude, ori, gof)
diff = (dip_test.pos - dip_fit.pos)
corr = numpy.corrcoef(dip_test.pos.ravel(), dip_fit.pos.ravel())[(0, 1)]
dist = numpy.sqrt(numpy.mean(numpy.sum((diff * diff), axis=1)))
gc_dist = ((180 / numpy.pi) * numpy.mean(numpy.arccos(numpy.sum((dip_test.ori * dip_fit.ori), axis=1))))
amp_err = numpy.sqrt(numpy.mean(((dip_test.amplitude - dip_fit.amplitude) ** 2)))
assert_allclose(dip_fit.pos, dip_test.pos, rtol=0, atol=0.01, err_msg='position mismatch')
assert_true((dist < 0.01), ('dist: %s' % dist))
assert_true((corr > (1 - 0.01)), ('corr: %s' % corr))
assert_true((gc_dist < 20), ('gc_dist: %s' % gc_dist))
assert_true((amp_err < 1e-08), ('amp_err: %s' % amp_err))
dip_outside = Dipole(times=[0.0, 0.001], pos=[[0.0, 0.0, 1.0], [0.0, 0.0, 0.04]], amplitude=[1e-07, 1e-07], ori=[[1.0, 0.0, 0.0], [1.0, 0.0, 0.0]], gof=1)
assert_raises(ValueError, make_forward_dipole, dip_outside, fname_bem, info, fname_trans)
times = [0.0, 0.0, 0.0, 0.001, 0.001, 0.002]
pos = ((numpy.random.rand(6, 3) * 0.02) + numpy.array([0.0, 0.0, 0.04])[numpy.newaxis, :])
amplitude = (numpy.random.rand(6) * 1e-07)
ori = (numpy.eye(6, 3) + numpy.eye(6, 3, (- 3)))
gof = (numpy.arange(len(times)) / len(times))
dip_even_samp = Dipole(times, pos, amplitude, ori, gof)
(fwd, stc) = make_forward_dipole(dip_even_samp, sphere, info, trans=fname_trans)
assert_true(isinstance, VolSourceEstimate)
tempResult = arange(0.0, 0.003, 0.001)
	
===================================================================	
_make_dipoles_sparse: 130	
----------------------------	

tempResult = arange(X.shape[1])
	
===================================================================	
mixed_norm_solver: 259	
----------------------------	

'Solve L1/L2 mixed-norm inverse problem with active set strategy.\n\n    Parameters\n    ----------\n    M : array, shape (n_sensors, n_times)\n        The data.\n    G : array, shape (n_sensors, n_dipoles)\n        The gain matrix a.k.a. lead field.\n    alpha : float\n        The regularization parameter. It should be between 0 and 100.\n        A value of 100 will lead to an empty active set (no active source).\n    maxit : int\n        The number of iterations.\n    tol : float\n        Tolerance on dual gap for convergence checking.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n    active_set_size : int\n        Size of active set increase at each iteration.\n    debias : bool\n        Debias source estimates.\n    n_orient : int\n        The number of orientation (1 : fixed or 3 : free or loose).\n    solver : \'prox\' | \'cd\' | \'bcd\' | \'auto\'\n        The algorithm to use for the optimization.\n    return_gap : bool\n        Return final duality gap.\n\n    Returns\n    -------\n    X : array, shape (n_active, n_times)\n        The source estimates.\n    active_set : array\n        The mask of active sources.\n    E : list\n        The value of the objective function over the iterations.\n    gap : float\n        Final duality gap. Returned only if return_gap is True.\n\n    References\n    ----------\n    .. [1] A. Gramfort, M. Kowalski, M. Hamalainen,\n       "Mixed-norm estimates for the M/EEG inverse problem using accelerated\n       gradient methods", Physics in Medicine and Biology, 2012.\n       http://dx.doi.org/10.1088/0031-9155/57/7/1937\n\n    .. [2] D. Strohmeier, Y. Bekhti, J. Haueisen, A. Gramfort,\n       "The Iterative Reweighted Mixed-Norm Estimate for Spatio-Temporal\n       MEG/EEG Source Reconstruction", IEEE Transactions of Medical Imaging,\n       Volume 35 (10), pp. 2218-2228, 15 April 2013.\n    '
n_dipoles = G.shape[1]
n_positions = (n_dipoles // n_orient)
(n_sensors, n_times) = M.shape
alpha_max = norm_l2inf(numpy.dot(G.T, M), n_orient, copy=False)
utils.logger.info(('-- ALPHA MAX : %s' % alpha_max))
alpha = float(alpha)
has_sklearn = True
try:
    from sklearn.linear_model.coordinate_descent import MultiTaskLasso
except ImportError:
    has_sklearn = False
if (solver == 'auto'):
    if (has_sklearn and (n_orient == 1)):
        solver = 'cd'
    else:
        solver = 'bcd'
if (solver == 'cd'):
    if ((n_orient == 1) and (not has_sklearn)):
        warn('Scikit-learn >= 0.12 cannot be found. Using block coordinate descent instead of coordinate descent.')
        solver = 'bcd'
    if (n_orient > 1):
        warn('Coordinate descent is only available for fixed orientation. Using block coordinate descent instead of coordinate descent')
        solver = 'bcd'
if (solver == 'cd'):
    utils.logger.info('Using coordinate descent')
    l21_solver = _mixed_norm_solver_cd
    lc = None
elif (solver == 'bcd'):
    utils.logger.info('Using block coordinate descent')
    l21_solver = _mixed_norm_solver_bcd
    G = numpy.asfortranarray(G)
    if (n_orient == 1):
        lc = numpy.sum((G * G), axis=0)
    else:
        lc = numpy.empty(n_positions)
        for j in range(n_positions):
            G_tmp = G[:, (j * n_orient):((j + 1) * n_orient)]
            lc[j] = scipy.linalg.norm(numpy.dot(G_tmp.T, G_tmp), ord=2)
else:
    utils.logger.info('Using proximal iterations')
    l21_solver = _mixed_norm_solver_prox
    lc = (1.01 * (scipy.linalg.norm(G, ord=2) ** 2))
if (active_set_size is not None):
    E = list()
    highest_d_obj = (- numpy.inf)
    X_init = None
    active_set = numpy.zeros(n_dipoles, dtype=numpy.bool)
    idx_large_corr = numpy.argsort(groups_norm2(numpy.dot(G.T, M), n_orient))
    new_active_idx = idx_large_corr[(- active_set_size):]
    if (n_orient > 1):
        tempResult = arange(n_orient)
	
===================================================================	
mixed_norm_solver: 284	
----------------------------	

'Solve L1/L2 mixed-norm inverse problem with active set strategy.\n\n    Parameters\n    ----------\n    M : array, shape (n_sensors, n_times)\n        The data.\n    G : array, shape (n_sensors, n_dipoles)\n        The gain matrix a.k.a. lead field.\n    alpha : float\n        The regularization parameter. It should be between 0 and 100.\n        A value of 100 will lead to an empty active set (no active source).\n    maxit : int\n        The number of iterations.\n    tol : float\n        Tolerance on dual gap for convergence checking.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n    active_set_size : int\n        Size of active set increase at each iteration.\n    debias : bool\n        Debias source estimates.\n    n_orient : int\n        The number of orientation (1 : fixed or 3 : free or loose).\n    solver : \'prox\' | \'cd\' | \'bcd\' | \'auto\'\n        The algorithm to use for the optimization.\n    return_gap : bool\n        Return final duality gap.\n\n    Returns\n    -------\n    X : array, shape (n_active, n_times)\n        The source estimates.\n    active_set : array\n        The mask of active sources.\n    E : list\n        The value of the objective function over the iterations.\n    gap : float\n        Final duality gap. Returned only if return_gap is True.\n\n    References\n    ----------\n    .. [1] A. Gramfort, M. Kowalski, M. Hamalainen,\n       "Mixed-norm estimates for the M/EEG inverse problem using accelerated\n       gradient methods", Physics in Medicine and Biology, 2012.\n       http://dx.doi.org/10.1088/0031-9155/57/7/1937\n\n    .. [2] D. Strohmeier, Y. Bekhti, J. Haueisen, A. Gramfort,\n       "The Iterative Reweighted Mixed-Norm Estimate for Spatio-Temporal\n       MEG/EEG Source Reconstruction", IEEE Transactions of Medical Imaging,\n       Volume 35 (10), pp. 2218-2228, 15 April 2013.\n    '
n_dipoles = G.shape[1]
n_positions = (n_dipoles // n_orient)
(n_sensors, n_times) = M.shape
alpha_max = norm_l2inf(numpy.dot(G.T, M), n_orient, copy=False)
utils.logger.info(('-- ALPHA MAX : %s' % alpha_max))
alpha = float(alpha)
has_sklearn = True
try:
    from sklearn.linear_model.coordinate_descent import MultiTaskLasso
except ImportError:
    has_sklearn = False
if (solver == 'auto'):
    if (has_sklearn and (n_orient == 1)):
        solver = 'cd'
    else:
        solver = 'bcd'
if (solver == 'cd'):
    if ((n_orient == 1) and (not has_sklearn)):
        warn('Scikit-learn >= 0.12 cannot be found. Using block coordinate descent instead of coordinate descent.')
        solver = 'bcd'
    if (n_orient > 1):
        warn('Coordinate descent is only available for fixed orientation. Using block coordinate descent instead of coordinate descent')
        solver = 'bcd'
if (solver == 'cd'):
    utils.logger.info('Using coordinate descent')
    l21_solver = _mixed_norm_solver_cd
    lc = None
elif (solver == 'bcd'):
    utils.logger.info('Using block coordinate descent')
    l21_solver = _mixed_norm_solver_bcd
    G = numpy.asfortranarray(G)
    if (n_orient == 1):
        lc = numpy.sum((G * G), axis=0)
    else:
        lc = numpy.empty(n_positions)
        for j in range(n_positions):
            G_tmp = G[:, (j * n_orient):((j + 1) * n_orient)]
            lc[j] = scipy.linalg.norm(numpy.dot(G_tmp.T, G_tmp), ord=2)
else:
    utils.logger.info('Using proximal iterations')
    l21_solver = _mixed_norm_solver_prox
    lc = (1.01 * (scipy.linalg.norm(G, ord=2) ** 2))
if (active_set_size is not None):
    E = list()
    highest_d_obj = (- numpy.inf)
    X_init = None
    active_set = numpy.zeros(n_dipoles, dtype=numpy.bool)
    idx_large_corr = numpy.argsort(groups_norm2(numpy.dot(G.T, M), n_orient))
    new_active_idx = idx_large_corr[(- active_set_size):]
    if (n_orient > 1):
        new_active_idx = ((n_orient * new_active_idx[:, None]) + np.arange(n_orient)[None, :]).ravel()
    active_set[new_active_idx] = True
    as_size = numpy.sum(active_set)
    for k in range(maxit):
        if (solver == 'bcd'):
            lc_tmp = lc[active_set[::n_orient]]
        elif (solver == 'cd'):
            lc_tmp = None
        else:
            lc_tmp = (1.01 * (scipy.linalg.norm(G[:, active_set], ord=2) ** 2))
        (X, as_, _) = l21_solver(M, G[:, active_set], alpha, lc_tmp, maxit=maxit, tol=tol, init=X_init, n_orient=n_orient)
        active_set[active_set] = as_.copy()
        idx_old_active_set = numpy.where(active_set)[0]
        (_, p_obj, d_obj, R) = dgap_l21(M, G, X, active_set, alpha, n_orient)
        highest_d_obj = max(d_obj, highest_d_obj)
        gap = (p_obj - highest_d_obj)
        E.append(p_obj)
        utils.logger.info(('Iteration %d :: p_obj %f :: dgap %f ::n_active_start %d :: n_active_end %d' % ((k + 1), p_obj, gap, (as_size // n_orient), (numpy.sum(active_set) // n_orient))))
        if (gap < tol):
            utils.logger.info(('Convergence reached ! (gap: %s < %s)' % (gap, tol)))
            break
        if (k < (maxit - 1)):
            idx_large_corr = numpy.argsort(groups_norm2(numpy.dot(G.T, R), n_orient))
            new_active_idx = idx_large_corr[(- active_set_size):]
            if (n_orient > 1):
                tempResult = arange(n_orient)
	
===================================================================	
_tf_mixed_norm_solver_bcd_active_set: 536	
----------------------------	

(n_sensors, n_times) = M.shape
n_sources = G.shape[1]
n_positions = (n_sources // n_orient)
tempResult = arange(n_positions)
	
===================================================================	
_tf_mixed_norm_solver_bcd_active_set: 552	
----------------------------	

(n_sensors, n_times) = M.shape
n_sources = G.shape[1]
n_positions = (n_sources // n_orient)
Z = dict.fromkeys(numpy.arange(n_positions), 0.0)
active_set = numpy.zeros(n_sources, dtype=numpy.bool)
active = []
if (Z_init is not None):
    if (Z_init.shape != (n_sources, (shape[1] * shape[2]))):
        raise Exception('Z_init must be None or an array with shape (n_sources, n_coefs).')
    for ii in range(n_positions):
        if numpy.any(Z_init[(ii * n_orient):((ii + 1) * n_orient)]):
            active_set[(ii * n_orient):((ii + 1) * n_orient)] = True
            active.append(ii)
    if len(active):
        Z.update(dict(zip(active, numpy.vsplit(Z_init[active_set], len(active)))))
E = []
candidates = range(n_positions)
d_obj = (- numpy.inf)
while True:
    tempResult = arange(n_positions)
	
===================================================================	
_tf_mixed_norm_solver_bcd_: 456	
----------------------------	

G = numpy.asfortranarray(G)
(n_sensors, n_times) = M.shape
n_sources = G.shape[1]
n_positions = (n_sources // n_orient)
Gd = G.copy()
tempResult = arange(n_positions)
	
===================================================================	
_gamma_map_opt: 28	
----------------------------	

'Hierarchical Bayes (Gamma-MAP).\n\n    Parameters\n    ----------\n    M : array, shape=(n_sensors, n_times)\n        Observation.\n    G : array, shape=(n_sensors, n_sources)\n        Forward operator.\n    alpha : float\n        Regularization parameter (noise variance).\n    maxit : int\n        Maximum number of iterations.\n    tol : float\n        Tolerance parameter for convergence.\n    group_size : int\n        Number of consecutive sources which use the same gamma.\n    update_mode : int\n        Update mode, 1: MacKay update (default), 3: Modified MacKay update.\n    gammas : array, shape=(n_sources,)\n        Initial values for posterior variances (gammas). If None, a\n        variance of 1.0 is used.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    X : array, shape=(n_active, n_times)\n        Estimated source time courses.\n    active_set : array, shape=(n_active,)\n        Indices of active sources.\n    '
G = G.copy()
M = M.copy()
if (gammas is None):
    gammas = numpy.ones(G.shape[1], dtype=numpy.float)
eps = np.finfo(float).eps
n_sources = G.shape[1]
(n_sensors, n_times) = M.shape
M_normalize_constant = scipy.linalg.norm(numpy.dot(M, M.T), ord='fro')
M /= numpy.sqrt(M_normalize_constant)
alpha /= M_normalize_constant
G_normalize_constant = scipy.linalg.norm(G, ord=numpy.inf)
G /= G_normalize_constant
if ((n_sources % group_size) != 0):
    raise ValueError('Number of sources has to be evenly dividable by the group size')
n_active = n_sources
tempResult = arange(n_sources)
	
===================================================================	
test_compute_debiasing: 11	
----------------------------	

'Test source amplitude debiasing'
rng = numpy.random.RandomState(42)
G = rng.randn(10, 4)
X = rng.randn(4, 20)
tempResult = arange(1, 5, dtype=numpy.float)
	
===================================================================	
BaseRaw.apply_gradient_compensation: 211	
----------------------------	

'Apply CTF gradient compensation.\n\n        .. warning:: The compensation matrices are stored with single\n                     precision, so repeatedly switching between different\n                     of compensation (e.g., 0->1->3->2) can increase\n                     numerical noise, especially if data are saved to\n                     disk in between changing grades. It is thus best to\n                     only use a single gradient compensation level in\n                     final analyses.\n\n        Parameters\n        ----------\n        grade : int\n            CTF gradient compensation level.\n        verbose : bool, str, int, or None\n            If not None, override default verbose level (see\n            :func:`mne.verbose` and :ref:`Logging documentation <tut_logging>`\n            for more).\n\n        Returns\n        -------\n        raw : instance of Raw\n            The modified Raw instance. Works in-place.\n        '
grade = int(grade)
current_comp = self.compensation_grade
if (current_comp != grade):
    if self.proj:
        raise RuntimeError('Cannot change compensation on data where projectors have been applied')
    from_comp = (current_comp if self.preload else self._read_comp_grade)
    comp = make_compensator(self.info, from_comp, grade)
    utils.logger.info(('Compensator constructed to change %d -> %d' % (current_comp, grade)))
    set_current_comp(self.info, grade)
    if self.preload:
        utils.logger.info('Applying compensator to loaded data')
        tempResult = arange(0, len(self.times), 10000)
	
===================================================================	
ToDataFrameMixin._get_check_picks: 33	
----------------------------	

'Get and check picks.'
if (picks is None):
    picks = list(range(self.info['nchan']))
tempResult = arange(len(picks_check))
	
===================================================================	
BaseRaw.get_data: 465	
----------------------------	

"Get data in the given range.\n\n        Parameters\n        ----------\n        picks : array-like of int | None\n            Indices of channels to get data from. If None, data from all\n            channels is returned\n        start : int\n            The first sample to include. Defaults to 0.\n        stop : int | None\n            End sample (first not to include). If None (default), the end of\n            the data is  used.\n        reject_by_annotation : None | 'omit' | 'NaN'\n            Whether to reject by annotation. If None (default), no rejection is\n            done. If 'omit', segments annotated with description starting with\n            'bad' are omitted. If 'NaN', the bad samples are filled with NaNs.\n        return_times : bool\n            Whether to return times as well. Defaults to False.\n\n        Returns\n        -------\n        data : ndarray, shape (n_channels, n_times)\n            Copy of the data in the given range.\n        times : ndarray, shape (n_times,)\n            Times associated with the data samples. Only returned if\n            return_times=True.\n\n        Notes\n        -----\n        .. versionadded:: 0.14.0\n        "
if (picks is None):
    tempResult = arange(self.info['nchan'])
	
===================================================================	
BaseRaw._update_times: 311	
----------------------------	

'Update times.'
tempResult = arange(self.n_times)
	
===================================================================	
ToDataFrameMixin.to_data_frame: 71	
----------------------------	

"Export data in tabular structure as a pandas DataFrame.\n\n        Columns and indices will depend on the object being converted.\n        Generally this will include as much relevant information as\n        possible for the data type being converted. This makes it easy\n        to convert data for use in packages that utilize dataframes,\n        such as statsmodels or seaborn.\n\n        Parameters\n        ----------\n        picks : array-like of int | None\n            If None only MEG and EEG channels are kept\n            otherwise the channels indices in picks are kept.\n        index : tuple of str | None\n            Column to be used as index for the data. Valid string options\n            are 'epoch', 'time' and 'condition'. If None, all three info\n            columns will be included in the table as categorial data.\n        scaling_time : float\n            Scaling to be applied to time units.\n        scalings : dict | None\n            Scaling to be applied to the channels picked. If None, defaults to\n            ``scalings=dict(eeg=1e6, grad=1e13, mag=1e15, misc=1.0)``.\n        copy : bool\n            If true, data will be copied. Else data may be modified in place.\n        start : int | None\n            If it is a Raw object, this defines a starting index for creating\n            the dataframe from a slice. The times will be interpolated from the\n            index and the sampling rate of the signal.\n        stop : int | None\n            If it is a Raw object, this defines a stop index for creating\n            the dataframe from a slice. The times will be interpolated from the\n            index and the sampling rate of the signal.\n\n        Returns\n        -------\n        df : instance of pandas.core.DataFrame\n            A dataframe suitable for usage with other\n            statistical/plotting/analysis packages. Column/Index values will\n            depend on the object type being converted, but should be\n            human-readable.\n        "
from ..epochs import BaseEpochs
from ..evoked import Evoked
from ..source_estimate import _BaseSourceEstimate
scaling_time = _scale_dep(scaling_time, scale_time, 'scaling_time', 'scale_time')
del scale_time
pd = _check_pandas_installed()
mindex = list()
if isinstance(self, _BaseSourceEstimate):
    if (self.subject is None):
        default_index = ['time']
    else:
        default_index = ['subject', 'time']
    data = self.data.T
    times = self.times
    shape = data.shape
    mindex.append(('subject', numpy.repeat(self.subject, shape[0])))
    if isinstance(self.vertices, list):
        col_names = [i for e in [['{0} {1}'.format(('LH' if (ii < 1) else 'RH'), vert) for vert in vertno] for (ii, vertno) in enumerate(self.vertices)] for i in e]
    else:
        col_names = ['VOL {0}'.format(vert) for vert in self.vertices]
elif isinstance(self, (BaseEpochs, BaseRaw, Evoked)):
    picks = self._get_check_picks(picks, self.ch_names)
    if isinstance(self, BaseEpochs):
        default_index = ['condition', 'epoch', 'time']
        data = self.get_data()[:, picks, :]
        times = self.times
        (n_epochs, n_picks, n_times) = data.shape
        data = np.hstack(data).T
        times = numpy.tile(times, n_epochs)
        id_swapped = dict(((v, k) for (k, v) in self.event_id.items()))
        names = [id_swapped[k] for k in self.events[:, 2]]
        mindex.append(('condition', numpy.repeat(names, n_times)))
        tempResult = arange(n_epochs)
	
===================================================================	
create_info: 893	
----------------------------	

"Create a basic Info instance suitable for use with create_raw.\n\n    Parameters\n    ----------\n    ch_names : list of str | int\n        Channel names. If an int, a list of channel names will be created\n        from :func:`range(ch_names) <range>`.\n    sfreq : float\n        Sample rate of the data.\n    ch_types : list of str | str\n        Channel types. If None, data are assumed to be misc.\n        Currently supported fields are 'ecg', 'bio', 'stim', 'eog', 'misc',\n        'seeg', 'ecog', 'mag', 'eeg', 'ref_meg', 'grad', 'emg', 'hbr' or 'hbo'.\n        If str, then all channels are assumed to be of the same type.\n    montage : None | str | Montage | DigMontage | list\n        A montage containing channel positions. If str or Montage is\n        specified, the channel info will be updated with the channel\n        positions. Default is None. If DigMontage is specified, the\n        digitizer information will be updated. A list of unique montages,\n        can be specifed and applied to the info. See also the documentation of\n        :func:`mne.channels.read_montage` for more information.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    info : instance of Info\n        The measurement info.\n\n    Notes\n    -----\n    The info dictionary will be sparsely populated to enable functionality\n    within the rest of the package. Advanced functionality such as source\n    localization can only be obtained through substantial, proper\n    modifications of the info structure (not recommended).\n\n    Note that the MEG device-to-head transform ``info['dev_head_t']`` will\n    be initialized to the identity transform.\n\n    Proper units of measure:\n    * V: eeg, eog, seeg, emg, ecg, bio, ecog\n    * T: mag\n    * T/m: grad\n    * M: hbo, hbr\n    * Am: dipole\n    * AU: misc\n    "
if isinstance(ch_names, int):
    tempResult = arange(ch_names)
	
===================================================================	
_make_dig_points: 278	
----------------------------	

"Construct digitizer info for the info.\n\n    Parameters\n    ----------\n    nasion : array-like | numpy.ndarray, shape (3,) | None\n        Point designated as the nasion point.\n    lpa : array-like |  numpy.ndarray, shape (3,) | None\n        Point designated as the left auricular point.\n    rpa : array-like |  numpy.ndarray, shape (3,) | None\n        Point designated as the right auricular point.\n    hpi : array-like | numpy.ndarray, shape (n_points, 3) | None\n        Points designated as head position indicator points.\n    extra_points : array-like | numpy.ndarray, shape (n_points, 3)\n        Points designed as the headshape points.\n    dig_ch_pos : dict\n        Dict of EEG channel positions.\n\n    Returns\n    -------\n    dig : list\n        List of digitizer points to be added to the info['dig'].\n    "
dig = []
if (lpa is not None):
    lpa = numpy.asarray(lpa)
    if (lpa.shape != (3,)):
        raise ValueError(('LPA should have the shape (3,) instead of %s' % (lpa.shape,)))
    dig.append({'r': lpa, 'ident': constants.FIFF.FIFFV_POINT_LPA, 'kind': constants.FIFF.FIFFV_POINT_CARDINAL, 'coord_frame': constants.FIFF.FIFFV_COORD_HEAD})
if (nasion is not None):
    nasion = numpy.asarray(nasion)
    if (nasion.shape != (3,)):
        raise ValueError(('Nasion should have the shape (3,) instead of %s' % (nasion.shape,)))
    dig.append({'r': nasion, 'ident': constants.FIFF.FIFFV_POINT_NASION, 'kind': constants.FIFF.FIFFV_POINT_CARDINAL, 'coord_frame': constants.FIFF.FIFFV_COORD_HEAD})
if (rpa is not None):
    rpa = numpy.asarray(rpa)
    if (rpa.shape != (3,)):
        raise ValueError(('RPA should have the shape (3,) instead of %s' % (rpa.shape,)))
    dig.append({'r': rpa, 'ident': constants.FIFF.FIFFV_POINT_RPA, 'kind': constants.FIFF.FIFFV_POINT_CARDINAL, 'coord_frame': constants.FIFF.FIFFV_COORD_HEAD})
if (hpi is not None):
    hpi = numpy.asarray(hpi)
    if ((hpi.ndim != 2) or (hpi.shape[1] != 3)):
        raise ValueError(('HPI should have the shape (n_points, 3) instead of %s' % (hpi.shape,)))
    for (idx, point) in enumerate(hpi):
        dig.append({'r': point, 'ident': (idx + 1), 'kind': constants.FIFF.FIFFV_POINT_HPI, 'coord_frame': constants.FIFF.FIFFV_COORD_HEAD})
if (extra_points is not None):
    extra_points = numpy.asarray(extra_points)
    if (extra_points.shape[1] != 3):
        raise ValueError(('Points should have the shape (n_points, 3) instead of %s' % (extra_points.shape,)))
    for (idx, point) in enumerate(extra_points):
        dig.append({'r': point, 'ident': (idx + 1), 'kind': constants.FIFF.FIFFV_POINT_EXTRA, 'coord_frame': constants.FIFF.FIFFV_COORD_HEAD})
if (dig_ch_pos is not None):
    keys = sorted(dig_ch_pos.keys())
    try:
        idents = [int(key[(- 3):]) for key in keys]
    except ValueError:
        tempResult = arange(1, (len(keys) + 1))
	
===================================================================	
read_big: 40	
----------------------------	

"Read large chunks of data (>16MB) Windows-friendly.\n\n    Parameters\n    ----------\n    fid : file\n        Open file to read from.\n    size : int or None\n        Number of bytes to read. If None, the whole file is read.\n\n    Returns\n    -------\n    buf : bytes\n        The data.\n\n    Notes\n    -----\n    Windows (argh) can't handle reading large chunks of data, so we\n    have to do it piece-wise, possibly related to:\n       http://stackoverflow.com/questions/4226941\n\n    Examples\n    --------\n    This code should work for normal files and .gz files:\n\n        >>> import numpy as np\n        >>> import gzip, os, tempfile, shutil\n        >>> fname = tempfile.mkdtemp()\n        >>> fname_gz = os.path.join(fname, 'temp.gz')\n        >>> fname = os.path.join(fname, 'temp.bin')\n        >>> randgen = np.random.RandomState(9)\n        >>> x = randgen.randn(3000000)  # > 16MB data\n        >>> with open(fname, 'wb') as fid: x.tofile(fid)\n        >>> with open(fname, 'rb') as fid: y = np.fromstring(read_big(fid))\n        >>> assert np.all(x == y)\n        >>> fid_gz = gzip.open(fname_gz, 'wb')\n        >>> _ = fid_gz.write(x.tostring())\n        >>> fid_gz.close()\n        >>> fid_gz = gzip.open(fname_gz, 'rb')\n        >>> y = np.fromstring(read_big(fid_gz))\n        >>> assert np.all(x == y)\n        >>> fid_gz.close()\n        >>> shutil.rmtree(os.path.dirname(fname))\n\n    "
buf_size = 16777216
if (size is None):
    if (not isinstance(fid, gzip.GzipFile)):
        size = (os.fstat(fid.fileno()).st_size - fid.tell())
if (size is not None):
    tempResult = arange(0, size, buf_size)
	
===================================================================	
_read_segments_file: 70	
----------------------------	

'Read a chunk of raw data.'
if (n_channels is None):
    n_channels = raw.info['nchan']
n_bytes = np.dtype(dtype).itemsize
data_offset = (((n_channels * start) * n_bytes) + offset)
data_left = ((stop - start) * n_channels)
block_size = (((int(100000000.0) // n_bytes) // n_channels) * n_channels)
block_size = min(data_left, block_size)
with open(raw._filenames[fi], 'rb', buffering=0) as fid:
    fid.seek(data_offset)
    tempResult = arange(0, data_left, block_size)
	
===================================================================	
RawArtemis123.__init__: 217	
----------------------------	

from scipy.spatial.distance import cdist
(fname, ext) = os.path.splitext(input_fname)
if (ext == '.txt'):
    input_fname = (fname + '.bin')
elif (ext != '.bin'):
    raise RuntimeError(('Valid artemis123 files must end in "txt"' + ' or ".bin".'))
if (not os.path.exists(input_fname)):
    raise RuntimeError(('%s - Not Found' % input_fname))
(info, header_info) = _get_artemis123_info(input_fname, pos_fname=pos_fname)
last_samps = [(header_info.get('num_samples', 1) - 1)]
super(RawArtemis123, self).__init__(info, preload, filenames=[input_fname], raw_extras=[header_info], last_samps=last_samps, orig_format=numpy.float32, verbose=verbose)
self.info['hpi_results'] = []
if add_head_trans:
    n_hpis = 0
    for d in info['hpi_subsystem']['hpi_coils']:
        if (d['event_bits'] == [256]):
            n_hpis += 1
    if (n_hpis < 3):
        warn((('%d HPIs active. At least 3 needed to perform' % n_hpis) + 'head localization\n *NO* head localization performed'))
    else:
        (hpi_dev, hpi_g) = _fit_device_hpi_positions(self, t_win=[0, 0.25])
        if (pos_fname is not None):
            utils.logger.info((('No Digitized cHPI locations found.\n' + 'Assuming cHPIs are placed at cardinal ') + 'fiducial locations. (Nasion, LPA, RPA'))
            hpi_head = numpy.array([d['r'] for d in self.info.get('dig', []) if (d['kind'] == constants.FIFF.FIFFV_POINT_HPI)])
            if (len(hpi_head) != len(hpi_dev)):
                mesg = (('number of digitized (%d) and ' + 'active (%d) HPI coils are ') + 'not the same.')
                raise RuntimeError((mesg % (len(hpi_head), len(hpi_dev))))
            (head_to_dev_t, order) = _fit_coil_order_dev_head_trans(hpi_dev, hpi_head)
            self.info['dev_head_t'] = Transform(constants.FIFF.FIFFV_COORD_DEVICE, constants.FIFF.FIFFV_COORD_HEAD, head_to_dev_t)
            dig_dists = cdist(hpi_head, hpi_head)
            dev_dists = cdist(hpi_dev, hpi_dev)
            tmp_dists = numpy.abs((dig_dists - dev_dists))
            dist_limit = (tmp_dists.max() * 1.1)
        else:
            utils.logger.info('Assuming Cardinal HPIs')
            nas = hpi_dev[0]
            lpa = hpi_dev[2]
            rpa = hpi_dev[1]
            t = get_ras_to_neuromag_trans(nas, lpa, rpa)
            self.info['dev_head_t'] = Transform(constants.FIFF.FIFFV_COORD_DEVICE, constants.FIFF.FIFFV_COORD_HEAD, t)
            nas = apply_trans(t, nas)
            lpa = apply_trans(t, lpa)
            rpa = apply_trans(t, rpa)
            hpi = [nas, rpa, lpa]
            self.info['dig'] = _make_dig_points(nasion=nas, lpa=lpa, rpa=rpa, hpi=hpi)
            order = numpy.array([0, 1, 2])
            dist_limit = 0.005
        hpi_result = dict()
        dig = []
        for (idx, point) in enumerate(hpi_dev):
            dig.append({'r': point, 'ident': (idx + 1), 'kind': constants.FIFF.FIFFV_POINT_HPI, 'coord_frame': constants.FIFF.FIFFV_COORD_DEVICE})
        hpi_result['dig_points'] = dig
        hpi_result['coord_trans'] = self.info['dev_head_t']
        hpi_result['order'] = (order + 1)
        tempResult = arange(3)
	
===================================================================	
_read_segments_c: 93	
----------------------------	

'Read chunk of vectorized raw data.'
n_samples = raw._n_samples
dtype = _fmt_dtype_dict[raw.orig_format]
n_bytes = _fmt_byte_dict[raw.orig_format]
n_channels = len(raw.ch_names)
trigger_ch = raw._event_ch
block = numpy.zeros((n_channels, (stop - start)))
with open(raw._filenames[fi], 'rb', buffering=0) as fid:
    tempResult = arange(n_channels)
	
===================================================================	
_get_vhdr_info: 280	
----------------------------	

"Extract all the information from the header file.\n\n    Parameters\n    ----------\n    vhdr_fname : str\n        Raw EEG header to be read.\n    eog : list of str\n        Names of channels that should be designated EOG channels. Names should\n        correspond to the vhdr file.\n    misc : list or tuple of str | 'auto'\n        Names of channels or list of indices that should be designated\n        MISC channels. Values should correspond to the electrodes\n        in the vhdr file. If 'auto', units in vhdr file are used for inferring\n        misc channels. Default is ``'auto'``.\n    scale : float\n        The scaling factor for EEG data. Unless specified otherwise by\n        header file, units are in microvolts. Default scale factor is 1.\n    montage : str | None | instance of Montage\n        Path or instance of montage containing electrode positions. If None,\n        read sensor locations from header file if present, otherwise (0, 0, 0).\n        See the documentation of :func:`mne.channels.read_montage` for more\n        information.\n\n    Returns\n    -------\n    info : Info\n        The measurement info.\n    fmt : str\n        The data format in the file.\n    edf_info : dict\n        A dict containing Brain Vision specific parameters.\n    events : array, shape (n_events, 3)\n        Events from the corresponding vmrk file.\n    "
scale = float(scale)
ext = os.path.splitext(vhdr_fname)[(- 1)]
if (ext != '.vhdr'):
    raise IOError(("The header file must be given to read the data, not a file with extension '%s'." % ext))
with open(vhdr_fname, 'rb') as f:
    header = f.readline()
    codepage = 'utf-8'
    header = header.decode('ascii', 'ignore').strip()
    _check_hdr_version(header)
    settings = f.read()
    try:
        cp_setting = re.search('Codepage=(.+)', settings.decode('ascii', 'ignore'), (re.IGNORECASE & re.MULTILINE))
        if cp_setting:
            codepage = cp_setting.group(1).strip()
        if (codepage == 'ANSI'):
            codepage = 'cp1252'
        settings = settings.decode(codepage)
    except UnicodeDecodeError:
        settings = settings.decode('latin-1')
if (settings.find('[Comment]') != (- 1)):
    (params, settings) = settings.split('[Comment]')
else:
    (params, settings) = (settings, '')
cfg = externals.six.moves.configparser.ConfigParser()
if hasattr(cfg, 'read_file'):
    cfg.read_file(StringIO(params))
else:
    cfg.readfp(StringIO(params))
sfreq = (1000000.0 / cfg.getfloat('Common Infos', 'SamplingInterval'))
info = _empty_info(sfreq)
order = cfg.get('Common Infos', 'DataOrientation')
if (order not in _orientation_dict):
    raise NotImplementedError(('Data Orientation %s is not supported' % order))
order = _orientation_dict[order]
data_format = cfg.get('Common Infos', 'DataFormat')
if (data_format == 'BINARY'):
    fmt = cfg.get('Binary Infos', 'BinaryFormat')
    if (fmt not in _fmt_dict):
        raise NotImplementedError(('Datatype %s is not supported' % fmt))
    fmt = _fmt_dict[fmt]
else:
    fmt = dict(((key, cfg.get('ASCII Infos', key)) for key in cfg.options('ASCII Infos')))
path = os.path.dirname(vhdr_fname)
data_filename = os.path.join(path, cfg.get('Common Infos', 'DataFile'))
info['meas_date'] = int(time.time())
info['buffer_size_sec'] = 1.0
nchan = (cfg.getint('Common Infos', 'NumberOfChannels') + 1)
n_samples = None
if (order == 'C'):
    try:
        n_samples = cfg.getint('Common Infos', 'DataPoints')
    except externals.six.moves.configparser.NoOptionError:
        utils.logger.warning('No info on DataPoints found. Inferring number of samples from the data file size.')
        with open(data_filename, 'rb') as fid:
            fid.seek(0, 2)
            n_bytes = fid.tell()
            n_samples = ((n_bytes // _fmt_byte_dict[fmt]) // (nchan - 1))
ch_names = ([''] * nchan)
cals = numpy.empty(nchan)
ranges = numpy.empty(nchan)
cals.fill(numpy.nan)
ch_dict = dict()
misc_chs = dict()
for (chan, props) in cfg.items('Channel Infos'):
    n = (int(re.findall('ch(\\d+)', chan)[0]) - 1)
    props = props.split(',')
    if (len(props) < 4):
        props += ('µV',)
    (name, _, resolution, unit) = props[:4]
    ch_dict[chan] = name
    ch_names[n] = name
    if (resolution == ''):
        if (not unit):
            resolution = 1e-06
        else:
            resolution = 1.0
    unit = unit.replace('Â', '')
    cals[n] = float(resolution)
    ranges[n] = (_unit_dict.get(unit, 1) * scale)
    if (unit not in ('V', 'µV', 'uV')):
        misc_chs[name] = (constants.FIFF.FIFF_UNIT_CEL if (unit == 'C') else constants.FIFF.FIFF_UNIT_NONE)
misc = (list(misc_chs.keys()) if (misc == 'auto') else misc)
if (cfg.has_section('Coordinates') and (montage is None)):
    from ...transforms import _sph_to_cart
    from ...channels.montage import Montage
    montage_pos = list()
    montage_names = list()
    to_misc = list()
    for ch in cfg.items('Coordinates'):
        ch_name = ch_dict[ch[0]]
        montage_names.append(ch_name)
        (radius, theta, phi) = map(float, ch[1].split(','))
        pol = numpy.deg2rad(theta)
        az = numpy.deg2rad(phi)
        pos = _sph_to_cart(numpy.array([[(radius * 85.0), az, pol]]))[0]
        if ((pos == 0).all() and (ch_name not in (list(eog) + misc))):
            to_misc.append(ch_name)
        montage_pos.append(pos)
    tempResult = arange(len(montage_pos))
	
===================================================================	
RawBTi._read_segment_file: 566	
----------------------------	

'Read a segment of data from a file.'
bti_info = self._raw_extras[fi]
fname = bti_info['pdf_fname']
dtype = bti_info['dtype']
n_channels = self.info['nchan']
n_bytes = np.dtype(dtype).itemsize
data_left = ((stop - start) * n_channels)
read_cals = numpy.empty((bti_info['total_chans'],))
for ch in bti_info['chs']:
    read_cals[ch['index']] = ch['cal']
block_size = (((int(100000000.0) // n_bytes) // n_channels) * n_channels)
block_size = min(data_left, block_size)
with _bti_open(fname, 'rb') as fid:
    fid.seek((bti_info['bytes_per_slice'] * start), 0)
    tempResult = arange(0, data_left, block_size)
	
===================================================================	
_read_bti_header: 524	
----------------------------	

'Read bti PDF header.'
info = (_read_bti_header_pdf(pdf_fname) if (pdf_fname is not None) else dict())
cfg = _read_config(config_fname)
info['bti_transform'] = cfg['transforms']
chans = info.get('chs', None)
if (chans is not None):
    chans_cfg = [c for c in cfg['chs'] if (c['chan_no'] in [c_['chan_no'] for c_ in chans])]
    match = ([c['chan_no'] for c in chans_cfg] == [c['chan_no'] for c in chans])
    if (not match):
        raise RuntimeError('Could not match raw data channels with config channels. Some of the channels found are not described in config.')
else:
    chans_cfg = cfg['chs']
    chans = [dict() for d in chans_cfg]
for (ch, ch_cfg) in zip(chans, chans_cfg):
    ch['upb'] = ch_cfg['units_per_bit']
    ch['gain'] = ch_cfg['gain']
    ch['name'] = ch_cfg['name']
    if (ch_cfg.get('dev', dict()).get('transform', None) is not None):
        ch['loc'] = _coil_trans_to_loc(ch_cfg['dev']['transform'])
    else:
        ch['loc'] = None
    if (pdf_fname is not None):
        if (info['data_format'] <= 2):
            ch['cal'] = ((ch['scale'] * ch['upb']) / float(ch['gain']))
        else:
            ch['cal'] = (ch['scale'] * ch['gain'])
    else:
        ch['cal'] = ch['scale'] = 1.0
if sort_by_ch_name:
    by_index = [(i, d['index']) for (i, d) in enumerate(chans)]
    by_index.sort(key=(lambda c: c[1]))
    by_index = [idx[0] for idx in by_index]
    chs = [chans[pos] for pos in by_index]
    sort_by_name_idx = [(i, d['name']) for (i, d) in enumerate(chs)]
    a_chs = [c for c in sort_by_name_idx if c[1].startswith('A')]
    other_chs = [c for c in sort_by_name_idx if (not c[1].startswith('A'))]
    sort_by_name_idx = (sorted(a_chs, key=(lambda c: int(c[1][1:]))) + sorted(other_chs))
    sort_by_name_idx = [idx[0] for idx in sort_by_name_idx]
    info['chs'] = [chans[pos] for pos in sort_by_name_idx]
    info['order'] = sort_by_name_idx
else:
    info['chs'] = chans
    tempResult = arange(len(chans))
	
===================================================================	
RawCNT._read_segment_file: 196	
----------------------------	

'Take a chunk of raw data, multiply by mult or cals, and store.'
n_channels = (self.info['nchan'] - 1)
channel_offset = self._raw_extras[0]['channel_offset']
baselines = self._raw_extras[0]['baselines']
stim_ch = self._raw_extras[0]['stim_channel']
n_bytes = self._raw_extras[0]['n_bytes']
dtype = ('<i4' if (n_bytes == 4) else '<i2')
tempResult = arange((n_channels + 1))
	
===================================================================	
RawCNT._read_segment_file: 204	
----------------------------	

'Take a chunk of raw data, multiply by mult or cals, and store.'
n_channels = (self.info['nchan'] - 1)
channel_offset = self._raw_extras[0]['channel_offset']
baselines = self._raw_extras[0]['baselines']
stim_ch = self._raw_extras[0]['stim_channel']
n_bytes = self._raw_extras[0]['n_bytes']
dtype = ('<i4' if (n_bytes == 4) else '<i2')
sel = numpy.arange((n_channels + 1))[idx]
chunk_size = (channel_offset * n_channels)
data_left = ((stop - start) * n_channels)
block_size = (((int(100000000.0) // n_bytes) // chunk_size) * chunk_size)
block_size = min(data_left, block_size)
s_offset = (start % channel_offset)
with open(self._filenames[fi], 'rb', buffering=0) as fid:
    fid.seek((900 + (n_channels * (75 + ((start - s_offset) * n_bytes)))))
    tempResult = arange(0, data_left, block_size)
	
===================================================================	
test_read_ctf: 115	
----------------------------	

'Test CTF reader'
temp_dir = _TempDir()
out_fname = os.path.join(temp_dir, 'test_py_raw.fif')
os.mkdir(os.path.join(temp_dir, 'randpos'))
ctf_eeg_fname = os.path.join(temp_dir, 'randpos', ctf_fname_catch)
shutil.copytree(os.path.join(ctf_dir, ctf_fname_catch), ctf_eeg_fname)
with warnings.catch_warnings(record=True) as w:
    raw = _test_raw_reader(read_raw_ctf, directory=ctf_eeg_fname)
assert_true(all((('MISC channel' in str(ww.message)) for ww in w)))
picks = pick_types(raw.info, meg=False, eeg=True)
pos = np.random.RandomState(42).randn(len(picks), 3)
fake_eeg_fname = os.path.join(ctf_eeg_fname, 'catch-alp-good-f.eeg')
with open(fake_eeg_fname, 'wb') as fid:
    fid.write('foo\n'.encode('ascii'))
assert_raises(RuntimeError, read_raw_ctf, ctf_eeg_fname)
with open(fake_eeg_fname, 'wb') as fid:
    for (ii, ch_num) in enumerate(picks):
        args = ((str((ch_num + 1)), raw.ch_names[ch_num]) + tuple((('%0.5f' % x) for x in (100 * pos[ii]))))
        fid.write(('\t'.join(args) + '\n').encode('ascii'))
pos_read_old = numpy.array([raw.info['chs'][p]['loc'][:3] for p in picks])
with warnings.catch_warnings(record=True) as w:
    raw = read_raw_ctf(ctf_eeg_fname)
assert_true(all((('MISC channel' in str(ww.message)) for ww in w)))
pos_read = numpy.array([raw.info['chs'][p]['loc'][:3] for p in picks])
assert_allclose(apply_trans(raw.info['ctf_head_t'], pos), pos_read, rtol=1e-05, atol=1e-05)
assert_true(((pos_read == pos_read_old).mean() < 0.1))
shutil.copy(os.path.join(ctf_dir, 'catch-alp-good-f.ds_randpos_raw.fif'), os.path.join(temp_dir, 'randpos', 'catch-alp-good-f.ds_raw.fif'))
os.mkdir(os.path.join(temp_dir, 'nohc'))
ctf_no_hc_fname = os.path.join(temp_dir, 'no_hc', ctf_fname_catch)
shutil.copytree(ctf_eeg_fname, ctf_no_hc_fname)
remove_base = os.path.join(ctf_no_hc_fname, os.path.basename(ctf_fname_catch[:(- 3)]))
os.remove((remove_base + '.hc'))
with warnings.catch_warnings(record=True):
    assert_raises(RuntimeError, read_raw_ctf, ctf_no_hc_fname)
os.remove((remove_base + '.eeg'))
shutil.copy(os.path.join(ctf_dir, 'catch-alp-good-f.ds_nohc_raw.fif'), os.path.join(temp_dir, 'no_hc', 'catch-alp-good-f.ds_raw.fif'))
use_fnames = [os.path.join(ctf_dir, c) for c in ctf_fnames]
for fname in use_fnames:
    raw_c = read_raw_fif((fname + '_raw.fif'), preload=True)
    with warnings.catch_warnings(record=True) as w:
        raw = read_raw_ctf(fname)
    assert_true(all((('MISC channel' in str(ww.message)) for ww in w)))
    assert_array_equal(raw.ch_names, raw_c.ch_names)
    assert_allclose(raw.times, raw_c.times)
    assert_allclose(raw._cals, raw_c._cals)
    for key in ('version', 'usecs'):
        assert_equal(raw.info['meas_id'][key], raw_c.info['meas_id'][key])
    py_time = raw.info['meas_id']['secs']
    c_time = raw_c.info['meas_id']['secs']
    max_offset = ((24 * 60) * 60)
    assert_true(((c_time - max_offset) <= py_time <= c_time))
    for t in ('dev_head_t', 'dev_ctf_t', 'ctf_head_t'):
        assert_allclose(raw.info[t]['trans'], raw_c.info[t]['trans'], rtol=0.0001, atol=1e-07)
    for key in ('acq_pars', 'acq_stim', 'bads', 'ch_names', 'custom_ref_applied', 'description', 'events', 'experimenter', 'highpass', 'line_freq', 'lowpass', 'nchan', 'proj_id', 'proj_name', 'projs', 'sfreq', 'subject_info'):
        assert_equal(raw.info[key], raw_c.info[key], key)
    if (os.path.basename(fname) not in single_trials):
        assert_equal(raw.info['buffer_size_sec'], raw_c.info['buffer_size_sec'])
    assert_equal(len(raw.info['comps']), len(raw_c.info['comps']))
    for (c1, c2) in zip(raw.info['comps'], raw_c.info['comps']):
        for key in ('colcals', 'rowcals'):
            assert_allclose(c1[key], c2[key])
        assert_equal(c1['save_calibrated'], c2['save_calibrated'])
        for key in ('row_names', 'col_names', 'nrow', 'ncol'):
            assert_array_equal(c1['data'][key], c2['data'][key])
        assert_allclose(c1['data']['data'], c2['data']['data'], atol=1e-07, rtol=1e-05)
    assert_allclose(raw.info['hpi_results'][0]['coord_trans']['trans'], raw_c.info['hpi_results'][0]['coord_trans']['trans'], rtol=1e-05, atol=1e-07)
    assert_equal(len(raw.info['chs']), len(raw_c.info['chs']))
    for (ii, (c1, c2)) in enumerate(zip(raw.info['chs'], raw_c.info['chs'])):
        for key in ('kind', 'scanno', 'unit', 'ch_name', 'unit_mul', 'range', 'coord_frame', 'coil_type', 'logno'):
            if ((c1['ch_name'] == 'RMSP') and ('catch-alp-good-f' in fname) and (key in ('kind', 'unit', 'coord_frame', 'coil_type', 'logno'))):
                continue
            assert_equal(c1[key], c2[key], err_msg=key)
        for key in ('cal',):
            assert_allclose(c1[key], c2[key], atol=1e-06, rtol=0.0001, err_msg=('raw.info["chs"][%d][%s]' % (ii, key)))
        for key in ('loc',):
            if ((c1['ch_name'] == 'RMSP') and ('catch-alp-good-f' in fname)):
                continue
            assert_allclose(c1[key][:3], c2[key][:3], atol=1e-06, rtol=0.0001, err_msg=('raw.info["chs"][%d][%s]' % (ii, key)))
            assert_allclose(c1[key][9:12], c2[key][9:12], atol=1e-06, rtol=0.0001, err_msg=('raw.info["chs"][%d][%s]' % (ii, key)))
    if fname.endswith('catch-alp-good-f.ds'):
        raw.info['dig'] = raw.info['dig'][:(- 10)]
    raw_c.save(out_fname, overwrite=True, buffer_size_sec=1.0)
    raw_read = read_raw_fif(out_fname)
    rng = numpy.random.RandomState(0)
    tempResult = arange(len(raw.ch_names))
	
===================================================================	
RawEDF._read_segment_file: 47	
----------------------------	

'Read a chunk of raw data.'
from scipy.interpolate import interp1d
if (mult is not None):
    raise NotImplementedError('mult is not supported yet')
exclude = self._raw_extras[fi]['exclude']
tempResult = arange(self.info['nchan'])
	
===================================================================	
_get_info: 73	
----------------------------	

'Get measurement info.'
from scipy import io
info = _empty_info(sfreq=eeg.srate)
update_ch_names = True
path = None
if ((not isinstance(eeg.chanlocs, numpy.ndarray)) and (eeg.nbchan == 1)):
    eeg.chanlocs = [eeg.chanlocs]
if (len(eeg.chanlocs) > 0):
    pos_fields = ['X', 'Y', 'Z']
    if (isinstance(eeg.chanlocs, numpy.ndarray) and (not isinstance(eeg.chanlocs[0], scipy.io.matlab.mio5_params.mat_struct))):
        has_pos = all(((fld in eeg.chanlocs[0].dtype.names) for fld in pos_fields))
    else:
        has_pos = all((hasattr(eeg.chanlocs[0], fld) for fld in pos_fields))
    get_pos = (has_pos and (montage is None))
    (pos_ch_names, ch_names, pos) = (list(), list(), list())
    kind = 'user_defined'
    update_ch_names = False
    for chanloc in eeg.chanlocs:
        ch_names.append(chanloc.labels)
        if get_pos:
            loc_x = _to_loc(chanloc.X)
            loc_y = _to_loc(chanloc.Y)
            loc_z = _to_loc(chanloc.Z)
            locs = numpy.r_[((- loc_y), loc_x, loc_z)]
            if (not numpy.any(numpy.isnan(locs))):
                pos_ch_names.append(chanloc.labels)
                pos.append(locs)
    n_channels_with_pos = len(pos_ch_names)
    info = create_info(ch_names, eeg.srate, ch_types='eeg')
    if (n_channels_with_pos > 0):
        tempResult = arange(n_channels_with_pos)
	
===================================================================	
RawEGI.__init__: 92	
----------------------------	

if (eog is None):
    eog = []
if (misc is None):
    misc = []
with open(input_fname, 'rb') as fid:
    utils.logger.info(('Reading EGI header from %s...' % input_fname))
    egi_info = _read_header(fid)
    utils.logger.info('    Reading events ...')
    egi_events = _read_events(fid, egi_info)
    if ((egi_info['value_range'] != 0) and (egi_info['bits'] != 0)):
        cal = (egi_info['value_range'] / (2 ** egi_info['bits']))
    else:
        cal = 1e-06
utils.logger.info('    Assembling measurement info ...')
event_codes = []
if (egi_info['n_events'] > 0):
    event_codes = list(egi_info['event_codes'])
    if (include is None):
        exclude_list = (['sync', 'TREV'] if (exclude is None) else exclude)
        exclude_inds = [i for (i, k) in enumerate(event_codes) if (k in exclude_list)]
        more_excludes = []
        if (exclude is None):
            for (ii, event) in enumerate(egi_events):
                if ((event.sum() <= 1) and event_codes[ii]):
                    more_excludes.append(ii)
        if ((len(exclude_inds) + len(more_excludes)) == len(event_codes)):
            warn('Did not find any event code with more than one event.', RuntimeWarning)
        else:
            exclude_inds.extend(more_excludes)
        exclude_inds.sort()
        tempResult = arange(egi_info['n_events'])
	
===================================================================	
RawEGI.__init__: 104	
----------------------------	

if (eog is None):
    eog = []
if (misc is None):
    misc = []
with open(input_fname, 'rb') as fid:
    utils.logger.info(('Reading EGI header from %s...' % input_fname))
    egi_info = _read_header(fid)
    utils.logger.info('    Reading events ...')
    egi_events = _read_events(fid, egi_info)
    if ((egi_info['value_range'] != 0) and (egi_info['bits'] != 0)):
        cal = (egi_info['value_range'] / (2 ** egi_info['bits']))
    else:
        cal = 1e-06
utils.logger.info('    Assembling measurement info ...')
event_codes = []
if (egi_info['n_events'] > 0):
    event_codes = list(egi_info['event_codes'])
    if (include is None):
        exclude_list = (['sync', 'TREV'] if (exclude is None) else exclude)
        exclude_inds = [i for (i, k) in enumerate(event_codes) if (k in exclude_list)]
        more_excludes = []
        if (exclude is None):
            for (ii, event) in enumerate(egi_events):
                if ((event.sum() <= 1) and event_codes[ii]):
                    more_excludes.append(ii)
        if ((len(exclude_inds) + len(more_excludes)) == len(event_codes)):
            warn('Did not find any event code with more than one event.', RuntimeWarning)
        else:
            exclude_inds.extend(more_excludes)
        exclude_inds.sort()
        include_ = [i for i in numpy.arange(egi_info['n_events']) if (i not in exclude_inds)]
        include_names = [k for (i, k) in enumerate(event_codes) if (i in include_)]
    else:
        include_ = [i for (i, k) in enumerate(event_codes) if (k in include)]
        include_names = include
    for (kk, v) in [('include', include_names), ('exclude', exclude)]:
        if isinstance(v, list):
            for k in v:
                if (k not in event_codes):
                    raise ValueError(('Could find event named "%s"' % k))
        elif (v is not None):
            raise ValueError(('`%s` must be None or of type list' % kk))
    tempResult = arange(len(include_))
	
===================================================================	
RawMff.__init__: 169	
----------------------------	

'Init the RawMff class.'
utils.logger.info(('Reading EGI MFF Header from %s...' % input_fname))
egi_info = _read_header(input_fname)
if (eog is None):
    eog = []
if (misc is None):
    misc = np.where((np.array(egi_info['chan_type']) != 'eeg'))[0].tolist()
utils.logger.info('    Reading events ...')
(egi_events, egi_info) = _read_events(input_fname, egi_info)
gains = _get_gains(os.path.join(input_fname, egi_info['info_fname']))
if ((egi_info['value_range'] != 0) and (egi_info['bits'] != 0)):
    cals = [(egi_info['value_range'] / (2 ** egi_info['bits'])) for i in range(len(egi_info['chan_type']))]
else:
    cal_scales = {'uV': 1e-06, 'V': 1}
    cals = [cal_scales[t] for t in egi_info['chan_unit']]
if ('gcal' in gains):
    cals *= gains['gcal']
if ('ical' in gains):
    pass
utils.logger.info('    Assembling measurement info ...')
if (egi_info['n_events'] > 0):
    event_codes = list(egi_info['event_codes'])
    if (include is None):
        exclude_list = (['sync', 'TREV'] if (exclude is None) else exclude)
        exclude_inds = [i for (i, k) in enumerate(event_codes) if (k in exclude_list)]
        more_excludes = []
        if (exclude is None):
            for (ii, event) in enumerate(egi_events):
                if ((event.sum() <= 1) and event_codes[ii]):
                    more_excludes.append(ii)
        if ((len(exclude_inds) + len(more_excludes)) == len(event_codes)):
            warn('Did not find any event code with more than one event.', RuntimeWarning)
        else:
            exclude_inds.extend(more_excludes)
        exclude_inds.sort()
        tempResult = arange(egi_info['n_events'])
	
===================================================================	
RawMff.__init__: 183	
----------------------------	

'Init the RawMff class.'
utils.logger.info(('Reading EGI MFF Header from %s...' % input_fname))
egi_info = _read_header(input_fname)
if (eog is None):
    eog = []
if (misc is None):
    misc = np.where((np.array(egi_info['chan_type']) != 'eeg'))[0].tolist()
utils.logger.info('    Reading events ...')
(egi_events, egi_info) = _read_events(input_fname, egi_info)
gains = _get_gains(os.path.join(input_fname, egi_info['info_fname']))
if ((egi_info['value_range'] != 0) and (egi_info['bits'] != 0)):
    cals = [(egi_info['value_range'] / (2 ** egi_info['bits'])) for i in range(len(egi_info['chan_type']))]
else:
    cal_scales = {'uV': 1e-06, 'V': 1}
    cals = [cal_scales[t] for t in egi_info['chan_unit']]
if ('gcal' in gains):
    cals *= gains['gcal']
if ('ical' in gains):
    pass
utils.logger.info('    Assembling measurement info ...')
if (egi_info['n_events'] > 0):
    event_codes = list(egi_info['event_codes'])
    if (include is None):
        exclude_list = (['sync', 'TREV'] if (exclude is None) else exclude)
        exclude_inds = [i for (i, k) in enumerate(event_codes) if (k in exclude_list)]
        more_excludes = []
        if (exclude is None):
            for (ii, event) in enumerate(egi_events):
                if ((event.sum() <= 1) and event_codes[ii]):
                    more_excludes.append(ii)
        if ((len(exclude_inds) + len(more_excludes)) == len(event_codes)):
            warn('Did not find any event code with more than one event.', RuntimeWarning)
        else:
            exclude_inds.extend(more_excludes)
        exclude_inds.sort()
        include_ = [i for i in numpy.arange(egi_info['n_events']) if (i not in exclude_inds)]
        include_names = [k for (i, k) in enumerate(event_codes) if (i in include_)]
    else:
        include_ = [i for (i, k) in enumerate(event_codes) if (k in include)]
        include_names = include
    for (kk, v) in [('include', include_names), ('exclude', exclude)]:
        if isinstance(v, list):
            for k in v:
                if (k not in event_codes):
                    raise ValueError(('Could find event named "%s"' % k))
        elif (v is not None):
            raise ValueError(('`%s` must be None or of type list' % kk))
    utils.logger.info('    Synthesizing trigger channel "STI 014" ...')
    utils.logger.info(('    Excluding events {%s} ...' % ', '.join([k for (i, k) in enumerate(event_codes) if (i not in include_)])))
    tempResult = arange(len(include_))
	
===================================================================	
_combine_triggers: 112	
----------------------------	

'Combine binary triggers.'
new_trigger = numpy.zeros(data.shape[1])
if (data.astype(bool).sum(axis=0).max() > 1):
    utils.logger.info('    Found multiple events at the same time sample. Cannot create trigger channel.')
    return
if (remapping is None):
    tempResult = arange(data)
	
===================================================================	
test_multiple_files: 185	
----------------------------	

'Test loading multiple files simultaneously.'
tempdir = _TempDir()
raw = read_raw_fif(fif_fname).crop(0, 10)
raw.load_data()
raw.load_data()
split_size = 3.0
sfreq = raw.info['sfreq']
nsamp = (raw.last_samp - raw.first_samp)
tempResult = arange(0.0, nsamp, (split_size * sfreq))
	
===================================================================	
test_filter: 613	
----------------------------	

'Test filtering (FIR and IIR) and Raw.apply_function interface.'
raw = read_raw_fif(fif_fname).crop(0, 7)
raw.load_data()
sig_dec_notch = 12
sig_dec_notch_fit = 12
picks_meg = pick_types(raw.info, meg=True, exclude='bads')
picks = picks_meg[:4]
trans = 2.0
filter_params = dict(picks=picks, filter_length='auto', h_trans_bandwidth=trans, l_trans_bandwidth=trans, fir_design='firwin')
raw_lp = raw.copy().filter(None, 8.0, **filter_params)
raw_hp = raw.copy().filter(16.0, None, **filter_params)
raw_bp = raw.copy().filter((8.0 + trans), (16.0 - trans), **filter_params)
raw_bs = raw.copy().filter(16.0, 8.0, **filter_params)
(data, _) = raw[picks, :]
(lp_data, _) = raw_lp[picks, :]
(hp_data, _) = raw_hp[picks, :]
(bp_data, _) = raw_bp[picks, :]
(bs_data, _) = raw_bs[picks, :]
tols = dict(atol=1e-20, rtol=1e-05)
assert_allclose(bs_data, (lp_data + hp_data), **tols)
assert_allclose(data, ((lp_data + bp_data) + hp_data), **tols)
assert_allclose(data, (bp_data + bs_data), **tols)
filter_params_iir = dict(picks=picks, n_jobs=2, method='iir', iir_params=dict(output='ba'))
raw_lp_iir = raw.copy().filter(None, 4.0, **filter_params_iir)
raw_hp_iir = raw.copy().filter(8.0, None, **filter_params_iir)
raw_bp_iir = raw.copy().filter(4.0, 8.0, **filter_params_iir)
del filter_params_iir
(lp_data_iir, _) = raw_lp_iir[picks, :]
(hp_data_iir, _) = raw_hp_iir[picks, :]
(bp_data_iir, _) = raw_bp_iir[picks, :]
summation = ((lp_data_iir + hp_data_iir) + bp_data_iir)
assert_array_almost_equal(data[:, 100:(- 100)], summation[:, 100:(- 100)], 11)
(data, _) = raw[picks_meg[4:], :]
(bp_data, _) = raw_bp[picks_meg[4:], :]
assert_array_equal(data, bp_data)
(bp_data_iir, _) = raw_bp_iir[picks_meg[4:], :]
assert_array_equal(data, bp_data_iir)
raw_copy = raw.copy()
raw_copy.filter(None, 20.0, n_jobs=2, **filter_params)
assert_true((raw._data[(0, 0)] != raw_copy._data[(0, 0)]))
assert_equal(raw.copy().filter(None, 20.0, **filter_params)._data, raw_copy._data)
with warnings.catch_warnings(record=True):
    warnings.simplefilter('always')
    raw_bs = raw.copy().filter((60.0 + trans), (60.0 - trans), **filter_params)
    (data_bs, _) = raw_bs[picks, :]
    raw_notch = raw.copy().notch_filter(60.0, picks=picks, n_jobs=2, method='fir', trans_bandwidth=(2 * trans))
(data_notch, _) = raw_notch[picks, :]
assert_array_almost_equal(data_bs, data_notch, sig_dec_notch)
raw_notch = raw.copy().notch_filter(None, picks=picks, n_jobs=2, method='spectrum_fit')
(data_notch, _) = raw_notch[picks, :]
(data, _) = raw[picks, :]
assert_array_almost_equal(data, data_notch, sig_dec_notch_fit)
raw = RawArray(numpy.random.randn(3, 1000), create_info(3, 1000.0, ((['eeg'] * 2) + ['stim'])))
raw.info['lowpass'] = raw.info['highpass'] = None
for kind in ('none', 'lowpass', 'highpass', 'bandpass', 'bandstop'):
    print(kind)
    h_freq = l_freq = None
    if (kind in ('lowpass', 'bandpass')):
        h_freq = 70
    if (kind in ('highpass', 'bandpass')):
        l_freq = 30
    if (kind == 'bandstop'):
        (l_freq, h_freq) = (70, 30)
    assert_true((raw.info['lowpass'] is None))
    assert_true((raw.info['highpass'] is None))
    kwargs = dict(l_trans_bandwidth=20, h_trans_bandwidth=20, filter_length='auto', phase='zero', fir_design='firwin')
    tempResult = arange(1)
	
===================================================================	
test_filter: 621	
----------------------------	

'Test filtering (FIR and IIR) and Raw.apply_function interface.'
raw = read_raw_fif(fif_fname).crop(0, 7)
raw.load_data()
sig_dec_notch = 12
sig_dec_notch_fit = 12
picks_meg = pick_types(raw.info, meg=True, exclude='bads')
picks = picks_meg[:4]
trans = 2.0
filter_params = dict(picks=picks, filter_length='auto', h_trans_bandwidth=trans, l_trans_bandwidth=trans, fir_design='firwin')
raw_lp = raw.copy().filter(None, 8.0, **filter_params)
raw_hp = raw.copy().filter(16.0, None, **filter_params)
raw_bp = raw.copy().filter((8.0 + trans), (16.0 - trans), **filter_params)
raw_bs = raw.copy().filter(16.0, 8.0, **filter_params)
(data, _) = raw[picks, :]
(lp_data, _) = raw_lp[picks, :]
(hp_data, _) = raw_hp[picks, :]
(bp_data, _) = raw_bp[picks, :]
(bs_data, _) = raw_bs[picks, :]
tols = dict(atol=1e-20, rtol=1e-05)
assert_allclose(bs_data, (lp_data + hp_data), **tols)
assert_allclose(data, ((lp_data + bp_data) + hp_data), **tols)
assert_allclose(data, (bp_data + bs_data), **tols)
filter_params_iir = dict(picks=picks, n_jobs=2, method='iir', iir_params=dict(output='ba'))
raw_lp_iir = raw.copy().filter(None, 4.0, **filter_params_iir)
raw_hp_iir = raw.copy().filter(8.0, None, **filter_params_iir)
raw_bp_iir = raw.copy().filter(4.0, 8.0, **filter_params_iir)
del filter_params_iir
(lp_data_iir, _) = raw_lp_iir[picks, :]
(hp_data_iir, _) = raw_hp_iir[picks, :]
(bp_data_iir, _) = raw_bp_iir[picks, :]
summation = ((lp_data_iir + hp_data_iir) + bp_data_iir)
assert_array_almost_equal(data[:, 100:(- 100)], summation[:, 100:(- 100)], 11)
(data, _) = raw[picks_meg[4:], :]
(bp_data, _) = raw_bp[picks_meg[4:], :]
assert_array_equal(data, bp_data)
(bp_data_iir, _) = raw_bp_iir[picks_meg[4:], :]
assert_array_equal(data, bp_data_iir)
raw_copy = raw.copy()
raw_copy.filter(None, 20.0, n_jobs=2, **filter_params)
assert_true((raw._data[(0, 0)] != raw_copy._data[(0, 0)]))
assert_equal(raw.copy().filter(None, 20.0, **filter_params)._data, raw_copy._data)
with warnings.catch_warnings(record=True):
    warnings.simplefilter('always')
    raw_bs = raw.copy().filter((60.0 + trans), (60.0 - trans), **filter_params)
    (data_bs, _) = raw_bs[picks, :]
    raw_notch = raw.copy().notch_filter(60.0, picks=picks, n_jobs=2, method='fir', trans_bandwidth=(2 * trans))
(data_notch, _) = raw_notch[picks, :]
assert_array_almost_equal(data_bs, data_notch, sig_dec_notch)
raw_notch = raw.copy().notch_filter(None, picks=picks, n_jobs=2, method='spectrum_fit')
(data_notch, _) = raw_notch[picks, :]
(data, _) = raw[picks, :]
assert_array_almost_equal(data, data_notch, sig_dec_notch_fit)
raw = RawArray(numpy.random.randn(3, 1000), create_info(3, 1000.0, ((['eeg'] * 2) + ['stim'])))
raw.info['lowpass'] = raw.info['highpass'] = None
for kind in ('none', 'lowpass', 'highpass', 'bandpass', 'bandstop'):
    print(kind)
    h_freq = l_freq = None
    if (kind in ('lowpass', 'bandpass')):
        h_freq = 70
    if (kind in ('highpass', 'bandpass')):
        l_freq = 30
    if (kind == 'bandstop'):
        (l_freq, h_freq) = (70, 30)
    assert_true((raw.info['lowpass'] is None))
    assert_true((raw.info['highpass'] is None))
    kwargs = dict(l_trans_bandwidth=20, h_trans_bandwidth=20, filter_length='auto', phase='zero', fir_design='firwin')
    raw_filt = raw.copy().filter(l_freq, h_freq, picks=numpy.arange(1), **kwargs)
    assert_true((raw.info['lowpass'] is None))
    assert_true((raw.info['highpass'] is None))
    raw_filt = raw.copy().filter(l_freq, h_freq, **kwargs)
    wanted_h = (h_freq if (kind != 'bandstop') else None)
    wanted_l = (l_freq if (kind != 'bandstop') else None)
    assert_equal(raw_filt.info['lowpass'], wanted_h)
    assert_equal(raw_filt.info['highpass'], wanted_l)
    tempResult = arange(2)
	
===================================================================	
test_crop: 649	
----------------------------	

'Test cropping raw files.'
raw = concatenate_raws([read_raw_fif(f) for f in [fif_fname, fif_fname]])
split_size = 10.0
sfreq = raw.info['sfreq']
nsamp = ((raw.last_samp - raw.first_samp) + 1)
tempResult = arange(0.0, (nsamp - 1), (split_size * sfreq))
	
===================================================================	
test_crop: 661	
----------------------------	

'Test cropping raw files.'
raw = concatenate_raws([read_raw_fif(f) for f in [fif_fname, fif_fname]])
split_size = 10.0
sfreq = raw.info['sfreq']
nsamp = ((raw.last_samp - raw.first_samp) + 1)
tmins = numpy.r_[(1.0, numpy.round(numpy.arange(0.0, (nsamp - 1), (split_size * sfreq))))]
tmins = numpy.sort(tmins)
tmaxs = numpy.concatenate(((tmins[1:] - 1), [(nsamp - 1)]))
tmaxs /= sfreq
tmins /= sfreq
raws = ([None] * len(tmins))
for (ri, (tmin, tmax)) in enumerate(zip(tmins, tmaxs)):
    raws[ri] = raw.copy().crop(tmin, tmax)
all_raw_2 = concatenate_raws(raws, preload=False)
assert_equal(raw.first_samp, all_raw_2.first_samp)
assert_equal(raw.last_samp, all_raw_2.last_samp)
assert_array_equal(raw[:, :][0], all_raw_2[:, :][0])
tempResult = arange(0.0, (nsamp - 1), (split_size * sfreq))
	
===================================================================	
test_io_complex: 424	
----------------------------	

'Test IO with complex data types.'
rng = numpy.random.RandomState(0)
tempdir = _TempDir()
dtypes = [numpy.complex64, numpy.complex128]
raw = _test_raw_reader(partial(read_raw_fif), fname=fif_fname)
tempResult = arange(5)
	
===================================================================	
test_fix_types: 52	
----------------------------	

'Test fixing of channel types.'
for (fname, change) in ((hp_fif_fname, True), (test_fif_fname, False), (ctf_fname, False)):
    raw = read_raw_fif(fname)
    mag_picks = pick_types(raw.info, meg='mag')
    tempResult = arange(len(raw.ch_names))
	
===================================================================	
RawKIT._read_segment_file: 112	
----------------------------	

'Read a chunk of raw data.'
nchan = self._raw_extras[fi]['nchan']
data_left = ((stop - start) * nchan)
conv_factor = self._raw_extras[fi]['conv_factor']
n_bytes = 2
blk_size = min(data_left, (((100000000 // n_bytes) // nchan) * nchan))
with open(self._filenames[fi], 'rb', buffering=0) as fid:
    fid.seek(144)
    data_offset = unpack('i', fid.read(constants.KIT.INT))[0]
    pointer = ((start * nchan) * constants.KIT.SHORT)
    fid.seek((data_offset + pointer))
    stim = self._raw_extras[fi]['stim']
    tempResult = arange(0, data_left, blk_size)
	
===================================================================	
_make_stim_channel: 138	
----------------------------	

'Create synthetic stim channel from multiple trigger channels.'
if (slope == '+'):
    trig_chs_bin = (trigger_chs > threshold)
elif (slope == '-'):
    trig_chs_bin = (trigger_chs < threshold)
else:
    raise ValueError("slope needs to be '+' or '-'")
if (stim_code == 'binary'):
    tempResult = arange(len(trigger_chs))
	
===================================================================	
_test_raw_reader: 19	
----------------------------	

'Test reading, writing and slicing of raw classes.\n\n    Parameters\n    ----------\n    reader : function\n        Function to test.\n    test_preloading : bool\n        Whether not preloading is implemented for the reader. If True, both\n        cases and memory mapping to file are tested.\n    **kwargs :\n        Arguments for the reader. Note: Do not use preload as kwarg.\n        Use ``test_preloading`` instead.\n\n    Returns\n    -------\n    raw : Instance of Raw\n        A preloaded Raw object.\n    '
tempdir = _TempDir()
rng = numpy.random.RandomState(0)
if test_preloading:
    raw = reader(preload=True, **kwargs)
    buffer_fname = os.path.join(tempdir, 'buffer')
    tempResult = arange((len(raw.ch_names) - 1))
	
===================================================================	
compute_source_psd: 179	
----------------------------	

'Compute source power spectrum density (PSD).\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        The raw data\n    inverse_operator : instance of InverseOperator\n        The inverse operator\n    lambda2: float\n        The regularization parameter\n    method: "MNE" | "dSPM" | "sLORETA"\n        Use mininum norm, dSPM or sLORETA\n    tmin : float | None\n        The beginning of the time interval of interest (in seconds). If None\n        start from the beginning of the file.\n    tmax : float | None\n        The end of the time interval of interest (in seconds). If None\n        stop at the end of the file.\n    fmin : float\n        The lower frequency of interest\n    fmax : float\n        The upper frequency of interest\n    n_fft: int\n        Window size for the FFT. Should be a power of 2.\n    overlap: float\n        The overlap fraction between windows. Should be between 0 and 1.\n        0 means no overlap.\n    pick_ori : None | "normal"\n        If "normal", rather than pooling the orientations by taking the norm,\n        only the radial component is kept. This is only implemented\n        when working with loose orientations.\n    label: Label\n        Restricts the source estimates to a given label\n    nave : int\n        The number of averages used to scale the noise covariance matrix.\n    pca: bool\n        If True, the true dimension of data is estimated before running\n        the time-frequency transforms. It reduces the computation times\n        e.g. with a dataset that was maxfiltered (true dim is 64).\n    prepared : bool\n        If True, do not call `prepare_inverse_operator`.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    stc : SourceEstimate | VolSourceEstimate\n        The PSD (in dB) of each of the sources.\n    '
from scipy.signal import hanning
_check_ori(pick_ori, inverse_operator['source_ori'])
utils.logger.info(('Considering frequencies %g ... %g Hz' % (fmin, fmax)))
(K, sel, Vh, vertno, is_free_ori, noise_norm) = _prepare_source_params(inst=raw, inverse_operator=inverse_operator, label=label, lambda2=lambda2, method=method, nave=nave, pca=pca, pick_ori=pick_ori, prepared=prepared, verbose=verbose)
(start, stop) = (0, ((raw.last_samp + 1) - raw.first_samp))
if (tmin is not None):
    start = raw.time_as_index(tmin)[0]
if (tmax is not None):
    stop = (raw.time_as_index(tmax)[0] + 1)
n_fft = int(n_fft)
Fs = raw.info['sfreq']
window = hanning(n_fft)
freqs = scipy.fftpack.fftfreq(n_fft, (1.0 / Fs))
freqs_mask = (((freqs >= 0) & (freqs >= fmin)) & (freqs <= fmax))
freqs = freqs[freqs_mask]
fstep = numpy.mean(numpy.diff(freqs))
if (is_free_ori and (pick_ori is None)):
    psd = numpy.zeros(((K.shape[0] // 3), numpy.sum(freqs_mask)))
else:
    psd = numpy.zeros((K.shape[0], numpy.sum(freqs_mask)))
n_windows = 0
tempResult = arange(start, stop, int((n_fft * (1.0 - overlap))))
	
===================================================================	
source_band_induced_power: 39	
----------------------------	

'Compute source space induced power in given frequency bands.\n\n    Parameters\n    ----------\n    epochs : instance of Epochs\n        The epochs.\n    inverse_operator : instance of inverse operator\n        The inverse operator.\n    bands : dict\n        Example : bands = dict(alpha=[8, 9]).\n    label : Label\n        Restricts the source estimates to a given label.\n    lambda2 : float\n        The regularization parameter of the minimum norm.\n    method : "MNE" | "dSPM" | "sLORETA"\n        Use mininum norm, dSPM or sLORETA.\n    nave : int\n        The number of averages used to scale the noise covariance matrix.\n    n_cycles : float | array of float\n        Number of cycles. Fixed number or one per frequency.\n    df : float\n        delta frequency within bands.\n    use_fft : bool\n        Do convolutions in time or frequency domain with FFT.\n    decim : int\n        Temporal decimation factor.\n    baseline : None (default) or tuple of length 2\n        The time interval to apply baseline correction. If None do not apply\n        it. If baseline is (a, b) the interval is between "a (s)" and "b (s)".\n        If a is None the beginning of the data is used and if b is None then b\n        is set to the end of the interval. If baseline is equal to (None, None)\n        all the time interval is used.\n    baseline_mode : \'mean\' | \'ratio\' | \'logratio\' | \'percent\' | \'zscore\' | \'zlogratio\' | None\n        Perform baseline correction by\n\n          - subtracting the mean baseline power (\'mean\')\n          - dividing by the mean baseline power (\'ratio\')\n          - dividing by the mean baseline power and taking the log (\'logratio\')\n          - subtracting the mean baseline power followed by dividing by the\n            mean baseline power (\'percent\')\n          - subtracting the mean baseline power and dividing by the standard\n            deviation of the baseline power (\'zscore\')\n          - dividing by the mean baseline power, taking the log, and dividing\n            by the standard deviation of the baseline power (\'zlogratio\')\n\n        If None no baseline correction is applied.\n    pca : bool\n        If True, the true dimension of data is estimated before running\n        the time-frequency transforms. It reduces the computation times\n        e.g. with a dataset that was maxfiltered (true dim is 64).\n    n_jobs : int\n        Number of jobs to run in parallel.\n    prepared : bool\n        If True, do not call `prepare_inverse_operator`.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    stcs : dict with a SourceEstimate (or VolSourceEstimate) for each band\n        The estimated source space induced power estimates.\n    '
_check_method(method)
tempResult = arange(band[0], (band[1] + (df / 2.0)), df)
	
===================================================================	
test_inverse_operator_channel_ordering: 189	
----------------------------	

'Test MNE inverse computation is immune to channel reorderings.'
evoked = _get_evoked()
noise_cov = read_cov(fname_cov)
fwd_orig = make_forward_solution(evoked.info, fname_trans, src_fname, fname_bem, eeg=True, mindist=5.0)
fwd_orig = convert_forward_solution(fwd_orig, surf_ori=True)
inv_orig = make_inverse_operator(evoked.info, fwd_orig, noise_cov, loose=0.2, depth=0.8, limit_depth_chs=False)
stc_1 = apply_inverse(evoked, inv_orig, lambda2, 'dSPM')
tempResult = arange(len(evoked.info['ch_names']))
	
===================================================================	
test_tfr_with_inverse_operator: 45	
----------------------------	

'Test time freq with MNE inverse computation.'
(tmin, tmax, event_id) = ((- 0.2), 0.5, 1)
raw = read_raw_fif(fname_data)
events = find_events(raw, stim_channel='STI 014')
inverse_operator = read_inverse_operator(fname_inv)
inv = prepare_inverse_operator(inverse_operator, nave=1, lambda2=(1.0 / 9.0), method='dSPM')
raw.info['bads'] += ['MEG 2443', 'EEG 053']
picks = pick_types(raw.info, meg=True, eeg=False, eog=True, stim=False, exclude='bads')
event_id = 1
events3 = events[:3]
epochs = Epochs(raw, events3, event_id, tmin, tmax, picks=picks, baseline=(None, 0), reject=dict(grad=4e-10, eog=0.00015), preload=True)
bands = dict(alpha=[10, 10])
label = read_label(fname_label)
stcs = source_band_induced_power(epochs, inv, bands, n_cycles=2, use_fft=False, pca=True, label=label, prepared=True)
stc = stcs['alpha']
assert_true((len(stcs) == len(list(bands.keys()))))
assert_true(numpy.all((stc.data > 0)))
assert_array_almost_equal(stc.times, epochs.times)
stcs_no_pca = source_band_induced_power(epochs, inv, bands, n_cycles=2, use_fft=False, pca=False, label=label, prepared=True)
assert_array_almost_equal(stcs['alpha'].data, stcs_no_pca['alpha'].data)
epochs = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, baseline=(None, 0), reject=dict(grad=4e-10, eog=0.00015), preload=True)
tempResult = arange(7, 30, 2)
	
===================================================================	
_prob_kuiper: 50	
----------------------------	

'Test for statistical significance against uniform distribution.\n\n    Parameters\n    ----------\n    d : float\n        The kuiper distance value.\n    n_eff : int\n        The effective number of elements.\n    dtype : str | obj\n        The data type to be used. Defaults to double precision floats.\n\n    Returns\n    -------\n    pk_norm : float\n        The normalized Kuiper value such that 0 < ``pk_norm`` < 1.\n\n    References\n    ----------\n    [1] Stephens MA 1970. Journal of the Royal Statistical Society, ser. B,\n    vol 32, pp 115-122.\n\n    [2] Kuiper NH 1962. Proceedings of the Koninklijke Nederlands Akademie\n    van Wetenschappen, ser Vol 63 pp 38-47\n    '
n_time_slices = numpy.size(d)
n_points = 100
en = math.sqrt(n_eff)
k_lambda = (((en + 0.155) + (0.24 / en)) * d)
l2 = (k_lambda ** 2.0)
tempResult = arange(n_points)
	
===================================================================	
kuiper: 32	
----------------------------	

"Kuiper's test of uniform distribution.\n\n    Parameters\n    ----------\n    data : ndarray, shape (n_sources,) | (n_sources, n_times)\n           Empirical distribution.\n    dtype : str | obj\n        The data type to be used.\n\n    Returns\n    -------\n    ks : ndarray\n        Kuiper's statistic.\n    pk : ndarray\n        Normalized probability of Kuiper's statistic [0, 1].\n    "
data = np.sort(data, axis=0).astype(dtype)
shape = data.shape
n_dim = len(shape)
n_trials = shape[0]
tempResult = arange(n_trials, dtype=dtype)
	
===================================================================	
kuiper: 33	
----------------------------	

"Kuiper's test of uniform distribution.\n\n    Parameters\n    ----------\n    data : ndarray, shape (n_sources,) | (n_sources, n_times)\n           Empirical distribution.\n    dtype : str | obj\n        The data type to be used.\n\n    Returns\n    -------\n    ks : ndarray\n        Kuiper's statistic.\n    pk : ndarray\n        Normalized probability of Kuiper's statistic [0, 1].\n    "
data = np.sort(data, axis=0).astype(dtype)
shape = data.shape
n_dim = len(shape)
n_trials = shape[0]
j1 = ((numpy.arange(n_trials, dtype=dtype) + 1.0) / float(n_trials))
tempResult = arange(n_trials, dtype=dtype)
	
===================================================================	
qrs_detector: 28	
----------------------------	

'Detect QRS component in ECG channels.\n\n    QRS is the main wave on the heart beat.\n\n    Parameters\n    ----------\n    sfreq : float\n        Sampling rate\n    ecg : array\n        ECG signal\n    thresh_value : float | str\n        qrs detection threshold. Can also be "auto" for automatic\n        selection of threshold.\n    levels : float\n        number of std from mean to include for detection\n    n_thresh : int\n        max number of crossings\n    l_freq : float\n        Low pass frequency\n    h_freq : float\n        High pass frequency\n    tstart : float\n        Start detection after tstart seconds.\n    filter_length : str | int | None\n        Number of taps to use for filtering.\n\n    Returns\n    -------\n    events : array\n        Indices of ECG peaks\n    '
win_size = int(round(((60.0 * sfreq) / 120.0)))
filtecg = filter_data(ecg, sfreq, l_freq, h_freq, None, filter_length, 0.5, 0.5, phase='zero-double', fir_window='hann', fir_design='firwin2')
ecg_abs = numpy.abs(filtecg)
init = int(sfreq)
n_samples_start = int((sfreq * tstart))
ecg_abs = ecg_abs[n_samples_start:]
n_points = len(ecg_abs)
maxpt = numpy.empty(3)
maxpt[0] = numpy.max(ecg_abs[:init])
maxpt[1] = numpy.max(ecg_abs[init:(init * 2)])
maxpt[2] = numpy.max(ecg_abs[(init * 2):(init * 3)])
init_max = numpy.mean(maxpt)
if (thresh_value == 'auto'):
    tempResult = arange(0.3, 1.1, 0.05)
	
===================================================================	
corrmap: 1056	
----------------------------	

'Find similar Independent Components across subjects by map similarity.\n\n    Corrmap (Viola et al. 2009 Clin Neurophysiol) identifies the best group\n    match to a supplied template. Typically, feed it a list of fitted ICAs and\n    a template IC, for example, the blink for the first subject, to identify\n    specific ICs across subjects.\n\n    The specific procedure consists of two iterations. In a first step, the\n    maps best correlating with the template are identified. In the next step,\n    the analysis is repeated with the mean of the maps identified in the first\n    stage.\n\n    Run with `plot` and `show` set to `True` and `label=False` to find\n    good parameters. Then, run with labelling enabled to apply the\n    labelling in the IC objects. (Running with both `plot` and `labels`\n    off does nothing.)\n\n    Outputs a list of fitted ICAs with the indices of the marked ICs in a\n    specified field.\n\n    The original Corrmap website: www.debener.de/corrmap/corrmapplugin1.html\n\n    Parameters\n    ----------\n    icas : list of mne.preprocessing.ICA\n        A list of fitted ICA objects.\n    template : tuple | np.ndarray, shape (n_components,)\n        Either a tuple with two elements (int, int) representing the list\n        indices of the set from which the template should be chosen, and the\n        template. E.g., if template=(1, 0), the first IC of the 2nd ICA object\n        is used.\n        Or a numpy array whose size corresponds to each IC map from the\n        supplied maps, in which case this map is chosen as the template.\n    threshold : "auto" | list of float | float\n        Correlation threshold for identifying ICs\n        If "auto", search for the best map by trying all correlations between\n        0.6 and 0.95. In the original proposal, lower values are considered,\n        but this is not yet implemented.\n        If list of floats, search for the best map in the specified range of\n        correlation strengths. As correlation values, must be between 0 and 1\n        If float > 0, select ICs correlating better than this.\n        If float > 1, use find_outliers to identify ICs within subjects (not in\n        original Corrmap)\n        Defaults to "auto".\n    label : None | str\n        If not None, categorised ICs are stored in a dictionary ``labels_``\n        under the given name. Preexisting entries will be appended to\n        (excluding repeats), not overwritten. If None, a dry run is performed\n        and the supplied ICs are not changed.\n    ch_type : \'mag\' | \'grad\' | \'planar1\' | \'planar2\' | \'eeg\'\n        The channel type to plot. Defaults to \'eeg\'.\n    plot : bool\n        Should constructed template and selected maps be plotted? Defaults\n        to True.\n    show : bool\n        Show figures if True.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n    outlines : \'head\' | dict | None\n        The outlines to be drawn. If \'head\', a head scheme will be drawn. If\n        dict, each key refers to a tuple of x and y positions. The values in\n        \'mask_pos\' will serve as image mask. If None, nothing will be drawn.\n        Defaults to \'head\'. If dict, the \'autoshrink\' (bool) field will\n        trigger automated shrinking of the positions due to points outside the\n        outline. Moreover, a matplotlib patch object can be passed for\n        advanced masking options, either directly or as a function that returns\n        patches (required for multi-axis plots).\n    layout : None | Layout | list of Layout\n        Layout instance specifying sensor positions (does not need to be\n        specified for Neuromag data). Or a list of Layout if projections\n        are from different sensor types.\n    sensors : bool | str\n        Add markers for sensor locations to the plot. Accepts matplotlib plot\n        format string (e.g., \'r+\' for red plusses). If True, a circle will be\n        used (via .add_artist). Defaults to True.\n    contours : int | array of float\n        The number of contour lines to draw. If 0, no contours will be drawn.\n        When an integer, matplotlib ticker locator is used to find suitable\n        values for the contour thresholds (may sometimes be inaccurate, use\n        array for accuracy). If an array, the values represent the levels for\n        the contours. Defaults to 6.\n    cmap : None | matplotlib colormap\n        Colormap for the plot. If ``None``, defaults to \'Reds_r\' for norm data,\n        otherwise to \'RdBu_r\'.\n\n    Returns\n    -------\n    template_fig : fig\n        Figure showing the template.\n    labelled_ics : fig\n        Figure showing the labelled ICs in all ICA decompositions.\n    '
if (not isinstance(plot, bool)):
    raise ValueError('`plot` must be of type `bool`')
if (threshold == 'auto'):
    tempResult = arange(60, 95, dtype=numpy.float64)
	
===================================================================	
ICA._pick_sources: 631	
----------------------------	

'Aux function.'
if (exclude is None):
    exclude = self.exclude
else:
    exclude = list(set((self.exclude + list(exclude))))
_n_pca_comp = self._check_n_pca_components(self.n_pca_components)
if (not (self.n_components_ <= _n_pca_comp <= self.max_pca_components)):
    raise ValueError('n_pca_components must be >= n_components and <= max_pca_components.')
n_components = self.n_components_
utils.logger.info(('Transforming to ICA space (%i components)' % n_components))
if (self.pca_mean_ is not None):
    data -= self.pca_mean_[:, None]
tempResult = arange(n_components)
	
===================================================================	
ICA._pick_sources: 635	
----------------------------	

'Aux function.'
if (exclude is None):
    exclude = self.exclude
else:
    exclude = list(set((self.exclude + list(exclude))))
_n_pca_comp = self._check_n_pca_components(self.n_pca_components)
if (not (self.n_components_ <= _n_pca_comp <= self.max_pca_components)):
    raise ValueError('n_pca_components must be >= n_components and <= max_pca_components.')
n_components = self.n_components_
utils.logger.info(('Transforming to ICA space (%i components)' % n_components))
if (self.pca_mean_ is not None):
    data -= self.pca_mean_[:, None]
sel_keep = numpy.arange(n_components)
if (include not in (None, [])):
    sel_keep = numpy.unique(include)
elif (exclude not in (None, [])):
    tempResult = arange(n_components)
	
===================================================================	
_regularize_in: 898	
----------------------------	

'Regularize basis set using idealized SNR measure.'
(n_in, n_out) = _get_n_moments([int_order, ext_order])
tempResult = arange((int_order + 1))
	
===================================================================	
_regularize_in: 904	
----------------------------	

'Regularize basis set using idealized SNR measure.'
(n_in, n_out) = _get_n_moments([int_order, ext_order])
(a_lm_sq, rho_i) = _compute_sphere_activation_in(numpy.arange((int_order + 1)))
(degrees, orders) = _get_degrees_orders(int_order)
a_lm_sq = a_lm_sq[degrees]
I_tots = numpy.zeros(n_in)
in_keepers = list(range(n_in))
out_removes = _regularize_out(int_order, ext_order, mag_or_fine)
tempResult = arange(n_in, (n_in + n_out))
	
===================================================================	
_trans_starts_stops_quats: 257	
----------------------------	

'Get all trans and limits we need.'
tempResult = arange(*numpy.searchsorted(pos[1], [start, stop]))
	
===================================================================	
_prep_mf_coils: 250	
----------------------------	

'Get all coil integration information loaded and sorted.'
(coils, comp_coils) = _prep_meg_channels(info, accurate=True, elekta_defs=True, head_frame=False, ignore_ref=ignore_ref, do_picking=False, verbose=False)[:2]
mag_mask = _get_mag_mask(coils)
if (len(comp_coils) > 0):
    meg_picks = pick_types(info, meg=True, ref_meg=False, exclude=[])
    ref_picks = pick_types(info, meg=False, ref_meg=True, exclude=[])
    inserts = numpy.searchsorted(meg_picks, ref_picks)
    for (idx, comp_coil) in zip(inserts[::(- 1)], comp_coils[::(- 1)]):
        coils.insert(idx, comp_coil)
n_coils = len(coils)
rmags = numpy.concatenate([coil['rmag'] for coil in coils])
cosmags = numpy.concatenate([coil['cosmag'] for coil in coils])
ws = numpy.concatenate([coil['w'] for coil in coils])
cosmags *= ws[:, numpy.newaxis]
del ws
n_int = numpy.array([len(coil['rmag']) for coil in coils])
tempResult = arange(len(n_int))
	
===================================================================	
_regularize: 407	
----------------------------	

'Regularize a decomposition matrix.'
(int_order, ext_order) = (exp['int_order'], exp['ext_order'])
(n_in, n_out) = _get_n_moments([int_order, ext_order])
t_str = ('%8.3f' % t)
if (regularize is not None):
    (in_removes, out_removes) = _regularize_in(int_order, ext_order, S_decomp, mag_or_fine)
else:
    in_removes = []
    out_removes = _regularize_out(int_order, ext_order, mag_or_fine)
tempResult = arange(n_in)
	
===================================================================	
_regularize: 408	
----------------------------	

'Regularize a decomposition matrix.'
(int_order, ext_order) = (exp['int_order'], exp['ext_order'])
(n_in, n_out) = _get_n_moments([int_order, ext_order])
t_str = ('%8.3f' % t)
if (regularize is not None):
    (in_removes, out_removes) = _regularize_in(int_order, ext_order, S_decomp, mag_or_fine)
else:
    in_removes = []
    out_removes = _regularize_out(int_order, ext_order, mag_or_fine)
reg_in_moments = numpy.setdiff1d(numpy.arange(n_in), in_removes)
tempResult = arange(n_in, (n_in + n_out))
	
===================================================================	
_regularize_out: 892	
----------------------------	

'Regularize out components based on norm.'
n_in = _get_n_moments(int_order)
tempResult = arange((0 if mag_or_fine.any() else 3))
	
===================================================================	
_update_sensor_geometry: 860	
----------------------------	

'Replace sensor geometry information and reorder cal_chs.'
from ._fine_cal import read_fine_calibration
utils.logger.info(('    Using fine calibration %s' % os.path.basename(fine_cal)))
fine_cal = read_fine_calibration(fine_cal)
ch_names = _clean_names(info['ch_names'], remove_whitespace=True)
info_to_cal = dict()
missing = list()
for (ci, name) in enumerate(fine_cal['ch_names']):
    if (name not in ch_names):
        missing.append(name)
    else:
        oi = ch_names.index(name)
        info_to_cal[oi] = ci
meg_picks = pick_types(info, meg=True, exclude=[])
if (len(info_to_cal) != len(meg_picks)):
    raise RuntimeError(('Not all MEG channels found in fine calibration file, missing:\n%s' % sorted(list((set((ch_names[pick] for pick in meg_picks)) - set(fine_cal['ch_names']))))))
if len(missing):
    warn(('Found cal channel%s not in data: %s' % (_pl(missing), missing)))
grad_picks = pick_types(info, meg='grad', exclude=())
mag_picks = pick_types(info, meg='mag', exclude=())
grad_imbalances = np.array([fine_cal['imb_cals'][info_to_cal[gi]] for gi in grad_picks]).T
if (grad_imbalances.shape[0] not in [1, 3]):
    raise ValueError(('Must have 1 (x) or 3 (x, y, z) point-like ' + ('magnetometers. Currently have %i' % grad_imbalances.shape[0])))
mag_cals = numpy.array([fine_cal['imb_cals'][info_to_cal[mi]] for mi in mag_picks])
grad_coilsets = _get_grad_point_coilsets(info, n_types=len(grad_imbalances), ignore_ref=ignore_ref)
calibration = dict(grad_imbalances=grad_imbalances, grad_coilsets=grad_coilsets, mag_cals=mag_cals)
ang_shift = numpy.zeros((len(fine_cal['ch_names']), 3))
used = numpy.zeros(len(info['chs']), bool)
cal_corrs = list()
cal_chans = list()
adjust_logged = False
for (oi, ci) in info_to_cal.items():
    assert (ch_names[oi] == fine_cal['ch_names'][ci])
    assert (not used[oi])
    used[oi] = True
    info_ch = info['chs'][oi]
    ch_num = int(fine_cal['ch_names'][ci].lstrip('MEG').lstrip('0'))
    cal_chans.append([ch_num, info_ch['coil_type']])
    ch_coil_rot = _loc_to_coil_trans(info_ch['loc'])[:3, :3]
    cal_loc = fine_cal['locs'][ci].copy()
    cal_coil_rot = _loc_to_coil_trans(cal_loc)[:3, :3]
    if (numpy.max([numpy.abs(numpy.dot(cal_coil_rot[:, ii], cal_coil_rot[:, 2])) for ii in range(2)]) > 1e-06):
        if (not adjust_logged):
            utils.logger.info('        Adjusting non-orthogonal EX and EY')
            adjust_logged = True
        this_trans = _find_vector_rotation(ch_coil_rot[:, 2], cal_coil_rot[:, 2])
        cal_loc[3:] = np.dot(this_trans, ch_coil_rot).T.ravel()
    v1 = _loc_to_coil_trans(cal_loc)[:3, :3]
    _normalize_vectors(v1)
    v2 = _loc_to_coil_trans(info_ch['loc'])[:3, :3]
    _normalize_vectors(v2)
    ang_shift[ci] = numpy.sum((v1 * v2), axis=0)
    if (oi in grad_picks):
        extra = [1.0, fine_cal['imb_cals'][ci][0]]
    else:
        extra = [fine_cal['imb_cals'][ci][0], 0.0]
    cal_corrs.append(numpy.concatenate([extra, cal_loc]))
    info_ch['loc'][3:] = cal_loc[3:]
    assert (info_ch['coord_frame'] == io.constants.FIFF.FIFFV_COORD_DEVICE)
assert used[meg_picks].all()
tempResult = arange(len(used))
	
===================================================================	
_copy_preload_add_channels: 332	
----------------------------	

'Load data for processing and (maybe) add cHPI pos channels.'
raw = raw.copy()
if add_channels:
    kinds = [io.constants.FIFF.FIFFV_QUAT_1, io.constants.FIFF.FIFFV_QUAT_2, io.constants.FIFF.FIFFV_QUAT_3, io.constants.FIFF.FIFFV_QUAT_4, io.constants.FIFF.FIFFV_QUAT_5, io.constants.FIFF.FIFFV_QUAT_6, io.constants.FIFF.FIFFV_HPI_G, io.constants.FIFF.FIFFV_HPI_ERR, io.constants.FIFF.FIFFV_HPI_MOV]
    out_shape = ((len(raw.ch_names) + len(kinds)), len(raw.times))
    out_data = numpy.zeros(out_shape, numpy.float64)
    msg = '    Appending head position result channels and '
    if raw.preload:
        utils.logger.info((msg + 'copying original raw data'))
        out_data[:len(raw.ch_names)] = raw._data
        raw._data = out_data
    else:
        utils.logger.info((msg + 'loading raw data from disk'))
        raw._preload_data(out_data[:len(raw.ch_names)], verbose=False)
        raw._data = out_data
    assert (raw.preload is True)
    off = len(raw.ch_names)
    chpi_chs = [dict(ch_name=('CHPI%03d' % (ii + 1)), logno=(ii + 1), scanno=((off + ii) + 1), unit_mul=(- 1), range=1.0, unit=(- 1), kind=kinds[ii], coord_frame=io.constants.FIFF.FIFFV_COORD_UNKNOWN, cal=0.0001, coil_type=io.constants.FIFF.FWD_COIL_UNKNOWN, loc=numpy.zeros(12)) for ii in range(len(kinds))]
    raw.info['chs'].extend(chpi_chs)
    raw.info._update_redundant()
    raw.info._check_consistency()
    assert (raw._data.shape == (raw.info['nchan'], len(raw.times)))
    tempResult = arange((len(raw.ch_names) - len(chpi_chs)), len(raw.ch_names))
	
===================================================================	
_concatenate_sph_coils: 493	
----------------------------	

'Concatenate MEG coil parameters for spherical harmoncs.'
rs = numpy.concatenate([coil['r0_exey'] for coil in coils])
wcoils = numpy.concatenate([coil['w'] for coil in coils])
ezs = numpy.concatenate([numpy.tile(coil['ez'][numpy.newaxis, :], (len(coil['rmag']), 1)) for coil in coils])
tempResult = arange(len(coils))
	
===================================================================	
maxwell_filter: 109	
----------------------------	

'Apply Maxwell filter to data using multipole moments.\n\n    .. warning:: Automatic bad channel detection is not currently implemented.\n                 It is critical to mark bad channels before running Maxwell\n                 filtering to prevent artifact spreading.\n\n    .. warning:: Maxwell filtering in MNE is not designed or certified\n                 for clinical use.\n\n    Parameters\n    ----------\n    raw : instance of mne.io.Raw\n        Data to be filtered\n    origin : array-like, shape (3,) | str\n        Origin of internal and external multipolar moment space in meters.\n        The default is ``\'auto\'``, which means a head-digitization-based\n        origin fit when ``coord_frame=\'head\'``, and ``(0., 0., 0.)`` when\n        ``coord_frame=\'meg\'``.\n    int_order : int\n        Order of internal component of spherical expansion.\n    ext_order : int\n        Order of external component of spherical expansion.\n    calibration : str | None\n        Path to the ``\'.dat\'`` file with fine calibration coefficients.\n        File can have 1D or 3D gradiometer imbalance correction.\n        This file is machine/site-specific.\n    cross_talk : str | None\n        Path to the FIF file with cross-talk correction information.\n    st_duration : float | None\n        If not None, apply spatiotemporal SSS with specified buffer duration\n        (in seconds). Elekta\'s default is 10.0 seconds in MaxFilter™ v2.2.\n        Spatiotemporal SSS acts as implicitly as a high-pass filter where the\n        cut-off frequency is 1/st_dur Hz. For this (and other) reasons, longer\n        buffers are generally better as long as your system can handle the\n        higher memory usage. To ensure that each window is processed\n        identically, choose a buffer length that divides evenly into your data.\n        Any data at the trailing edge that doesn\'t fit evenly into a whole\n        buffer window will be lumped into the previous buffer.\n    st_correlation : float\n        Correlation limit between inner and outer subspaces used to reject\n        ovwrlapping intersecting inner/outer signals during spatiotemporal SSS.\n    coord_frame : str\n        The coordinate frame that the ``origin`` is specified in, either\n        ``\'meg\'`` or ``\'head\'``. For empty-room recordings that do not have\n        a head<->meg transform ``info[\'dev_head_t\']``, the MEG coordinate\n        frame should be used.\n    destination : str | array-like, shape (3,) | None\n        The destination location for the head. Can be ``None``, which\n        will not change the head position, or a string path to a FIF file\n        containing a MEG device<->head transformation, or a 3-element array\n        giving the coordinates to translate to (with no rotations).\n        For example, ``destination=(0, 0, 0.04)`` would translate the bases\n        as ``--trans default`` would in MaxFilter™ (i.e., to the default\n        head location).\n    regularize : str | None\n        Basis regularization type, must be "in" or None.\n        "in" is the same algorithm as the "-regularize in" option in\n        MaxFilter™.\n    ignore_ref : bool\n        If True, do not include reference channels in compensation. This\n        option should be True for KIT files, since Maxwell filtering\n        with reference channels is not currently supported.\n    bad_condition : str\n        How to deal with ill-conditioned SSS matrices. Can be "error"\n        (default), "warning", or "ignore".\n    head_pos : array | None\n        If array, movement compensation will be performed.\n        The array should be of shape (N, 10), holding the position\n        parameters as returned by e.g. `read_head_pos`.\n\n        .. versionadded:: 0.12\n\n    st_fixed : bool\n        If True (default), do tSSS using the median head position during the\n        ``st_duration`` window. This is the default behavior of MaxFilter\n        and has been most extensively tested.\n\n        .. versionadded:: 0.12\n\n    st_only : bool\n        If True, only tSSS (temporal) projection of MEG data will be\n        performed on the output data. The non-tSSS parameters (e.g.,\n        ``int_order``, ``calibration``, ``head_pos``, etc.) will still be\n        used to form the SSS bases used to calculate temporal projectors,\n        but the ouptut MEG data will *only* have temporal projections\n        performed. Noise reduction from SSS basis multiplication,\n        cross-talk cancellation, movement compensation, and so forth\n        will not be applied to the data. This is useful, for example, when\n        evoked movement compensation will be performed with\n        :func:`mne.epochs.average_movements`.\n\n        .. versionadded:: 0.12\n\n    mag_scale : float | str\n        The magenetometer scale-factor used to bring the magnetometers\n        to approximately the same order of magnitude as the gradiometers\n        (default 100.), as they have different units (T vs T/m).\n        Can be ``\'auto\'`` to use the reciprocal of the physical distance\n        between the gradiometer pickup loops (e.g., 0.0168 m yields\n        59.5 for VectorView).\n\n        .. versionadded:: 0.13\n\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    raw_sss : instance of mne.io.Raw\n        The raw data with Maxwell filtering applied.\n\n    See Also\n    --------\n    mne.chpi.filter_chpi\n    mne.chpi.read_head_pos\n    mne.epochs.average_movements\n\n    Notes\n    -----\n    .. versionadded:: 0.11\n\n    Some of this code was adapted and relicensed (with BSD form) with\n    permission from Jussi Nurminen. These algorithms are based on work\n    from [1]_ and [2]_.\n\n    .. note:: This code may use multiple CPU cores, see the\n              :ref:`FAQ <faq_cpu>` for more information.\n\n    Compared to Elekta\'s MaxFilter™ software, the MNE Maxwell filtering\n    routines currently provide the following features:\n\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Feature                                                                     | MNE | MaxFilter |\n    +=============================================================================+=====+===========+\n    | Maxwell filtering software shielding                                        | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Bad channel reconstruction                                                  | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Cross-talk cancellation                                                     | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Fine calibration correction (1D)                                            | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Fine calibration correction (3D)                                            | X   |           |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Spatio-temporal SSS (tSSS)                                                  | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Coordinate frame translation                                                | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Regularization using information theory                                     | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Movement compensation (raw)                                                 | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Movement compensation (:func:`epochs <mne.epochs.average_movements>`)       | X   |           |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | :func:`cHPI subtraction <mne.chpi.filter_chpi>`                             | X   | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Double floating point precision                                             | X   |           |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Seamless processing of split (``-1.fif``) and concatenated files            | X   |           |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Certified for clinical use                                                  |     | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Automatic bad channel detection                                             |     | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n    | Head position estimation                                                    |     | X         |\n    +-----------------------------------------------------------------------------+-----+-----------+\n\n    Epoch-based movement compensation is described in [1]_.\n\n    Use of Maxwell filtering routines with non-Elekta systems is currently\n    **experimental**. Worse results for non-Elekta systems are expected due\n    to (at least):\n\n        * Missing fine-calibration and cross-talk cancellation data for\n          other systems.\n        * Processing with reference sensors has not been vetted.\n        * Regularization of components may not work well for all systems.\n        * Coil integration has not been optimized using Abramowitz/Stegun\n          definitions.\n\n    .. note:: Various Maxwell filtering algorithm components are covered by\n              patents owned by Elekta Oy, Helsinki, Finland.\n              These patents include, but may not be limited to:\n\n                  - US2006031038 (Signal Space Separation)\n                  - US6876196 (Head position determination)\n                  - WO2005067789 (DC fields)\n                  - WO2005078467 (MaxShield)\n                  - WO2006114473 (Temporal Signal Space Separation)\n\n              These patents likely preclude the use of Maxwell filtering code\n              in commercial applications. Consult a lawyer if necessary.\n\n    Currently, in order to perform Maxwell filtering, the raw data must not\n    have any projectors applied. During Maxwell filtering, the spatial\n    structure of the data is modified, so projectors are discarded (unless\n    in ``st_only=True`` mode).\n\n    References\n    ----------\n    .. [1] Taulu S. and Kajola M. "Presentation of electromagnetic\n           multichannel data: The signal space separation method,"\n           Journal of Applied Physics, vol. 97, pp. 124905 1-10, 2005.\n\n           http://lib.tkk.fi/Diss/2008/isbn9789512295654/article2.pdf\n\n    .. [2] Taulu S. and Simola J. "Spatiotemporal signal space separation\n           method for rejecting nearby interference in MEG measurements,"\n           Physics in Medicine and Biology, vol. 51, pp. 1759-1768, 2006.\n\n           http://lib.tkk.fi/Diss/2008/isbn9789512295654/article3.pdf\n    '
if (not isinstance(raw, BaseRaw)):
    raise TypeError(('raw must be Raw, not %s' % type(raw)))
_check_usable(raw)
_check_regularize(regularize)
st_correlation = float(st_correlation)
if ((st_correlation <= 0.0) or (st_correlation > 1.0)):
    raise ValueError(('Need 0 < st_correlation <= 1., got %s' % st_correlation))
if (coord_frame not in ('head', 'meg')):
    raise ValueError(('coord_frame must be either "head" or "meg", not "%s"' % coord_frame))
head_frame = (True if (coord_frame == 'head') else False)
recon_trans = _check_destination(destination, raw.info, head_frame)
if (st_duration is not None):
    st_duration = float(st_duration)
    if (not (0.0 < st_duration <= (raw.times[(- 1)] + (1.0 / raw.info['sfreq'])))):
        raise ValueError(('st_duration (%0.1fs) must be between 0 and the duration of the data (%0.1fs).' % (st_duration, raw.times[(- 1)])))
    st_correlation = float(st_correlation)
    st_duration = int(round((st_duration * raw.info['sfreq'])))
    if (not (0.0 < st_correlation <= 1)):
        raise ValueError('st_correlation must be between 0. and 1.')
if ((not isinstance(bad_condition, string_types)) or (bad_condition not in ['error', 'warning', 'ignore'])):
    raise ValueError(('bad_condition must be "error", "warning", or "ignore", not %s' % bad_condition))
if ((raw.info['dev_head_t'] is None) and (coord_frame == 'head')):
    raise RuntimeError('coord_frame cannot be "head" because info["dev_head_t"] is None; if this is an empty room recording, consider using coord_frame="meg"')
if (st_only and (st_duration is None)):
    raise ValueError('st_duration must not be None if st_only is True')
head_pos = _check_pos(head_pos, head_frame, raw, st_fixed, raw.info['sfreq'])
_check_info(raw.info, sss=(not st_only), tsss=(st_duration is not None), calibration=((not st_only) and (calibration is not None)), ctc=((not st_only) and (cross_talk is not None)))
utils.logger.info('Maxwell filtering raw data')
add_channels = ((head_pos[0] is not None) and (not st_only))
(raw_sss, pos_picks) = _copy_preload_add_channels(raw, add_channels=add_channels)
del raw
if (not st_only):
    _remove_meg_projs(raw_sss)
info = raw_sss.info
(meg_picks, mag_picks, grad_picks, good_picks, mag_or_fine) = _get_mf_picks(info, int_order, ext_order, ignore_ref)
(coil_scale, mag_scale) = _get_coil_scale(meg_picks, mag_picks, grad_picks, mag_scale, info)
sss_cal = dict()
if (calibration is not None):
    (calibration, sss_cal) = _update_sensor_geometry(info, calibration, ignore_ref)
    mag_or_fine.fill(True)
origin = _check_origin(origin, info, coord_frame, disp=True)
origin.setflags(write=False)
(n_in, n_out) = _get_n_moments([int_order, ext_order])
if (cross_talk is not None):
    sss_ctc = _read_ctc(cross_talk)
    ctc_chs = sss_ctc['proj_items_chs']
    meg_ch_names = [info['ch_names'][p] for p in meg_picks]
    if (meg_ch_names[0] not in ctc_chs):
        ctc_chs = _clean_names(ctc_chs, remove_whitespace=True)
    missing = sorted(list((set(meg_ch_names) - set(ctc_chs))))
    if (len(missing) != 0):
        raise RuntimeError(('Missing MEG channels in cross-talk matrix:\n%s' % missing))
    missing = sorted(list((set(ctc_chs) - set(meg_ch_names))))
    if (len(missing) > 0):
        warn(('Not all cross-talk channels in raw:\n%s' % missing))
    ctc_picks = [ctc_chs.index(info['ch_names'][c]) for c in meg_picks[good_picks]]
    assert (len(ctc_picks) == len(good_picks))
    ctc = sss_ctc['decoupler'][ctc_picks][:, ctc_picks]
    sss_ctc['decoupler'] = sss_ctc['decoupler'].T.tocsc()
else:
    sss_ctc = dict()
exp = dict(origin=origin, int_order=int_order, ext_order=0)
all_coils = _prep_mf_coils(info, ignore_ref)
S_recon = _trans_sss_basis(exp, all_coils, recon_trans, coil_scale)
exp['ext_order'] = ext_order
S_recon /= coil_scale
if (recon_trans is not None):
    diff = (1000 * (info['dev_head_t']['trans'][:3, 3] - recon_trans['trans'][:3, 3]))
    dist = numpy.sqrt(numpy.sum(_sq(diff)))
    if (dist > 25.0):
        warn(('Head position change is over 25 mm (%s) = %0.1f mm' % (', '.join((('%0.1f' % x) for x in diff)), dist)))
max_st = dict()
if (st_duration is not None):
    max_st.update(job=10, subspcorr=st_correlation, buflen=(st_duration / info['sfreq']))
    utils.logger.info(('    Processing data using tSSS with st_duration=%s' % max_st['buflen']))
    st_when = ('before' if st_fixed else 'after')
else:
    st_duration = max(int(round((10.0 * info['sfreq']))), 1)
    st_correlation = None
    st_when = 'never'
st_duration = min(len(raw_sss.times), st_duration)
del st_fixed
tempResult = arange(0, (len(raw_sss.times) + 1), st_duration)
	
===================================================================	
fix_stim_artifact: 47	
----------------------------	

"Eliminate stimulation's artifacts from instance.\n\n    .. note:: This function operates in-place, consider passing\n              ``inst.copy()`` if this is not desired.\n\n    Parameters\n    ----------\n    inst : instance of Raw or Epochs or Evoked\n        The data.\n    events : array, shape (n_events, 3)\n        The list of events. Required only when inst is Raw.\n    event_id : int\n        The id of the events generating the stimulation artifacts.\n        If None, read all events. Required only when inst is Raw.\n    tmin : float\n        Start time of the interpolation window in seconds.\n    tmax : float\n        End time of the interpolation window in seconds.\n    mode : 'linear' | 'window'\n        Way to fill the artifacted time interval.\n        'linear' does linear interpolation\n        'window' applies a (1 - hanning) window.\n    stim_channel : str | None\n        Stim channel to use.\n\n    Returns\n    -------\n    inst : instance of Raw or Evoked or Epochs\n        Instance with modified data\n    "
if (mode not in ('linear', 'window')):
    raise ValueError(("mode has to be 'linear' or 'window' (got %s)" % mode))
s_start = int(numpy.ceil((inst.info['sfreq'] * tmin)))
s_end = int(numpy.ceil((inst.info['sfreq'] * tmax)))
if ((mode == 'window') and ((s_end - s_start) < 4)):
    raise ValueError('Time range is too short. Use a larger interval or set mode to "linear".')
window = None
if (mode == 'window'):
    window = _get_window(s_start, s_end)
picks = _pick_data_channels(inst.info)
_check_preload(inst, 'fix_stim_artifact')
if isinstance(inst, BaseRaw):
    if (events is None):
        events = find_events(inst, stim_channel=stim_channel)
    if (len(events) == 0):
        raise ValueError('No events are found')
    if (event_id is None):
        tempResult = arange(len(events))
	
===================================================================	
_fix_artifact: 22	
----------------------------	

'Modify original data by using parameter data.'
from scipy.interpolate import interp1d
if (mode == 'linear'):
    x = numpy.array([first_samp, last_samp])
    f = interp1d(x, data[:, (first_samp, last_samp)][picks])
    tempResult = arange(first_samp, last_samp)
	
===================================================================	
test_ica_core: 160	
----------------------------	

'Test ICA on raw and epochs.'
raw = read_raw_fif(raw_fname).crop(1.5, stop).load_data()
test_cov = read_cov(test_cov_name)
events = read_events(event_name)
picks = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=False, exclude='bads')
epochs = Epochs(raw, events[:4], event_id, tmin, tmax, picks=picks, baseline=(None, 0), preload=True)
noise_cov = [None, test_cov]
n_components = [2, 1.0]
max_pca_components = [3]
picks_ = [picks]
methods = ['fastica']
iter_ica_params = product(noise_cov, n_components, max_pca_components, picks_, methods)
assert_raises(ValueError, ICA, n_components=3, max_pca_components=2)
assert_raises(ValueError, ICA, n_components=2.3, max_pca_components=2)
for (n_cov, n_comp, max_n, pcks, method) in iter_ica_params:
    ica = ICA(noise_cov=n_cov, n_components=n_comp, max_pca_components=max_n, n_pca_components=max_n, random_state=0, method=method, max_iter=1)
    assert_raises(ValueError, ica.__contains__, 'mag')
    print(ica)
    assert_raises(RuntimeError, ica.get_sources, raw)
    assert_raises(RuntimeError, ica.get_sources, epochs)
    with warnings.catch_warnings(record=True):
        ica.fit(raw, picks=pcks, start=start, stop=stop)
        repr(ica)
    assert_true(('mag' in ica))
    unmixing1 = ica.unmixing_matrix_
    with warnings.catch_warnings(record=True):
        ica.fit(raw, picks=pcks, start=start, stop=stop)
    assert_array_almost_equal(unmixing1, ica.unmixing_matrix_)
    raw_sources = ica.get_sources(raw)
    assert_equal(raw_sources._filenames, [None])
    print(raw_sources)
    sources = raw_sources[:, :][0]
    assert_true((sources.shape[0] == ica.n_components_))
    raw3 = raw.copy()
    raw3.preload = False
    assert_raises(ValueError, ica.apply, raw3, include=[1, 2])
    ica = ICA(noise_cov=n_cov, n_components=n_comp, max_pca_components=max_n, n_pca_components=max_n, random_state=0)
    with warnings.catch_warnings(record=True):
        ica.fit(epochs, picks=picks)
    data = epochs.get_data()[:, 0, :]
    n_samples = numpy.prod(data.shape)
    assert_equal(ica.n_samples_, n_samples)
    print(ica)
    sources = ica.get_sources(epochs).get_data()
    assert_true((sources.shape[1] == ica.n_components_))
    tempResult = arange(1)
	
===================================================================	
test_ica_additional: 314	
----------------------------	

'Test additional ICA functionality.'
import matplotlib.pyplot as plt
tempdir = _TempDir()
stop2 = 500
raw = read_raw_fif(raw_fname).crop(1.5, stop).load_data()
raw.annotations = Annotations([0.5], [0.5], ['BAD'])
test_cov = read_cov(test_cov_name)
events = read_events(event_name)
picks = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=False, exclude='bads')
epochs = Epochs(raw, events[:4], event_id, tmin, tmax, picks=picks, baseline=(None, 0), preload=True)
with warnings.catch_warnings(record=True):
    ica = ICA(n_components=None, max_pca_components=None, n_pca_components=None, random_state=0)
    ica.fit(epochs, picks=picks, decim=3)
picks2 = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=True, exclude='bads')
epochs_eog = Epochs(raw, events[:4], event_id, tmin, tmax, picks=picks2, baseline=(None, 0), preload=True)
test_cov2 = test_cov.copy()
ica = ICA(noise_cov=test_cov2, n_components=3, max_pca_components=4, n_pca_components=4)
assert_true((ica.info is None))
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks[:5])
assert_true(isinstance(ica.info, Info))
assert_true((ica.n_components_ < 5))
ica = ICA(n_components=3, max_pca_components=4, n_pca_components=4)
assert_raises(RuntimeError, ica.save, '')
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks=[1, 2, 3, 4, 5], start=start, stop=stop2)
with warnings.catch_warnings(record=True):
    (_, scores_1) = ica.find_bads_ecg(raw)
    (_, scores_2) = ica.find_bads_ecg(raw, raw.ch_names[1])
assert_false((scores_1[0] == scores_2[0]))
ica2 = ica.copy()
ica3 = ica.copy()
corrmap([ica, ica2], (0, 0), threshold='auto', label='blinks', plot=True, ch_type='mag')
corrmap([ica, ica2], (0, 0), threshold=2, plot=False, show=False)
assert_true((ica.labels_['blinks'] == ica2.labels_['blinks']))
assert_true((0 in ica.labels_['blinks']))
components = ica.get_components()
template = components[:, 0]
EvokedArray(components, ica.info, tmin=0.0).plot_topomap([0])
corrmap([ica, ica3], template, threshold='auto', label='blinks', plot=True, ch_type='mag')
assert_true((ica2.labels_['blinks'] == ica3.labels_['blinks']))
matplotlib.pyplot.close('all')
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    ica_badname = os.path.join(os.path.dirname(tempdir), 'test-bad-name.fif.gz')
    ica.save(ica_badname)
    read_ica(ica_badname)
assert_naming(w, 'test_ica.py', 2)
ica = ICA(n_components=3, max_pca_components=4, n_pca_components=4)
raw_ = raw.copy()
for _ in range(3):
    raw_.append(raw_)
n_samples = raw_._data.shape[1]
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks=None, decim=3)
assert_true(raw_._data.shape[1], n_samples)
ica = ICA(n_components=1.0, max_pca_components=4, n_pca_components=4)
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks=None, decim=3)
assert_true((ica.n_components_ == 4))
ica_var = _ica_explained_variance(ica, raw, normalize=True)
assert_true(numpy.all((ica_var[:(- 1)] >= ica_var[1:])))
ica.exclude = [0]
ica.labels_ = dict(blink=[0], think=[1])
ica_sorted = _sort_components(ica, [3, 2, 1, 0], copy=True)
assert_equal(ica_sorted.exclude, [3])
assert_equal(ica_sorted.labels_, dict(blink=[3], think=[2]))
assert_raises(RuntimeError, ica.get_sources, epochs)
test_ica_fname = os.path.join(os.path.dirname(tempdir), 'test-ica.fif')
for cov in (None, test_cov):
    ica = ICA(noise_cov=cov, n_components=2, max_pca_components=4, n_pca_components=4)
    with warnings.catch_warnings(record=True):
        ica.fit(raw, picks=picks, start=start, stop=stop2)
    sources = ica.get_sources(epochs).get_data()
    assert_true((ica.mixing_matrix_.shape == (2, 2)))
    assert_true((ica.unmixing_matrix_.shape == (2, 2)))
    assert_true((ica.pca_components_.shape == (4, len(picks))))
    assert_true((sources.shape[1] == ica.n_components_))
    for exclude in [[], [0]]:
        ica.exclude = exclude
        ica.labels_ = {'foo': [0]}
        ica.save(test_ica_fname)
        ica_read = read_ica(test_ica_fname)
        assert_true((ica.exclude == ica_read.exclude))
        assert_equal(ica.labels_, ica_read.labels_)
        ica.exclude = []
        ica.apply(raw, exclude=[1])
        assert_true((ica.exclude == []))
        ica.exclude = [0, 1]
        ica.apply(raw, exclude=[1])
        assert_true((ica.exclude == [0, 1]))
        ica_raw = ica.get_sources(raw)
        assert_true((ica.exclude == [ica_raw.ch_names.index(e) for e in ica_raw.info['bads']]))
    d1 = ica_raw._data[0].copy()
    ica_raw.filter(4, 20, fir_design='firwin2')
    assert_equal(ica_raw.info['lowpass'], 20.0)
    assert_equal(ica_raw.info['highpass'], 4.0)
    assert_true((d1 != ica_raw._data[0]).any())
    d1 = ica_raw._data[0].copy()
    ica_raw.notch_filter([10], trans_bandwidth=10, fir_design='firwin')
    assert_true((d1 != ica_raw._data[0]).any())
    ica.n_pca_components = 2
    ica.method = 'fake'
    ica.save(test_ica_fname)
    ica_read = read_ica(test_ica_fname)
    assert_true((ica.n_pca_components == ica_read.n_pca_components))
    assert_equal(ica.method, ica_read.method)
    assert_equal(ica.labels_, ica_read.labels_)
    attrs = 'mixing_matrix_ unmixing_matrix_ pca_components_ pca_explained_variance_ _pre_whitener'

    def f(x, y):
        return getattr(x, y).dtype
    for attr in attrs.split():
        assert_equal(f(ica_read, attr), f(ica, attr))
    ica.n_pca_components = 4
    ica_read.n_pca_components = 4
    ica.exclude = []
    ica.save(test_ica_fname)
    ica_read = read_ica(test_ica_fname)
    for attr in ['mixing_matrix_', 'unmixing_matrix_', 'pca_components_', 'pca_mean_', 'pca_explained_variance_', '_pre_whitener']:
        assert_array_almost_equal(getattr(ica, attr), getattr(ica_read, attr))
    assert_true((ica.ch_names == ica_read.ch_names))
    assert_true(isinstance(ica_read.info, Info))
    sources = ica.get_sources(raw)[:, :][0]
    sources2 = ica_read.get_sources(raw)[:, :][0]
    assert_array_almost_equal(sources, sources2)
    _raw1 = ica.apply(raw, exclude=[1])
    _raw2 = ica_read.apply(raw, exclude=[1])
    assert_array_almost_equal(_raw1[:, :][0], _raw2[:, :][0])
os.remove(test_ica_fname)
for (name, func) in get_score_funcs().items():
    if (name in score_funcs_unsuited):
        continue
    scores = ica.score_sources(raw, target='EOG 061', score_func=func, start=0, stop=10)
    assert_true((ica.n_components_ == len(scores)))
scores = ica.score_sources(raw, score_func=scipy.stats.skew)
tempResult = arange(1)
	
===================================================================	
test_ica_additional: 354	
----------------------------	

'Test additional ICA functionality.'
import matplotlib.pyplot as plt
tempdir = _TempDir()
stop2 = 500
raw = read_raw_fif(raw_fname).crop(1.5, stop).load_data()
raw.annotations = Annotations([0.5], [0.5], ['BAD'])
test_cov = read_cov(test_cov_name)
events = read_events(event_name)
picks = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=False, exclude='bads')
epochs = Epochs(raw, events[:4], event_id, tmin, tmax, picks=picks, baseline=(None, 0), preload=True)
with warnings.catch_warnings(record=True):
    ica = ICA(n_components=None, max_pca_components=None, n_pca_components=None, random_state=0)
    ica.fit(epochs, picks=picks, decim=3)
picks2 = pick_types(raw.info, meg=True, stim=False, ecg=False, eog=True, exclude='bads')
epochs_eog = Epochs(raw, events[:4], event_id, tmin, tmax, picks=picks2, baseline=(None, 0), preload=True)
test_cov2 = test_cov.copy()
ica = ICA(noise_cov=test_cov2, n_components=3, max_pca_components=4, n_pca_components=4)
assert_true((ica.info is None))
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks[:5])
assert_true(isinstance(ica.info, Info))
assert_true((ica.n_components_ < 5))
ica = ICA(n_components=3, max_pca_components=4, n_pca_components=4)
assert_raises(RuntimeError, ica.save, '')
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks=[1, 2, 3, 4, 5], start=start, stop=stop2)
with warnings.catch_warnings(record=True):
    (_, scores_1) = ica.find_bads_ecg(raw)
    (_, scores_2) = ica.find_bads_ecg(raw, raw.ch_names[1])
assert_false((scores_1[0] == scores_2[0]))
ica2 = ica.copy()
ica3 = ica.copy()
corrmap([ica, ica2], (0, 0), threshold='auto', label='blinks', plot=True, ch_type='mag')
corrmap([ica, ica2], (0, 0), threshold=2, plot=False, show=False)
assert_true((ica.labels_['blinks'] == ica2.labels_['blinks']))
assert_true((0 in ica.labels_['blinks']))
components = ica.get_components()
template = components[:, 0]
EvokedArray(components, ica.info, tmin=0.0).plot_topomap([0])
corrmap([ica, ica3], template, threshold='auto', label='blinks', plot=True, ch_type='mag')
assert_true((ica2.labels_['blinks'] == ica3.labels_['blinks']))
matplotlib.pyplot.close('all')
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    ica_badname = os.path.join(os.path.dirname(tempdir), 'test-bad-name.fif.gz')
    ica.save(ica_badname)
    read_ica(ica_badname)
assert_naming(w, 'test_ica.py', 2)
ica = ICA(n_components=3, max_pca_components=4, n_pca_components=4)
raw_ = raw.copy()
for _ in range(3):
    raw_.append(raw_)
n_samples = raw_._data.shape[1]
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks=None, decim=3)
assert_true(raw_._data.shape[1], n_samples)
ica = ICA(n_components=1.0, max_pca_components=4, n_pca_components=4)
with warnings.catch_warnings(record=True):
    ica.fit(raw, picks=None, decim=3)
assert_true((ica.n_components_ == 4))
ica_var = _ica_explained_variance(ica, raw, normalize=True)
assert_true(numpy.all((ica_var[:(- 1)] >= ica_var[1:])))
ica.exclude = [0]
ica.labels_ = dict(blink=[0], think=[1])
ica_sorted = _sort_components(ica, [3, 2, 1, 0], copy=True)
assert_equal(ica_sorted.exclude, [3])
assert_equal(ica_sorted.labels_, dict(blink=[3], think=[2]))
assert_raises(RuntimeError, ica.get_sources, epochs)
test_ica_fname = os.path.join(os.path.dirname(tempdir), 'test-ica.fif')
for cov in (None, test_cov):
    ica = ICA(noise_cov=cov, n_components=2, max_pca_components=4, n_pca_components=4)
    with warnings.catch_warnings(record=True):
        ica.fit(raw, picks=picks, start=start, stop=stop2)
    sources = ica.get_sources(epochs).get_data()
    assert_true((ica.mixing_matrix_.shape == (2, 2)))
    assert_true((ica.unmixing_matrix_.shape == (2, 2)))
    assert_true((ica.pca_components_.shape == (4, len(picks))))
    assert_true((sources.shape[1] == ica.n_components_))
    for exclude in [[], [0]]:
        ica.exclude = exclude
        ica.labels_ = {'foo': [0]}
        ica.save(test_ica_fname)
        ica_read = read_ica(test_ica_fname)
        assert_true((ica.exclude == ica_read.exclude))
        assert_equal(ica.labels_, ica_read.labels_)
        ica.exclude = []
        ica.apply(raw, exclude=[1])
        assert_true((ica.exclude == []))
        ica.exclude = [0, 1]
        ica.apply(raw, exclude=[1])
        assert_true((ica.exclude == [0, 1]))
        ica_raw = ica.get_sources(raw)
        assert_true((ica.exclude == [ica_raw.ch_names.index(e) for e in ica_raw.info['bads']]))
    d1 = ica_raw._data[0].copy()
    ica_raw.filter(4, 20, fir_design='firwin2')
    assert_equal(ica_raw.info['lowpass'], 20.0)
    assert_equal(ica_raw.info['highpass'], 4.0)
    assert_true((d1 != ica_raw._data[0]).any())
    d1 = ica_raw._data[0].copy()
    ica_raw.notch_filter([10], trans_bandwidth=10, fir_design='firwin')
    assert_true((d1 != ica_raw._data[0]).any())
    ica.n_pca_components = 2
    ica.method = 'fake'
    ica.save(test_ica_fname)
    ica_read = read_ica(test_ica_fname)
    assert_true((ica.n_pca_components == ica_read.n_pca_components))
    assert_equal(ica.method, ica_read.method)
    assert_equal(ica.labels_, ica_read.labels_)
    attrs = 'mixing_matrix_ unmixing_matrix_ pca_components_ pca_explained_variance_ _pre_whitener'

    def f(x, y):
        return getattr(x, y).dtype
    for attr in attrs.split():
        assert_equal(f(ica_read, attr), f(ica, attr))
    ica.n_pca_components = 4
    ica_read.n_pca_components = 4
    ica.exclude = []
    ica.save(test_ica_fname)
    ica_read = read_ica(test_ica_fname)
    for attr in ['mixing_matrix_', 'unmixing_matrix_', 'pca_components_', 'pca_mean_', 'pca_explained_variance_', '_pre_whitener']:
        assert_array_almost_equal(getattr(ica, attr), getattr(ica_read, attr))
    assert_true((ica.ch_names == ica_read.ch_names))
    assert_true(isinstance(ica_read.info, Info))
    sources = ica.get_sources(raw)[:, :][0]
    sources2 = ica_read.get_sources(raw)[:, :][0]
    assert_array_almost_equal(sources, sources2)
    _raw1 = ica.apply(raw, exclude=[1])
    _raw2 = ica_read.apply(raw, exclude=[1])
    assert_array_almost_equal(_raw1[:, :][0], _raw2[:, :][0])
os.remove(test_ica_fname)
for (name, func) in get_score_funcs().items():
    if (name in score_funcs_unsuited):
        continue
    scores = ica.score_sources(raw, target='EOG 061', score_func=func, start=0, stop=10)
    assert_true((ica.n_components_ == len(scores)))
scores = ica.score_sources(raw, score_func=scipy.stats.skew)
assert_raises(ValueError, ica.score_sources, raw, target=numpy.arange(1))
params = []
params += [(None, (- 1), slice(2), [0, 1])]
params += [(None, 'MEG 1531')]
for (idx, ch_name) in product(*params):
    ica.detect_artifacts(raw, start_find=0, stop_find=50, ecg_ch=ch_name, eog_ch=ch_name, skew_criterion=idx, var_criterion=idx, kurt_criterion=idx)
evoked = epochs.average()
evoked_data = evoked.data.copy()
raw_data = raw[:][0].copy()
epochs_data = epochs.get_data().copy()
with warnings.catch_warnings(record=True):
    (idx, scores) = ica.find_bads_ecg(raw, method='ctps')
    assert_equal(len(scores), ica.n_components_)
    (idx, scores) = ica.find_bads_ecg(raw, method='correlation')
    assert_equal(len(scores), ica.n_components_)
    (idx, scores) = ica.find_bads_eog(raw)
    assert_equal(len(scores), ica.n_components_)
    (idx, scores) = ica.find_bads_ecg(epochs, method='ctps')
    assert_equal(len(scores), ica.n_components_)
    assert_raises(ValueError, ica.find_bads_ecg, epochs.average(), method='ctps')
    assert_raises(ValueError, ica.find_bads_ecg, raw, method='crazy-coupling')
    (idx, scores) = ica.find_bads_eog(raw)
    assert_equal(len(scores), ica.n_components_)
    raw.info['chs'][(raw.ch_names.index('EOG 061') - 1)]['kind'] = 202
    (idx, scores) = ica.find_bads_eog(raw)
    assert_true(isinstance(scores, list))
    assert_equal(len(scores[0]), ica.n_components_)
    (idx, scores) = ica.find_bads_eog(evoked, ch_name='MEG 1441')
    assert_equal(len(scores), ica.n_components_)
    (idx, scores) = ica.find_bads_ecg(evoked, method='correlation')
    assert_equal(len(scores), ica.n_components_)
assert_array_equal(raw_data, raw[:][0])
assert_array_equal(epochs_data, epochs.get_data())
assert_array_equal(evoked_data, evoked.data)
for (name, func) in get_score_funcs().items():
    if (name in score_funcs_unsuited):
        continue
    scores = ica.score_sources(epochs_eog, target='EOG 061', score_func=func)
    assert_true((ica.n_components_ == len(scores)))
scores = ica.score_sources(epochs, score_func=scipy.stats.skew)
tempResult = arange(1)
	
===================================================================	
test_xdawn_apply_transform: 70	
----------------------------	

'Test Xdawn apply and transform.'
(raw, events, picks) = _get_data()
raw.pick_types(eeg=True, meg=False)
epochs = Epochs(raw, events, event_id, tmin, tmax, proj=False, preload=True, baseline=None, verbose=False)
n_components = 2
xd = Xdawn(n_components=n_components, correct_overlap=False)
xd.fit(epochs)
for inst in [raw, epochs.average(), epochs]:
    denoise = xd.apply(inst)
assert_raises(ValueError, xd.apply, 42)
xd.transform(epochs)
xd.transform(epochs.average())
xd.transform(epochs._data)
xd.transform(epochs._data[0])
assert_raises(ValueError, xd.transform, 42)
numpy.random.seed(0)
tempResult = arange(len(epochs))
	
===================================================================	
simulate_raw: 160	
----------------------------	

'Simulate raw data.\n\n    Head movements can optionally be simulated using the ``head_pos``\n    parameter.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        The raw template to use for simulation. The ``info``, ``times``,\n        and potentially ``first_samp`` properties will be used.\n    stc : instance of SourceEstimate\n        The source estimate to use to simulate data. Must have the same\n        sample rate as the raw data.\n    trans : dict | str | None\n        Either a transformation filename (usually made using mne_analyze)\n        or an info dict (usually opened using read_trans()).\n        If string, an ending of `.fif` or `.fif.gz` will be assumed to\n        be in FIF format, any other ending will be assumed to be a text\n        file with a 4x4 transformation matrix (like the `--trans` MNE-C\n        option). If trans is None, an identity transform will be used.\n    src : str | instance of SourceSpaces\n        Source space corresponding to the stc. If string, should be a source\n        space filename. Can also be an instance of loaded or generated\n        SourceSpaces.\n    bem : str | dict\n        BEM solution  corresponding to the stc. If string, should be a BEM\n        solution filename (e.g., "sample-5120-5120-5120-bem-sol.fif").\n    cov : instance of Covariance | str | None\n        The sensor covariance matrix used to generate noise. If None,\n        no noise will be added. If \'simple\', a basic (diagonal) ad-hoc\n        noise covariance will be used. If a string, then the covariance\n        will be loaded.\n    blink : bool\n        If true, add simulated blink artifacts. See Notes for details.\n    ecg : bool\n        If true, add simulated ECG artifacts. See Notes for details.\n    chpi : bool\n        If true, simulate continuous head position indicator information.\n        Valid cHPI information must encoded in ``raw.info[\'hpi_meas\']``\n        to use this option.\n    head_pos : None | str | dict | tuple | array\n        Name of the position estimates file. Should be in the format of\n        the files produced by maxfilter. If dict, keys should\n        be the time points and entries should be 4x4 ``dev_head_t``\n        matrices. If None, the original head position (from\n        ``info[\'dev_head_t\']``) will be used. If tuple, should have the\n        same format as data returned by `head_pos_to_trans_rot_t`.\n        If array, should be of the form returned by\n        :func:`mne.chpi.read_head_pos`.\n    mindist : float\n        Minimum distance between sources and the inner skull boundary\n        to use during forward calculation.\n    interp : str\n        Either \'hann\', \'cos2\', \'linear\', or \'zero\', the type of\n        forward-solution interpolation to use between forward solutions\n        at different head positions.\n    iir_filter : None | array\n        IIR filter coefficients (denominator) e.g. [1, -1, 0.2].\n    n_jobs : int\n        Number of jobs to use.\n    random_state : None | int | np.random.RandomState\n        The random generator state used for blink, ECG, and sensor\n        noise randomization.\n    use_cps : None | bool (default None)\n        Whether to use cortical patch statistics to define normal\n        orientations. Only used when surf_ori and/or force_fixed are True.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    raw : instance of Raw\n        The simulated raw file.\n\n    See Also\n    --------\n    mne.chpi.read_head_pos\n    simulate_evoked\n    simulate_stc\n    simulate_sparse_stc\n\n    Notes\n    -----\n    Events coded with the position number (starting at 1) will be stored\n    in the trigger channel (if available) at times corresponding to t=0\n    in the ``stc``.\n\n    The resulting SNR will be determined by the structure of the noise\n    covariance, the amplitudes of ``stc``, and the head position(s) provided.\n\n    The blink and ECG artifacts are generated by 1) placing impulses at\n    random times of activation, and 2) convolving with activation kernel\n    functions. In both cases, the scale-factors of the activation functions\n    (and for the resulting EOG and ECG channel traces) were chosen based on\n    visual inspection to yield amplitudes generally consistent with those\n    seen in experimental data. Noisy versions of the blink and ECG\n    activations will be stored in the first EOG and ECG channel in the\n    raw file, respectively, if they exist.\n\n    For blink artifacts:\n\n        1. Random activation times are drawn from an inhomogeneous poisson\n           process whose blink rate oscillates between 4.5 blinks/minute\n           and 17 blinks/minute based on the low (reading) and high (resting)\n           blink rates from [1]_.\n        2. The activation kernel is a 250 ms Hanning window.\n        3. Two activated dipoles are located in the z=0 plane (in head\n           coordinates) at ±30 degrees away from the y axis (nasion).\n        4. Activations affect MEG and EEG channels.\n\n    For ECG artifacts:\n\n        1. Random inter-beat intervals are drawn from a uniform distribution\n           of times corresponding to 40 and 80 beats per minute.\n        2. The activation function is the sum of three Hanning windows with\n           varying durations and scales to make a more complex waveform.\n        3. The activated dipole is located one (estimated) head radius to\n           the left (-x) of head center and three head radii below (+z)\n           head center; this dipole is oriented in the +x direction.\n        4. Activations only affect MEG channels.\n\n    .. versionadded:: 0.10.0\n\n    References\n    ----------\n    .. [1] Bentivoglio et al. "Analysis of blink rate patterns in normal\n           subjects" Movement Disorders, 1997 Nov;12(6):1028-34.\n    '
if (not isinstance(raw, BaseRaw)):
    raise TypeError('raw should be an instance of Raw')
(times, info, first_samp) = (raw.times, raw.info, raw.first_samp)
raw_verbose = raw.verbose
if (not isinstance(stc, _BaseSourceEstimate)):
    raise TypeError('stc must be a SourceEstimate')
if (not numpy.allclose(info['sfreq'], (1.0 / stc.tstep))):
    raise ValueError('stc and info must have same sample rate')
if (len(stc.times) <= 2):
    raise ValueError('stc must have at least three time points')
stim = (False if (len(pick_types(info, meg=False, stim=True)) == 0) else True)
n_jobs = check_n_jobs(n_jobs)
rng = check_random_state(random_state)
interper = _Interp2(interp)
if (head_pos is None):
    head_pos = dict()
if isinstance(head_pos, string_types):
    head_pos = read_head_pos(head_pos)
if isinstance(head_pos, numpy.ndarray):
    head_pos = head_pos_to_trans_rot_t(head_pos)
if isinstance(head_pos, tuple):
    (transs, rots, ts) = head_pos
    ts -= (first_samp / info['sfreq'])
    dev_head_ts = [numpy.r_[(numpy.c_[(r, t[:, numpy.newaxis])], [[0, 0, 0, 1]])] for (r, t) in zip(rots, transs)]
    del transs, rots
elif isinstance(head_pos, dict):
    ts = numpy.array(list(head_pos.keys()), float)
    ts.sort()
    dev_head_ts = [head_pos[float(tt)] for tt in ts]
else:
    raise TypeError(('unknown head_pos type %s' % type(head_pos)))
bad = (ts < 0)
if bad.any():
    raise RuntimeError(('All position times must be >= 0, found %s/%s< 0' % (bad.sum(), len(bad))))
bad = (ts > times[(- 1)])
if bad.any():
    raise RuntimeError(('All position times must be <= t_end (%0.1f sec), found %s/%s bad values (is this a split file?)' % (times[(- 1)], bad.sum(), len(bad))))
if ((len(ts) == 0) or (ts[0] > 0)):
    ts = numpy.r_[([0.0], ts)]
    dev_head_ts.insert(0, info['dev_head_t']['trans'])
dev_head_ts = [{'trans': d, 'to': info['dev_head_t']['to'], 'from': info['dev_head_t']['from']} for d in dev_head_ts]
offsets = raw.time_as_index(ts)
offsets = numpy.concatenate([offsets, [len(times)]])
assert (offsets[(- 2)] != offsets[(- 1)])
assert numpy.array_equal(offsets, numpy.unique(offsets))
assert (len(offsets) == (len(dev_head_ts) + 1))
del ts
src = _ensure_src(src, verbose=False)
if isinstance(bem, string_types):
    bem = read_bem_solution(bem, verbose=False)
if isinstance(cov, string_types):
    if (cov == 'simple'):
        cov = make_ad_hoc_cov(info, verbose=False)
    else:
        cov = read_cov(cov, verbose=False)
approx_events = int(((len(times) / info['sfreq']) / (stc.times[(- 1)] - stc.times[0])))
utils.logger.info(('Provided parameters will provide approximately %s event%s' % (approx_events, _pl(approx_events))))
meeg_picks = pick_types(info, meg=True, eeg=True, exclude=[])
meg_picks = pick_types(info, meg=True, eeg=False, exclude=[])
fwd_info = pick_info(info, meeg_picks)
fwd_info['projs'] = []
utils.logger.info(('Setting up raw simulation: %s position%s, "%s" interpolation' % (len(dev_head_ts), _pl(dev_head_ts), interp)))
del interp
verts = stc.vertices
verts = ([verts] if isinstance(stc, VolSourceEstimate) else verts)
src = _restrict_source_space_to(src, verts)
raw_data = numpy.zeros((len(info['ch_names']), len(times)))
ecg_rr = blink_rrs = exg_bem = hpi_rrs = None
ecg = (ecg and (len(meg_picks) > 0))
chpi = (chpi and (len(meg_picks) > 0))
if chpi:
    (hpi_freqs, hpi_pick, hpi_ons) = _get_hpi_info(info)
    hpi_rrs = _get_hpi_initial_fit(info, verbose='error')
    hpi_nns = (hpi_rrs / numpy.sqrt(numpy.sum((hpi_rrs * hpi_rrs), axis=1))[:, numpy.newaxis])
    raw_data[hpi_pick, :] = hpi_ons.sum()
    _log_ch('cHPI status bits enbled and', info, hpi_pick)
if (blink or ecg):
    (R, r0) = fit_sphere_to_headshape(info, units='m', verbose=False)[:2]
    exg_bem = make_sphere_model(r0, head_radius=R, relative_radii=(0.97, 0.98, 0.99, 1.0), sigmas=(0.33, 1.0, 0.004, 0.33), verbose=False)
if blink:
    blink_rrs = numpy.array([[numpy.cos((numpy.pi / 3.0)), numpy.sin((numpy.pi / 3.0)), 0.0], [(- numpy.cos((numpy.pi / 3.0))), numpy.sin((numpy.pi / 3)), 0.0]])
    blink_rrs /= numpy.sqrt(numpy.sum((blink_rrs * blink_rrs), axis=1))[:, numpy.newaxis]
    blink_rrs *= (0.96 * R)
    blink_rrs += r0
    blink_nns = numpy.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    blink_rate = ((1 + numpy.cos(((((2 * numpy.pi) * 1.0) / 60.0) * times))) / 2.0)
    blink_rate *= (12.5 / 60.0)
    blink_rate += (4.5 / 60.0)
    blink_data = (rng.rand(len(times)) < (blink_rate / info['sfreq']))
    blink_data = (blink_data * (rng.rand(len(times)) + 0.5))
    blink_kernel = numpy.hanning(int((0.25 * info['sfreq'])))
    blink_data = (numpy.convolve(blink_data, blink_kernel, 'same')[numpy.newaxis, :] * 1e-07)
    ch = pick_types(info, meg=False, eeg=False, eog=True)
    noise = (rng.randn(blink_data.shape[1]) * 5e-06)
    if (len(ch) >= 1):
        ch = ch[(- 1)]
        raw_data[ch, :] = ((blink_data * 1000.0) + noise)
    else:
        ch = None
    _log_ch('Blinks simulated and trace', info, ch)
    del blink_kernel, blink_rate, noise
if ecg:
    ecg_rr = numpy.array([[(- R), 0, ((- 3) * R)]])
    max_beats = int(numpy.ceil(((times[(- 1)] * 80.0) / 60.0)))
    cardiac_idx = np.cumsum((rng.uniform((60.0 / 80.0), (60.0 / 40.0), max_beats) * info['sfreq'])).astype(int)
    cardiac_idx = cardiac_idx[(cardiac_idx < len(times))]
    cardiac_data = numpy.zeros(len(times))
    cardiac_data[cardiac_idx] = 1
    cardiac_kernel = numpy.concatenate([(2 * numpy.hanning(int((0.04 * info['sfreq'])))), ((- 0.3) * numpy.hanning(int((0.05 * info['sfreq'])))), (0.2 * numpy.hanning(int((0.26 * info['sfreq']))))], axis=(- 1))
    ecg_data = (numpy.convolve(cardiac_data, cardiac_kernel, 'same')[numpy.newaxis, :] * 1.5e-07)
    ch = pick_types(info, meg=False, eeg=False, ecg=True)
    noise = (rng.randn(ecg_data.shape[1]) * 1.5e-05)
    if (len(ch) >= 1):
        ch = ch[(- 1)]
        raw_data[ch, :] = ((ecg_data * 2000.0) + noise)
    else:
        ch = None
    _log_ch('ECG simulated and trace', info, ch)
    del cardiac_data, cardiac_kernel, max_beats, cardiac_idx
stc_event_idx = numpy.argmin(numpy.abs(stc.times))
if stim:
    event_ch = pick_channels(info['ch_names'], _get_stim_channel(None, info))[0]
    raw_data[event_ch, :] = 0.0
else:
    event_ch = None
_log_ch('Event information', info, event_ch)
used = numpy.zeros(len(times), bool)
tempResult = arange(len(times))
	
===================================================================	
simulate_raw: 163	
----------------------------	

'Simulate raw data.\n\n    Head movements can optionally be simulated using the ``head_pos``\n    parameter.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        The raw template to use for simulation. The ``info``, ``times``,\n        and potentially ``first_samp`` properties will be used.\n    stc : instance of SourceEstimate\n        The source estimate to use to simulate data. Must have the same\n        sample rate as the raw data.\n    trans : dict | str | None\n        Either a transformation filename (usually made using mne_analyze)\n        or an info dict (usually opened using read_trans()).\n        If string, an ending of `.fif` or `.fif.gz` will be assumed to\n        be in FIF format, any other ending will be assumed to be a text\n        file with a 4x4 transformation matrix (like the `--trans` MNE-C\n        option). If trans is None, an identity transform will be used.\n    src : str | instance of SourceSpaces\n        Source space corresponding to the stc. If string, should be a source\n        space filename. Can also be an instance of loaded or generated\n        SourceSpaces.\n    bem : str | dict\n        BEM solution  corresponding to the stc. If string, should be a BEM\n        solution filename (e.g., "sample-5120-5120-5120-bem-sol.fif").\n    cov : instance of Covariance | str | None\n        The sensor covariance matrix used to generate noise. If None,\n        no noise will be added. If \'simple\', a basic (diagonal) ad-hoc\n        noise covariance will be used. If a string, then the covariance\n        will be loaded.\n    blink : bool\n        If true, add simulated blink artifacts. See Notes for details.\n    ecg : bool\n        If true, add simulated ECG artifacts. See Notes for details.\n    chpi : bool\n        If true, simulate continuous head position indicator information.\n        Valid cHPI information must encoded in ``raw.info[\'hpi_meas\']``\n        to use this option.\n    head_pos : None | str | dict | tuple | array\n        Name of the position estimates file. Should be in the format of\n        the files produced by maxfilter. If dict, keys should\n        be the time points and entries should be 4x4 ``dev_head_t``\n        matrices. If None, the original head position (from\n        ``info[\'dev_head_t\']``) will be used. If tuple, should have the\n        same format as data returned by `head_pos_to_trans_rot_t`.\n        If array, should be of the form returned by\n        :func:`mne.chpi.read_head_pos`.\n    mindist : float\n        Minimum distance between sources and the inner skull boundary\n        to use during forward calculation.\n    interp : str\n        Either \'hann\', \'cos2\', \'linear\', or \'zero\', the type of\n        forward-solution interpolation to use between forward solutions\n        at different head positions.\n    iir_filter : None | array\n        IIR filter coefficients (denominator) e.g. [1, -1, 0.2].\n    n_jobs : int\n        Number of jobs to use.\n    random_state : None | int | np.random.RandomState\n        The random generator state used for blink, ECG, and sensor\n        noise randomization.\n    use_cps : None | bool (default None)\n        Whether to use cortical patch statistics to define normal\n        orientations. Only used when surf_ori and/or force_fixed are True.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    raw : instance of Raw\n        The simulated raw file.\n\n    See Also\n    --------\n    mne.chpi.read_head_pos\n    simulate_evoked\n    simulate_stc\n    simulate_sparse_stc\n\n    Notes\n    -----\n    Events coded with the position number (starting at 1) will be stored\n    in the trigger channel (if available) at times corresponding to t=0\n    in the ``stc``.\n\n    The resulting SNR will be determined by the structure of the noise\n    covariance, the amplitudes of ``stc``, and the head position(s) provided.\n\n    The blink and ECG artifacts are generated by 1) placing impulses at\n    random times of activation, and 2) convolving with activation kernel\n    functions. In both cases, the scale-factors of the activation functions\n    (and for the resulting EOG and ECG channel traces) were chosen based on\n    visual inspection to yield amplitudes generally consistent with those\n    seen in experimental data. Noisy versions of the blink and ECG\n    activations will be stored in the first EOG and ECG channel in the\n    raw file, respectively, if they exist.\n\n    For blink artifacts:\n\n        1. Random activation times are drawn from an inhomogeneous poisson\n           process whose blink rate oscillates between 4.5 blinks/minute\n           and 17 blinks/minute based on the low (reading) and high (resting)\n           blink rates from [1]_.\n        2. The activation kernel is a 250 ms Hanning window.\n        3. Two activated dipoles are located in the z=0 plane (in head\n           coordinates) at ±30 degrees away from the y axis (nasion).\n        4. Activations affect MEG and EEG channels.\n\n    For ECG artifacts:\n\n        1. Random inter-beat intervals are drawn from a uniform distribution\n           of times corresponding to 40 and 80 beats per minute.\n        2. The activation function is the sum of three Hanning windows with\n           varying durations and scales to make a more complex waveform.\n        3. The activated dipole is located one (estimated) head radius to\n           the left (-x) of head center and three head radii below (+z)\n           head center; this dipole is oriented in the +x direction.\n        4. Activations only affect MEG channels.\n\n    .. versionadded:: 0.10.0\n\n    References\n    ----------\n    .. [1] Bentivoglio et al. "Analysis of blink rate patterns in normal\n           subjects" Movement Disorders, 1997 Nov;12(6):1028-34.\n    '
if (not isinstance(raw, BaseRaw)):
    raise TypeError('raw should be an instance of Raw')
(times, info, first_samp) = (raw.times, raw.info, raw.first_samp)
raw_verbose = raw.verbose
if (not isinstance(stc, _BaseSourceEstimate)):
    raise TypeError('stc must be a SourceEstimate')
if (not numpy.allclose(info['sfreq'], (1.0 / stc.tstep))):
    raise ValueError('stc and info must have same sample rate')
if (len(stc.times) <= 2):
    raise ValueError('stc must have at least three time points')
stim = (False if (len(pick_types(info, meg=False, stim=True)) == 0) else True)
n_jobs = check_n_jobs(n_jobs)
rng = check_random_state(random_state)
interper = _Interp2(interp)
if (head_pos is None):
    head_pos = dict()
if isinstance(head_pos, string_types):
    head_pos = read_head_pos(head_pos)
if isinstance(head_pos, numpy.ndarray):
    head_pos = head_pos_to_trans_rot_t(head_pos)
if isinstance(head_pos, tuple):
    (transs, rots, ts) = head_pos
    ts -= (first_samp / info['sfreq'])
    dev_head_ts = [numpy.r_[(numpy.c_[(r, t[:, numpy.newaxis])], [[0, 0, 0, 1]])] for (r, t) in zip(rots, transs)]
    del transs, rots
elif isinstance(head_pos, dict):
    ts = numpy.array(list(head_pos.keys()), float)
    ts.sort()
    dev_head_ts = [head_pos[float(tt)] for tt in ts]
else:
    raise TypeError(('unknown head_pos type %s' % type(head_pos)))
bad = (ts < 0)
if bad.any():
    raise RuntimeError(('All position times must be >= 0, found %s/%s< 0' % (bad.sum(), len(bad))))
bad = (ts > times[(- 1)])
if bad.any():
    raise RuntimeError(('All position times must be <= t_end (%0.1f sec), found %s/%s bad values (is this a split file?)' % (times[(- 1)], bad.sum(), len(bad))))
if ((len(ts) == 0) or (ts[0] > 0)):
    ts = numpy.r_[([0.0], ts)]
    dev_head_ts.insert(0, info['dev_head_t']['trans'])
dev_head_ts = [{'trans': d, 'to': info['dev_head_t']['to'], 'from': info['dev_head_t']['from']} for d in dev_head_ts]
offsets = raw.time_as_index(ts)
offsets = numpy.concatenate([offsets, [len(times)]])
assert (offsets[(- 2)] != offsets[(- 1)])
assert numpy.array_equal(offsets, numpy.unique(offsets))
assert (len(offsets) == (len(dev_head_ts) + 1))
del ts
src = _ensure_src(src, verbose=False)
if isinstance(bem, string_types):
    bem = read_bem_solution(bem, verbose=False)
if isinstance(cov, string_types):
    if (cov == 'simple'):
        cov = make_ad_hoc_cov(info, verbose=False)
    else:
        cov = read_cov(cov, verbose=False)
approx_events = int(((len(times) / info['sfreq']) / (stc.times[(- 1)] - stc.times[0])))
utils.logger.info(('Provided parameters will provide approximately %s event%s' % (approx_events, _pl(approx_events))))
meeg_picks = pick_types(info, meg=True, eeg=True, exclude=[])
meg_picks = pick_types(info, meg=True, eeg=False, exclude=[])
fwd_info = pick_info(info, meeg_picks)
fwd_info['projs'] = []
utils.logger.info(('Setting up raw simulation: %s position%s, "%s" interpolation' % (len(dev_head_ts), _pl(dev_head_ts), interp)))
del interp
verts = stc.vertices
verts = ([verts] if isinstance(stc, VolSourceEstimate) else verts)
src = _restrict_source_space_to(src, verts)
raw_data = numpy.zeros((len(info['ch_names']), len(times)))
ecg_rr = blink_rrs = exg_bem = hpi_rrs = None
ecg = (ecg and (len(meg_picks) > 0))
chpi = (chpi and (len(meg_picks) > 0))
if chpi:
    (hpi_freqs, hpi_pick, hpi_ons) = _get_hpi_info(info)
    hpi_rrs = _get_hpi_initial_fit(info, verbose='error')
    hpi_nns = (hpi_rrs / numpy.sqrt(numpy.sum((hpi_rrs * hpi_rrs), axis=1))[:, numpy.newaxis])
    raw_data[hpi_pick, :] = hpi_ons.sum()
    _log_ch('cHPI status bits enbled and', info, hpi_pick)
if (blink or ecg):
    (R, r0) = fit_sphere_to_headshape(info, units='m', verbose=False)[:2]
    exg_bem = make_sphere_model(r0, head_radius=R, relative_radii=(0.97, 0.98, 0.99, 1.0), sigmas=(0.33, 1.0, 0.004, 0.33), verbose=False)
if blink:
    blink_rrs = numpy.array([[numpy.cos((numpy.pi / 3.0)), numpy.sin((numpy.pi / 3.0)), 0.0], [(- numpy.cos((numpy.pi / 3.0))), numpy.sin((numpy.pi / 3)), 0.0]])
    blink_rrs /= numpy.sqrt(numpy.sum((blink_rrs * blink_rrs), axis=1))[:, numpy.newaxis]
    blink_rrs *= (0.96 * R)
    blink_rrs += r0
    blink_nns = numpy.array([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])
    blink_rate = ((1 + numpy.cos(((((2 * numpy.pi) * 1.0) / 60.0) * times))) / 2.0)
    blink_rate *= (12.5 / 60.0)
    blink_rate += (4.5 / 60.0)
    blink_data = (rng.rand(len(times)) < (blink_rate / info['sfreq']))
    blink_data = (blink_data * (rng.rand(len(times)) + 0.5))
    blink_kernel = numpy.hanning(int((0.25 * info['sfreq'])))
    blink_data = (numpy.convolve(blink_data, blink_kernel, 'same')[numpy.newaxis, :] * 1e-07)
    ch = pick_types(info, meg=False, eeg=False, eog=True)
    noise = (rng.randn(blink_data.shape[1]) * 5e-06)
    if (len(ch) >= 1):
        ch = ch[(- 1)]
        raw_data[ch, :] = ((blink_data * 1000.0) + noise)
    else:
        ch = None
    _log_ch('Blinks simulated and trace', info, ch)
    del blink_kernel, blink_rate, noise
if ecg:
    ecg_rr = numpy.array([[(- R), 0, ((- 3) * R)]])
    max_beats = int(numpy.ceil(((times[(- 1)] * 80.0) / 60.0)))
    cardiac_idx = np.cumsum((rng.uniform((60.0 / 80.0), (60.0 / 40.0), max_beats) * info['sfreq'])).astype(int)
    cardiac_idx = cardiac_idx[(cardiac_idx < len(times))]
    cardiac_data = numpy.zeros(len(times))
    cardiac_data[cardiac_idx] = 1
    cardiac_kernel = numpy.concatenate([(2 * numpy.hanning(int((0.04 * info['sfreq'])))), ((- 0.3) * numpy.hanning(int((0.05 * info['sfreq'])))), (0.2 * numpy.hanning(int((0.26 * info['sfreq']))))], axis=(- 1))
    ecg_data = (numpy.convolve(cardiac_data, cardiac_kernel, 'same')[numpy.newaxis, :] * 1.5e-07)
    ch = pick_types(info, meg=False, eeg=False, ecg=True)
    noise = (rng.randn(ecg_data.shape[1]) * 1.5e-05)
    if (len(ch) >= 1):
        ch = ch[(- 1)]
        raw_data[ch, :] = ((ecg_data * 2000.0) + noise)
    else:
        ch = None
    _log_ch('ECG simulated and trace', info, ch)
    del cardiac_data, cardiac_kernel, max_beats, cardiac_idx
stc_event_idx = numpy.argmin(numpy.abs(stc.times))
if stim:
    event_ch = pick_channels(info['ch_names'], _get_stim_channel(None, info))[0]
    raw_data[event_ch, :] = 0.0
else:
    event_ch = None
_log_ch('Event information', info, event_ch)
used = numpy.zeros(len(times), bool)
stc_indices = (numpy.arange(len(times)) % len(stc.times))
raw_data[meeg_picks, :] = 0.0
if chpi:
    tempResult = arange(len(times))
	
===================================================================	
simulate_sparse_stc: 45	
----------------------------	

'Generate sparse (n_dipoles) sources time courses from data_fun.\n\n    This function randomly selects ``n_dipoles`` vertices in the whole\n    cortex or one single vertex (randomly in or in the center of) each\n    label if ``labels is not None``. It uses ``data_fun`` to generate\n    waveforms for each vertex.\n\n    Parameters\n    ----------\n    src : instance of SourceSpaces\n        The source space.\n    n_dipoles : int\n        Number of dipoles to simulate.\n    times : array\n        Time array\n    data_fun : callable\n        Function to generate the waveforms. The default is a 100 nAm, 10 Hz\n        sinusoid as ``1e-7 * np.sin(20 * pi * t)``. The function should take\n        as input the array of time samples in seconds and return an array of\n        the same length containing the time courses.\n    labels : None | list of Labels\n        The labels. The default is None, otherwise its size must be n_dipoles.\n    random_state : None | int | np.random.RandomState\n        To specify the random generator state.\n    location : str\n        The label location to choose. Can be \'random\' (default) or \'center\'\n        to use :func:`mne.Label.center_of_mass`. Note that for \'center\'\n        mode the label values are used as weights.\n\n        .. versionadded:: 0.13\n\n    subject : string | None\n        The subject the label is defined for.\n        Only used with ``location=\'center\'``.\n\n        .. versionadded:: 0.13\n\n    subjects_dir : str, or None\n        Path to the SUBJECTS_DIR. If None, the path is obtained by using\n        the environment variable SUBJECTS_DIR.\n        Only used with ``location=\'center\'``.\n\n        .. versionadded:: 0.13\n\n    surf : str\n        The surface to use for Euclidean distance center of mass\n        finding. The default here is "sphere", which finds the center\n        of mass on the spherical surface to help avoid potential issues\n        with cortical folding.\n\n        .. versionadded:: 0.13\n\n    Returns\n    -------\n    stc : SourceEstimate\n        The generated source time courses.\n\n    See Also\n    --------\n    simulate_raw\n    simulate_evoked\n    simulate_stc\n\n    Notes\n    -----\n    .. versionadded:: 0.10.0\n    '
rng = check_random_state(random_state)
src = _ensure_src(src, verbose=False)
subject_src = src[0].get('subject_his_id')
if (subject is None):
    subject = subject_src
elif ((subject_src is not None) and (subject != subject_src)):
    raise ValueError(('subject argument (%s) did not match the source space subject_his_id (%s)' % (subject, subject_src)))
data = numpy.zeros((n_dipoles, len(times)))
for i_dip in range(n_dipoles):
    data[i_dip, :] = data_fun(times)
if (labels is None):
    offsets = np.linspace(0, n_dipoles, (len(src) + 1)).astype(int)
    n_dipoles_ss = numpy.diff(offsets)
    tempResult = arange(s['nuse'])
	
===================================================================	
test_metrics: 19	
----------------------------	

'Test simulation metrics'
src = read_source_spaces(src_fname)
tempResult = arange(600)
	
===================================================================	
test_simulate_raw_sphere: 81	
----------------------------	

'Test simulation of raw data with sphere model.'
seed = 42
(raw, src, stc, trans, sphere) = _get_data()
assert_true((len(pick_types(raw.info, meg=False, ecg=True)) == 1))
head_pos_sim = dict()
shifts = [[0.001, 0.0, (- 0.001)], [(- 0.001), 0.001, 0.0]]
for (time_key, shift) in enumerate(shifts):
    temp_trans = deepcopy(raw.info['dev_head_t'])
    temp_trans['trans'][:3, 3] += shift
    head_pos_sim[(time_key + 1.0)] = temp_trans['trans']
raw_sim = simulate_raw(raw, stc, trans, src, sphere, read_cov(cov_fname), head_pos=head_pos_sim, blink=True, ecg=True, random_state=seed, use_cps=True)
raw_sim_2 = simulate_raw(raw, stc, trans_fname, src_fname, sphere, cov_fname, head_pos=head_pos_sim, blink=True, ecg=True, random_state=seed, use_cps=True)
assert_array_equal(raw_sim_2[:][0], raw_sim[:][0])
tempdir = _TempDir()
test_outname = os.path.join(tempdir, 'sim_test_raw.fif')
raw_sim.save(test_outname)
raw_sim_loaded = read_raw_fif(test_outname, preload=True)
assert_allclose(raw_sim_loaded[:][0], raw_sim[:][0], rtol=1e-06, atol=1e-20)
del raw_sim, raw_sim_2
for (ecg, eog) in ((True, False), (False, True), (True, True)):
    raw_sim_3 = simulate_raw(raw, stc, trans, src, sphere, cov=None, head_pos=head_pos_sim, blink=eog, ecg=ecg, random_state=seed, use_cps=True)
    raw_sim_4 = simulate_raw(raw, stc, trans, src, sphere, cov=None, head_pos=head_pos_sim, blink=False, ecg=False, random_state=seed, use_cps=True)
    tempResult = arange(len(raw.ch_names))
	
===================================================================	
_make_stc: 35	
----------------------------	

'Helper to make a STC.'
seed = 42
sfreq = raw.info['sfreq']
tstep = (1.0 / sfreq)
n_samples = (len(raw.times) // 10)
tempResult = arange(0, n_samples)
	
===================================================================	
test_simulate_raw_chpi: 161	
----------------------------	

'Test simulation of raw data with cHPI.'
raw = read_raw_fif(raw_chpi_fname, allow_maxshield='yes')
tempResult = arange(len(raw.ch_names))
	
===================================================================	
test_simulate_sparse_stc: 78	
----------------------------	

' Test generation of sparse source estimate '
fwd = read_forward_solution_meg(fname_fwd, force_fixed=True, use_cps=True)
labels = [read_label(os.path.join(data_path, 'MEG', 'sample', 'labels', ('%s.label' % label))) for label in label_names]
n_times = 10
tmin = 0
tstep = 0.001
tempResult = arange(n_times, dtype=numpy.float)
	
===================================================================	
test_simulate_sparse_stc_single_hemi: 144	
----------------------------	

' Test generation of sparse source estimate '
fwd = read_forward_solution_meg(fname_fwd, force_fixed=True, use_cps=True)
n_times = 10
tmin = 0
tstep = 0.001
tempResult = arange(n_times, dtype=numpy.float)
	
===================================================================	
_get_clusters_st_multistep: 82	
----------------------------	

'Directly calculate connectivity.\n\n    This uses knowledge that time points are\n    only connected to adjacent neighbors for data organized as time x space.\n\n    This algorithm time increases linearly with the number of time points,\n    compared to with the square for the standard (graph) algorithm.\n    '
n_src = len(neighbors)
n_times = len(keepers)
t_border = list()
t_border.append(0)
for (ki, k) in enumerate(keepers):
    keepers[ki] = (k + (ki * n_src))
    t_border.append((t_border[ki] + len(k)))
t_border = numpy.array(t_border)
keepers = numpy.concatenate(keepers)
v = keepers
(t, s) = divmod(v, n_src)
r = numpy.ones(t.shape, dtype=bool)
clusters = list()
next_ind = 0
tempResult = arange(t_border[0], t_border[n_times])
	
===================================================================	
summarize_clusters_stc: 650	
----------------------------	

'Assemble summary SourceEstimate from spatiotemporal cluster results.\n\n    This helps visualizing results from spatio-temporal-clustering\n    permutation tests.\n\n    Parameters\n    ----------\n    clu : tuple\n        the output from clustering permutation tests.\n    p_thresh : float\n        The significance threshold for inclusion of clusters.\n    tstep : float\n        The temporal difference between two time samples.\n    tmin : float | int\n        The time of the first sample.\n    subject : str\n        The name of the subject.\n    vertices : list of arrays | None\n        The vertex numbers associated with the source space locations. Defaults\n        to None. If None, equals ```[np.arange(10242), np.arange(10242)]```.\n\n    Returns\n    -------\n    out : instance of SourceEstimate\n        A summary of the clusters. The first time point in this SourceEstimate\n        object is the summation of all the clusters. Subsequent time points\n        contain each individual cluster. The magnitude of the activity\n        corresponds to the length the cluster spans in time (in samples).\n    '
if (vertices is None):
    tempResult = arange(10242)
	
===================================================================	
summarize_clusters_stc: 650	
----------------------------	

'Assemble summary SourceEstimate from spatiotemporal cluster results.\n\n    This helps visualizing results from spatio-temporal-clustering\n    permutation tests.\n\n    Parameters\n    ----------\n    clu : tuple\n        the output from clustering permutation tests.\n    p_thresh : float\n        The significance threshold for inclusion of clusters.\n    tstep : float\n        The temporal difference between two time samples.\n    tmin : float | int\n        The time of the first sample.\n    subject : str\n        The name of the subject.\n    vertices : list of arrays | None\n        The vertex numbers associated with the source space locations. Defaults\n        to None. If None, equals ```[np.arange(10242), np.arange(10242)]```.\n\n    Returns\n    -------\n    out : instance of SourceEstimate\n        A summary of the clusters. The first time point in this SourceEstimate\n        object is the summation of all the clusters. Subsequent time points\n        contain each individual cluster. The magnitude of the activity\n        corresponds to the length the cluster spans in time (in samples).\n    '
if (vertices is None):
    tempResult = arange(10242)
	
===================================================================	
_find_clusters: 181	
----------------------------	

'Find all clusters which are above/below a certain threshold.\n\n    When doing a two-tailed test (tail == 0), only points with the same\n    sign will be clustered together.\n\n    Parameters\n    ----------\n    x : 1D array\n        Data\n    threshold : float | dict\n        Where to threshold the statistic. Should be negative for tail == -1,\n        and positive for tail == 0 or 1. Can also be an dict for\n        threshold-free cluster enhancement.\n    tail : -1 | 0 | 1\n        Type of comparison\n    connectivity : sparse matrix in COO format, None, or list\n        Defines connectivity between features. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        If connectivity is a list, it is assumed that each entry stores the\n        indices of the spatial neighbors in a spatio-temporal dataset x.\n        Default is None, i.e, a regular lattice connectivity.\n    max_step : int\n        If connectivity is a list, this defines the maximal number of steps\n        between vertices along the second dimension (typically time) to be\n        considered connected.\n    include : 1D bool array or None\n        Mask to apply to the data of points to cluster. If None, all points\n        are used.\n    partitions : array of int or None\n        An array (same size as X) of integers indicating which points belong\n        to each partition.\n    t_power : float\n        Power to raise the statistical values (usually t-values) by before\n        summing (sign will be retained). Note that t_power == 0 will give a\n        count of nodes in each cluster, t_power == 1 will weight each node by\n        its statistical score.\n    show_info : bool\n        If True, display information about thresholds used (for TFCE). Should\n        only be done for the standard permutation.\n\n    Returns\n    -------\n    clusters : list of slices or list of arrays (boolean masks)\n        We use slices for 1D signals and mask to multidimensional\n        arrays.\n    sums: array\n        Sum of x values in clusters.\n    '
from scipy import ndimage
if (tail not in [(- 1), 0, 1]):
    raise ValueError('invalid tail parameter')
x = numpy.asanyarray(x)
if (not numpy.isscalar(threshold)):
    if (not isinstance(threshold, dict)):
        raise TypeError('threshold must be a number, or a dict for threshold-free cluster enhancement')
    if (not all(((key in threshold) for key in ['start', 'step']))):
        raise KeyError('threshold, if dict, must have at least "start" and "step"')
    tfce = True
    if (tail == (- 1)):
        if (threshold['start'] > 0):
            raise ValueError('threshold["start"] must be <= 0 for tail == -1')
        if (threshold['step'] >= 0):
            raise ValueError('threshold["step"] must be < 0 for tail == -1')
        stop = numpy.min(x)
    elif (tail == 1):
        stop = numpy.max(x)
    else:
        stop = numpy.max(numpy.abs(x))
    tempResult = arange(threshold['start'], stop, threshold['step'], float)
	
===================================================================	
_find_clusters: 230	
----------------------------	

'Find all clusters which are above/below a certain threshold.\n\n    When doing a two-tailed test (tail == 0), only points with the same\n    sign will be clustered together.\n\n    Parameters\n    ----------\n    x : 1D array\n        Data\n    threshold : float | dict\n        Where to threshold the statistic. Should be negative for tail == -1,\n        and positive for tail == 0 or 1. Can also be an dict for\n        threshold-free cluster enhancement.\n    tail : -1 | 0 | 1\n        Type of comparison\n    connectivity : sparse matrix in COO format, None, or list\n        Defines connectivity between features. The matrix is assumed to\n        be symmetric and only the upper triangular half is used.\n        If connectivity is a list, it is assumed that each entry stores the\n        indices of the spatial neighbors in a spatio-temporal dataset x.\n        Default is None, i.e, a regular lattice connectivity.\n    max_step : int\n        If connectivity is a list, this defines the maximal number of steps\n        between vertices along the second dimension (typically time) to be\n        considered connected.\n    include : 1D bool array or None\n        Mask to apply to the data of points to cluster. If None, all points\n        are used.\n    partitions : array of int or None\n        An array (same size as X) of integers indicating which points belong\n        to each partition.\n    t_power : float\n        Power to raise the statistical values (usually t-values) by before\n        summing (sign will be retained). Note that t_power == 0 will give a\n        count of nodes in each cluster, t_power == 1 will weight each node by\n        its statistical score.\n    show_info : bool\n        If True, display information about thresholds used (for TFCE). Should\n        only be done for the standard permutation.\n\n    Returns\n    -------\n    clusters : list of slices or list of arrays (boolean masks)\n        We use slices for 1D signals and mask to multidimensional\n        arrays.\n    sums: array\n        Sum of x values in clusters.\n    '
from scipy import ndimage
if (tail not in [(- 1), 0, 1]):
    raise ValueError('invalid tail parameter')
x = numpy.asanyarray(x)
if (not numpy.isscalar(threshold)):
    if (not isinstance(threshold, dict)):
        raise TypeError('threshold must be a number, or a dict for threshold-free cluster enhancement')
    if (not all(((key in threshold) for key in ['start', 'step']))):
        raise KeyError('threshold, if dict, must have at least "start" and "step"')
    tfce = True
    if (tail == (- 1)):
        if (threshold['start'] > 0):
            raise ValueError('threshold["start"] must be <= 0 for tail == -1')
        if (threshold['step'] >= 0):
            raise ValueError('threshold["step"] must be < 0 for tail == -1')
        stop = numpy.min(x)
    elif (tail == 1):
        stop = numpy.max(x)
    else:
        stop = numpy.max(numpy.abs(x))
    thresholds = numpy.arange(threshold['start'], stop, threshold['step'], float)
    h_power = threshold.get('h_power', 2)
    e_power = threshold.get('e_power', 0.5)
    if (show_info is True):
        if (len(thresholds) == 0):
            warn(('threshold["start"] (%s) is more extreme than data statistics with most extreme value %s' % (threshold['start'], stop)))
        else:
            utils.logger.info(('Using %d thresholds from %0.2f to %0.2f for TFCE computation (h_power=%0.2f, e_power=%0.2f)' % (len(thresholds), thresholds[0], thresholds[(- 1)], h_power, e_power)))
    scores = numpy.zeros(x.size)
else:
    thresholds = [threshold]
    tfce = False
if (include is None):
    include = numpy.ones(x.shape, dtype=bool)
if (not numpy.all((numpy.diff(thresholds) > 0))):
    raise RuntimeError('Threshold misconfiguration, must be monotonically increasing')
clusters = list()
sums = numpy.empty(0)
for (ti, thresh) in enumerate(thresholds):
    clusters = list()
    sums = numpy.empty(0)
    if (tail == 0):
        x_ins = [numpy.logical_and((x > thresh), include), numpy.logical_and((x < (- thresh)), include)]
    elif (tail == (- 1)):
        x_ins = [numpy.logical_and((x < thresh), include)]
    else:
        x_ins = [numpy.logical_and((x > thresh), include)]
    for x_in in x_ins:
        if numpy.any(x_in):
            out = _find_clusters_1dir_parts(x, x_in, connectivity, max_step, partitions, t_power, ndimage)
            clusters += out[0]
            sums = numpy.concatenate((sums, out[1]))
    if (tfce is True):
        if (ti == 0):
            h = abs(thresh)
        else:
            h = abs((thresh - thresholds[(ti - 1)]))
        h = (h ** h_power)
        for c in clusters:
            if isinstance(c, slice):
                len_c = (c.stop - c.start)
            elif isinstance(c, tuple):
                len_c = len(c)
            elif (c.dtype == bool):
                len_c = numpy.sum(c)
            else:
                len_c = len(c)
            scores[c] += (h * (len_c ** e_power))
if (tfce is True):
    tempResult = arange(x.size)
	
===================================================================	
_permutation_cluster_test: 471	
----------------------------	

n_jobs = check_n_jobs(n_jobs)
'Aux Function.\n\n    Note. X is required to be a list. Depending on the length of X\n    either a 1 sample t-test or an f-test / more sample permutation scheme\n    is elicited.\n    '
if (out_type not in ['mask', 'indices']):
    raise ValueError("out_type must be either 'mask' or 'indices'")
if ((not isinstance(threshold, dict)) and (((tail < 0) and (threshold > 0)) or ((tail > 0) and (threshold < 0)) or ((tail == 0) and (threshold < 0)))):
    raise ValueError(('incompatible tail and threshold signs, got %s and %s' % (tail, threshold)))
X = [(x[:, numpy.newaxis] if (x.ndim == 1) else x) for x in X]
n_samples = X[0].shape[0]
n_times = X[0].shape[1]
sample_shape = X[0].shape[1:]
for x in X:
    if (x.shape[1:] != sample_shape):
        raise ValueError('All samples mush have the same size')
X = [numpy.reshape(x, (x.shape[0], (- 1))) for x in X]
n_tests = X[0].shape[1]
if (connectivity is not None):
    connectivity = _setup_connectivity(connectivity, n_tests, n_times)
if ((exclude is not None) and (not (exclude.size == n_tests))):
    raise ValueError('exclude must be the same shape as X[0]')
T_obs = stat_fun(*X)
utils.logger.info(('stat_fun(H1): min=%f max=%f' % (numpy.min(T_obs), numpy.max(T_obs))))
if (buffer_size is not None):
    T_obs_buffer = numpy.zeros_like(T_obs)
    for pos in range(0, n_tests, buffer_size):
        T_obs_buffer[pos:(pos + buffer_size)] = stat_fun(*[x[:, pos:(pos + buffer_size)] for x in X])
    if (not numpy.alltrue((T_obs == T_obs_buffer))):
        warn('Provided stat_fun does not treat variables independently. Setting buffer_size to None.')
        buffer_size = None
if (connectivity is None):
    T_obs.shape = sample_shape
if (exclude is not None):
    include = numpy.logical_not(exclude)
else:
    include = None
if ((check_disjoint is True) and (connectivity is not None)):
    partitions = _get_partitions_from_connectivity(connectivity, n_times)
else:
    partitions = None
utils.logger.info('Running initial clustering')
out = _find_clusters(T_obs, threshold, tail, connectivity, max_step=max_step, include=include, partitions=partitions, t_power=t_power, show_info=True)
(clusters, cluster_stats) = out
if isinstance(threshold, dict):
    T_obs = cluster_stats.copy()
utils.logger.info(('Found %d clusters' % len(clusters)))
if (connectivity is not None):
    if (out_type == 'mask'):
        clusters = _cluster_indices_to_mask(clusters, n_tests)
elif (out_type == 'indices'):
    clusters = _cluster_mask_to_indices(clusters)
T_obs.shape = sample_shape
extra = ''
rng = check_random_state(seed)
del seed
if (len(X) == 1):
    do_perm_func = _do_1samp_permutations
    X_full = X[0]
    slices = None
    max_perms = ((2 ** (n_samples - (tail == 0))) - 1)
    if (max_perms < n_permutations):
        tempResult = arange(max_perms)
	
===================================================================	
fdr_correction: 20	
----------------------------	

"P-value correction with False Discovery Rate (FDR).\n\n    Correction for multiple comparison using FDR.\n\n    This covers Benjamini/Hochberg for independent or positively correlated and\n    Benjamini/Yekutieli for general or negatively correlated tests.\n\n    Parameters\n    ----------\n    pvals : array_like\n        set of p-values of the individual tests.\n    alpha : float\n        error rate\n    method : 'indep' | 'negcorr'\n        If 'indep' it implements Benjamini/Hochberg for independent or if\n        'negcorr' it corresponds to Benjamini/Yekutieli.\n\n    Returns\n    -------\n    reject : array, bool\n        True if a hypothesis is rejected, False if not\n    pval_corrected : array\n        pvalues adjusted for multiple hypothesis testing to limit FDR\n\n    Notes\n    -----\n    Reference:\n    Genovese CR, Lazar NA, Nichols T.\n    Thresholding of statistical maps in functional neuroimaging using the false\n    discovery rate. Neuroimage. 2002 Apr;15(4):870-8.\n    "
pvals = numpy.asarray(pvals)
shape_init = pvals.shape
pvals = pvals.ravel()
pvals_sortind = numpy.argsort(pvals)
pvals_sorted = pvals[pvals_sortind]
sortrevind = pvals_sortind.argsort()
if (method in ['i', 'indep', 'p', 'poscorr']):
    ecdffactor = _ecdf(pvals_sorted)
elif (method in ['n', 'negcorr']):
    tempResult = arange(1, (len(pvals_sorted) + 1))
	
===================================================================	
_ecdf: 7	
----------------------------	

'No frills empirical cdf used in fdrcorrection.'
nobs = len(x)
tempResult = arange(1, (nobs + 1))
	
===================================================================	
_map_effects: 60	
----------------------------	

'Map effects to indices.'
if (n_factors > len(ascii_uppercase)):
    raise ValueError('Maximum number of factors supported is 26')
factor_names = list(ascii_uppercase[:n_factors])
if isinstance(effects, string_types):
    if (('*' in effects) and (':' in effects)):
        raise ValueError('Not "*" and ":" permitted in effects')
    elif (('+' in effects) and (':' in effects)):
        raise ValueError('Not "+" and ":" permitted in effects')
    elif (effects == 'all'):
        effects = None
    elif ((len(effects) == 1) or (':' in effects)):
        effects = [effects]
    elif ('+' in effects):
        effects = effects.split('+')
    elif ('*' in effects):
        pass
    else:
        raise ValueError('"{0}" is not a valid option for "effects"'.format(effects))
if isinstance(effects, list):
    bad_names = [e for e in effects if (e not in factor_names)]
    if (len(bad_names) > 1):
        raise ValueError('Effect names: {0} are not valid. They should the first `n_factors` ({1}) characters from thealphabet'.format(bad_names, n_factors))
tempResult = arange(((2 ** n_factors) - 1))
	
===================================================================	
_bootstrap_ci: 72	
----------------------------	

'Get confidence intervals from non-parametric bootstrap.'
if (stat_fun == 'mean'):

    def stat_fun(x):
        return x.mean(axis=0)
elif (stat_fun == 'median'):

    def stat_fun(x):
        return numpy.median(x, axis=0)
elif (not callable(stat_fun)):
    raise ValueError("stat_fun must be 'mean', 'median' or callable.")
n_trials = arr.shape[0]
tempResult = arange(n_trials, dtype=int)
	
===================================================================	
test_regression: 28	
----------------------------	

'Test Ordinary Least Squares Regression.'
(tmin, tmax) = ((- 0.2), 0.5)
event_id = dict(aud_l=1, aud_r=2)
raw = mne.io.read_raw_fif(raw_fname)
events = mne.read_events(event_fname)[:10]
epochs = mne.Epochs(raw, events, event_id, tmin, tmax, proj=True, baseline=(None, 0))
tempResult = arange(len(epochs.ch_names))
	
===================================================================	
assert_meg_snr: 43	
----------------------------	

'Helper to assert channel SNR of a certain level\n\n    Mostly useful for operations like Maxwell filtering that modify\n    MEG channels while leaving EEG and others intact.\n    '
picks = pick_types(desired.info, meg=True, exclude=[])
picks_desired = pick_types(desired.info, meg=True, exclude=[])
assert_array_equal(picks, picks_desired, err_msg='MEG pick mismatch')
chpis = pick_types(actual.info, meg=False, chpi=True, exclude=[])
chpis_desired = pick_types(desired.info, meg=False, chpi=True, exclude=[])
if (chpi_med_tol is not None):
    assert_array_equal(chpis, chpis_desired, err_msg='cHPI pick mismatch')
tempResult = arange(len(actual.ch_names))
	
===================================================================	
assert_meg_snr: 44	
----------------------------	

'Helper to assert channel SNR of a certain level\n\n    Mostly useful for operations like Maxwell filtering that modify\n    MEG channels while leaving EEG and others intact.\n    '
picks = pick_types(desired.info, meg=True, exclude=[])
picks_desired = pick_types(desired.info, meg=True, exclude=[])
assert_array_equal(picks, picks_desired, err_msg='MEG pick mismatch')
chpis = pick_types(actual.info, meg=False, chpi=True, exclude=[])
chpis_desired = pick_types(desired.info, meg=False, chpi=True, exclude=[])
if (chpi_med_tol is not None):
    assert_array_equal(chpis, chpis_desired, err_msg='cHPI pick mismatch')
others = numpy.setdiff1d(numpy.arange(len(actual.ch_names)), numpy.concatenate([picks, chpis]))
tempResult = arange(len(desired.ch_names))
	
===================================================================	
test_calculate_chpi_positions: 142	
----------------------------	

'Test calculation of cHPI positions.'
mf_quats = read_head_pos(pos_fname)
raw = read_raw_fif(chpi_fif_fname, allow_maxshield='yes', preload=True)
raw_dec = _decimate_chpi(raw, 15)
with catch_logging() as log:
    py_quats = _calculate_chpi_positions(raw_dec, verbose='debug')
assert_true(log.getvalue().startswith('HPIFIT'))
_assert_quats(py_quats, mf_quats, dist_tol=0.004, angle_tol=2.5)
raw_no_chpi = read_raw_fif(test_fif_fname)
assert_raises(RuntimeError, _calculate_chpi_positions, raw_no_chpi)
raw_bad = raw.copy()
for d in raw_bad.info['dig']:
    if (d['kind'] == mne.io.constants.FIFF.FIFFV_POINT_HPI):
        d['coord_frame'] = mne.io.constants.FIFF.FIFFV_COORD_UNKNOWN
        break
assert_raises(RuntimeError, _calculate_chpi_positions, raw_bad)
for d in raw_bad.info['dig']:
    if (d['kind'] == mne.io.constants.FIFF.FIFFV_POINT_HPI):
        d['coord_frame'] = mne.io.constants.FIFF.FIFFV_COORD_HEAD
        d['r'] = numpy.ones(3)
raw_bad.crop(0, 1.0)
tempResult = arange(306, len(raw_bad.ch_names))
	
===================================================================	
test_simulate_calculate_chpi_positions: 180	
----------------------------	

'Test calculation of cHPI positions with simulated data.'
info = read_info(raw_fname)
chpi_channel = 'STI201'
ncoil = len(info['hpi_results'][0]['order'])
tempResult = arange(ncoil)
	
===================================================================	
test_simulate_calculate_chpi_positions: 199	
----------------------------	

'Test calculation of cHPI positions with simulated data.'
info = read_info(raw_fname)
chpi_channel = 'STI201'
ncoil = len(info['hpi_results'][0]['order'])
coil_freq = (10 + (numpy.arange(ncoil) * 5))
hpi_subsystem = {'event_channel': chpi_channel, 'hpi_coils': [{'event_bits': numpy.array([256, 0, 256, 256], dtype=numpy.int32)}, {'event_bits': numpy.array([512, 0, 512, 512], dtype=numpy.int32)}, {'event_bits': numpy.array([1024, 0, 1024, 1024], dtype=numpy.int32)}, {'event_bits': numpy.array([2048, 0, 2048, 2048], dtype=numpy.int32)}], 'ncoil': ncoil}
info['hpi_subsystem'] = hpi_subsystem
for (l, freq) in enumerate(coil_freq):
    info['hpi_meas'][0]['hpi_coils'][l]['coil_freq'] = freq
picks = pick_types(info, meg=True, stim=True, eeg=False, exclude=[])
info['sfreq'] = 100.0
info = pick_info(info, picks)
info['chs'][info['ch_names'].index('STI 001')]['ch_name'] = 'STI201'
info._update_redundant()
info['projs'] = []
info_trans = info['dev_head_t']['trans'].copy()
dev_head_pos_ini = numpy.concatenate([rot_to_quat(info_trans[:3, :3]), info_trans[:3, 3]])
ez = numpy.array([0, 0, 1])
duration = 30
head_pos_sfreq_quotient = 0.1
S = int((duration / (info['sfreq'] * head_pos_sfreq_quotient)))
dz = 0.001
dev_head_pos = numpy.zeros((S, 10))
tempResult = arange(S)
	
===================================================================	
test_simulate_calculate_chpi_positions: 201	
----------------------------	

'Test calculation of cHPI positions with simulated data.'
info = read_info(raw_fname)
chpi_channel = 'STI201'
ncoil = len(info['hpi_results'][0]['order'])
coil_freq = (10 + (numpy.arange(ncoil) * 5))
hpi_subsystem = {'event_channel': chpi_channel, 'hpi_coils': [{'event_bits': numpy.array([256, 0, 256, 256], dtype=numpy.int32)}, {'event_bits': numpy.array([512, 0, 512, 512], dtype=numpy.int32)}, {'event_bits': numpy.array([1024, 0, 1024, 1024], dtype=numpy.int32)}, {'event_bits': numpy.array([2048, 0, 2048, 2048], dtype=numpy.int32)}], 'ncoil': ncoil}
info['hpi_subsystem'] = hpi_subsystem
for (l, freq) in enumerate(coil_freq):
    info['hpi_meas'][0]['hpi_coils'][l]['coil_freq'] = freq
picks = pick_types(info, meg=True, stim=True, eeg=False, exclude=[])
info['sfreq'] = 100.0
info = pick_info(info, picks)
info['chs'][info['ch_names'].index('STI 001')]['ch_name'] = 'STI201'
info._update_redundant()
info['projs'] = []
info_trans = info['dev_head_t']['trans'].copy()
dev_head_pos_ini = numpy.concatenate([rot_to_quat(info_trans[:3, :3]), info_trans[:3, 3]])
ez = numpy.array([0, 0, 1])
duration = 30
head_pos_sfreq_quotient = 0.1
S = int((duration / (info['sfreq'] * head_pos_sfreq_quotient)))
dz = 0.001
dev_head_pos = numpy.zeros((S, 10))
dev_head_pos[:, 0] = ((numpy.arange(S) * info['sfreq']) * head_pos_sfreq_quotient)
dev_head_pos[:, 1:4] = dev_head_pos_ini[:3]
tempResult = arange(S)
	
===================================================================	
test_cov_order: 63	
----------------------------	

'Test covariance ordering.'
info = read_info(raw_fname)
info['bads'] += ['MEG 0113']
ch_names = [info['ch_names'][pick] for pick in pick_types(info, meg=False, eeg=True)]
cov = read_cov(cov_fname)
prepare_noise_cov(cov, info, ch_names, verbose='error')
cov_reorder = cov.copy()
tempResult = arange(len(cov.ch_names))
	
===================================================================	
test_cov_order: 72	
----------------------------	

'Test covariance ordering.'
info = read_info(raw_fname)
info['bads'] += ['MEG 0113']
ch_names = [info['ch_names'][pick] for pick in pick_types(info, meg=False, eeg=True)]
cov = read_cov(cov_fname)
prepare_noise_cov(cov, info, ch_names, verbose='error')
cov_reorder = cov.copy()
order = np.random.RandomState(0).permutation(numpy.arange(len(cov.ch_names)))
cov_reorder['names'] = [cov['names'][ii] for ii in order]
cov_reorder['data'] = cov['data'][order][:, order]
_assert_reorder(cov_reorder, cov, order)
cov_reg = regularize(cov, info)
cov_reg_reorder = regularize(cov_reorder, info)
_assert_reorder(cov_reg_reorder, cov_reg, order)
cov_prep = prepare_noise_cov(cov, info, ch_names)
cov_prep_reorder = prepare_noise_cov(cov, info, ch_names)
tempResult = arange(len(cov_prep['names']))
	
===================================================================	
test_epochs_bad_baseline: 384	
----------------------------	

'Test Epochs initialization with bad baseline parameters.'
(raw, events) = _get_data()[:2]
assert_raises(ValueError, Epochs, raw, events, None, (- 0.1), 0.3, ((- 0.2), 0))
assert_raises(ValueError, Epochs, raw, events, None, (- 0.1), 0.3, (0, 0.4))
assert_raises(ValueError, Epochs, raw, events, None, (- 0.1), 0.3, (0.1, 0))
assert_raises(ValueError, Epochs, raw, events, None, 0.1, 0.3, (None, 0))
assert_raises(ValueError, Epochs, raw, events, None, (- 0.3), (- 0.1), (0, None))
epochs = Epochs(raw, events, None, 0.1, 0.3, baseline=None)
assert_raises(RuntimeError, epochs.apply_baseline, (0.1, 0.2))
epochs.load_data()
assert_raises(ValueError, epochs.apply_baseline, (None, 0))
assert_raises(ValueError, epochs.apply_baseline, (0, None))
tempResult = arange(100, dtype=float)
	
===================================================================	
test_epoch_eq: 1021	
----------------------------	

'Test epoch count equalization and condition combining.'
(raw, events, picks) = _get_data()
epochs_1 = Epochs(raw, events, event_id, tmin, tmax, picks=picks)
epochs_2 = Epochs(raw, events, event_id_2, tmin, tmax, picks=picks)
epochs_1.drop_bad()
assert_true((len([l for l in epochs_1.drop_log if (not l)]) == len(epochs_1.events)))
drop_log1 = epochs_1.drop_log = [[] for _ in range(len(epochs_1.events))]
drop_log2 = [([] if (log == ['EQUALIZED_COUNT']) else log) for log in epochs_1.drop_log]
assert_true((drop_log1 == drop_log2))
assert_true((len([l for l in epochs_1.drop_log if (not l)]) == len(epochs_1.events)))
assert_true((epochs_1.events.shape[0] != epochs_2.events.shape[0]))
equalize_epoch_counts([epochs_1, epochs_2], method='mintime')
assert_true((epochs_1.events.shape[0] == epochs_2.events.shape[0]))
epochs_3 = Epochs(raw, events, event_id, tmin, tmax, picks=picks)
epochs_4 = Epochs(raw, events, event_id_2, tmin, tmax, picks=picks)
equalize_epoch_counts([epochs_3, epochs_4], method='truncate')
assert_true((epochs_1.events.shape[0] == epochs_3.events.shape[0]))
assert_true((epochs_3.events.shape[0] == epochs_4.events.shape[0]))
epochs = Epochs(raw, events, {'a': 1, 'b': 2, 'c': 3, 'd': 4}, tmin, tmax, picks=picks, reject=reject)
epochs.drop_bad()
assert_true((len([l for l in epochs.drop_log if (not l)]) == len(epochs.events)))
drop_log1 = deepcopy(epochs.drop_log)
old_shapes = [epochs[key].events.shape[0] for key in ['a', 'b', 'c', 'd']]
epochs.equalize_event_counts(['a', 'b'])
drop_log2 = [([] if (log == ['EQUALIZED_COUNT']) else log) for log in epochs.drop_log]
assert_true((drop_log1 == drop_log2))
assert_true((len([l for l in epochs.drop_log if (not l)]) == len(epochs.events)))
new_shapes = [epochs[key].events.shape[0] for key in ['a', 'b', 'c', 'd']]
assert_true((new_shapes[0] == new_shapes[1]))
assert_true((new_shapes[2] == new_shapes[2]))
assert_true((new_shapes[3] == new_shapes[3]))
old_shapes = new_shapes
epochs.equalize_event_counts([['a', 'b'], 'c'])
new_shapes = [epochs[key].events.shape[0] for key in ['a', 'b', 'c', 'd']]
assert_true(((new_shapes[0] + new_shapes[1]) == new_shapes[2]))
assert_true((new_shapes[3] == old_shapes[3]))
assert_raises(KeyError, epochs.equalize_event_counts, [1, 'a'])
old_shapes = new_shapes
epochs.equalize_event_counts([['a', 'b'], ['c', 'd']])
new_shapes = [epochs[key].events.shape[0] for key in ['a', 'b', 'c', 'd']]
assert_true(((old_shapes[0] + old_shapes[1]) == (new_shapes[0] + new_shapes[1])))
assert_true(((new_shapes[0] + new_shapes[1]) == (new_shapes[2] + new_shapes[3])))
assert_raises(ValueError, combine_event_ids, epochs, ['a', 'b'], {'ab': 1})
combine_event_ids(epochs, ['a', 'b'], {'ab': 12}, copy=False)
caught = 0
for key in ['a', 'b']:
    try:
        epochs[key]
    except KeyError:
        caught += 1
assert_equal(caught, 2)
assert_true((not numpy.any((epochs.events[:, 2] == 1))))
assert_true((not numpy.any((epochs.events[:, 2] == 2))))
epochs = combine_event_ids(epochs, ['c', 'd'], {'cd': 34})
assert_true(numpy.all(numpy.logical_or((epochs.events[:, 2] == 12), (epochs.events[:, 2] == 34))))
assert_true((epochs['ab'].events.shape[0] == (old_shapes[0] + old_shapes[1])))
assert_true((epochs['ab'].events.shape[0] == epochs['cd'].events.shape[0]))
epochs = Epochs(raw, events, {'a/x': 1, 'b/x': 2, 'a/y': 3, 'b/y': 4}, tmin, tmax, picks=picks, reject=reject)
(cond1, cond2) = (['a', ['b/x', 'b/y']], [['a/x', 'a/y'], 'b'])
es = [epochs.copy().equalize_event_counts(c)[0] for c in (cond1, cond2)]
assert_array_equal(es[0].events[:, 0], es[1].events[:, 0])
(cond1, cond2) = (['a', ['b', 'b/y']], [['a/x', 'a/y'], 'x'])
for c in (cond1, cond2):
    assert_raises(ValueError, epochs.equalize_event_counts, c)
assert_raises(KeyError, epochs.equalize_event_counts, ['a/no_match', 'b'])
tempResult = arange(10)
	
===================================================================	
test_resample: 880	
----------------------------	

'Test of resample of epochs.'
(raw, events, picks) = _get_data()
epochs = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=False, reject=reject, flat=flat)
assert_raises(RuntimeError, epochs.resample, 100)
epochs_o = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=True, reject=reject, flat=flat)
epochs = epochs_o.copy()
data_normal = copy.deepcopy(epochs.get_data())
times_normal = copy.deepcopy(epochs.times)
sfreq_normal = epochs.info['sfreq']
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), npad=0)
data_up = copy.deepcopy(epochs.get_data())
times_up = copy.deepcopy(epochs.times)
sfreq_up = epochs.info['sfreq']
epochs.resample(sfreq_normal, npad=0)
data_new = copy.deepcopy(epochs.get_data())
times_new = copy.deepcopy(epochs.times)
sfreq_new = epochs.info['sfreq']
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_true((sfreq_up == (2 * sfreq_normal)))
assert_true((sfreq_new == sfreq_normal))
assert_true((len(times_up) == (2 * len(times_normal))))
assert_array_almost_equal(times_new, times_normal, 10)
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_array_almost_equal(data_new, data_normal, 5)
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), n_jobs=2, npad=0)
assert_true(numpy.allclose(data_up, epochs._data, rtol=1e-08, atol=1e-16))
epochs = epochs_o.copy()
epochs_resampled = epochs.copy().resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is not epochs))
epochs_resampled = epochs.resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is epochs))
(n_trial, n_chan, n_time, sfreq) = (1, 1, 10, 1000.0)
data = numpy.zeros((n_trial, n_chan, n_time))
events = numpy.zeros((n_trial, 3), int)
info = create_info(n_chan, sfreq, 'eeg')
epochs1 = EpochsArray(data, deepcopy(info), events)
epochs2 = EpochsArray(data, deepcopy(info), events)
epochs = concatenate_epochs([epochs1, epochs2])
epochs1.resample((epochs1.info['sfreq'] // 2), npad='auto')
epochs2.resample((epochs2.info['sfreq'] // 2), npad='auto')
epochs = concatenate_epochs([epochs1, epochs2])
for e in (epochs1, epochs2, epochs):
    assert_equal(e.times[0], epochs.tmin)
    assert_equal(e.times[(- 1)], epochs.tmax)
this_tmin = (- 0.002)
epochs = EpochsArray(data, deepcopy(info), events, tmin=this_tmin)
for times in (epochs.times, epochs._raw_times):
    tempResult = arange(n_time)
	
===================================================================	
test_resample: 883	
----------------------------	

'Test of resample of epochs.'
(raw, events, picks) = _get_data()
epochs = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=False, reject=reject, flat=flat)
assert_raises(RuntimeError, epochs.resample, 100)
epochs_o = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=True, reject=reject, flat=flat)
epochs = epochs_o.copy()
data_normal = copy.deepcopy(epochs.get_data())
times_normal = copy.deepcopy(epochs.times)
sfreq_normal = epochs.info['sfreq']
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), npad=0)
data_up = copy.deepcopy(epochs.get_data())
times_up = copy.deepcopy(epochs.times)
sfreq_up = epochs.info['sfreq']
epochs.resample(sfreq_normal, npad=0)
data_new = copy.deepcopy(epochs.get_data())
times_new = copy.deepcopy(epochs.times)
sfreq_new = epochs.info['sfreq']
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_true((sfreq_up == (2 * sfreq_normal)))
assert_true((sfreq_new == sfreq_normal))
assert_true((len(times_up) == (2 * len(times_normal))))
assert_array_almost_equal(times_new, times_normal, 10)
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_array_almost_equal(data_new, data_normal, 5)
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), n_jobs=2, npad=0)
assert_true(numpy.allclose(data_up, epochs._data, rtol=1e-08, atol=1e-16))
epochs = epochs_o.copy()
epochs_resampled = epochs.copy().resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is not epochs))
epochs_resampled = epochs.resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is epochs))
(n_trial, n_chan, n_time, sfreq) = (1, 1, 10, 1000.0)
data = numpy.zeros((n_trial, n_chan, n_time))
events = numpy.zeros((n_trial, 3), int)
info = create_info(n_chan, sfreq, 'eeg')
epochs1 = EpochsArray(data, deepcopy(info), events)
epochs2 = EpochsArray(data, deepcopy(info), events)
epochs = concatenate_epochs([epochs1, epochs2])
epochs1.resample((epochs1.info['sfreq'] // 2), npad='auto')
epochs2.resample((epochs2.info['sfreq'] // 2), npad='auto')
epochs = concatenate_epochs([epochs1, epochs2])
for e in (epochs1, epochs2, epochs):
    assert_equal(e.times[0], epochs.tmin)
    assert_equal(e.times[(- 1)], epochs.tmax)
this_tmin = (- 0.002)
epochs = EpochsArray(data, deepcopy(info), events, tmin=this_tmin)
for times in (epochs.times, epochs._raw_times):
    assert_allclose(times, ((numpy.arange(n_time) / sfreq) + this_tmin))
epochs.resample((info['sfreq'] * 2.0))
for times in (epochs.times, epochs._raw_times):
    tempResult = arange((2 * n_time))
	
===================================================================	
test_resample: 886	
----------------------------	

'Test of resample of epochs.'
(raw, events, picks) = _get_data()
epochs = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=False, reject=reject, flat=flat)
assert_raises(RuntimeError, epochs.resample, 100)
epochs_o = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=True, reject=reject, flat=flat)
epochs = epochs_o.copy()
data_normal = copy.deepcopy(epochs.get_data())
times_normal = copy.deepcopy(epochs.times)
sfreq_normal = epochs.info['sfreq']
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), npad=0)
data_up = copy.deepcopy(epochs.get_data())
times_up = copy.deepcopy(epochs.times)
sfreq_up = epochs.info['sfreq']
epochs.resample(sfreq_normal, npad=0)
data_new = copy.deepcopy(epochs.get_data())
times_new = copy.deepcopy(epochs.times)
sfreq_new = epochs.info['sfreq']
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_true((sfreq_up == (2 * sfreq_normal)))
assert_true((sfreq_new == sfreq_normal))
assert_true((len(times_up) == (2 * len(times_normal))))
assert_array_almost_equal(times_new, times_normal, 10)
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_array_almost_equal(data_new, data_normal, 5)
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), n_jobs=2, npad=0)
assert_true(numpy.allclose(data_up, epochs._data, rtol=1e-08, atol=1e-16))
epochs = epochs_o.copy()
epochs_resampled = epochs.copy().resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is not epochs))
epochs_resampled = epochs.resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is epochs))
(n_trial, n_chan, n_time, sfreq) = (1, 1, 10, 1000.0)
data = numpy.zeros((n_trial, n_chan, n_time))
events = numpy.zeros((n_trial, 3), int)
info = create_info(n_chan, sfreq, 'eeg')
epochs1 = EpochsArray(data, deepcopy(info), events)
epochs2 = EpochsArray(data, deepcopy(info), events)
epochs = concatenate_epochs([epochs1, epochs2])
epochs1.resample((epochs1.info['sfreq'] // 2), npad='auto')
epochs2.resample((epochs2.info['sfreq'] // 2), npad='auto')
epochs = concatenate_epochs([epochs1, epochs2])
for e in (epochs1, epochs2, epochs):
    assert_equal(e.times[0], epochs.tmin)
    assert_equal(e.times[(- 1)], epochs.tmax)
this_tmin = (- 0.002)
epochs = EpochsArray(data, deepcopy(info), events, tmin=this_tmin)
for times in (epochs.times, epochs._raw_times):
    assert_allclose(times, ((numpy.arange(n_time) / sfreq) + this_tmin))
epochs.resample((info['sfreq'] * 2.0))
for times in (epochs.times, epochs._raw_times):
    assert_allclose(times, ((numpy.arange((2 * n_time)) / (sfreq * 2)) + this_tmin))
epochs.crop(0, None)
for times in (epochs.times, epochs._raw_times):
    tempResult = arange(((n_time - 2) * 2))
	
===================================================================	
test_resample: 889	
----------------------------	

'Test of resample of epochs.'
(raw, events, picks) = _get_data()
epochs = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=False, reject=reject, flat=flat)
assert_raises(RuntimeError, epochs.resample, 100)
epochs_o = Epochs(raw, events[:10], event_id, tmin, tmax, picks=picks, preload=True, reject=reject, flat=flat)
epochs = epochs_o.copy()
data_normal = copy.deepcopy(epochs.get_data())
times_normal = copy.deepcopy(epochs.times)
sfreq_normal = epochs.info['sfreq']
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), npad=0)
data_up = copy.deepcopy(epochs.get_data())
times_up = copy.deepcopy(epochs.times)
sfreq_up = epochs.info['sfreq']
epochs.resample(sfreq_normal, npad=0)
data_new = copy.deepcopy(epochs.get_data())
times_new = copy.deepcopy(epochs.times)
sfreq_new = epochs.info['sfreq']
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_true((sfreq_up == (2 * sfreq_normal)))
assert_true((sfreq_new == sfreq_normal))
assert_true((len(times_up) == (2 * len(times_normal))))
assert_array_almost_equal(times_new, times_normal, 10)
assert_true((data_up.shape[2] == (2 * data_normal.shape[2])))
assert_array_almost_equal(data_new, data_normal, 5)
epochs = epochs_o.copy()
epochs.resample((sfreq_normal * 2), n_jobs=2, npad=0)
assert_true(numpy.allclose(data_up, epochs._data, rtol=1e-08, atol=1e-16))
epochs = epochs_o.copy()
epochs_resampled = epochs.copy().resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is not epochs))
epochs_resampled = epochs.resample((sfreq_normal * 2), npad=0)
assert_true((epochs_resampled is epochs))
(n_trial, n_chan, n_time, sfreq) = (1, 1, 10, 1000.0)
data = numpy.zeros((n_trial, n_chan, n_time))
events = numpy.zeros((n_trial, 3), int)
info = create_info(n_chan, sfreq, 'eeg')
epochs1 = EpochsArray(data, deepcopy(info), events)
epochs2 = EpochsArray(data, deepcopy(info), events)
epochs = concatenate_epochs([epochs1, epochs2])
epochs1.resample((epochs1.info['sfreq'] // 2), npad='auto')
epochs2.resample((epochs2.info['sfreq'] // 2), npad='auto')
epochs = concatenate_epochs([epochs1, epochs2])
for e in (epochs1, epochs2, epochs):
    assert_equal(e.times[0], epochs.tmin)
    assert_equal(e.times[(- 1)], epochs.tmax)
this_tmin = (- 0.002)
epochs = EpochsArray(data, deepcopy(info), events, tmin=this_tmin)
for times in (epochs.times, epochs._raw_times):
    assert_allclose(times, ((numpy.arange(n_time) / sfreq) + this_tmin))
epochs.resample((info['sfreq'] * 2.0))
for times in (epochs.times, epochs._raw_times):
    assert_allclose(times, ((numpy.arange((2 * n_time)) / (sfreq * 2)) + this_tmin))
epochs.crop(0, None)
for times in (epochs.times, epochs._raw_times):
    assert_allclose(times, (numpy.arange(((n_time - 2) * 2)) / (sfreq * 2)))
epochs.resample(sfreq)
for times in (epochs.times, epochs._raw_times):
    tempResult = arange((n_time - 2))
	
===================================================================	
test_seeg_ecog: 1513	
----------------------------	

'Test the compatibility of the Epoch object with SEEG and ECoG data.'
(n_epochs, n_channels, n_times, sfreq) = (5, 10, 20, 1000.0)
data = numpy.ones((n_epochs, n_channels, n_times))
tempResult = arange(n_epochs)
	
===================================================================	
test_array_epochs: 1394	
----------------------------	

'Test creating epochs from array.'
import matplotlib.pyplot as plt
tempdir = _TempDir()
data = rng.random_sample((10, 20, 300))
sfreq = 1000.0
ch_names = [('EEG %03d' % (i + 1)) for i in range(20)]
types = (['eeg'] * 20)
info = create_info(ch_names, sfreq, types)
tempResult = arange(1, 600, 60)
	
===================================================================	
test_array_epochs: 1437	
----------------------------	

'Test creating epochs from array.'
import matplotlib.pyplot as plt
tempdir = _TempDir()
data = rng.random_sample((10, 20, 300))
sfreq = 1000.0
ch_names = [('EEG %03d' % (i + 1)) for i in range(20)]
types = (['eeg'] * 20)
info = create_info(ch_names, sfreq, types)
events = numpy.c_[(numpy.arange(1, 600, 60), numpy.zeros(10, int), ([1, 2] * 5))]
event_id = {'a': 1, 'b': 2}
epochs = EpochsArray(data, info, events, tmin, event_id)
assert_true(str(epochs).startswith('<EpochsArray'))
assert_raises(ValueError, EpochsArray, data[:(- 1)], info, events, tmin, event_id)
assert_raises(ValueError, EpochsArray, data, info, events, tmin, dict(a=1))
temp_fname = os.path.join(tempdir, 'test-epo.fif')
epochs.save(temp_fname)
epochs2 = read_epochs(temp_fname)
data2 = epochs2.get_data()
assert_allclose(data, data2)
assert_allclose(epochs.times, epochs2.times)
assert_equal(epochs.event_id, epochs2.event_id)
assert_array_equal(epochs.events, epochs2.events)
epochs[0].plot()
matplotlib.pyplot.close('all')
assert_array_equal(numpy.unique(epochs['a'].events[:, 2]), numpy.array([1]))
assert_equal(len(epochs[:2]), 2)
data[(0, 5, 150)] = 3000
data[1, :, :] = 0
data[(2, 5, 210)] = 3000
data[(3, 5, 260)] = 0
epochs = EpochsArray(data, info, events=events, event_id=event_id, tmin=0, reject=dict(eeg=1000), flat=dict(eeg=0.1), reject_tmin=0.1, reject_tmax=0.2)
assert_equal(len(epochs), (len(events) - 2))
assert_equal(epochs.drop_log[0], ['EEG 006'])
assert_equal(len(epochs.drop_log), 10)
assert_equal(len(epochs.events), len(epochs.selection))
data = numpy.ones((10, 20, 300))
epochs = EpochsArray(data, info, events, event_id=event_id, tmin=(- 0.2), baseline=(None, 0))
ep_data = epochs.get_data()
assert_array_equal(ep_data, numpy.zeros_like(ep_data))
epochs = EpochsArray(data[:, :, :1], info, events=events, event_id=event_id, tmin=0.0)
assert_allclose(epochs.times, [0.0])
assert_allclose(epochs.get_data(), data[:, :, :1])
epochs.save(temp_fname)
epochs_read = read_epochs(temp_fname)
assert_allclose(epochs_read.times, [0.0])
assert_allclose(epochs_read.get_data(), data[:, :, :1])
mask = (events[:, 2] == 1)
data_1 = data[mask]
events_1 = events[mask]
epochs = EpochsArray(data_1, info, events=events_1, event_id=1, tmin=(- 0.2))
epochs = EpochsArray(data_1, info)
tempResult = arange(len(data_1))
	
===================================================================	
test_delayed_epochs: 1169	
----------------------------	

'Test delayed projection on Epochs.'
(raw, events, picks) = _get_data()
events = events[:10]
picks = numpy.concatenate([pick_types(raw.info, meg=True, eeg=True)[::22], pick_types(raw.info, meg=False, eeg=False, ecg=True, eog=True)])
picks = numpy.sort(picks)
raw.load_data().pick_channels([raw.ch_names[pick] for pick in picks])
raw.info.normalize_proj()
del picks
n_epochs = 2
raw.info['lowpass'] = 40.0
for decim in (1, 3):
    proj_data = Epochs(raw, events, event_id, tmin, tmax, proj=True, reject=reject, decim=decim)
    use_tmin = proj_data.tmin
    proj_data = proj_data.get_data()
    noproj_data = Epochs(raw, events, event_id, tmin, tmax, proj=False, reject=reject, decim=decim).get_data()
    assert_equal(proj_data.shape, noproj_data.shape)
    assert_equal(proj_data.shape[0], n_epochs)
    for preload in (True, False):
        for proj in (True, False, 'delayed'):
            for ii in range(3):
                print(decim, preload, proj, ii)
                comp = (proj_data if (proj is True) else noproj_data)
                if (ii in (0, 1)):
                    epochs = Epochs(raw, events, event_id, tmin, tmax, proj=proj, reject=reject, preload=preload, decim=decim)
                else:
                    fake_events = numpy.zeros((len(comp), 3), int)
                    tempResult = arange(len(comp))
	
===================================================================	
test_to_data_frame: 1085	
----------------------------	

'Test epochs Pandas exporter.'
(raw, events, picks) = _get_data()
epochs = Epochs(raw, events, {'a': 1, 'b': 2}, tmin, tmax, picks=picks)
assert_raises(ValueError, epochs.to_data_frame, index=['foo', 'bar'])
assert_raises(ValueError, epochs.to_data_frame, index='qux')
tempResult = arange(400)
	
===================================================================	
test_decim: 217	
----------------------------	

'Test epochs decimation.'
(dec_1, dec_2) = (2, 3)
decim = (dec_1 * dec_2)
(n_epochs, n_channels, n_times) = (5, 10, 20)
sfreq = 1000.0
sfreq_new = (sfreq / decim)
data = rng.randn(n_epochs, n_channels, n_times)
tempResult = arange(n_epochs)
	
===================================================================	
test_decim: 239	
----------------------------	

'Test epochs decimation.'
(dec_1, dec_2) = (2, 3)
decim = (dec_1 * dec_2)
(n_epochs, n_channels, n_times) = (5, 10, 20)
sfreq = 1000.0
sfreq_new = (sfreq / decim)
data = rng.randn(n_epochs, n_channels, n_times)
events = np.array([np.arange(n_epochs), ([0] * n_epochs), ([1] * n_epochs)]).T
info = create_info(n_channels, sfreq, 'eeg')
info['lowpass'] = (sfreq_new / float(decim))
epochs = EpochsArray(data, info, events)
data_epochs = epochs.copy().decimate(decim).get_data()
data_epochs_2 = epochs.copy().decimate(decim, offset=1).get_data()
data_epochs_3 = epochs.decimate(dec_1).decimate(dec_2).get_data()
assert_array_equal(data_epochs, data[:, :, ::decim])
assert_array_equal(data_epochs_2, data[:, :, 1::decim])
assert_array_equal(data_epochs, data_epochs_3)
(raw, events, picks) = _get_data()
events = events[(events[:, 2] == 1)][:2]
raw.load_data().pick_channels([raw.ch_names[pick] for pick in picks[::30]])
raw.info.normalize_proj()
del picks
sfreq_new = (raw.info['sfreq'] / decim)
raw.info['lowpass'] = (sfreq_new / 12.0)
assert_raises(ValueError, epochs.decimate, (- 1))
assert_raises(ValueError, epochs.decimate, 2, offset=(- 1))
assert_raises(ValueError, epochs.decimate, 2, offset=2)
for this_offset in range(decim):
    epochs = Epochs(raw, events, event_id, tmin=((- this_offset) / raw.info['sfreq']), tmax=tmax)
    tempResult = arange(decim)
	
===================================================================	
test_decim: 240	
----------------------------	

'Test epochs decimation.'
(dec_1, dec_2) = (2, 3)
decim = (dec_1 * dec_2)
(n_epochs, n_channels, n_times) = (5, 10, 20)
sfreq = 1000.0
sfreq_new = (sfreq / decim)
data = rng.randn(n_epochs, n_channels, n_times)
events = np.array([np.arange(n_epochs), ([0] * n_epochs), ([1] * n_epochs)]).T
info = create_info(n_channels, sfreq, 'eeg')
info['lowpass'] = (sfreq_new / float(decim))
epochs = EpochsArray(data, info, events)
data_epochs = epochs.copy().decimate(decim).get_data()
data_epochs_2 = epochs.copy().decimate(decim, offset=1).get_data()
data_epochs_3 = epochs.decimate(dec_1).decimate(dec_2).get_data()
assert_array_equal(data_epochs, data[:, :, ::decim])
assert_array_equal(data_epochs_2, data[:, :, 1::decim])
assert_array_equal(data_epochs, data_epochs_3)
(raw, events, picks) = _get_data()
events = events[(events[:, 2] == 1)][:2]
raw.load_data().pick_channels([raw.ch_names[pick] for pick in picks[::30]])
raw.info.normalize_proj()
del picks
sfreq_new = (raw.info['sfreq'] / decim)
raw.info['lowpass'] = (sfreq_new / 12.0)
assert_raises(ValueError, epochs.decimate, (- 1))
assert_raises(ValueError, epochs.decimate, 2, offset=(- 1))
assert_raises(ValueError, epochs.decimate, 2, offset=2)
for this_offset in range(decim):
    epochs = Epochs(raw, events, event_id, tmin=((- this_offset) / raw.info['sfreq']), tmax=tmax)
    idx_offsets = (numpy.arange(decim) + this_offset)
    tempResult = arange(decim)
	
===================================================================	
test_reject: 133	
----------------------------	

'Test epochs rejection.'
(raw, events, picks) = _get_data()
events = events[(events[:, 2] == event_id), :]
tempResult = arange(3)
	
===================================================================	
test_reject: 152	
----------------------------	

'Test epochs rejection.'
(raw, events, picks) = _get_data()
events = events[(events[:, 2] == event_id), :]
selection = numpy.arange(3)
drop_log = (([[]] * 3) + ([['MEG 2443']] * 4))
assert_raises(TypeError, pick_types, raw)
picks_meg = pick_types(raw.info, meg=True, eeg=False)
assert_raises(TypeError, Epochs, raw, events, event_id, tmin, tmax, picks=picks, preload=False, reject='foo')
assert_raises(ValueError, Epochs, raw, events, event_id, tmin, tmax, picks=picks_meg, preload=False, reject=dict(eeg=1.0))
Epochs(raw, events, event_id, tmin, tmax, picks=picks_meg, preload=False, reject=dict(eeg=numpy.inf))
for val in (None, (- 1)):
    for kwarg in ('reject', 'flat'):
        assert_raises(ValueError, Epochs, raw, events, event_id, tmin, tmax, picks=picks_meg, preload=False, **{kwarg: dict(grad=val)})
assert_raises(KeyError, Epochs, raw, events, event_id, tmin, tmax, picks=picks, preload=False, reject=dict(foo=1.0))
data_7 = dict()
keep_idx = [0, 1, 2]
for preload in (True, False):
    for proj in (True, False, 'delayed'):
        epochs = Epochs(raw, events, event_id, tmin, tmax, picks=picks, preload=preload)
        assert_raises(ValueError, epochs.drop_bad, reject='foo')
        epochs.drop_bad()
        assert_equal(len(epochs), len(events))
        tempResult = arange(len(events))
	
===================================================================	
test_find_events: 144	
----------------------------	

'Test find events in raw file.'
events = read_events(fname)
raw = read_raw_fif(raw_fname, preload=True)
extra_ends = ['', '_1']
orig_envs = [os.getenv(('MNE_STIM_CHANNEL%s' % s)) for s in extra_ends]
os.environ['MNE_STIM_CHANNEL'] = 'STI 014'
if ('MNE_STIM_CHANNEL_1' in os.environ):
    del os.environ['MNE_STIM_CHANNEL_1']
events2 = find_events(raw)
assert_array_almost_equal(events, events2)
events11 = find_events(raw, mask=3, mask_type='not_and')
with warnings.catch_warnings(record=True) as w:
    warnings.simplefilter('always')
    events22 = read_events(fname, mask=3, mask_type='not_and')
    assert_true((sum((('events masked' in str(ww.message)) for ww in w)) == 1))
assert_array_equal(events11, events22)
raw._first_samps[0] = 0
raw.info['sfreq'] = 1000
raw._update_times()
stim_channel = 'STI 014'
stim_channel_idx = pick_channels(raw.info['ch_names'], include=[stim_channel])
tempResult = arange(5)
	
===================================================================	
test_decim: 32	
----------------------------	

'Test evoked decimation.'
rng = numpy.random.RandomState(0)
(n_epochs, n_channels, n_times) = (5, 10, 20)
(dec_1, dec_2) = (2, 3)
decim = (dec_1 * dec_2)
sfreq = 1000.0
sfreq_new = (sfreq / decim)
data = rng.randn(n_epochs, n_channels, n_times)
tempResult = arange(n_epochs)
	
===================================================================	
test_to_data_frame: 211	
----------------------------	

'Test evoked Pandas exporter.'
ave = read_evokeds(fname, 0)
tempResult = arange(400)
	
===================================================================	
test_detrend: 327	
----------------------------	

'Test zeroth and first order detrending.'
tempResult = arange(10)
	
===================================================================	
test_notch_filters: 134	
----------------------------	

'Test notch filters.'
sfreq = 487.0
sig_len_secs = 20
tempResult = arange(0, int((sig_len_secs * sfreq)))
	
===================================================================	
test_notch_filters: 135	
----------------------------	

'Test notch filters.'
sfreq = 487.0
sig_len_secs = 20
t = (numpy.arange(0, int((sig_len_secs * sfreq))) / sfreq)
tempResult = arange(60, 241, 60)
	
===================================================================	
test_filter_auto: 265	
----------------------------	

'Test filter auto parameters'
N = 300
sfreq = 100.0
lp = 10.0
sine_freq = 1.0
x = numpy.ones(N)
tempResult = arange(N)
	
===================================================================	
test_label_addition: 120	
----------------------------	

'Test label addition.'
pos = np.random.RandomState(0).rand(10, 3)
tempResult = arange(10.0)
	
===================================================================	
test_morph: 456	
----------------------------	

'Test inter-subject label morphing.'
label_orig = read_label(real_label_fname)
label_orig.subject = 'sample'
vals = list()
tempResult = arange(10242)
	
===================================================================	
test_morph: 456	
----------------------------	

'Test inter-subject label morphing.'
label_orig = read_label(real_label_fname)
label_orig.subject = 'sample'
vals = list()
tempResult = arange(10242)
	
===================================================================	
test_morph: 456	
----------------------------	

'Test inter-subject label morphing.'
label_orig = read_label(real_label_fname)
label_orig.subject = 'sample'
vals = list()
tempResult = arange(10242)
	
===================================================================	
test_morph: 467	
----------------------------	

'Test inter-subject label morphing.'
label_orig = read_label(real_label_fname)
label_orig.subject = 'sample'
vals = list()
for grade in [5, [numpy.arange(10242), numpy.arange(10242)], numpy.arange(10242)]:
    label = label_orig.copy()
    assert_raises(ValueError, label.morph, 'sample', 'fsaverage')
    label.values.fill(1)
    label = label.morph(None, 'fsaverage', 5, grade, subjects_dir, 1)
    label = label.morph('fsaverage', 'sample', 5, None, subjects_dir, 2)
    assert_true(np.in1d(label_orig.vertices, label.vertices).all())
    assert_true((len(label.vertices) < (3 * len(label_orig.vertices))))
    vals.append(label.vertices)
assert_array_equal(vals[0], vals[1])
assert_equal(label.subject, 'sample')
tempResult = arange(10242)
	
===================================================================	
test_morph: 467	
----------------------------	

'Test inter-subject label morphing.'
label_orig = read_label(real_label_fname)
label_orig.subject = 'sample'
vals = list()
for grade in [5, [numpy.arange(10242), numpy.arange(10242)], numpy.arange(10242)]:
    label = label_orig.copy()
    assert_raises(ValueError, label.morph, 'sample', 'fsaverage')
    label.values.fill(1)
    label = label.morph(None, 'fsaverage', 5, grade, subjects_dir, 1)
    label = label.morph('fsaverage', 'sample', 5, None, subjects_dir, 2)
    assert_true(np.in1d(label_orig.vertices, label.vertices).all())
    assert_true((len(label.vertices) < (3 * len(label_orig.vertices))))
    vals.append(label.vertices)
assert_array_equal(vals[0], vals[1])
assert_equal(label.subject, 'sample')
tempResult = arange(10242)
	
===================================================================	
test_get_peak: 587	
----------------------------	

'Test peak getter.'
(n_vert, n_times) = (10, 5)
tempResult = arange(n_vert, dtype=numpy.int)
	
===================================================================	
_fake_stc: 135	
----------------------------	

tempResult = arange(10)
	
===================================================================	
_fake_stc: 135	
----------------------------	

tempResult = arange(90)
	
===================================================================	
test_stc_attributes: 179	
----------------------------	

'Test STC attributes.'
stc = _fake_stc(n_time=10)
vec_stc = _fake_vec_stc(n_time=10)
_test_stc_integrety(stc)
assert_array_almost_equal(stc.times, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

def attempt_times_mutation(stc):
    stc.times -= 1

def attempt_assignment(stc, attr, val):
    setattr(stc, attr, val)
assert_raises(ValueError, attempt_times_mutation, stc)
assert_raises(ValueError, attempt_assignment, stc, 'times', [1])
stc.tmin = 1
assert_true((type(stc.tmin) == float))
assert_array_almost_equal(stc.times, [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9])
stc.tstep = 1
assert_true((type(stc.tstep) == float))
assert_array_almost_equal(stc.times, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])
assert_raises(ValueError, attempt_assignment, stc, 'tstep', 0)
assert_raises(ValueError, attempt_assignment, stc, 'tstep', (- 1))
stc.data = numpy.random.rand(100, 5)
assert_array_almost_equal(stc.times, [1.0, 2.0, 3.0, 4.0, 5.0])
assert_raises(ValueError, attempt_assignment, stc, 'data', [[1]])
assert_raises(ValueError, attempt_assignment, stc, 'data', None)
tempResult = arange(100)
	
===================================================================	
test_stc_attributes: 180	
----------------------------	

'Test STC attributes.'
stc = _fake_stc(n_time=10)
vec_stc = _fake_vec_stc(n_time=10)
_test_stc_integrety(stc)
assert_array_almost_equal(stc.times, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

def attempt_times_mutation(stc):
    stc.times -= 1

def attempt_assignment(stc, attr, val):
    setattr(stc, attr, val)
assert_raises(ValueError, attempt_times_mutation, stc)
assert_raises(ValueError, attempt_assignment, stc, 'times', [1])
stc.tmin = 1
assert_true((type(stc.tmin) == float))
assert_array_almost_equal(stc.times, [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9])
stc.tstep = 1
assert_true((type(stc.tstep) == float))
assert_array_almost_equal(stc.times, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])
assert_raises(ValueError, attempt_assignment, stc, 'tstep', 0)
assert_raises(ValueError, attempt_assignment, stc, 'tstep', (- 1))
stc.data = numpy.random.rand(100, 5)
assert_array_almost_equal(stc.times, [1.0, 2.0, 3.0, 4.0, 5.0])
assert_raises(ValueError, attempt_assignment, stc, 'data', [[1]])
assert_raises(ValueError, attempt_assignment, stc, 'data', None)
assert_raises(ValueError, attempt_assignment, stc, 'data', numpy.arange(100))
tempResult = arange(100)
	
===================================================================	
test_stc_attributes: 181	
----------------------------	

'Test STC attributes.'
stc = _fake_stc(n_time=10)
vec_stc = _fake_vec_stc(n_time=10)
_test_stc_integrety(stc)
assert_array_almost_equal(stc.times, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

def attempt_times_mutation(stc):
    stc.times -= 1

def attempt_assignment(stc, attr, val):
    setattr(stc, attr, val)
assert_raises(ValueError, attempt_times_mutation, stc)
assert_raises(ValueError, attempt_assignment, stc, 'times', [1])
stc.tmin = 1
assert_true((type(stc.tmin) == float))
assert_array_almost_equal(stc.times, [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9])
stc.tstep = 1
assert_true((type(stc.tstep) == float))
assert_array_almost_equal(stc.times, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0])
assert_raises(ValueError, attempt_assignment, stc, 'tstep', 0)
assert_raises(ValueError, attempt_assignment, stc, 'tstep', (- 1))
stc.data = numpy.random.rand(100, 5)
assert_array_almost_equal(stc.times, [1.0, 2.0, 3.0, 4.0, 5.0])
assert_raises(ValueError, attempt_assignment, stc, 'data', [[1]])
assert_raises(ValueError, attempt_assignment, stc, 'data', None)
assert_raises(ValueError, attempt_assignment, stc, 'data', numpy.arange(100))
assert_raises(ValueError, attempt_assignment, vec_stc, 'data', [numpy.arange(100)])
tempResult = arange(100)
	
===================================================================	
_fake_vec_stc: 139	
----------------------------	

tempResult = arange(10)
	
===================================================================	
_fake_vec_stc: 139	
----------------------------	

tempResult = arange(90)
	
===================================================================	
test_extract_label_time_course: 341	
----------------------------	

'Test extraction of label time courses from stc.'
n_stcs = 3
n_times = 50
src = read_inverse_operator(fname_inv)['src']
vertices = [src[0]['vertno'], src[1]['vertno']]
n_verts = (len(vertices[0]) + len(vertices[1]))
labels_lh = read_labels_from_annot('sample', hemi='lh', subjects_dir=subjects_dir)
labels_rh = read_labels_from_annot('sample', hemi='rh', subjects_dir=subjects_dir)
labels = list()
labels.extend(labels_lh[:5])
labels.extend(labels_rh[:4])
n_labels = len(labels)
tempResult = arange(n_labels)
	
===================================================================	
test_extract_label_time_course: 342	
----------------------------	

'Test extraction of label time courses from stc.'
n_stcs = 3
n_times = 50
src = read_inverse_operator(fname_inv)['src']
vertices = [src[0]['vertno'], src[1]['vertno']]
n_verts = (len(vertices[0]) + len(vertices[1]))
labels_lh = read_labels_from_annot('sample', hemi='lh', subjects_dir=subjects_dir)
labels_rh = read_labels_from_annot('sample', hemi='rh', subjects_dir=subjects_dir)
labels = list()
labels.extend(labels_lh[:5])
labels.extend(labels_rh[:4])
n_labels = len(labels)
label_means = (numpy.arange(n_labels)[:, None] * numpy.ones((n_labels, n_times)))
tempResult = arange(n_labels)
	
===================================================================	
test_to_data_frame: 572	
----------------------------	

'Test stc Pandas exporter.'
(n_vert, n_times) = (10, 5)
tempResult = arange(n_vert, dtype=numpy.int)
	
===================================================================	
test_transform_data: 469	
----------------------------	

'Test applying linear (time) transform to data.'
(n_sensors, n_vertices, n_times) = (10, 20, 4)
kernel = rng.randn(n_vertices, n_sensors)
sens_data = rng.randn(n_sensors, n_times)
tempResult = arange(n_vertices)
	
===================================================================	
test_transform_data: 471	
----------------------------	

'Test applying linear (time) transform to data.'
(n_sensors, n_vertices, n_times) = (10, 20, 4)
kernel = rng.randn(n_vertices, n_sensors)
sens_data = rng.randn(n_sensors, n_times)
vertices = numpy.arange(n_vertices)
data = numpy.dot(kernel, sens_data)
tempResult = arange((n_vertices // 2), n_vertices)
	
===================================================================	
test_mixed_stc: 610	
----------------------------	

'Test source estimate from mixed source space.'
N = 90
T = 2
S = 3
data = rng.randn(N, T)
tempResult = arange((N // S))
	
===================================================================	
test_mixed_stc: 611	
----------------------------	

'Test source estimate from mixed source space.'
N = 90
T = 2
S = 3
data = rng.randn(N, T)
vertno = (S * [numpy.arange((N // S))])
tempResult = arange(N)
	
===================================================================	
test_transform: 485	
----------------------------	

'Test applying linear (time) transform to data.'
(n_verts_lh, n_verts_rh, n_times) = (10, 10, 10)
tempResult = arange(n_verts_lh)
	
===================================================================	
test_transform: 485	
----------------------------	

'Test applying linear (time) transform to data.'
(n_verts_lh, n_verts_rh, n_times) = (10, 10, 10)
tempResult = arange(n_verts_rh)
	
===================================================================	
test_transform: 501	
----------------------------	

'Test applying linear (time) transform to data.'
(n_verts_lh, n_verts_rh, n_times) = (10, 10, 10)
vertices = [numpy.arange(n_verts_lh), (n_verts_lh + numpy.arange(n_verts_rh))]
data = rng.randn((n_verts_lh + n_verts_rh), n_times)
stc = SourceEstimate(data, vertices=vertices, tmin=(- 0.1), tstep=0.1)
stcs_t = stc.transform(_my_trans, copy=True)
assert_true(isinstance(stcs_t, list))
assert_array_equal(stc.times, stcs_t[0].times)
assert_equal(stc.vertices, stcs_t[0].vertices)
data = numpy.concatenate((stcs_t[0].data[:, :, None], stcs_t[1].data[:, :, None]), axis=2)
data_t = stc.transform_data(_my_trans)
assert_array_equal(data, data_t)
assert_raises(ValueError, stc.transform, _my_trans, copy=False)
tmp = deepcopy(stc)
stc_t = stc.transform(numpy.abs, copy=True)
assert_true(isinstance(stc_t, SourceEstimate))
assert_array_equal(stc.data, tmp.data)
times = numpy.round((1000 * stc.times))
tempResult = arange(len(stc.lh_vertno), (len(stc.lh_vertno) + len(stc.rh_vertno)), 1)
	
===================================================================	
test_volume_stc: 66	
----------------------------	

'Test volume STCs.'
tempdir = _TempDir()
N = 100
tempResult = arange(N)
	
===================================================================	
test_volume_stc: 67	
----------------------------	

'Test volume STCs.'
tempdir = _TempDir()
N = 100
data = numpy.arange(N)[:, numpy.newaxis]
tempResult = arange(2)
	
===================================================================	
test_volume_stc: 68	
----------------------------	

'Test volume STCs.'
tempdir = _TempDir()
N = 100
data = numpy.arange(N)[:, numpy.newaxis]
datas = [data, data, numpy.arange(2)[:, numpy.newaxis]]
tempResult = arange(N)
	
===================================================================	
test_volume_stc: 69	
----------------------------	

'Test volume STCs.'
tempdir = _TempDir()
N = 100
data = numpy.arange(N)[:, numpy.newaxis]
datas = [data, data, numpy.arange(2)[:, numpy.newaxis]]
vertno = numpy.arange(N)
tempResult = arange(2)
	
===================================================================	
test_volume_stc: 70	
----------------------------	

'Test volume STCs.'
tempdir = _TempDir()
N = 100
data = numpy.arange(N)[:, numpy.newaxis]
datas = [data, data, numpy.arange(2)[:, numpy.newaxis]]
vertno = numpy.arange(N)
vertnos = [vertno, vertno[:, numpy.newaxis], numpy.arange(2)[:, numpy.newaxis]]
tempResult = arange(2)
	
===================================================================	
_test_stc_integrety: 151	
----------------------------	

'Test consistency of tmin, tstep, data.shape[-1] and times.'
n_times = len(stc.times)
assert_equal(stc._data.shape[(- 1)], n_times)
tempResult = arange(n_times)
	
===================================================================	
test_vol_mask: 670	
----------------------------	

'Test extraction of volume mask.'
src = read_source_spaces(fname_vsrc)
mask = _get_vol_mask(src)
vertices = src[0]['vertno']
n_vertices = len(vertices)
tempResult = arange(n_vertices)
	
===================================================================	
test_morphed_source_space_return: 435	
----------------------------	

'Test returning a morphed source space to the original subject.'
data = rng.randn(20484, 1)
(tmin, tstep) = (0, 1.0)
src_fs = read_source_spaces(fname_fs)
stc_fs = SourceEstimate(data, [s['vertno'] for s in src_fs], tmin, tstep, 'fsaverage')
src_morph = morph_source_spaces(src_fs, 'sample', subjects_dir=subjects_dir)
stc_morph = stc_fs.morph('sample', [s['vertno'] for s in src_morph], smooth=1, subjects_dir=subjects_dir)
tempResult = arange(len(v))
	
===================================================================	
test_setup_source_space: 256	
----------------------------	

'Test setting up ico, oct, and all source spaces.'
tempdir = _TempDir()
fname_ico = os.path.join(data_path, 'subjects', 'fsaverage', 'bem', 'fsaverage-ico-5-src.fif')
assert_raises(ValueError, setup_source_space, 'sample', spacing='oct', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='octo', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='oct6e', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='7emm', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='alls', add_dist=False, subjects_dir=subjects_dir)
src = read_source_spaces(fname_ico)
with warnings.catch_warnings(record=True):
    warnings.simplefilter('always')
    src_new = setup_source_space('fsaverage', spacing='ico5', subjects_dir=subjects_dir, add_dist=False)
_compare_source_spaces(src, src_new, mode='approx')
assert_equal(repr(src), repr(src_new))
assert_equal(repr(src).count('surface ('), 2)
tempResult = arange(10242)
	
===================================================================	
test_setup_source_space: 257	
----------------------------	

'Test setting up ico, oct, and all source spaces.'
tempdir = _TempDir()
fname_ico = os.path.join(data_path, 'subjects', 'fsaverage', 'bem', 'fsaverage-ico-5-src.fif')
assert_raises(ValueError, setup_source_space, 'sample', spacing='oct', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='octo', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='oct6e', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='7emm', add_dist=False, subjects_dir=subjects_dir)
assert_raises(ValueError, setup_source_space, 'sample', spacing='alls', add_dist=False, subjects_dir=subjects_dir)
src = read_source_spaces(fname_ico)
with warnings.catch_warnings(record=True):
    warnings.simplefilter('always')
    src_new = setup_source_space('fsaverage', spacing='ico5', subjects_dir=subjects_dir, add_dist=False)
_compare_source_spaces(src, src_new, mode='approx')
assert_equal(repr(src), repr(src_new))
assert_equal(repr(src).count('surface ('), 2)
assert_array_equal(src[0]['vertno'], numpy.arange(10242))
tempResult = arange(10242)
	
===================================================================	
test_compute_nearest: 58	
----------------------------	

'Test nearest neighbor searches.'
x = rng.randn(500, 3)
x /= numpy.sqrt(numpy.sum((x ** 2), axis=1))[:, None]
tempResult = arange(500, dtype=numpy.int)
	
===================================================================	
test_progressbar: 567	
----------------------------	

tempResult = arange(10)
	
===================================================================	
test_check_type_picks: 426	
----------------------------	

'Test checking type integrity checks of picks.'
tempResult = arange(12)
	
===================================================================	
test_time_mask: 474	
----------------------------	

'Test safe time masking.'
N = 10
tempResult = arange(N)
	
===================================================================	
_yule_walker: 11	
----------------------------	

'Compute Yule-Walker (adapted from statsmodels).\n\n    Operates in-place.\n    '
assert (X.ndim == 2)
tempResult = arange((order + 1))
	
===================================================================	
psd_array_multitaper: 194	
----------------------------	

'Compute power spectrum density (PSD) using a multi-taper method.\n\n    Parameters\n    ----------\n    x : array, shape=(..., n_times)\n        The data to compute PSD from.\n    sfreq : float\n        The sampling frequency.\n    fmin : float\n        The lower frequency of interest.\n    fmax : float\n        The upper frequency of interest.\n    bandwidth : float\n        The bandwidth of the multi taper windowing function in Hz.\n    adaptive : bool\n        Use adaptive weights to combine the tapered spectra into PSD\n        (slow, use n_jobs >> 1 to speed up computation).\n    low_bias : bool\n        Only use tapers with more than 90% spectral concentration within\n        bandwidth.\n    normalization : str\n        Either "full" or "length" (default). If "full", the PSD will\n        be normalized by the sampling rate as well as the length of\n        the signal (as in nitime).\n    n_jobs : int\n        Number of parallel jobs to use (only used if adaptive=True).\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    psds : ndarray, shape (..., n_freqs) or\n        The power spectral densities. All dimensions up to the last will\n        be the same as input.\n    freqs : array\n        The frequency points in Hz of the PSD.\n\n    See Also\n    --------\n    mne.io.Raw.plot_psd, mne.Epochs.plot_psd, csd_epochs, psd_multitaper\n\n    Notes\n    -----\n    .. versionadded:: 0.14.0\n    '
if (normalization not in ('length', 'full')):
    raise ValueError(('Normalization must be "length" or "full", not %s' % normalization))
ndim_in = x.ndim
x = numpy.atleast_2d(x)
n_times = x.shape[(- 1)]
dshape = x.shape[:(- 1)]
x = x.reshape((- 1), n_times)
if (bandwidth is not None):
    half_nbw = ((float(bandwidth) * n_times) / (2 * sfreq))
else:
    half_nbw = 4
n_tapers_max = int((2 * half_nbw))
(dpss, eigvals) = dpss_windows(n_times, half_nbw, n_tapers_max, low_bias=low_bias)
freqs = numpy.fft.rfftfreq(x.shape[1], (1.0 / sfreq))
freq_mask = ((freqs >= fmin) & (freqs <= fmax))
freqs = freqs[freq_mask]
if (adaptive and (len(eigvals) < 3)):
    warn('Not adaptively combining the spectral estimators due to a low number of tapers.')
    adaptive = False
psd = numpy.zeros((x.shape[0], freq_mask.sum()))
n_chunk = max((50000000 // ((len(freq_mask) * len(eigvals)) * 16)), n_jobs)
tempResult = arange(0, x.shape[0], n_chunk)
	
===================================================================	
dpss_windows: 49	
----------------------------	

"Compute Discrete Prolate Spheroidal Sequences.\n\n    Will give of orders [0,Kmax-1] for a given frequency-spacing multiple\n    NW and sequence length N.\n\n    .. note:: Copied from NiTime.\n\n    Parameters\n    ----------\n    N : int\n        Sequence length\n    half_nbw : float, unitless\n        Standardized half bandwidth corresponding to 2 * half_bw = BW*f0\n        = BW*N/dt but with dt taken as 1\n    Kmax : int\n        Number of DPSS windows to return is Kmax (orders 0 through Kmax-1)\n    low_bias : Bool\n        Keep only tapers with eigenvalues > 0.9\n    interp_from : int (optional)\n        The dpss can be calculated using interpolation from a set of dpss\n        with the same NW and Kmax, but shorter N. This is the length of this\n        shorter set of dpss windows.\n    interp_kind : str (optional)\n        This input variable is passed to scipy.interpolate.interp1d and\n        specifies the kind of interpolation as a string ('linear', 'nearest',\n        'zero', 'slinear', 'quadratic, 'cubic') or as an integer specifying the\n        order of the spline interpolator to use.\n\n\n    Returns\n    -------\n    v, e : tuple,\n        v is an array of DPSS windows shaped (Kmax, N)\n        e are the eigenvalues\n\n    Notes\n    -----\n    Tridiagonal form of DPSS calculation from:\n\n    Slepian, D. Prolate spheroidal wave functions, Fourier analysis, and\n    uncertainty V: The discrete case. Bell System Technical Journal,\n    Volume 57 (1978), 1371430\n    "
from scipy import interpolate
from ..filter import next_fast_len
Kmax = int(Kmax)
W = (float(half_nbw) / N)
tempResult = arange(N, dtype='d')
	
===================================================================	
dpss_windows: 59	
----------------------------	

"Compute Discrete Prolate Spheroidal Sequences.\n\n    Will give of orders [0,Kmax-1] for a given frequency-spacing multiple\n    NW and sequence length N.\n\n    .. note:: Copied from NiTime.\n\n    Parameters\n    ----------\n    N : int\n        Sequence length\n    half_nbw : float, unitless\n        Standardized half bandwidth corresponding to 2 * half_bw = BW*f0\n        = BW*N/dt but with dt taken as 1\n    Kmax : int\n        Number of DPSS windows to return is Kmax (orders 0 through Kmax-1)\n    low_bias : Bool\n        Keep only tapers with eigenvalues > 0.9\n    interp_from : int (optional)\n        The dpss can be calculated using interpolation from a set of dpss\n        with the same NW and Kmax, but shorter N. This is the length of this\n        shorter set of dpss windows.\n    interp_kind : str (optional)\n        This input variable is passed to scipy.interpolate.interp1d and\n        specifies the kind of interpolation as a string ('linear', 'nearest',\n        'zero', 'slinear', 'quadratic, 'cubic') or as an integer specifying the\n        order of the spline interpolator to use.\n\n\n    Returns\n    -------\n    v, e : tuple,\n        v is an array of DPSS windows shaped (Kmax, N)\n        e are the eigenvalues\n\n    Notes\n    -----\n    Tridiagonal form of DPSS calculation from:\n\n    Slepian, D. Prolate spheroidal wave functions, Fourier analysis, and\n    uncertainty V: The discrete case. Bell System Technical Journal,\n    Volume 57 (1978), 1371430\n    "
from scipy import interpolate
from ..filter import next_fast_len
Kmax = int(Kmax)
W = (float(half_nbw) / N)
nidx = numpy.arange(N, dtype='d')
if (interp_from is not None):
    if (interp_from > N):
        e_s = ('In dpss_windows, interp_from is: %s ' % interp_from)
        e_s += ('and N is: %s. ' % N)
        e_s += 'Please enter interp_from smaller than N.'
        raise ValueError(e_s)
    dpss = []
    (d, e) = dpss_windows(interp_from, half_nbw, Kmax, low_bias=False)
    for this_d in d:
        tempResult = arange(this_d.shape[(- 1)])
	
===================================================================	
psd_array_welch: 56	
----------------------------	

"Compute power spectral density (PSD) using Welch's method.\n\n    Parameters\n    ----------\n    x : array, shape=(..., n_times)\n        The data to compute PSD from.\n    sfreq : float\n        The sampling frequency.\n    fmin : float\n        The lower frequency of interest.\n    fmax : float\n        The upper frequency of interest.\n    n_fft : int\n        The length of FFT used, must be ``>= n_per_seg`` (default: 256).\n        The segments will be zero-padded if ``n_fft > n_per_seg``.\n    n_overlap : int\n        The number of points of overlap between segments. Will be adjusted\n        to be <= n_per_seg. The default value is 0.\n    n_per_seg : int | None\n        Length of each Welch segment (windowed with a Hamming window). Defaults\n        to None, which sets n_per_seg equal to n_fft.\n    n_jobs : int\n        Number of CPUs to use in the computation.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    psds : ndarray, shape (..., n_freqs) or\n        The power spectral densities. All dimensions up to the last will\n        be the same as input.\n    freqs : ndarray, shape (n_freqs,)\n        The frequencies.\n\n    Notes\n    -----\n    .. versionadded:: 0.14.0\n    "
spectrogram = get_spectrogram()
dshape = x.shape[:(- 1)]
n_times = x.shape[(- 1)]
x = x.reshape((- 1), n_times)
(n_fft, n_per_seg, n_overlap) = _check_nfft(n_times, n_fft, n_per_seg, n_overlap)
win_size = (n_fft / float(sfreq))
utils.logger.info(('Effective window size : %0.3f (s)' % win_size))
tempResult = arange(((n_fft // 2) + 1), dtype=float)
	
===================================================================	
istft: 68	
----------------------------	

'ISTFT Inverse Short-Term Fourier Transform using a sine window.\n\n    Parameters\n    ----------\n    X : 3d array of shape [n_signals, wsize / 2 + 1,  n_step]\n        The STFT coefficients for positive frequencies\n    tstep : int\n        step between successive windows in samples (must be a multiple of 2,\n        a divider of wsize and smaller than wsize/2) (default: wsize/2)\n    Tx : int\n        Length of returned signal. If None Tx = n_step * tstep\n\n    Returns\n    -------\n    x : 1d array of length Tx\n        vector containing the inverse STFT signal\n\n    Examples\n    --------\n    x = istft(X)\n    x = istft(X, tstep)\n\n    See Also\n    --------\n    stft\n    '
(n_signals, n_win, n_step) = X.shape
if ((n_win % 2) == 0):
    ValueError('The number of rows of the STFT matrix must be odd.')
wsize = (2 * (n_win - 1))
if (tstep is None):
    tstep = (wsize / 2)
if (wsize % tstep):
    raise ValueError('The step size must be a divider of two times the number of rows of the STFT matrix minus two.')
if (wsize % 2):
    raise ValueError('The step size must be a multiple of 2.')
if (tstep > (wsize / 2)):
    raise ValueError('The step size must be smaller than the number of rows of the STFT matrix minus one.')
if (Tx is None):
    Tx = (n_step * tstep)
T = (n_step * tstep)
x = numpy.zeros((n_signals, ((T + wsize) - tstep)), dtype=numpy.float)
if (n_signals == 0):
    return x[:, :Tx]
tempResult = arange(0.5, (wsize + 0.5))
	
===================================================================	
stft: 32	
----------------------------	

'STFT Short-Term Fourier Transform using a sine window.\n\n    The transformation is designed to be a tight frame that can be\n    perfectly inverted. It only returns the positive frequencies.\n\n    Parameters\n    ----------\n    x : 2d array of size n_signals x T\n        containing multi-channels signal\n    wsize : int\n        length of the STFT window in samples (must be a multiple of 4)\n    tstep : int\n        step between successive windows in samples (must be a multiple of 2,\n        a divider of wsize and smaller than wsize/2) (default: wsize/2)\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    X : 3d array of shape [n_signals, wsize / 2 + 1, n_step]\n        STFT coefficients for positive frequencies with\n        n_step = ceil(T / tstep)\n\n    Examples\n    --------\n    X = stft(x, wsize)\n    X = stft(x, wsize, tstep)\n\n    See Also\n    --------\n    istft\n    stftfreq\n    '
if (not numpy.isrealobj(x)):
    raise ValueError('x is not a real valued array')
if (x.ndim == 1):
    x = x[None, :]
(n_signals, T) = x.shape
wsize = int(wsize)
if (wsize % 4):
    raise ValueError('The window length must be a multiple of 4.')
if (tstep is None):
    tstep = (wsize / 2)
tstep = int(tstep)
if ((wsize % tstep) or (tstep % 2)):
    raise ValueError('The step size must be a multiple of 2 and a divider of the window length.')
if (tstep > (wsize / 2)):
    raise ValueError('The step size must be smaller than half the window length.')
n_step = int(ceil((T / float(tstep))))
n_freq = ((wsize // 2) + 1)
utils.logger.info(('Number of frequencies: %d' % n_freq))
utils.logger.info(('Number of time steps: %d' % n_step))
X = numpy.zeros((n_signals, n_freq, n_step), dtype=numpy.complex)
if (n_signals == 0):
    return X
tempResult = arange(0.5, (wsize + 0.5))
	
===================================================================	
_make_dpss: 67	
----------------------------	

'Compute DPSS tapers for the given frequency range.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sampling frequency.\n    freqs : ndarray, shape (n_freqs,)\n        The frequencies in Hz.\n    n_cycles : float | ndarray, shape (n_freqs,), defaults to 7.\n        The number of cycles globally or for each frequency.\n    time_bandwidth : float, defaults to 4.0\n        Time x Bandwidth product.\n        The number of good tapers (low-bias) is chosen automatically based on\n        this to equal floor(time_bandwidth - 1).\n        Default is 4.0, giving 3 good tapers.\n    zero_mean : bool | None, , defaults to False\n        Make sure the wavelet has a mean of zero.\n\n\n    Returns\n    -------\n    Ws : list of array\n        The wavelets time series.\n    '
Ws = list()
if (time_bandwidth < 2.0):
    raise ValueError('time_bandwidth should be >= 2.0 for good tapers')
n_taps = int(numpy.floor((time_bandwidth - 1)))
n_cycles = numpy.atleast_1d(n_cycles)
if ((n_cycles.size != 1) and (n_cycles.size != len(freqs))):
    raise ValueError('n_cycles should be fixed or defined for each frequency.')
for m in range(n_taps):
    Wm = list()
    for (k, f) in enumerate(freqs):
        if (len(n_cycles) != 1):
            this_n_cycles = n_cycles[k]
        else:
            this_n_cycles = n_cycles[0]
        t_win = (this_n_cycles / float(f))
        tempResult = arange(0.0, t_win, (1.0 / sfreq))
	
===================================================================	
morlet: 38	
----------------------------	

'Compute Morlet wavelets for the given frequency range.\n\n    Parameters\n    ----------\n    sfreq : float\n        The sampling Frequency.\n    freqs : array\n        frequency range of interest (1 x Frequencies)\n    n_cycles: float | array of float, defaults to 7.0\n        Number of cycles. Fixed number or one per frequency.\n    sigma : float, defaults to None\n        It controls the width of the wavelet ie its temporal\n        resolution. If sigma is None the temporal resolution\n        is adapted with the frequency like for all wavelet transform.\n        The higher the frequency the shorter is the wavelet.\n        If sigma is fixed the temporal resolution is fixed\n        like for the short time Fourier transform and the number\n        of oscillations increases with the frequency.\n    zero_mean : bool, defaults to False\n        Make sure the wavelet has a mean of zero.\n\n    Returns\n    -------\n    Ws : list of array\n        The wavelets time series.\n    '
Ws = list()
n_cycles = numpy.atleast_1d(n_cycles)
if ((n_cycles.size != 1) and (n_cycles.size != len(freqs))):
    raise ValueError('n_cycles should be fixed or defined for each frequency.')
for (k, f) in enumerate(freqs):
    if (len(n_cycles) != 1):
        this_n_cycles = n_cycles[k]
    else:
        this_n_cycles = n_cycles[0]
    if (sigma is None):
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * f))
    else:
        sigma_t = (this_n_cycles / ((2.0 * numpy.pi) * sigma))
    tempResult = arange(0.0, (5.0 * sigma_t), (1.0 / sfreq))
	
===================================================================	
_prepare_picks: 581	
----------------------------	

'Prepare the picks.'
if (picks is None):
    picks = pick_types(info, meg=True, eeg=True, ref_meg=False, exclude='bads')
tempResult = arange(len(data))
	
===================================================================	
_precompute_st_windows: 35	
----------------------------	

'Precompute stockwell gausian windows (in the freq domain).'
tw = (scipy.fftpack.fftfreq(n_samp, (1.0 / sfreq)) / n_samp)
tw = numpy.r_[(tw[:1], tw[1:][::(- 1)])]
k = width
tempResult = arange(start_f, stop_f, 1)
	
===================================================================	
test_stft: 13	
----------------------------	

'Test stft and istft tight frame property'
sfreq = 1000.0
f = 7.0
for T in [253, 256]:
    tempResult = arange(T)
	
===================================================================	
test_stockwell_core: 42	
----------------------------	

'Test stockwell transform.'
sfreq = 1000.0
dur = 0.5
(onset, offset) = (0.175, 0.275)
n_samp = int((sfreq * dur))
tempResult = arange(n_samp)
	
===================================================================	
test_time_frequency: 44	
----------------------------	

'Test the to-be-deprecated time-frequency transform (PSD and ITC).'
event_id = 1
tmin = (- 0.2)
tmax = 0.498
raw = read_raw_fif(raw_fname)
events = read_events(event_fname)
include = []
exclude = (raw.info['bads'] + ['MEG 2443', 'EEG 053'])
picks = pick_types(raw.info, meg='grad', eeg=False, stim=False, include=include, exclude=exclude)
picks = picks[:2]
epochs = Epochs(raw, events, event_id, tmin, tmax, picks=picks)
data = epochs.get_data()
times = epochs.times
nave = len(data)
epochs_nopicks = Epochs(raw, events, event_id, tmin, tmax)
tempResult = arange(6, 20, 5)
	
===================================================================	
test_tfr_multitaper: 169	
----------------------------	

'Test tfr_multitaper.'
sfreq = 200.0
ch_names = ['SIM0001', 'SIM0002']
ch_types = ['grad', 'grad']
info = create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)
n_times = int(sfreq)
n_epochs = 3
seed = 42
rng = numpy.random.RandomState(seed)
noise = (0.1 * rng.randn(n_epochs, len(ch_names), n_times))
tempResult = arange(n_times, dtype=numpy.float)
	
===================================================================	
test_tfr_multitaper: 182	
----------------------------	

'Test tfr_multitaper.'
sfreq = 200.0
ch_names = ['SIM0001', 'SIM0002']
ch_types = ['grad', 'grad']
info = create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)
n_times = int(sfreq)
n_epochs = 3
seed = 42
rng = numpy.random.RandomState(seed)
noise = (0.1 * rng.randn(n_epochs, len(ch_names), n_times))
t = (numpy.arange(n_times, dtype=numpy.float) / sfreq)
signal = numpy.sin((((numpy.pi * 2.0) * 50.0) * t))
signal[numpy.logical_or((t < 0.45), (t > 0.55))] = 0.0
on_time = numpy.logical_and((t >= 0.45), (t <= 0.55))
signal[on_time] *= numpy.hanning(on_time.sum())
dat = (noise + signal)
reject = dict(grad=4000.0)
events = numpy.empty((n_epochs, 3), int)
first_event_sample = 100
event_id = dict(sin50hz=1)
for k in range(n_epochs):
    events[k, :] = ((first_event_sample + (k * n_times)), 0, event_id['sin50hz'])
epochs = EpochsArray(data=dat, info=info, events=events, event_id=event_id, reject=reject)
tempResult = arange(35, 70, 5, dtype=numpy.float)
	
===================================================================	
test_tfr_multitaper: 185	
----------------------------	

'Test tfr_multitaper.'
sfreq = 200.0
ch_names = ['SIM0001', 'SIM0002']
ch_types = ['grad', 'grad']
info = create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)
n_times = int(sfreq)
n_epochs = 3
seed = 42
rng = numpy.random.RandomState(seed)
noise = (0.1 * rng.randn(n_epochs, len(ch_names), n_times))
t = (numpy.arange(n_times, dtype=numpy.float) / sfreq)
signal = numpy.sin((((numpy.pi * 2.0) * 50.0) * t))
signal[numpy.logical_or((t < 0.45), (t > 0.55))] = 0.0
on_time = numpy.logical_and((t >= 0.45), (t <= 0.55))
signal[on_time] *= numpy.hanning(on_time.sum())
dat = (noise + signal)
reject = dict(grad=4000.0)
events = numpy.empty((n_epochs, 3), int)
first_event_sample = 100
event_id = dict(sin50hz=1)
for k in range(n_epochs):
    events[k, :] = ((first_event_sample + (k * n_times)), 0, event_id['sin50hz'])
epochs = EpochsArray(data=dat, info=info, events=events, event_id=event_id, reject=reject)
freqs = numpy.arange(35, 70, 5, dtype=numpy.float)
(power, itc) = tfr_multitaper(epochs, freqs=freqs, n_cycles=(freqs / 2.0), time_bandwidth=4.0)
(power2, itc2) = tfr_multitaper(epochs, freqs=freqs, n_cycles=(freqs / 2.0), time_bandwidth=4.0, decim=slice(0, 2))
tempResult = arange(len(ch_names))
	
===================================================================	
test_dpsswavelet: 151	
----------------------------	

'Test DPSS tapers.'
tempResult = arange(5, 25, 3)
	
===================================================================	
test_compute_tfr: 337	
----------------------------	

'Test _compute_tfr function.'
event_id = 1
tmin = (- 0.2)
tmax = 0.498
raw = read_raw_fif(raw_fname)
events = read_events(event_fname)
exclude = (raw.info['bads'] + ['MEG 2443', 'EEG 053'])
picks = pick_types(raw.info, meg='grad', eeg=False, stim=False, include=[], exclude=exclude)
picks = picks[:2]
epochs = Epochs(raw, events, event_id, tmin, tmax, picks=picks)
data = epochs.get_data()
sfreq = epochs.info['sfreq']
tempResult = arange(10, 20, 3)
	
===================================================================	
test_compute_tfr: 370	
----------------------------	

'Test _compute_tfr function.'
event_id = 1
tmin = (- 0.2)
tmax = 0.498
raw = read_raw_fif(raw_fname)
events = read_events(event_fname)
exclude = (raw.info['bads'] + ['MEG 2443', 'EEG 053'])
picks = pick_types(raw.info, meg='grad', eeg=False, stim=False, include=[], exclude=exclude)
picks = picks[:2]
epochs = Epochs(raw, events, event_id, tmin, tmax, picks=picks)
data = epochs.get_data()
sfreq = epochs.info['sfreq']
freqs = np.arange(10, 20, 3).astype(float)
for (func, use_fft, zero_mean, output) in product((tfr_array_multitaper, tfr_array_morlet), (False, True), (False, True), ('complex', 'power', 'phase', 'avg_power_itc', 'avg_power', 'itc')):
    if ((func == tfr_array_multitaper) and (output == 'phase')):
        assert_raises(NotImplementedError, func, data, sfreq=sfreq, freqs=freqs, output=output)
        continue
    out = func(data, sfreq=sfreq, freqs=freqs, use_fft=use_fft, zero_mean=zero_mean, n_cycles=2.0, output=output)
    shape = numpy.r_[(data.shape[:2], len(freqs), data.shape[2])]
    if (('avg' in output) or ('itc' in output)):
        assert_array_equal(shape[1:], out.shape)
    else:
        assert_array_equal(shape, out.shape)
    if (output in ('complex', 'avg_power_itc')):
        assert_equal(numpy.complex, out.dtype)
    else:
        assert_equal(numpy.float, out.dtype)
    assert_true(numpy.all(numpy.isfinite(out)))
for _data in (None, 'foo', data[0]):
    assert_raises(ValueError, _compute_tfr, _data, freqs, sfreq)
for _freqs in (None, 'foo', [[0]]):
    assert_raises(ValueError, _compute_tfr, data, _freqs, sfreq)
for _sfreq in (None, 'foo'):
    assert_raises(ValueError, _compute_tfr, data, freqs, _sfreq)
for key in ('output', 'method', 'use_fft', 'decim', 'n_jobs'):
    for value in (None, 'foo'):
        kwargs = {key: value}
        assert_raises(ValueError, _compute_tfr, data, freqs, sfreq, **kwargs)
assert_raises(ValueError, _compute_tfr, data, freqs, sfreq, method='morlet', time_bandwidth=1)
assert_raises(NotImplementedError, _compute_tfr, data, freqs, sfreq, method='multitaper', output='phase')
out = _compute_tfr(data, freqs, sfreq, output='itc', n_cycles=2.0)
assert_true((numpy.sum((out >= 1)) == 0))
assert_true((numpy.sum((out <= 0)) == 0))
for decim in (2, 3, 8, 9, slice(0, 2), slice(1, 3), slice(2, 4)):
    _decim = (slice(None, None, decim) if isinstance(decim, int) else decim)
    tempResult = arange(data.shape[2])
	
===================================================================	
_plot_onkey: 958	
----------------------------	

'Handle key presses.'
import matplotlib.pyplot as plt
if (event.key == 'down'):
    if params['butterfly']:
        return
    params['ch_start'] += params['n_channels']
    _channels_changed(params, len(params['ch_names']))
elif (event.key == 'up'):
    if params['butterfly']:
        return
    params['ch_start'] -= params['n_channels']
    _channels_changed(params, len(params['ch_names']))
elif (event.key == 'left'):
    sample = (params['t_start'] - params['duration'])
    sample = numpy.max([0, sample])
    _plot_window(sample, params)
elif (event.key == 'right'):
    sample = (params['t_start'] + params['duration'])
    sample = numpy.min([sample, (params['times'][(- 1)] - params['duration'])])
    times = params['epoch_times']
    xdata = times.flat[np.abs((times - sample)).argmin()]
    _plot_window(xdata, params)
elif (event.key == '-'):
    if params['butterfly']:
        params['butterfly_scale'] /= 1.1
    else:
        params['scale_factor'] /= 1.1
    params['plot_fun']()
elif (event.key in ['+', '=']):
    if params['butterfly']:
        params['butterfly_scale'] *= 1.1
    else:
        params['scale_factor'] *= 1.1
    params['plot_fun']()
elif (event.key == 'f11'):
    mng = matplotlib.pyplot.get_current_fig_manager()
    mng.full_screen_toggle()
elif (event.key == 'pagedown'):
    if ((params['n_channels'] == 1) or params['butterfly']):
        return
    n_channels = (params['n_channels'] - 1)
    ylim = params['ax'].get_ylim()
    offset = (ylim[0] / n_channels)
    tempResult = arange(n_channels)
	
===================================================================	
_plot_onkey: 972	
----------------------------	

'Handle key presses.'
import matplotlib.pyplot as plt
if (event.key == 'down'):
    if params['butterfly']:
        return
    params['ch_start'] += params['n_channels']
    _channels_changed(params, len(params['ch_names']))
elif (event.key == 'up'):
    if params['butterfly']:
        return
    params['ch_start'] -= params['n_channels']
    _channels_changed(params, len(params['ch_names']))
elif (event.key == 'left'):
    sample = (params['t_start'] - params['duration'])
    sample = numpy.max([0, sample])
    _plot_window(sample, params)
elif (event.key == 'right'):
    sample = (params['t_start'] + params['duration'])
    sample = numpy.min([sample, (params['times'][(- 1)] - params['duration'])])
    times = params['epoch_times']
    xdata = times.flat[np.abs((times - sample)).argmin()]
    _plot_window(xdata, params)
elif (event.key == '-'):
    if params['butterfly']:
        params['butterfly_scale'] /= 1.1
    else:
        params['scale_factor'] /= 1.1
    params['plot_fun']()
elif (event.key in ['+', '=']):
    if params['butterfly']:
        params['butterfly_scale'] *= 1.1
    else:
        params['scale_factor'] *= 1.1
    params['plot_fun']()
elif (event.key == 'f11'):
    mng = matplotlib.pyplot.get_current_fig_manager()
    mng.full_screen_toggle()
elif (event.key == 'pagedown'):
    if ((params['n_channels'] == 1) or params['butterfly']):
        return
    n_channels = (params['n_channels'] - 1)
    ylim = params['ax'].get_ylim()
    offset = (ylim[0] / n_channels)
    params['offsets'] = ((numpy.arange(n_channels) * offset) + (offset / 2.0))
    params['n_channels'] = n_channels
    params['ax'].collections.pop()
    params['ax'].set_yticks(params['offsets'])
    params['lines'].pop()
    params['vsel_patch'].set_height(n_channels)
    params['plot_fun']()
elif (event.key == 'pageup'):
    if params['butterfly']:
        return
    from matplotlib.collections import LineCollection
    n_channels = (params['n_channels'] + 1)
    ylim = params['ax'].get_ylim()
    offset = (ylim[0] / n_channels)
    tempResult = arange(n_channels)
	
===================================================================	
_plot_epochs_image: 256	
----------------------------	

'Plot epochs image. Helper function for plot_epochs_image.'
if (cmap is None):
    cmap = ('Reds' if (data.min() >= 0) else 'RdBu_r')
ax = axes_dict['image']
fig = ax.get_figure()
cmap = _setup_cmap(cmap)
n_epochs = len(data)
extent = [(1000.0 * epochs.times[0]), (1000.0 * epochs.times[(- 1)]), 0, n_epochs]
im = ax.imshow(data, vmin=vmin, vmax=vmax, cmap=cmap[0], aspect='auto', origin='lower', interpolation='nearest', extent=extent)
if (overlay_times is not None):
    tempResult = arange(n_epochs)
	
===================================================================	
_update_channels_epochs: 1122	
----------------------------	

'Change the amount of channels and epochs per view.'
from matplotlib.collections import LineCollection
n_channels = int(numpy.around(params['channel_slider'].val))
offset = (params['ax'].get_ylim()[0] / n_channels)
tempResult = arange(n_channels)
	
===================================================================	
_prepare_mne_browse_epochs: 541	
----------------------------	

'Set up the mne_browse_epochs window.'
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.collections import LineCollection
from matplotlib.colors import colorConverter
epochs = params['epochs']
if (picks is None):
    picks = _handle_picks(epochs)
if (len(picks) < 1):
    raise RuntimeError('No appropriate channels found. Please check your picks')
picks = sorted(picks)
inds = list()
types = list()
for t in ['grad', 'mag']:
    idxs = pick_types(params['info'], meg=t, ref_meg=False, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
for t in ['hbo', 'hbr']:
    idxs = pick_types(params['info'], meg=False, ref_meg=False, fnirs=t, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
pick_kwargs = dict(meg=False, ref_meg=False, exclude=[])
if (order is None):
    order = ['eeg', 'seeg', 'ecog', 'eog', 'ecg', 'emg', 'ref_meg', 'stim', 'resp', 'misc', 'chpi', 'syst', 'ias', 'exci']
for ch_type in order:
    pick_kwargs[ch_type] = True
    idxs = pick_types(params['info'], **pick_kwargs)
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([ch_type] * len(inds[(- 1)]))
    pick_kwargs[ch_type] = False
inds = np.concatenate(inds).astype(int)
if (not (len(inds) == len(picks))):
    raise RuntimeError('Some channels not classified. Please check your picks')
ch_names = [params['info']['ch_names'][x] for x in inds]
size = get_config('MNE_BROWSE_RAW_SIZE')
n_epochs = min(n_epochs, len(epochs.events))
duration = (len(epochs.times) * n_epochs)
n_channels = min(n_channels, len(picks))
if (size is not None):
    size = size.split(',')
    size = tuple((float(s) for s in size))
if (title is None):
    title = epochs._name
    if ((title is None) or (len(title) == 0)):
        title = ''
fig = figure_nobar(facecolor='w', figsize=size, dpi=80)
fig.canvas.set_window_title('mne_browse_epochs')
ax = matplotlib.pyplot.subplot2grid((10, 15), (0, 1), colspan=13, rowspan=9)
ax.annotate(title, xy=(0.5, 1), xytext=(0, (ax.get_ylim()[1] + 15)), ha='center', va='bottom', size=12, xycoords='axes fraction', textcoords='offset points')
color = _handle_default('color', None)
ax.axis([0, duration, 0, 200])
ax2 = ax.twiny()
ax2.set_zorder((- 1))
ax2.axis([0, duration, 0, 200])
ax_hscroll = matplotlib.pyplot.subplot2grid((10, 15), (9, 1), colspan=13)
ax_hscroll.get_yaxis().set_visible(False)
ax_hscroll.set_xlabel('Epochs')
ax_vscroll = matplotlib.pyplot.subplot2grid((10, 15), (0, 14), rowspan=9)
ax_vscroll.set_axis_off()
ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, 0), 1, len(picks), facecolor='w', zorder=3))
ax_help_button = matplotlib.pyplot.subplot2grid((10, 15), (9, 0), colspan=1)
help_button = matplotlib.widgets.Button(ax_help_button, 'Help')
help_button.on_clicked(partial(_onclick_help, params=params))
for ci in range(len(picks)):
    if (ch_names[ci] in params['info']['bads']):
        this_color = params['bad_color']
    else:
        this_color = color[types[ci]]
    ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, ci), 1, 1, facecolor=this_color, edgecolor=this_color, zorder=4))
vsel_patch = matplotlib.patches.Rectangle((0, 0), 1, n_channels, alpha=0.5, edgecolor='w', facecolor='w', zorder=5)
ax_vscroll.add_patch(vsel_patch)
ax_vscroll.set_ylim(len(types), 0)
ax_vscroll.set_title('Ch.')
type_colors = [matplotlib.colors.colorConverter.to_rgba(color[c]) for c in types]
colors = list()
for color_idx in range(len(type_colors)):
    colors.append(([type_colors[color_idx]] * len(epochs.events)))
lines = list()
n_times = len(epochs.times)
for ch_idx in range(n_channels):
    if ((len(colors) - 1) < ch_idx):
        break
    lc = LineCollection(list(), antialiased=True, linewidths=0.5, zorder=3, picker=3.0)
    ax.add_collection(lc)
    lines.append(lc)
times = epochs.times
data = numpy.zeros((params['info']['nchan'], (len(times) * n_epochs)))
ylim = (25.0, 0.0)
offset = (ylim[0] / n_channels)
tempResult = arange(n_channels)
	
===================================================================	
_prepare_mne_browse_epochs: 542	
----------------------------	

'Set up the mne_browse_epochs window.'
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.collections import LineCollection
from matplotlib.colors import colorConverter
epochs = params['epochs']
if (picks is None):
    picks = _handle_picks(epochs)
if (len(picks) < 1):
    raise RuntimeError('No appropriate channels found. Please check your picks')
picks = sorted(picks)
inds = list()
types = list()
for t in ['grad', 'mag']:
    idxs = pick_types(params['info'], meg=t, ref_meg=False, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
for t in ['hbo', 'hbr']:
    idxs = pick_types(params['info'], meg=False, ref_meg=False, fnirs=t, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
pick_kwargs = dict(meg=False, ref_meg=False, exclude=[])
if (order is None):
    order = ['eeg', 'seeg', 'ecog', 'eog', 'ecg', 'emg', 'ref_meg', 'stim', 'resp', 'misc', 'chpi', 'syst', 'ias', 'exci']
for ch_type in order:
    pick_kwargs[ch_type] = True
    idxs = pick_types(params['info'], **pick_kwargs)
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([ch_type] * len(inds[(- 1)]))
    pick_kwargs[ch_type] = False
inds = np.concatenate(inds).astype(int)
if (not (len(inds) == len(picks))):
    raise RuntimeError('Some channels not classified. Please check your picks')
ch_names = [params['info']['ch_names'][x] for x in inds]
size = get_config('MNE_BROWSE_RAW_SIZE')
n_epochs = min(n_epochs, len(epochs.events))
duration = (len(epochs.times) * n_epochs)
n_channels = min(n_channels, len(picks))
if (size is not None):
    size = size.split(',')
    size = tuple((float(s) for s in size))
if (title is None):
    title = epochs._name
    if ((title is None) or (len(title) == 0)):
        title = ''
fig = figure_nobar(facecolor='w', figsize=size, dpi=80)
fig.canvas.set_window_title('mne_browse_epochs')
ax = matplotlib.pyplot.subplot2grid((10, 15), (0, 1), colspan=13, rowspan=9)
ax.annotate(title, xy=(0.5, 1), xytext=(0, (ax.get_ylim()[1] + 15)), ha='center', va='bottom', size=12, xycoords='axes fraction', textcoords='offset points')
color = _handle_default('color', None)
ax.axis([0, duration, 0, 200])
ax2 = ax.twiny()
ax2.set_zorder((- 1))
ax2.axis([0, duration, 0, 200])
ax_hscroll = matplotlib.pyplot.subplot2grid((10, 15), (9, 1), colspan=13)
ax_hscroll.get_yaxis().set_visible(False)
ax_hscroll.set_xlabel('Epochs')
ax_vscroll = matplotlib.pyplot.subplot2grid((10, 15), (0, 14), rowspan=9)
ax_vscroll.set_axis_off()
ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, 0), 1, len(picks), facecolor='w', zorder=3))
ax_help_button = matplotlib.pyplot.subplot2grid((10, 15), (9, 0), colspan=1)
help_button = matplotlib.widgets.Button(ax_help_button, 'Help')
help_button.on_clicked(partial(_onclick_help, params=params))
for ci in range(len(picks)):
    if (ch_names[ci] in params['info']['bads']):
        this_color = params['bad_color']
    else:
        this_color = color[types[ci]]
    ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, ci), 1, 1, facecolor=this_color, edgecolor=this_color, zorder=4))
vsel_patch = matplotlib.patches.Rectangle((0, 0), 1, n_channels, alpha=0.5, edgecolor='w', facecolor='w', zorder=5)
ax_vscroll.add_patch(vsel_patch)
ax_vscroll.set_ylim(len(types), 0)
ax_vscroll.set_title('Ch.')
type_colors = [matplotlib.colors.colorConverter.to_rgba(color[c]) for c in types]
colors = list()
for color_idx in range(len(type_colors)):
    colors.append(([type_colors[color_idx]] * len(epochs.events)))
lines = list()
n_times = len(epochs.times)
for ch_idx in range(n_channels):
    if ((len(colors) - 1) < ch_idx):
        break
    lc = LineCollection(list(), antialiased=True, linewidths=0.5, zorder=3, picker=3.0)
    ax.add_collection(lc)
    lines.append(lc)
times = epochs.times
data = numpy.zeros((params['info']['nchan'], (len(times) * n_epochs)))
ylim = (25.0, 0.0)
offset = (ylim[0] / n_channels)
offsets = ((numpy.arange(n_channels) * offset) + (offset / 2.0))
tempResult = arange((len(times) * len(epochs.events)))
	
===================================================================	
_prepare_mne_browse_epochs: 543	
----------------------------	

'Set up the mne_browse_epochs window.'
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.collections import LineCollection
from matplotlib.colors import colorConverter
epochs = params['epochs']
if (picks is None):
    picks = _handle_picks(epochs)
if (len(picks) < 1):
    raise RuntimeError('No appropriate channels found. Please check your picks')
picks = sorted(picks)
inds = list()
types = list()
for t in ['grad', 'mag']:
    idxs = pick_types(params['info'], meg=t, ref_meg=False, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
for t in ['hbo', 'hbr']:
    idxs = pick_types(params['info'], meg=False, ref_meg=False, fnirs=t, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
pick_kwargs = dict(meg=False, ref_meg=False, exclude=[])
if (order is None):
    order = ['eeg', 'seeg', 'ecog', 'eog', 'ecg', 'emg', 'ref_meg', 'stim', 'resp', 'misc', 'chpi', 'syst', 'ias', 'exci']
for ch_type in order:
    pick_kwargs[ch_type] = True
    idxs = pick_types(params['info'], **pick_kwargs)
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([ch_type] * len(inds[(- 1)]))
    pick_kwargs[ch_type] = False
inds = np.concatenate(inds).astype(int)
if (not (len(inds) == len(picks))):
    raise RuntimeError('Some channels not classified. Please check your picks')
ch_names = [params['info']['ch_names'][x] for x in inds]
size = get_config('MNE_BROWSE_RAW_SIZE')
n_epochs = min(n_epochs, len(epochs.events))
duration = (len(epochs.times) * n_epochs)
n_channels = min(n_channels, len(picks))
if (size is not None):
    size = size.split(',')
    size = tuple((float(s) for s in size))
if (title is None):
    title = epochs._name
    if ((title is None) or (len(title) == 0)):
        title = ''
fig = figure_nobar(facecolor='w', figsize=size, dpi=80)
fig.canvas.set_window_title('mne_browse_epochs')
ax = matplotlib.pyplot.subplot2grid((10, 15), (0, 1), colspan=13, rowspan=9)
ax.annotate(title, xy=(0.5, 1), xytext=(0, (ax.get_ylim()[1] + 15)), ha='center', va='bottom', size=12, xycoords='axes fraction', textcoords='offset points')
color = _handle_default('color', None)
ax.axis([0, duration, 0, 200])
ax2 = ax.twiny()
ax2.set_zorder((- 1))
ax2.axis([0, duration, 0, 200])
ax_hscroll = matplotlib.pyplot.subplot2grid((10, 15), (9, 1), colspan=13)
ax_hscroll.get_yaxis().set_visible(False)
ax_hscroll.set_xlabel('Epochs')
ax_vscroll = matplotlib.pyplot.subplot2grid((10, 15), (0, 14), rowspan=9)
ax_vscroll.set_axis_off()
ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, 0), 1, len(picks), facecolor='w', zorder=3))
ax_help_button = matplotlib.pyplot.subplot2grid((10, 15), (9, 0), colspan=1)
help_button = matplotlib.widgets.Button(ax_help_button, 'Help')
help_button.on_clicked(partial(_onclick_help, params=params))
for ci in range(len(picks)):
    if (ch_names[ci] in params['info']['bads']):
        this_color = params['bad_color']
    else:
        this_color = color[types[ci]]
    ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, ci), 1, 1, facecolor=this_color, edgecolor=this_color, zorder=4))
vsel_patch = matplotlib.patches.Rectangle((0, 0), 1, n_channels, alpha=0.5, edgecolor='w', facecolor='w', zorder=5)
ax_vscroll.add_patch(vsel_patch)
ax_vscroll.set_ylim(len(types), 0)
ax_vscroll.set_title('Ch.')
type_colors = [matplotlib.colors.colorConverter.to_rgba(color[c]) for c in types]
colors = list()
for color_idx in range(len(type_colors)):
    colors.append(([type_colors[color_idx]] * len(epochs.events)))
lines = list()
n_times = len(epochs.times)
for ch_idx in range(n_channels):
    if ((len(colors) - 1) < ch_idx):
        break
    lc = LineCollection(list(), antialiased=True, linewidths=0.5, zorder=3, picker=3.0)
    ax.add_collection(lc)
    lines.append(lc)
times = epochs.times
data = numpy.zeros((params['info']['nchan'], (len(times) * n_epochs)))
ylim = (25.0, 0.0)
offset = (ylim[0] / n_channels)
offsets = ((numpy.arange(n_channels) * offset) + (offset / 2.0))
times = numpy.arange((len(times) * len(epochs.events)))
tempResult = arange(0, len(times), n_times)
	
===================================================================	
_prepare_mne_browse_epochs: 554	
----------------------------	

'Set up the mne_browse_epochs window.'
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib.collections import LineCollection
from matplotlib.colors import colorConverter
epochs = params['epochs']
if (picks is None):
    picks = _handle_picks(epochs)
if (len(picks) < 1):
    raise RuntimeError('No appropriate channels found. Please check your picks')
picks = sorted(picks)
inds = list()
types = list()
for t in ['grad', 'mag']:
    idxs = pick_types(params['info'], meg=t, ref_meg=False, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
for t in ['hbo', 'hbr']:
    idxs = pick_types(params['info'], meg=False, ref_meg=False, fnirs=t, exclude=[])
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([t] * len(inds[(- 1)]))
pick_kwargs = dict(meg=False, ref_meg=False, exclude=[])
if (order is None):
    order = ['eeg', 'seeg', 'ecog', 'eog', 'ecg', 'emg', 'ref_meg', 'stim', 'resp', 'misc', 'chpi', 'syst', 'ias', 'exci']
for ch_type in order:
    pick_kwargs[ch_type] = True
    idxs = pick_types(params['info'], **pick_kwargs)
    if (len(idxs) < 1):
        continue
    mask = numpy.in1d(idxs, picks, assume_unique=True)
    inds.append(idxs[mask])
    types += ([ch_type] * len(inds[(- 1)]))
    pick_kwargs[ch_type] = False
inds = np.concatenate(inds).astype(int)
if (not (len(inds) == len(picks))):
    raise RuntimeError('Some channels not classified. Please check your picks')
ch_names = [params['info']['ch_names'][x] for x in inds]
size = get_config('MNE_BROWSE_RAW_SIZE')
n_epochs = min(n_epochs, len(epochs.events))
duration = (len(epochs.times) * n_epochs)
n_channels = min(n_channels, len(picks))
if (size is not None):
    size = size.split(',')
    size = tuple((float(s) for s in size))
if (title is None):
    title = epochs._name
    if ((title is None) or (len(title) == 0)):
        title = ''
fig = figure_nobar(facecolor='w', figsize=size, dpi=80)
fig.canvas.set_window_title('mne_browse_epochs')
ax = matplotlib.pyplot.subplot2grid((10, 15), (0, 1), colspan=13, rowspan=9)
ax.annotate(title, xy=(0.5, 1), xytext=(0, (ax.get_ylim()[1] + 15)), ha='center', va='bottom', size=12, xycoords='axes fraction', textcoords='offset points')
color = _handle_default('color', None)
ax.axis([0, duration, 0, 200])
ax2 = ax.twiny()
ax2.set_zorder((- 1))
ax2.axis([0, duration, 0, 200])
ax_hscroll = matplotlib.pyplot.subplot2grid((10, 15), (9, 1), colspan=13)
ax_hscroll.get_yaxis().set_visible(False)
ax_hscroll.set_xlabel('Epochs')
ax_vscroll = matplotlib.pyplot.subplot2grid((10, 15), (0, 14), rowspan=9)
ax_vscroll.set_axis_off()
ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, 0), 1, len(picks), facecolor='w', zorder=3))
ax_help_button = matplotlib.pyplot.subplot2grid((10, 15), (9, 0), colspan=1)
help_button = matplotlib.widgets.Button(ax_help_button, 'Help')
help_button.on_clicked(partial(_onclick_help, params=params))
for ci in range(len(picks)):
    if (ch_names[ci] in params['info']['bads']):
        this_color = params['bad_color']
    else:
        this_color = color[types[ci]]
    ax_vscroll.add_patch(matplotlib.patches.Rectangle((0, ci), 1, 1, facecolor=this_color, edgecolor=this_color, zorder=4))
vsel_patch = matplotlib.patches.Rectangle((0, 0), 1, n_channels, alpha=0.5, edgecolor='w', facecolor='w', zorder=5)
ax_vscroll.add_patch(vsel_patch)
ax_vscroll.set_ylim(len(types), 0)
ax_vscroll.set_title('Ch.')
type_colors = [matplotlib.colors.colorConverter.to_rgba(color[c]) for c in types]
colors = list()
for color_idx in range(len(type_colors)):
    colors.append(([type_colors[color_idx]] * len(epochs.events)))
lines = list()
n_times = len(epochs.times)
for ch_idx in range(n_channels):
    if ((len(colors) - 1) < ch_idx):
        break
    lc = LineCollection(list(), antialiased=True, linewidths=0.5, zorder=3, picker=3.0)
    ax.add_collection(lc)
    lines.append(lc)
times = epochs.times
data = numpy.zeros((params['info']['nchan'], (len(times) * n_epochs)))
ylim = (25.0, 0.0)
offset = (ylim[0] / n_channels)
offsets = ((numpy.arange(n_channels) * offset) + (offset / 2.0))
times = numpy.arange((len(times) * len(epochs.events)))
epoch_times = numpy.arange(0, len(times), n_times)
ax.set_yticks(offsets)
ax.set_ylim(ylim)
ticks = (epoch_times + (0.5 * n_times))
ax.set_xticks(ticks)
ax2.set_xticks(ticks[:n_epochs])
labels = list(range(1, (len(ticks) + 1)))
ax.set_xticklabels(labels)
xlim = (epoch_times[(- 1)] + len(epochs.times))
ax_hscroll.set_xlim(0, xlim)
vertline_t = ax_hscroll.text(0, 1, '', color='y', va='bottom', ha='right')
tempResult = arange(0, xlim, (xlim / 7.0))
	
===================================================================	
plot_drop_log: 299	
----------------------------	

'Show the channel stats based on a drop_log from Epochs.\n\n    Parameters\n    ----------\n    drop_log : list of lists\n        Epoch drop log from Epochs.drop_log.\n    threshold : float\n        The percentage threshold to use to decide whether or not to\n        plot. Default is zero (always plot).\n    n_max_plot : int\n        Maximum number of channels to show stats for.\n    subject : str\n        The subject name to use in the title of the plot.\n    color : tuple | str\n        Color to use for the bars.\n    width : float\n        Width of the bars.\n    ignore : list\n        The drop reasons to ignore.\n    show : bool\n        Show figure if True.\n\n    Returns\n    -------\n    fig : Instance of matplotlib.figure.Figure\n        The figure.\n    '
import matplotlib.pyplot as plt
from ..epochs import _drop_log_stats
perc = _drop_log_stats(drop_log, ignore)
scores = Counter([ch for d in drop_log for ch in d if (ch not in ignore)])
ch_names = numpy.array(list(scores.keys()))
fig = matplotlib.pyplot.figure()
if ((perc < threshold) or (len(ch_names) == 0)):
    matplotlib.pyplot.text(0, 0, 'No drops')
    return fig
n_used = 0
for d in drop_log:
    if ((len(d) == 0) or any(((ch not in ignore) for ch in d))):
        n_used += 1
counts = ((100 * numpy.array(list(scores.values()), dtype=float)) / n_used)
n_plot = min(n_max_plot, len(ch_names))
order = numpy.flipud(numpy.argsort(counts))
matplotlib.pyplot.title(('%s: %0.1f%%' % (subject, perc)))
tempResult = arange(n_plot)
	
===================================================================	
_prepare_butterfly: 1043	
----------------------------	

'Set up butterfly plot.'
from matplotlib.collections import LineCollection
butterfly = (not params['butterfly'])
if butterfly:
    types = (set(['grad', 'mag', 'eeg', 'eog', 'ecg']) & set(params['types']))
    if (len(types) < 1):
        return
    params['ax_vscroll'].set_visible(False)
    ax = params['ax']
    labels = ax.yaxis.get_ticklabels()
    for label in labels:
        label.set_visible(True)
    ylim = ((5.0 * len(types)), 0.0)
    ax.set_ylim(ylim)
    offset = (ylim[0] / (4.0 * len(types)))
    tempResult = arange(0, ylim[0], offset)
	
===================================================================	
_prepare_butterfly: 1090	
----------------------------	

'Set up butterfly plot.'
from matplotlib.collections import LineCollection
butterfly = (not params['butterfly'])
if butterfly:
    types = (set(['grad', 'mag', 'eeg', 'eog', 'ecg']) & set(params['types']))
    if (len(types) < 1):
        return
    params['ax_vscroll'].set_visible(False)
    ax = params['ax']
    labels = ax.yaxis.get_ticklabels()
    for label in labels:
        label.set_visible(True)
    ylim = ((5.0 * len(types)), 0.0)
    ax.set_ylim(ylim)
    offset = (ylim[0] / (4.0 * len(types)))
    ticks = numpy.arange(0, ylim[0], offset)
    ticks = [(ticks[x] if (x < len(ticks)) else 0) for x in range(20)]
    ax.set_yticks(ticks)
    used_types = 0
    params['offsets'] = [ticks[2]]
    if ('grad' in types):
        pos = (0, (1 - (ticks[2] / ylim[0])))
        params['ax2'].annotate('Grad (fT/cm)', xy=pos, xytext=((- 70), 0), ha='left', size=12, va='center', xycoords='axes fraction', rotation=90, textcoords='offset points')
        used_types += 1
    params['offsets'].append(ticks[(2 + (used_types * 4))])
    if ('mag' in types):
        pos = (0, (1 - (ticks[(2 + (used_types * 4))] / ylim[0])))
        params['ax2'].annotate('Mag (fT)', xy=pos, xytext=((- 70), 0), ha='left', size=12, va='center', xycoords='axes fraction', rotation=90, textcoords='offset points')
        used_types += 1
    params['offsets'].append(ticks[(2 + (used_types * 4))])
    if ('eeg' in types):
        pos = (0, (1 - (ticks[(2 + (used_types * 4))] / ylim[0])))
        params['ax2'].annotate('EEG (uV)', xy=pos, xytext=((- 70), 0), ha='left', size=12, va='center', xycoords='axes fraction', rotation=90, textcoords='offset points')
        used_types += 1
    params['offsets'].append(ticks[(2 + (used_types * 4))])
    if ('eog' in types):
        pos = (0, (1 - (ticks[(2 + (used_types * 4))] / ylim[0])))
        params['ax2'].annotate('EOG (uV)', xy=pos, xytext=((- 70), 0), ha='left', size=12, va='center', xycoords='axes fraction', rotation=90, textcoords='offset points')
        used_types += 1
    params['offsets'].append(ticks[(2 + (used_types * 4))])
    if ('ecg' in types):
        pos = (0, (1 - (ticks[(2 + (used_types * 4))] / ylim[0])))
        params['ax2'].annotate('ECG (uV)', xy=pos, xytext=((- 70), 0), ha='left', size=12, va='center', xycoords='axes fraction', rotation=90, textcoords='offset points')
        used_types += 1
    while (len(params['lines']) < len(params['picks'])):
        lc = LineCollection(list(), antialiased=True, linewidths=0.5, zorder=3, picker=3.0)
        ax.add_collection(lc)
        params['lines'].append(lc)
else:
    labels = params['ax'].yaxis.get_ticklabels()
    for label in labels:
        label.set_visible(params['settings'][0])
    params['ax_vscroll'].set_visible(True)
    while (len(params['ax2'].texts) > 0):
        params['ax2'].texts.pop()
    n_channels = params['n_channels']
    while (len(params['lines']) > n_channels):
        params['ax'].collections.pop()
        params['lines'].pop()
    ylim = (25.0, 0.0)
    params['ax'].set_ylim(ylim)
    offset = (ylim[0] / n_channels)
    tempResult = arange(n_channels)
	
===================================================================	
plot_ica_scores: 225	
----------------------------	

"Plot scores related to detected components.\n\n    Use this function to asses how well your score describes outlier\n    sources and how well you were detecting them.\n\n    Parameters\n    ----------\n    ica : instance of mne.preprocessing.ICA\n        The ICA object.\n    scores : array_like of float, shape (n ica components) | list of arrays\n        Scores based on arbitrary metric to characterize ICA components.\n    exclude : array_like of int\n        The components marked for exclusion. If None (default), ICA.exclude\n        will be used.\n    labels : str | list | 'ecg' | 'eog' | None\n        The labels to consider for the axes tests. Defaults to None.\n        If list, should match the outer shape of `scores`.\n        If 'ecg' or 'eog', the ``labels_`` attributes will be looked up.\n        Note that '/' is used internally for sublabels specifying ECG and\n        EOG channels.\n    axhline : float\n        Draw horizontal line to e.g. visualize rejection threshold.\n    title : str\n        The figure title.\n    figsize : tuple of int | None\n        The figure size. If None it gets set automatically.\n    show : bool\n        Show figure if True.\n\n    Returns\n    -------\n    fig : instance of matplotlib.pyplot.Figure\n        The figure object\n    "
import matplotlib.pyplot as plt
tempResult = arange(ica.n_components_)
	
===================================================================	
_plot_ica_sources_evoked: 169	
----------------------------	

'Plot average over epochs in ICA space.\n\n    Parameters\n    ----------\n    evoked : instance of mne.Evoked\n        The Evoked to be used.\n    picks : int | array_like of int | None.\n        The components to be displayed. If None, plot will show the\n        sources in the order as fitted.\n    exclude : array_like of int\n        The components marked for exclusion. If None (default), ICA.exclude\n        will be used.\n    title : str\n        The figure title.\n    show : bool\n        Show figure if True.\n    labels : None | dict\n        The ICA labels attribute.\n    '
import matplotlib.pyplot as plt
if (title is None):
    title = 'Reconstructed latent sources, time-locked'
(fig, axes) = matplotlib.pyplot.subplots(1)
ax = axes
axes = [axes]
times = (evoked.times * 1000.0)
lines = list()
texts = list()
if (picks is None):
    tempResult = arange(evoked.data.shape[0])
	
===================================================================	
plot_source_spectrogram: 81	
----------------------------	

'Plot source power in time-freqency grid.\n\n    Parameters\n    ----------\n    stcs : list of SourceEstimate\n        Source power for consecutive time windows, one SourceEstimate object\n        should be provided for each frequency bin.\n    freq_bins : list of tuples of float\n        Start and end points of frequency bins of interest.\n    tmin : float\n        Minimum time instant to show.\n    tmax : float\n        Maximum time instant to show.\n    source_index : int | None\n        Index of source for which the spectrogram will be plotted. If None,\n        the source with the largest activation will be selected.\n    colorbar : bool\n        If true, a colorbar will be added to the plot.\n    show : bool\n        Show figure if True.\n    '
import matplotlib.pyplot as plt
if (len(stcs) == 0):
    raise ValueError('cannot plot spectrogram if len(stcs) == 0')
stc = stcs[0]
if ((tmin is not None) and (tmin < stc.times[0])):
    raise ValueError('tmin cannot be smaller than the first time point provided in stcs')
if ((tmax is not None) and (tmax > (stc.times[(- 1)] + stc.tstep))):
    raise ValueError('tmax cannot be larger than the sum of the last time point and the time step, which are provided in stcs')
if (tmin is None):
    tmin = stc.times[0]
if (tmax is None):
    tmax = (stc.times[(- 1)] + stc.tstep)
tempResult = arange(tmin, (tmax + stc.tstep), stc.tstep)
	
===================================================================	
plot_filter: 403	
----------------------------	

'Plot properties of a filter.\n\n    Parameters\n    ----------\n    h : dict or ndarray\n        An IIR dict or 1D ndarray of coefficients (for FIR filter).\n    sfreq : float\n        Sample rate of the data (Hz).\n    freq : array-like or None\n        The ideal response frequencies to plot (must be in ascending order).\n        If None (default), do not plot the ideal response.\n    gain : array-like or None\n        The ideal response gains to plot.\n        If None (default), do not plot the ideal response.\n    title : str | None\n        The title to use. If None (default), deteremine the title based\n        on the type of the system.\n    color : color object\n        The color to use (default \'#1f77b4\').\n    flim : tuple or None\n        If not None, the x-axis frequency limits (Hz) to use.\n        If None, freq will be used. If None (default) and freq is None,\n        ``(0.1, sfreq / 2.)`` will be used.\n    fscale : str\n        Frequency scaling to use, can be "log" (default) or "linear".\n    alim : tuple\n        The y-axis amplitude limits (dB) to use (default: (-60, 10)).\n    show : bool\n        Show figure if True (default).\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The figure containing the plots.\n\n    See Also\n    --------\n    mne.filter.create_filter\n    plot_ideal_filter\n\n    Notes\n    -----\n    .. versionadded:: 0.14\n    '
from scipy.signal import freqz, group_delay
import matplotlib.pyplot as plt
sfreq = float(sfreq)
_check_fscale(fscale)
flim = _get_flim(flim, fscale, freq, sfreq)
if (fscale == 'log'):
    omega = numpy.logspace(numpy.log10(flim[0]), numpy.log10(flim[1]), 1000)
else:
    omega = numpy.linspace(flim[0], flim[1], 1000)
omega /= (sfreq / (2 * numpy.pi))
if isinstance(h, dict):
    if ('sos' in h):
        from scipy.signal import sosfilt
        h = h['sos']
        H = numpy.ones(len(omega), numpy.complex128)
        gd = numpy.zeros(len(omega))
        for section in h:
            this_H = freqz(section[:3], section[3:], omega)[1]
            H *= this_H
            with warnings.catch_warnings(record=True):
                gd += group_delay((section[:3], section[3:]), omega)[1]
        n = estimate_ringing_samples(h)
        delta = numpy.zeros(n)
        delta[0] = 1
        h = sosfilt(h, delta)
    else:
        from scipy.signal import lfilter
        n = estimate_ringing_samples((h['b'], h['a']))
        delta = numpy.zeros(n)
        delta[0] = 1
        H = freqz(h['b'], h['a'], omega)[1]
        with warnings.catch_warnings(record=True):
            gd = group_delay((h['b'], h['a']), omega)[1]
        h = lfilter(h['b'], h['a'], delta)
    title = ('SOS (IIR) filter' if (title is None) else title)
else:
    H = freqz(h, worN=omega)[1]
    with warnings.catch_warnings(record=True):
        gd = group_delay((h, [1.0]), omega)[1]
    title = ('FIR filter' if (title is None) else title)
gd /= sfreq
(fig, axes) = matplotlib.pyplot.subplots(3)
tempResult = arange(len(h))
	
===================================================================	
plot_events: 269	
----------------------------	

"Plot events to get a visual display of the paradigm.\n\n    Parameters\n    ----------\n    events : array, shape (n_events, 3)\n        The events.\n    sfreq : float | None\n        The sample frequency. If None, data will be displayed in samples (not\n        seconds).\n    first_samp : int\n        The index of the first sample. Typically the raw.first_samp\n        attribute. It is needed for recordings on a Neuromag\n        system as the events are defined relative to the system\n        start and not to the beginning of the recording.\n    color : dict | None\n        Dictionary of event_id value and its associated color. If None,\n        colors are automatically drawn from a default list (cycled through if\n        number of events longer than list of default colors).\n    event_id : dict | None\n        Dictionary of event label (e.g. 'aud_l') and its associated\n        event_id value. Label used to plot a legend. If None, no legend is\n        drawn.\n    axes : instance of matplotlib.axes.AxesSubplot\n       The subplot handle.\n    equal_spacing : bool\n        Use equal spacing between events in y-axis.\n    show : bool\n        Show figure if True.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        The figure object containing the plot.\n\n    Notes\n    -----\n    .. versionadded:: 0.9.0\n    "
if (sfreq is None):
    sfreq = 1.0
    xlabel = 'samples'
else:
    xlabel = 'Time (s)'
events = numpy.asarray(events)
unique_events = numpy.unique(events[:, 2])
if (event_id is not None):
    event_id_rev = dict(((v, k) for (k, v) in event_id.items()))
    (conditions, unique_events_id) = zip(*sorted(event_id.items(), key=(lambda x: x[1])))
    for this_event in unique_events_id:
        if (this_event not in unique_events):
            raise ValueError(('%s from event_id is not present in events.' % this_event))
    for this_event in unique_events:
        if (this_event not in unique_events_id):
            warn(('event %s missing from event_id will be ignored' % this_event))
else:
    unique_events_id = unique_events
color = _handle_event_colors(unique_events, color, unique_events_id)
import matplotlib.pyplot as plt
fig = None
if (axes is None):
    fig = matplotlib.pyplot.figure()
ax = (axes if axes else matplotlib.pyplot.gca())
unique_events_id = numpy.array(unique_events_id)
min_event = numpy.min(unique_events_id)
max_event = numpy.max(unique_events_id)
for (idx, ev) in enumerate(unique_events_id):
    ev_mask = (events[:, 2] == ev)
    kwargs = {}
    if (event_id is not None):
        event_label = '{0} ({1})'.format(event_id_rev[ev], numpy.sum(ev_mask))
        kwargs['label'] = event_label
    if (ev in color):
        kwargs['color'] = color[ev]
    if equal_spacing:
        ax.plot(((events[(ev_mask, 0)] - first_samp) / sfreq), ((idx + 1) * numpy.ones(ev_mask.sum())), '.', **kwargs)
    else:
        ax.plot(((events[(ev_mask, 0)] - first_samp) / sfreq), events[(ev_mask, 2)], '.', **kwargs)
if equal_spacing:
    ax.set_ylim(0, (unique_events_id.size + 1))
    tempResult = arange(unique_events_id.size)
	
===================================================================	
plot_montage: 38	
----------------------------	

"Plot a montage.\n\n    Parameters\n    ----------\n    montage : instance of Montage or DigMontage\n        The montage to visualize.\n    scale_factor : float\n        Determines the size of the points.\n    show_names : bool\n        Whether to show the channel names.\n    kind : str\n        Whether to plot the montage as '3d' or 'topomap' (default).\n    show : bool\n        Show figure if True.\n\n    Returns\n    -------\n    fig : Instance of matplotlib.figure.Figure\n        The figure object.\n    "
from scipy.spatial.distance import cdist
from ..channels import Montage, DigMontage
from .. import create_info
if isinstance(montage, Montage):
    ch_names = montage.ch_names
    title = montage.kind
elif isinstance(montage, DigMontage):
    ch_names = montage.point_names
    title = None
else:
    raise TypeError('montage must be an instance of mne.channels.montage.Montage ormne.channels.montage.DigMontage')
if (kind not in ['topomap', '3d']):
    raise ValueError("kind must be 'topomap' or '3d'")
if isinstance(montage, Montage):
    dists = cdist(montage.pos, montage.pos)
    dists[numpy.tril_indices(dists.shape[0])] = numpy.nan
    dupes = numpy.argwhere(numpy.isclose(dists, 0))
    if dupes.any():
        montage = deepcopy(montage)
        n_chans = montage.pos.shape[0]
        n_dupes = dupes.shape[0]
        idx = np.setdiff1d(montage.selection, dupes[:, 1]).tolist()
        utils.logger.info('{} duplicate electrode labels found:'.format(n_dupes))
        utils.logger.info(', '.join([((ch_names[d[0]] + '/') + ch_names[d[1]]) for d in dupes]))
        utils.logger.info('Plotting {} unique labels.'.format((n_chans - n_dupes)))
        montage.ch_names = [montage.ch_names[i] for i in idx]
        ch_names = montage.ch_names
        montage.pos = montage.pos[idx, :]
        tempResult = arange((n_chans - n_dupes))
	
===================================================================	
plot_raw: 147	
----------------------------	

'Plot raw data.\n\n    Parameters\n    ----------\n    raw : instance of Raw\n        The raw data to plot.\n    events : array | None\n        Events to show with vertical bars.\n    duration : float\n        Time window (sec) to plot. The lesser of this value and the duration\n        of the raw file will be used.\n    start : float\n        Initial time to show (can be changed dynamically once plotted). If\n        show_first_samp is True, then it is taken relative to\n        ``raw.first_samp``.\n    n_channels : int\n        Number of channels to plot at once. Defaults to 20. Has no effect if\n        ``order`` is \'position\', \'selection\' or \'butterfly\'.\n    bgcolor : color object\n        Color of the background.\n    color : dict | color object | None\n        Color for the data traces. If None, defaults to::\n\n            dict(mag=\'darkblue\', grad=\'b\', eeg=\'k\', eog=\'k\', ecg=\'m\',\n                 emg=\'k\', ref_meg=\'steelblue\', misc=\'k\', stim=\'k\',\n                 resp=\'k\', chpi=\'k\')\n\n    bad_color : color object\n        Color to make bad channels.\n    event_color : color object | dict\n        Color to use for events. Can also be a dict with\n        ``{event_number: color}`` pairings. Use ``event_number==-1`` for\n        any event numbers in the events list that are not in the dictionary.\n    scalings : dict | None\n        Scaling factors for the traces. If any fields in scalings are \'auto\',\n        the scaling factor is set to match the 99.5th percentile of a subset of\n        the corresponding data. If scalings == \'auto\', all scalings fields are\n        set to \'auto\'. If any fields are \'auto\' and data is not preloaded, a\n        subset of times up to 100mb will be loaded. If None, defaults to::\n\n            dict(mag=1e-12, grad=4e-11, eeg=20e-6, eog=150e-6, ecg=5e-4,\n                 emg=1e-3, ref_meg=1e-12, misc=1e-3, stim=1,\n                 resp=1, chpi=1e-4)\n\n    remove_dc : bool\n        If True remove DC component when plotting data.\n    order : array of int | None\n        Order in which to plot data. If the array is shorter than the number of\n        channels, only the given channels are plotted. If None (default), all\n        channels are plotted. If ``group_by`` is ``\'position\'`` or\n        ``\'selection\'``, the ``order`` parameter is used only for selecting the\n        channels to be plotted.\n    show_options : bool\n        If True, a dialog for options related to projection is shown.\n    title : str | None\n        The title of the window. If None, and either the filename of the\n        raw object or \'<unknown>\' will be displayed as title.\n    show : bool\n        Show figure if True.\n    block : bool\n        Whether to halt program execution until the figure is closed.\n        Useful for setting bad channels on the fly by clicking on a line.\n        May not work on all systems / platforms.\n    highpass : float | None\n        Highpass to apply when displaying data.\n    lowpass : float | None\n        Lowpass to apply when displaying data.\n    filtorder : int\n        Filtering order. Note that for efficiency and simplicity,\n        filtering during plotting uses forward-backward IIR filtering,\n        so the effective filter order will be twice ``filtorder``.\n        Filtering the lines for display may also produce some edge\n        artifacts (at the left and right edges) of the signals\n        during display. Filtering requires scipy >= 0.10.\n    clipping : str | None\n        If None, channels are allowed to exceed their designated bounds in\n        the plot. If "clamp", then values are clamped to the appropriate\n        range for display, creating step-like artifacts. If "transparent",\n        then excessive values are not shown, creating gaps in the traces.\n    show_first_samp : bool\n        If True, show time axis relative to the ``raw.first_samp``.\n    proj : bool\n        Whether to apply projectors prior to plotting (default is ``True``).\n        Individual projectors can be enabled/disabled interactively (see\n        Notes). This argument only affects the plot; use ``raw.apply_proj()``\n        to modify the data stored in the Raw object.\n    group_by : str\n        How to group channels. ``\'type\'`` groups by channel type,\n        ``\'original\'`` plots in the order of ch_names, ``\'selection\'`` uses\n        Elekta\'s channel groupings (only works for Neuromag data),\n        ``\'position\'`` groups the channels by the positions of the sensors.\n        ``\'selection\'`` and ``\'position\'`` modes allow custom selections by\n        using lasso selector on the topomap. Pressing ``ctrl`` key while\n        selecting allows appending to the current selection. Channels marked as\n        bad appear with red edges on the topomap. ``\'type\'`` and ``\'original\'``\n        groups the channels by type in butterfly mode whereas ``\'selection\'``\n        and ``\'position\'`` use regional grouping. ``\'type\'`` and ``\'original\'``\n        modes are overrided with ``order`` keyword.\n    butterfly : bool\n        Whether to start in butterfly mode. Defaults to False.\n    decim : int | \'auto\'\n        Amount to decimate the data during display for speed purposes.\n        You should only decimate if the data are sufficiently low-passed,\n        otherwise aliasing can occur. The \'auto\' mode (default) uses\n        the decimation that results in a sampling rate least three times\n        larger than ``min(info[\'lowpass\'], lowpass)`` (e.g., a 40 Hz lowpass\n        will result in at least a 120 Hz displayed sample rate).\n\n    Returns\n    -------\n    fig : Instance of matplotlib.figure.Figure\n        Raw traces.\n\n    Notes\n    -----\n    The arrow keys (up/down/left/right) can typically be used to navigate\n    between channels and time ranges, but this depends on the backend\n    matplotlib is configured to use (e.g., mpl.use(\'TkAgg\') should work). The\n    scaling can be adjusted with - and + (or =) keys. The viewport dimensions\n    can be adjusted with page up/page down and home/end keys. Full screen mode\n    can be to toggled with f11 key. To mark or un-mark a channel as bad, click\n    on the rather flat segments of a channel\'s time series. The changes will be\n    reflected immediately in the raw object\'s ``raw.info[\'bads\']`` entry.\n\n    If projectors are present, a button labelled "Proj" in the lower right\n    corner of the plot window opens a secondary control window, which allows\n    enabling/disabling specific projectors individually. This provides a means\n    of interactively observing how each projector would affect the raw data if\n    it were applied.\n\n    Annotation mode is toggled by pressing \'a\' and butterfly mode by pressing\n    \'b\'.\n    '
import matplotlib.pyplot as plt
import matplotlib as mpl
from scipy.signal import butter
color = _handle_default('color', color)
scalings = _compute_scalings(scalings, raw)
scalings = _handle_default('scalings_plot_raw', scalings)
if ((clipping is not None) and (clipping not in ('clamp', 'transparent'))):
    raise ValueError(('clipping must be None, "clamp", or "transparent", not %s' % clipping))
nyq = (raw.info['sfreq'] / 2.0)
if ((highpass is None) and (lowpass is None)):
    ba = None
else:
    filtorder = int(filtorder)
    if (filtorder <= 0):
        raise ValueError(('filtorder (%s) must be >= 1' % filtorder))
    if ((highpass is not None) and (highpass <= 0)):
        raise ValueError(('highpass must be > 0, not %s' % highpass))
    if ((lowpass is not None) and (lowpass >= nyq)):
        raise ValueError(('lowpass must be < nyquist (%s), not %s' % (nyq, lowpass)))
    if (highpass is None):
        ba = butter(filtorder, (lowpass / nyq), 'lowpass', analog=False)
    elif (lowpass is None):
        ba = butter(filtorder, (highpass / nyq), 'highpass', analog=False)
    else:
        if (lowpass <= highpass):
            raise ValueError(('lowpass (%s) must be > highpass (%s)' % (lowpass, highpass)))
        ba = butter(filtorder, [(highpass / nyq), (lowpass / nyq)], 'bandpass', analog=False)
info = raw.info.copy()
projs = info['projs']
info['projs'] = []
n_times = raw.n_times
if (title is None):
    title = raw._filenames
    if (len(title) == 0):
        title = '<unknown>'
    elif (len(title) == 1):
        title = title[0]
    else:
        title = ('%s ... (+ %d more) ' % (title[0], (len(title) - 1)))
        if (len(title) > 60):
            title = ('...' + title[(- 60):])
elif (not isinstance(title, string_types)):
    raise TypeError('title must be None or a string')
if (events is not None):
    event_times = (events[:, 0].astype(float) - raw.first_samp)
    event_times /= info['sfreq']
    event_nums = events[:, 2]
else:
    event_times = event_nums = None
inds = list()
types = list()
for t in ['grad', 'mag']:
    inds += [pick_types(info, meg=t, ref_meg=False, exclude=[])]
    types += ([t] * len(inds[(- 1)]))
for t in ['hbo', 'hbr']:
    inds += [pick_types(info, meg=False, ref_meg=False, fnirs=t, exclude=[])]
    types += ([t] * len(inds[(- 1)]))
pick_kwargs = dict(meg=False, ref_meg=False, exclude=[])
for key in _PICK_TYPES_KEYS:
    if (key not in ['meg', 'fnirs']):
        pick_kwargs[key] = True
        inds += [pick_types(raw.info, **pick_kwargs)]
        types += ([key] * len(inds[(- 1)]))
        pick_kwargs[key] = False
inds = np.concatenate(inds).astype(int)
if (not (len(inds) == len(info['ch_names']))):
    raise RuntimeError('Some channels not classified, please report this problem')
reord = numpy.argsort(inds)
types = [types[ri] for ri in reord]
if isinstance(order, string_types):
    group_by = order
    warn('Using string order is deprecated and will not be allowed in 0.16. Use group_by instead.')
elif isinstance(order, (numpy.ndarray, list)):
    inds = inds[reord][order]
elif (order is not None):
    raise ValueError(('Unkown order type. Got %s.' % type(order)))
if (group_by in ['selection', 'position']):
    (selections, fig_selection) = _setup_browser_selection(raw, group_by)
    selections = {k: numpy.intersect1d(v, inds) for (k, v) in selections.items()}
elif (group_by == 'original'):
    if (order is None):
        tempResult = arange(len(inds))
	
===================================================================	
plot_raw_psd: 421	
----------------------------	

'Plot the power spectral density across channels.\n\n    Different channel types are drawn in sub-plots. When the data has been\n    processed with a bandpass, lowpass or highpass filter, dashed lines\n    indicate the boundaries of the filter (--). The line noise frequency is\n    also indicated with a dashed line (-.).\n\n    Parameters\n    ----------\n    raw : instance of io.Raw\n        The raw instance to use.\n    tmin : float\n        Start time for calculations.\n    tmax : float\n        End time for calculations.\n    fmin : float\n        Start frequency to consider.\n    fmax : float\n        End frequency to consider.\n    proj : bool\n        Apply projection.\n    n_fft : int | None\n        Number of points to use in Welch FFT calculations.\n        Default is None, which uses the minimum of 2048 and the\n        number of time points.\n    picks : array-like of int | None\n        List of channels to use. Cannot be None if `ax` is supplied. If both\n        `picks` and `ax` are None, separate subplots will be created for\n        each standard channel type (`mag`, `grad`, and `eeg`).\n    ax : instance of matplotlib Axes | None\n        Axes to plot into. If None, axes will be created.\n    color : str | tuple\n        A matplotlib-compatible color to use. Has no effect when\n        spatial_colors=True.\n    area_mode : str | None\n        Mode for plotting area. If \'std\', the mean +/- 1 STD (across channels)\n        will be plotted. If \'range\', the min and max (across channels) will be\n        plotted. Bad channels will be excluded from these calculations.\n        If None, no area will be plotted. If average=False, no area is plotted.\n    area_alpha : float\n        Alpha for the area.\n    n_overlap : int\n        The number of points of overlap between blocks. The default value\n        is 0 (no overlap).\n    dB : bool\n        Plot Power Spectral Density (PSD), in units (amplitude**2/Hz (dB)) if\n        ``dB=True``, and ``estimate=\'power\'`` or ``estimate=\'auto\'``. Plot PSD\n        in units (amplitude**2/Hz) if ``dB=False`` and,\n        ``estimate=\'power\'``. Plot Amplitude Spectral Density (ASD), in units\n        (amplitude/sqrt(Hz)), if ``dB=False`` and ``estimate=\'amplitude\'`` or\n        ``estimate=\'auto\'``. Plot ASD, in units (amplitude/sqrt(Hz) (db)), if\n        ``dB=True`` and ``estimate=\'amplitude\'``.\n    estimate : str, {\'auto\', \'power\', \'amplitude\'}\n        Can be "power" for power spectral density (PSD), "amplitude" for\n        amplitude spectrum density (ASD), or "auto" (default), which uses\n        "power" when dB is True and "amplitude" otherwise.\n    average : bool\n        If False, the PSDs of all channels is displayed. No averaging\n        is done and parameters area_mode and area_alpha are ignored. When\n        False, it is possible to paint an area (hold left mouse button and\n        drag) to plot a topomap.\n    show : bool\n        Show figure if True.\n    n_jobs : int\n        Number of jobs to run in parallel.\n    line_alpha : float | None\n        Alpha for the PSD line. Can be None (default) to use 1.0 when\n        ``average=True`` and 0.1 when ``average=False``.\n    spatial_colors : bool\n        Whether to use spatial colors. Only used when ``average=False``.\n    xscale : str\n        Can be \'linear\' (default) or \'log\'.\n    reject_by_annotation : bool\n        Whether to omit bad segments from the data while computing the\n        PSD. If True, annotated segments with a description that starts\n        with \'bad\' are omitted. Has no effect if ``inst`` is an Epochs or\n        Evoked object. Defaults to True.\n\n        .. versionadded:: 0.15.0\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    fig : instance of matplotlib figure\n        Figure with frequency spectra of the data channels.\n    '
from matplotlib.ticker import ScalarFormatter
if (average is None):
    warn('In version 0.15 average will default to False and spatial_colors will default to True.', DeprecationWarning)
    average = True
if (average and spatial_colors):
    raise ValueError('Average and spatial_colors cannot be enabled simultaneously.')
if (spatial_colors is None):
    spatial_colors = (False if average else True)
(fig, picks_list, titles_list, units_list, scalings_list, ax_list, make_label) = _set_psd_plot_params(raw.info, proj, picks, ax, area_mode)
del ax
if (line_alpha is None):
    line_alpha = (1.0 if average else 0.1)
line_alpha = float(line_alpha)
psd_list = list()
ylabels = list()
if (n_fft is None):
    tmax = (raw.times[(- 1)] if (not numpy.isfinite(tmax)) else tmax)
    n_fft = min((numpy.diff(raw.time_as_index([tmin, tmax]))[0] + 1), 2048)
for (ii, picks) in enumerate(picks_list):
    ax = ax_list[ii]
    (psds, freqs) = psd_welch(raw, tmin=tmin, tmax=tmax, picks=picks, fmin=fmin, fmax=fmax, proj=proj, n_fft=n_fft, n_overlap=n_overlap, n_jobs=n_jobs, reject_by_annotation=reject_by_annotation)
    ylabel = _convert_psds(psds, dB, estimate, scalings_list[ii], units_list[ii], [raw.ch_names[pi] for pi in picks])
    if average:
        psd_mean = numpy.mean(psds, axis=0)
        if (area_mode == 'std'):
            psd_std = numpy.std(psds, axis=0)
            hyp_limits = ((psd_mean - psd_std), (psd_mean + psd_std))
        elif (area_mode == 'range'):
            hyp_limits = (numpy.min(psds, axis=0), numpy.max(psds, axis=0))
        else:
            hyp_limits = None
        ax.plot(freqs, psd_mean, color=color, alpha=line_alpha, linewidth=0.5)
        if (hyp_limits is not None):
            ax.fill_between(freqs, hyp_limits[0], y2=hyp_limits[1], color=color, alpha=area_alpha)
    else:
        psd_list.append(psds)
    if make_label:
        if (ii == (len(picks_list) - 1)):
            ax.set_xlabel('Frequency (Hz)')
        ax.set_ylabel(ylabel)
        ax.set_title(titles_list[ii])
        ax.set_xlim(freqs[0], freqs[(- 1)])
    ylabels.append(ylabel)
for (key, ls) in zip(['lowpass', 'highpass', 'line_freq'], ['--', '--', '-.']):
    if (raw.info[key] is not None):
        for ax in ax_list:
            ax.axvline(raw.info[key], color='k', linestyle=ls, alpha=0.25, linewidth=2, zorder=2)
if (not average):
    picks = numpy.concatenate(picks_list)
    psd_list = numpy.concatenate(psd_list)
    types = numpy.array([channel_type(raw.info, idx) for idx in picks])
    info = create_info([raw.ch_names[p] for p in picks], raw.info['sfreq'], types)
    info['chs'] = [raw.info['chs'][p] for p in picks]
    valid_channel_types = ['mag', 'grad', 'eeg', 'seeg', 'eog', 'ecg', 'emg', 'dipole', 'gof', 'bio', 'ecog', 'hbo', 'hbr', 'misc']
    ch_types_used = list()
    for this_type in valid_channel_types:
        if (this_type in types):
            ch_types_used.append(this_type)
    unit = ''
    units = {t: yl for (t, yl) in zip(ch_types_used, ylabels)}
    titles = {c: t for (c, t) in zip(ch_types_used, titles_list)}
    tempResult = arange(len(psd_list))
	
===================================================================	
plot_topomap: 396	
----------------------------	

"Plot a topographic map as image.\n\n    Parameters\n    ----------\n    data : array, shape (n_chan,)\n        The data values to plot.\n    pos : array, shape (n_chan, 2) | instance of Info\n        Location information for the data points(/channels).\n        If an array, for each data point, the x and y coordinates.\n        If an Info object, it must contain only one data type and\n        exactly `len(data)` data channels, and the x/y coordinates will\n        be inferred from this Info object.\n    vmin : float | callable | None\n        The value specifying the lower bound of the color range.\n        If None, and vmax is None, -vmax is used. Else np.min(data).\n        If callable, the output equals vmin(data). Defaults to None.\n    vmax : float | callable | None\n        The value specifying the upper bound of the color range.\n        If None, the maximum absolute value is used. If callable, the output\n        equals vmax(data). Defaults to None.\n    cmap : matplotlib colormap | None\n        Colormap to use. If None, 'Reds' is used for all positive data,\n        otherwise defaults to 'RdBu_r'.\n    sensors : bool | str\n        Add markers for sensor locations to the plot. Accepts matplotlib plot\n        format string (e.g., 'r+' for red plusses). If True (default), circles\n        will be used.\n    res : int\n        The resolution of the topomap image (n pixels along each side).\n    axes : instance of Axes | None\n        The axes to plot to. If None, the current axes will be used.\n    names : list | None\n        List of channel names. If None, channel names are not plotted.\n    show_names : bool | callable\n        If True, show channel names on top of the map. If a callable is\n        passed, channel names will be formatted using the callable; e.g., to\n        delete the prefix 'MEG ' from all channel names, pass the function\n        lambda x: x.replace('MEG ', ''). If `mask` is not None, only\n        significant sensors will be shown.\n        If `True`, a list of names must be provided (see `names` keyword).\n    mask : ndarray of bool, shape (n_channels, n_times) | None\n        The channels to be marked as significant at a given time point.\n        Indices set to `True` will be considered. Defaults to None.\n    mask_params : dict | None\n        Additional plotting parameters for plotting significant sensors.\n        Default (None) equals::\n\n           dict(marker='o', markerfacecolor='w', markeredgecolor='k',\n                linewidth=0, markersize=4)\n\n    outlines : 'head' | 'skirt' | dict | None\n        The outlines to be drawn. If 'head', the default head scheme will be\n        drawn. If 'skirt' the head scheme will be drawn, but sensors are\n        allowed to be plotted outside of the head circle. If dict, each key\n        refers to a tuple of x and y positions, the values in 'mask_pos' will\n        serve as image mask, and the 'autoshrink' (bool) field will trigger\n        automated shrinking of the positions due to points outside the outline.\n        Alternatively, a matplotlib patch object can be passed for advanced\n        masking options, either directly or as a function that returns patches\n        (required for multi-axes plots). If None, nothing will be drawn.\n        Defaults to 'head'.\n    image_mask : ndarray of bool, shape (res, res) | None\n        The image mask to cover the interpolated surface. If None, it will be\n        computed from the outline.\n    contours : int | array of float\n        The number of contour lines to draw. If 0, no contours will be drawn.\n        If an array, the values represent the levels for the contours. The\n        values are in uV for EEG, fT for magnetometers and fT/m for\n        gradiometers. Defaults to 6.\n    image_interp : str\n        The image interpolation to be used. All matplotlib options are\n        accepted.\n    show : bool\n        Show figure if True.\n    head_pos : dict | None\n        If None (default), the sensors are positioned such that they span\n        the head circle. If dict, can have entries 'center' (tuple) and\n        'scale' (tuple) for what the center and scale of the head should be\n        relative to the electrode locations.\n    onselect : callable | None\n        Handle for a function that is called when the user selects a set of\n        channels by rectangle selection (matplotlib ``RectangleSelector``). If\n        None interactive selection is disabled. Defaults to None.\n\n    Returns\n    -------\n    im : matplotlib.image.AxesImage\n        The interpolated data.\n    cn : matplotlib.contour.ContourSet\n        The fieldlines.\n    "
import matplotlib.pyplot as plt
from matplotlib.widgets import RectangleSelector
if ((contours is None) or (contours is False)):
    warn(('Using %s as contours is deprecated and will not be allowed in 0.16. Use 0 instead.' % str(contours)), DeprecationWarning)
    contours = 0
data = numpy.asarray(data)
utils.logger.debug(('Plotting topomap for data shape %s' % (data.shape,)))
if isinstance(pos, Info):
    picks = _pick_data_channels(pos)
    pos = pick_info(pos, picks)
    ch_type = set((channel_type(pos, idx) for (idx, _) in enumerate(pos['chs'])))
    info_help = 'Pick Info with e.g. mne.pick_info and mne.channels.channel_indices_by_type.'
    if (len(ch_type) > 1):
        raise ValueError(('Multiple channel types in Info structure. ' + info_help))
    elif (len(pos['chs']) != data.shape[0]):
        raise ValueError(('Number of channels in the Info object and the data array does not match. ' + info_help))
    else:
        ch_type = ch_type.pop()
    if any(((type_ in ch_type) for type_ in ('planar', 'grad'))):
        from ..channels.layout import _merge_grad_data, find_layout, _pair_grad_sensors
        (picks, pos) = _pair_grad_sensors(pos, find_layout(pos))
        data = _merge_grad_data(data[picks]).reshape((- 1))
    else:
        picks = list(range(data.shape[0]))
        pos = _find_topomap_coords(pos, picks=picks)
if (data.ndim > 1):
    raise ValueError(('Data needs to be array of shape (n_sensors,); got shape %s.' % str(data.shape)))
pos_help = 'Electrode positions should be specified as a 2D array with shape (n_channels, 2). Each row in this matrix contains the (x, y) position of an electrode.'
if (pos.ndim != 2):
    error = '{ndim}D array supplied as electrode positions, where a 2D array was expected'.format(ndim=pos.ndim)
    raise ValueError(((error + ' ') + pos_help))
elif (pos.shape[1] == 3):
    error = 'The supplied electrode positions matrix contains 3 columns. Are you trying to specify XYZ coordinates? Perhaps the mne.channels.create_eeg_layout function is useful for you.'
    raise ValueError(((error + ' ') + pos_help))
elif ((pos.shape[1] == 1) or (pos.shape[1] > 4)):
    raise ValueError(pos_help)
if (len(data) != len(pos)):
    raise ValueError(('Data and pos need to be of same length. Got data of length %s, pos of length %s' % (len(data), len(pos))))
norm = (min(data) >= 0)
(vmin, vmax) = _setup_vmin_vmax(data, vmin, vmax, norm)
if (cmap is None):
    cmap = ('Reds' if norm else 'RdBu_r')
(pos, outlines) = _check_outlines(pos, outlines, head_pos)
ax = (axes if axes else matplotlib.pyplot.gca())
(pos_x, pos_y) = _prepare_topomap(pos, ax)
if (outlines is None):
    (xmin, xmax) = (pos_x.min(), pos_x.max())
    (ymin, ymax) = (pos_y.min(), pos_y.max())
else:
    xlim = (numpy.inf, (- numpy.inf))
    ylim = (numpy.inf, (- numpy.inf))
    mask_ = numpy.c_[outlines['mask_pos']]
    (xmin, xmax) = (numpy.min(numpy.r_[(xlim[0], mask_[:, 0])]), numpy.max(numpy.r_[(xlim[1], mask_[:, 0])]))
    (ymin, ymax) = (numpy.min(numpy.r_[(ylim[0], mask_[:, 1])]), numpy.max(numpy.r_[(ylim[1], mask_[:, 1])]))
xi = numpy.linspace(xmin, xmax, res)
yi = numpy.linspace(ymin, ymax, res)
(Xi, Yi) = numpy.meshgrid(xi, yi)
Zi = _GridData(np.array((pos_x, pos_y)).T).set_values(data)(Xi, Yi)
if (outlines is None):
    _is_default_outlines = False
elif isinstance(outlines, dict):
    _is_default_outlines = any((k.startswith('head') for k in outlines))
if (_is_default_outlines and (image_mask is None)):
    (image_mask, pos) = _make_image_mask(outlines, pos, res)
mask_params = _handle_default('mask_params', mask_params)
linewidth = mask_params['markeredgewidth']
patch = None
if ('patch' in outlines):
    patch = outlines['patch']
    patch_ = (patch() if callable(patch) else patch)
    patch_.set_clip_on(False)
    ax.add_patch(patch_)
    ax.set_transform(ax.transAxes)
    ax.set_clip_path(patch_)
im = ax.imshow(Zi, cmap=cmap, vmin=vmin, vmax=vmax, origin='lower', aspect='equal', extent=(xmin, xmax, ymin, ymax), interpolation=image_interp)
no_contours = False
if isinstance(contours, (numpy.ndarray, list)):
    pass
elif (contours == 0):
    (contours, no_contours) = (1, True)
if (Zi == Zi[(0, 0)]).all():
    cont = None
else:
    cont = ax.contour(Xi, Yi, Zi, contours, colors='k', linewidths=linewidth)
if (no_contours and (cont is not None)):
    for col in cont.collections:
        col.set_visible(False)
if _is_default_outlines:
    from matplotlib import patches
    patch_ = matplotlib.patches.Ellipse((0, 0), (2 * outlines['clip_radius'][0]), (2 * outlines['clip_radius'][1]), clip_on=True, transform=ax.transData)
if (_is_default_outlines or (patch is not None)):
    im.set_clip_path(patch_)
    if (cont is not None):
        for col in cont.collections:
            col.set_clip_path(patch_)
if ((sensors is not False) and (mask is None)):
    _plot_sensors(pos_x, pos_y, sensors=sensors, ax=ax)
elif (sensors and (mask is not None)):
    idx = numpy.where(mask)[0]
    ax.plot(pos_x[idx], pos_y[idx], **mask_params)
    idx = numpy.where((~ mask))[0]
    _plot_sensors(pos_x[idx], pos_y[idx], sensors=sensors, ax=ax)
elif ((not sensors) and (mask is not None)):
    idx = numpy.where(mask)[0]
    ax.plot(pos_x[idx], pos_y[idx], **mask_params)
if isinstance(outlines, dict):
    _draw_outlines(ax, outlines)
if show_names:
    if (names is None):
        raise ValueError('To show names, a list of names must be provided (see `names` keyword).')
    if (show_names is True):

        def _show_names(x):
            return x
    else:
        _show_names = show_names
    tempResult = arange(len(names))
	
===================================================================	
_GridData.__init__: 254	
----------------------------	

from scipy.spatial.qhull import Delaunay
assert ((pos.ndim == 2) and (pos.shape[1] == 2))
extremes = numpy.array([pos.min(axis=0), pos.max(axis=0)])
diffs = (extremes[1] - extremes[0])
extremes[0] -= diffs
extremes[1] += diffs
eidx = numpy.array(list(itertools.product(*([(([0] * (pos.shape[1] - 1)) + [1])] * pos.shape[1]))))
tempResult = arange(pos.shape[1])
	
===================================================================	
_grad_pair_pick_and_name: 1617	
----------------------------	

'Deal with grads. (Helper for a few viz functions).'
from ..channels.layout import _pair_grad_sensors
picked_chans = list()
pairpicks = _pair_grad_sensors(info, topomap_coords=False)
tempResult = arange(0, len(pairpicks), 2)
	
===================================================================	
_setup_browser_offsets: 785	
----------------------------	

'Compute viewport height and adjust offsets.'
ylim = [((n_channels * 2) + 1), 0]
offset = (ylim[0] / n_channels)
tempResult = arange(n_channels)
	
===================================================================	
_setup_butterfly: 1449	
----------------------------	

'Set butterfly view of raw plotter.'
from .raw import _setup_browser_selection
if ('ica' in params):
    return
butterfly = (not params['butterfly'])
ax = params['ax']
params['butterfly'] = butterfly
if butterfly:
    types = numpy.array(params['types'])[params['orig_inds']]
    if (params['group_by'] in ['type', 'original']):
        inds = params['inds']
        eeg = ('seeg' if ('seeg' in types) else 'eeg')
        labels = ([t for t in ['grad', 'mag', eeg, 'eog', 'ecg'] if (t in types)] + ['misc'])
        tempResult = arange(5, (5 * (len(labels) + 1)), 5)
	
===================================================================	
_setup_butterfly: 1477	
----------------------------	

'Set butterfly view of raw plotter.'
from .raw import _setup_browser_selection
if ('ica' in params):
    return
butterfly = (not params['butterfly'])
ax = params['ax']
params['butterfly'] = butterfly
if butterfly:
    types = numpy.array(params['types'])[params['orig_inds']]
    if (params['group_by'] in ['type', 'original']):
        inds = params['inds']
        eeg = ('seeg' if ('seeg' in types) else 'eeg')
        labels = ([t for t in ['grad', 'mag', eeg, 'eog', 'ecg'] if (t in types)] + ['misc'])
        ticks = numpy.arange(5, (5 * (len(labels) + 1)), 5)
        offs = {l: t for (l, t) in zip(labels, ticks)}
        params['offsets'] = numpy.zeros(len(params['types']))
        for ind in inds:
            params['offsets'][ind] = offs.get(params['types'][ind], (5 * len(labels)))
        ax.set_yticks(ticks)
        params['ax'].set_ylim((5 * (len(labels) + 1)), 0)
        ax.set_yticklabels(labels)
    else:
        if ('selections' not in params):
            params['selections'] = _setup_browser_selection(params['raw'], 'position', selector=False)
        sels = params['selections']
        selections = _SELECTIONS[1:]
        if (('Misc' in sels) and (len(sels['Misc']) > 0)):
            selections += ['Misc']
        if ((params['group_by'] == 'selection') and ('eeg' in types)):
            for sel in _EEG_SELECTIONS:
                if (sel in sels):
                    selections += [sel]
        picks = list()
        for selection in selections:
            picks.append(sels.get(selection, list()))
        labels = ax.yaxis.get_ticklabels()
        for label in labels:
            label.set_visible(True)
        ylim = ((5.0 * len(picks)), 0.0)
        ax.set_ylim(ylim)
        offset = (ylim[0] / (len(picks) + 1))
        tempResult = arange(0, ylim[0], offset)
	
===================================================================	
_setup_butterfly: 1490	
----------------------------	

'Set butterfly view of raw plotter.'
from .raw import _setup_browser_selection
if ('ica' in params):
    return
butterfly = (not params['butterfly'])
ax = params['ax']
params['butterfly'] = butterfly
if butterfly:
    types = numpy.array(params['types'])[params['orig_inds']]
    if (params['group_by'] in ['type', 'original']):
        inds = params['inds']
        eeg = ('seeg' if ('seeg' in types) else 'eeg')
        labels = ([t for t in ['grad', 'mag', eeg, 'eog', 'ecg'] if (t in types)] + ['misc'])
        ticks = numpy.arange(5, (5 * (len(labels) + 1)), 5)
        offs = {l: t for (l, t) in zip(labels, ticks)}
        params['offsets'] = numpy.zeros(len(params['types']))
        for ind in inds:
            params['offsets'][ind] = offs.get(params['types'][ind], (5 * len(labels)))
        ax.set_yticks(ticks)
        params['ax'].set_ylim((5 * (len(labels) + 1)), 0)
        ax.set_yticklabels(labels)
    else:
        if ('selections' not in params):
            params['selections'] = _setup_browser_selection(params['raw'], 'position', selector=False)
        sels = params['selections']
        selections = _SELECTIONS[1:]
        if (('Misc' in sels) and (len(sels['Misc']) > 0)):
            selections += ['Misc']
        if ((params['group_by'] == 'selection') and ('eeg' in types)):
            for sel in _EEG_SELECTIONS:
                if (sel in sels):
                    selections += [sel]
        picks = list()
        for selection in selections:
            picks.append(sels.get(selection, list()))
        labels = ax.yaxis.get_ticklabels()
        for label in labels:
            label.set_visible(True)
        ylim = ((5.0 * len(picks)), 0.0)
        ax.set_ylim(ylim)
        offset = (ylim[0] / (len(picks) + 1))
        ticks = numpy.arange(0, ylim[0], offset)
        ticks = [(ticks[x] if (x < len(ticks)) else 0) for x in range(20)]
        ax.set_yticks(ticks)
        offsets = numpy.zeros(len(params['types']))
        for (group_idx, group) in enumerate(picks):
            for (idx, pick) in enumerate(group):
                offsets[pick] = (offset * (group_idx + 1))
        params['inds'] = params['orig_inds'].copy()
        params['offsets'] = offsets
        ax.set_yticklabels(([''] + selections), color='black', rotation=45, va='top')
else:
    params['inds'] = params['orig_inds'].copy()
    if ('fig_selection' not in params):
        tempResult = arange(params['n_channels'], len(params['lines']))
	
===================================================================	
mne_analyze_colormap: 124	
----------------------------	

"Return a colormap similar to that used by mne_analyze.\n\n    Parameters\n    ----------\n    limits : list (or array) of length 3 or 6\n        Bounds for the colormap, which will be mirrored across zero if length\n        3, or completely specified (and potentially asymmetric) if length 6.\n    format : str\n        Type of colormap to return. If 'matplotlib', will return a\n        matplotlib.colors.LinearSegmentedColormap. If 'mayavi', will\n        return an RGBA array of shape (256, 4).\n\n    Returns\n    -------\n    cmap : instance of matplotlib.pyplot.colormap | array\n        A teal->blue->gray->red->yellow colormap.\n\n    Notes\n    -----\n    For this will return a colormap that will display correctly for data\n    that are scaled by the plotting function to span [-fmax, fmax].\n\n    Examples\n    --------\n    The following code will plot a STC using standard MNE limits:\n\n        colormap = mne.viz.mne_analyze_colormap(limits=[5, 10, 15])\n        brain = stc.plot('fsaverage', 'inflated', 'rh', colormap)\n        brain.scale_data_colormap(fmin=-15, fmid=0, fmax=15, transparent=False)\n\n    "
limits = numpy.asarray(limits, dtype='float')
if ((len(limits) != 3) and (len(limits) != 6)):
    raise ValueError('limits must have 3 or 6 elements')
if ((len(limits) == 3) and any((limits < 0.0))):
    raise ValueError('if 3 elements, limits must all be non-negative')
if any((numpy.diff(limits) <= 0)):
    raise ValueError('limits must be monotonically increasing')
if (format == 'matplotlib'):
    from matplotlib import colors
    if (len(limits) == 3):
        limits = ((numpy.concatenate(((- numpy.flipud(limits)), limits)) + limits[(- 1)]) / (2 * limits[(- 1)]))
    else:
        limits = ((limits - numpy.min(limits)) / numpy.max((limits - numpy.min(limits))))
    cdict = {'red': ((limits[0], 0.0, 0.0), (limits[1], 0.0, 0.0), (limits[2], 0.5, 0.5), (limits[3], 0.5, 0.5), (limits[4], 1.0, 1.0), (limits[5], 1.0, 1.0)), 'green': ((limits[0], 1.0, 1.0), (limits[1], 0.0, 0.0), (limits[2], 0.5, 0.5), (limits[3], 0.5, 0.5), (limits[4], 0.0, 0.0), (limits[5], 1.0, 1.0)), 'blue': ((limits[0], 1.0, 1.0), (limits[1], 1.0, 1.0), (limits[2], 0.5, 0.5), (limits[3], 0.5, 0.5), (limits[4], 0.0, 0.0), (limits[5], 0.0, 0.0))}
    return matplotlib.colors.LinearSegmentedColormap('mne_analyze', cdict)
elif (format == 'mayavi'):
    if (len(limits) == 3):
        limits = (numpy.concatenate(((- numpy.flipud(limits)), [0], limits)) / limits[(- 1)])
    else:
        limits = numpy.concatenate((limits[:3], [0], limits[3:]))
        limits /= numpy.max(numpy.abs(limits))
    r = numpy.array([0, 0, 0, 0, 1, 1, 1])
    g = numpy.array([1, 0, 0, 0, 0, 0, 1])
    b = numpy.array([1, 1, 1, 0, 0, 0, 0])
    a = numpy.array([1, 1, 0, 0, 0, 1, 1])
    tempResult = arange(256)
	
===================================================================	
ClickableImage.plot_clicks: 825	
----------------------------	

'Plot the x/y positions stored in self.coords.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Arguments are passed to imshow in displaying the bg image.\n        '
from matplotlib.pyplot import subplots
if (len(self.coords) == 0):
    raise ValueError('No coordinates found, make sure you click on the image that is first shown.')
(f, ax) = subplots()
ax.imshow(self.imdata, extent=(0, self.xmax, 0, self.ymax), **kwargs)
(xlim, ylim) = [ax.get_xlim(), ax.get_ylim()]
(xcoords, ycoords) = zip(*self.coords)
ax.scatter(xcoords, ycoords, c='r')
tempResult = arange(len(self.coords))
	
===================================================================	
plot_trans: 399	
----------------------------	

'Plot head, sensor, and source space alignment in 3D.\n\n    Parameters\n    ----------\n    info : dict\n        The measurement info.\n    trans : str | \'auto\' | dict | None\n        The full path to the head<->MRI transform ``*-trans.fif`` file\n        produced during coregistration. If trans is None, an identity matrix\n        is assumed.\n    subject : str | None\n        The subject name corresponding to FreeSurfer environment\n        variable SUBJECT. Can be omitted if ``src`` is provided.\n    subjects_dir : str\n        The path to the freesurfer subjects reconstructions.\n        It corresponds to Freesurfer environment variable SUBJECTS_DIR.\n    source : str | list\n        Type to load. Common choices would be `\'bem\'`, `\'head\'` or\n        `\'outer_skin\'`. If list, the sources are looked up in the given order\n        and first found surface is used. We first try loading\n        `\'$SUBJECTS_DIR/$SUBJECT/bem/$SUBJECT-$SOURCE.fif\'`, and then look for\n        `\'$SUBJECT*$SOURCE.fif\'` in the same directory. For `\'outer_skin\'`,\n        the subjects bem and bem/flash folders are searched. Defaults to \'bem\'.\n        Note. For single layer bems it is recommended to use \'head\'.\n    coord_frame : str\n        Coordinate frame to use, \'head\', \'meg\', or \'mri\'.\n    meg_sensors : bool | str | list\n        Can be "helmet" (equivalent to False) or "sensors" to show the MEG\n        helmet or sensors, respectively, or a combination of the two like\n        ``[\'helmet\', \'sensors\']`` (equivalent to True, default) or ``[]``.\n    eeg_sensors : bool | str | list\n        Can be "original" (default; equivalent to True) or "projected" to\n        show EEG sensors in their digitized locations or projected onto the\n        scalp, or a list of these options including ``[]`` (equivalent of\n        False).\n    dig : bool | \'fiducials\'\n        If True, plot the digitization points; \'fiducials\' to plot fiducial\n        points only.\n    ref_meg : bool\n        If True (default False), include reference MEG sensors.\n    ecog_sensors : bool\n        If True (default), show ECoG sensors.\n    head : bool | None\n        If True, show head surface. Can also be None, which will show the\n        head surface for MEG and EEG, but hide it if ECoG sensors are\n        present.\n    brain : bool | str | None\n        If True, show the brain surfaces. Can also be a str for\n        surface type (e.g., \'pial\', same as True), or None (True for ECoG,\n        False otherwise).\n    skull : bool | str | list of str | list of dict\n        Whether to plot skull surface. If string, common choices would be\n        \'inner_skull\', or \'outer_skull\'. Can also be a list to plot\n        multiple skull surfaces. If a list of dicts, each dict must\n        contain the complete surface info (such as you get from\n        :func:`mne.make_bem_model`). True is an alias of \'outer_skull\'.\n        The subjects bem and bem/flash folders are searched for the \'surf\'\n        files. Defaults to False.\n    src : instance of SourceSpaces | None\n        If not None, also plot the source space points.\n\n        .. versionadded:: 0.14\n\n    mri_fiducials : bool | str\n        Plot MRI fiducials (default False). If ``True``, look for a file with\n        the canonical name (``bem/{subject}-fiducials.fif``). If ``str`` it\n        should provide the full path to the fiducials file.\n\n        .. versionadded:: 0.14\n\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    fig : instance of mlab.Figure\n        The mayavi figure.\n    '
from ..forward import _create_meg_coils
mlab = _import_mlab()
if (meg_sensors is False):
    meg_sensors = 'helmet'
elif (meg_sensors is True):
    meg_sensors = ['helmet', 'sensors']
if (eeg_sensors is False):
    eeg_sensors = []
elif (eeg_sensors is True):
    eeg_sensors = 'original'
if isinstance(eeg_sensors, string_types):
    eeg_sensors = [eeg_sensors]
if isinstance(meg_sensors, string_types):
    meg_sensors = [meg_sensors]
for (kind, var) in zip(('eeg', 'meg'), (eeg_sensors, meg_sensors)):
    if ((not isinstance(var, (list, tuple))) or (not all((isinstance(x, string_types) for x in var)))):
        raise TypeError(('%s_sensors must be list or tuple of str, got %s' % (type(var),)))
if (not all(((x in ('helmet', 'sensors')) for x in meg_sensors))):
    raise ValueError(('meg_sensors must only contain "helmet" and "points", got %s' % (meg_sensors,)))
if (not all(((x in ('original', 'projected')) for x in eeg_sensors))):
    raise ValueError(('eeg_sensors must only contain "original" and "projected", got %s' % (eeg_sensors,)))
if (not isinstance(info, Info)):
    raise TypeError(('info must be an instance of Info, got %s' % type(info)))
valid_coords = ['head', 'meg', 'mri']
if (coord_frame not in valid_coords):
    raise ValueError(('coord_frame must be one of %s' % (valid_coords,)))
if (src is not None):
    if (not isinstance(src, SourceSpaces)):
        raise TypeError(('src must be None or SourceSpaces, got %s' % (type(src),)))
    src_subject = src[0].get('subject_his_id', None)
    subject = (src_subject if (subject is None) else subject)
    if ((src_subject is not None) and (subject != src_subject)):
        raise ValueError(('subject ("%s") did not match the subject name  in src ("%s")' % (subject, src_subject)))
    src_rr = numpy.concatenate([s['rr'][s['inuse'].astype(bool)] for s in src])
    src_nn = numpy.concatenate([s['nn'][s['inuse'].astype(bool)] for s in src])
else:
    src_rr = src_nn = numpy.empty((0, 3))
meg_picks = pick_types(info, meg=True, ref_meg=ref_meg)
eeg_picks = pick_types(info, meg=False, eeg=True, ref_meg=False)
ecog_picks = pick_types(info, meg=False, ecog=True, ref_meg=False)
if (head is None):
    head = ((len(ecog_picks) == 0) and (subject is not None))
if (head and (subject is None)):
    raise ValueError('If head is True, subject must be provided')
if isinstance(trans, string_types):
    if (trans == 'auto'):
        subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
        trans = _find_trans(subject, subjects_dir)
    trans = read_trans(trans, return_all=True)
    exp = None
    for trans in trans:
        try:
            trans = _ensure_trans(trans, 'head', 'mri')
        except Exception as exp:
            pass
        else:
            break
    else:
        raise exp
elif (trans is None):
    trans = Transform('head', 'mri')
elif (not isinstance(trans, dict)):
    raise TypeError('trans must be str, dict, or None')
head_mri_t = _ensure_trans(trans, 'head', 'mri')
dev_head_t = info['dev_head_t']
del trans
if (coord_frame == 'meg'):
    head_trans = invert_transform(dev_head_t)
    meg_trans = Transform('meg', 'meg')
    mri_trans = invert_transform(combine_transforms(dev_head_t, head_mri_t, 'meg', 'mri'))
elif (coord_frame == 'mri'):
    head_trans = head_mri_t
    meg_trans = combine_transforms(dev_head_t, head_mri_t, 'meg', 'mri')
    mri_trans = Transform('mri', 'mri')
else:
    head_trans = Transform('head', 'head')
    meg_trans = info['dev_head_t']
    mri_trans = invert_transform(head_mri_t)
surfs = dict()
if head:
    subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
    head_surf = _get_head_surface(subject, source=source, subjects_dir=subjects_dir, raise_error=False)
    if (head_surf is None):
        if isinstance(source, string_types):
            source = [source]
        for this_surf in source:
            if (not this_surf.endswith('outer_skin')):
                continue
            surf_fname = os.path.join(subjects_dir, subject, 'bem', 'flash', ('%s.surf' % this_surf))
            if (not os.path.exists(surf_fname)):
                surf_fname = os.path.join(subjects_dir, subject, 'bem', ('%s.surf' % this_surf))
                if (not os.path.exists(surf_fname)):
                    continue
            utils.logger.info(('Using %s for head surface.' % this_surf))
            (rr, tris) = read_surface(surf_fname)
            head_surf = dict(rr=(rr / 1000.0), tris=tris, ntri=len(tris), np=len(rr), coord_frame=io.constants.FIFF.FIFFV_COORD_MRI)
            complete_surface_info(head_surf, copy=False, verbose=False)
            break
    if (head_surf is None):
        raise IOError(('No head surface found for subject %s.' % subject))
    surfs['head'] = head_surf
if mri_fiducials:
    if (mri_fiducials is True):
        subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
        if (subject is None):
            raise ValueError('Subject needs to be specified to automatically find the fiducials file.')
        mri_fiducials = os.path.join(subjects_dir, subject, 'bem', (subject + '-fiducials.fif'))
    if isinstance(mri_fiducials, string_types):
        (mri_fiducials, cf) = read_fiducials(mri_fiducials)
        if (cf != io.constants.FIFF.FIFFV_COORD_MRI):
            raise ValueError('Fiducials are not in MRI space')
    fid_loc = _fiducial_coords(mri_fiducials, io.constants.FIFF.FIFFV_COORD_MRI)
    fid_loc = apply_trans(mri_trans, fid_loc)
else:
    fid_loc = []
if (('helmet' in meg_sensors) and (len(meg_picks) > 0)):
    surfs['helmet'] = get_meg_helmet_surf(info, head_mri_t)
if (brain is None):
    if ((len(ecog_picks) > 0) and (subject is not None)):
        brain = 'pial'
    else:
        brain = False
if brain:
    subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
    brain = ('pial' if (brain is True) else brain)
    for hemi in ['lh', 'rh']:
        fname = os.path.join(subjects_dir, subject, 'surf', ('%s.%s' % (hemi, brain)))
        (rr, tris) = read_surface(fname)
        rr *= 0.001
        surfs[hemi] = dict(rr=rr, tris=tris, ntri=len(tris), np=len(rr), coord_frame=io.constants.FIFF.FIFFV_COORD_MRI)
        complete_surface_info(surfs[hemi], copy=False, verbose=False)
if (skull is True):
    skull = 'outer_skull'
if isinstance(skull, string_types):
    skull = [skull]
elif (not skull):
    skull = []
if ((len(skull) > 0) and (not isinstance(skull[0], dict))):
    skull = sorted(skull)
skull_alpha = dict()
skull_colors = dict()
hemi_val = 0.5
if ((src is None) or (brain and any(((s['type'] == 'surf') for s in src)))):
    hemi_val = 1.0
tempResult = arange((len(skull) + 1))
	
===================================================================	
_sensor_shape: 878	
----------------------------	

'Get the sensor shape vertices.'
rrs = numpy.empty([0, 2])
tris = numpy.empty([0, 3], int)
id_ = (coil['type'] & 65535)
if (id_ in (2, 3012, 3013, 3011)):
    long_side = coil['size']
    offset = 0.0025
    rrs = numpy.array([[offset, ((- long_side) / 2.0)], [(long_side / 2.0), ((- long_side) / 2.0)], [(long_side / 2.0), (long_side / 2.0)], [offset, (long_side / 2.0)], [(- offset), ((- long_side) / 2.0)], [((- long_side) / 2.0), ((- long_side) / 2.0)], [((- long_side) / 2.0), (long_side / 2.0)], [(- offset), (long_side / 2.0)]])
    tris = numpy.concatenate((_make_tris_fan(4), (_make_tris_fan(4) + 4)), axis=0)
elif (id_ in (2000, 3022, 3023, 3024)):
    size = (0.001 if (id_ == 2000) else (coil['size'] / 2.0))
    rrs = (numpy.array([[(- 1.0), 1.0], [1.0, 1.0], [1.0, (- 1.0)], [(- 1.0), (- 1.0)]]) * size)
    tris = _make_tris_fan(4)
elif (id_ in (4001, 4003, 5002, 7002, 7003, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_MAG)):
    n_pts = 15
    tempResult = arange(n_pts)
	
===================================================================	
_sensor_shape: 886	
----------------------------	

'Get the sensor shape vertices.'
rrs = numpy.empty([0, 2])
tris = numpy.empty([0, 3], int)
id_ = (coil['type'] & 65535)
if (id_ in (2, 3012, 3013, 3011)):
    long_side = coil['size']
    offset = 0.0025
    rrs = numpy.array([[offset, ((- long_side) / 2.0)], [(long_side / 2.0), ((- long_side) / 2.0)], [(long_side / 2.0), (long_side / 2.0)], [offset, (long_side / 2.0)], [(- offset), ((- long_side) / 2.0)], [((- long_side) / 2.0), ((- long_side) / 2.0)], [((- long_side) / 2.0), (long_side / 2.0)], [(- offset), (long_side / 2.0)]])
    tris = numpy.concatenate((_make_tris_fan(4), (_make_tris_fan(4) + 4)), axis=0)
elif (id_ in (2000, 3022, 3023, 3024)):
    size = (0.001 if (id_ == 2000) else (coil['size'] / 2.0))
    rrs = (numpy.array([[(- 1.0), 1.0], [1.0, 1.0], [1.0, (- 1.0)], [(- 1.0), (- 1.0)]]) * size)
    tris = _make_tris_fan(4)
elif (id_ in (4001, 4003, 5002, 7002, 7003, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_MAG)):
    n_pts = 15
    circle = numpy.exp((((2j * numpy.pi) * numpy.arange(n_pts)) / float(n_pts)))
    circle = numpy.concatenate(([0.0], circle))
    circle *= (coil['size'] / 2.0)
    rrs = np.array([circle.real, circle.imag]).T
    tris = _make_tris_fan((n_pts + 1))
elif (id_ in (4002, 5001, 5003, 5004, 4004, 4005, 6001, 7001, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_GRAD, io.constants.FIFF.FIFFV_COIL_ARTEMIS123_REF_GRAD)):
    baseline = (coil['base'] if (id_ in (5004, 4005)) else 0.0)
    n_pts = 16
    tempResult = arange((- 1), n_pts)
	
===================================================================	
plot_alignment: 710	
----------------------------	

'Plot head, sensor, and source space alignment in 3D.\n\n    Parameters\n    ----------\n    info : dict\n        The measurement info.\n    trans : str | \'auto\' | dict | None\n        The full path to the head<->MRI transform ``*-trans.fif`` file\n        produced during coregistration. If trans is None, an identity matrix\n        is assumed.\n    subject : str | None\n        The subject name corresponding to FreeSurfer environment\n        variable SUBJECT. Can be omitted if ``src`` is provided.\n    subjects_dir : str | None\n        The path to the freesurfer subjects reconstructions.\n        It corresponds to Freesurfer environment variable SUBJECTS_DIR.\n    surfaces : str | list\n        Surfaces to plot. Supported values: \'head\', \'outer_skin\',\n        \'outer_skull\', \'inner_skull\', \'brain\', \'pial\', \'white\', \'inflated\'.\n        Defaults to (\'head\',).\n\n        .. note:: For single layer BEMs it is recommended to use \'brain\'.\n    coord_frame : str\n        Coordinate frame to use, \'head\', \'meg\', or \'mri\'.\n    meg : str | list | bool\n        Can be "helmet", "sensors" or "ref" to show the MEG helmet, sensors or\n        reference sensors respectively, or a combination like\n        ``[\'helmet\', \'sensors\']``. True translates to\n        ``(\'helmet\', \'sensors\', \'ref\')``.\n    eeg : bool | str | list\n        Can be "original" (default; equivalent to True) or "projected" to\n        show EEG sensors in their digitized locations or projected onto the\n        scalp, or a list of these options including ``[]`` (equivalent of\n        False).\n    dig : bool | \'fiducials\'\n        If True, plot the digitization points; \'fiducials\' to plot fiducial\n        points only.\n    ecog : bool\n        If True (default), show ECoG sensors.\n    src : instance of SourceSpaces | None\n        If not None, also plot the source space points.\n    mri_fiducials : bool | str\n        Plot MRI fiducials (default False). If ``True``, look for a file with\n        the canonical name (``bem/{subject}-fiducials.fif``). If ``str`` it\n        should provide the full path to the fiducials file.\n    bem : list of dict | Instance of ConductorModel | None\n        Can be either the BEM surfaces (list of dict), a BEM solution or a\n        sphere model. If None, we first try loading\n        `\'$SUBJECTS_DIR/$SUBJECT/bem/$SUBJECT-$SOURCE.fif\'`, and then look for\n        `\'$SUBJECT*$SOURCE.fif\'` in the same directory. For `\'outer_skin\'`,\n        the subjects bem and bem/flash folders are searched. Defaults to None.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see :func:`mne.verbose`\n        and :ref:`Logging documentation <tut_logging>` for more).\n\n    Returns\n    -------\n    fig : instance of mlab.Figure\n        The mayavi figure.\n\n    See Also\n    --------\n    mne.viz.plot_bem\n\n    Notes\n    -----\n    This function serves the purpose of checking the validity of the many\n    different steps of source reconstruction:\n\n    - Transform matrix (keywords ``trans``, ``meg`` and ``mri_fiducials``),\n    - BEM surfaces (keywords ``bem`` and ``surfaces``),\n    - sphere conductor model (keywords ``bem`` and ``surfaces``) and\n    - source space (keywords ``surfaces`` and ``src``).\n\n    .. versionadded:: 0.15\n    '
from ..forward import _create_meg_coils
mlab = _import_mlab()
if (eeg is False):
    eeg = list()
elif (eeg is True):
    eeg = 'original'
if (meg is True):
    meg = ('helmet', 'sensors', 'ref')
elif (meg is False):
    meg = list()
elif isinstance(meg, string_types):
    meg = [meg]
if isinstance(eeg, string_types):
    eeg = [eeg]
for (kind, var) in zip(('eeg', 'meg'), (eeg, meg)):
    if ((not isinstance(var, (list, tuple))) or (not all((isinstance(x, string_types) for x in var)))):
        raise TypeError(('%s must be list or tuple of str, got %s' % (kind, type(var))))
if (not all(((x in ('helmet', 'sensors', 'ref')) for x in meg))):
    raise ValueError(('meg must only contain "helmet", "sensors" or "ref", got %s' % (meg,)))
if (not all(((x in ('original', 'projected')) for x in eeg))):
    raise ValueError(('eeg must only contain "original" and "projected", got %s' % (eeg,)))
if (not isinstance(info, Info)):
    raise TypeError(('info must be an instance of Info, got %s' % type(info)))
is_sphere = False
if (isinstance(bem, ConductorModel) and bem['is_sphere']):
    if ((len(bem['layers']) != 4) and (len(surfaces) > 1)):
        raise ValueError('The sphere conductor model must have three layers for plotting skull and head.')
    is_sphere = True
skull = list()
if ('outer_skull' in surfaces):
    if (isinstance(bem, ConductorModel) and (not bem['is_sphere'])):
        skull.append(_bem_find_surface(bem, io.constants.FIFF.FIFFV_BEM_SURF_ID_SKULL))
    else:
        skull.append('outer_skull')
if ('inner_skull' in surfaces):
    if (isinstance(bem, ConductorModel) and (not bem['is_sphere'])):
        skull.append(_bem_find_surface(bem, io.constants.FIFF.FIFFV_BEM_SURF_ID_BRAIN))
    else:
        skull.append('inner_skull')
surf_dict = bem._surf_dict.copy()
surf_dict['outer_skin'] = io.constants.FIFF.FIFFV_BEM_SURF_ID_HEAD
if ((len(skull) > 0) and (not isinstance(skull[0], dict))):
    skull = sorted(skull)
    if ((bem is not None) and (not isinstance(bem, ConductorModel))):
        for (idx, surf_name) in enumerate(skull):
            for this_surf in bem:
                if (this_surf['id'] == surf_dict[surf_name]):
                    skull[idx] = this_surf
                    break
            else:
                raise ValueError(('Could not find the surface for %s.' % surf_name))
valid_coords = ['head', 'meg', 'mri']
if (coord_frame not in valid_coords):
    raise ValueError(('coord_frame must be one of %s' % (valid_coords,)))
if (src is not None):
    if (not isinstance(src, SourceSpaces)):
        raise TypeError(('src must be None or SourceSpaces, got %s' % (type(src),)))
    src_subject = src[0].get('subject_his_id', None)
    subject = (src_subject if (subject is None) else subject)
    if ((src_subject is not None) and (subject != src_subject)):
        raise ValueError(('subject ("%s") did not match the subject name  in src ("%s")' % (subject, src_subject)))
    src_rr = numpy.concatenate([s['rr'][s['inuse'].astype(bool)] for s in src])
    src_nn = numpy.concatenate([s['nn'][s['inuse'].astype(bool)] for s in src])
else:
    src_rr = src_nn = numpy.empty((0, 3))
ref_meg = ('ref' in meg)
meg_picks = pick_types(info, meg=True, ref_meg=ref_meg)
eeg_picks = pick_types(info, meg=False, eeg=True, ref_meg=False)
ecog_picks = pick_types(info, meg=False, ecog=True, ref_meg=False)
if isinstance(trans, string_types):
    if (trans == 'auto'):
        subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
        trans = _find_trans(subject, subjects_dir)
    trans = read_trans(trans, return_all=True)
    exp = None
    for trans in trans:
        try:
            trans = _ensure_trans(trans, 'head', 'mri')
        except Exception as exp:
            pass
        else:
            break
    else:
        raise exp
elif (trans is None):
    trans = Transform('head', 'mri')
elif (not isinstance(trans, dict)):
    raise TypeError('trans must be str, dict, or None')
head_mri_t = _ensure_trans(trans, 'head', 'mri')
dev_head_t = info['dev_head_t']
del trans
if (coord_frame == 'meg'):
    head_trans = invert_transform(dev_head_t)
    meg_trans = Transform('meg', 'meg')
    mri_trans = invert_transform(combine_transforms(dev_head_t, head_mri_t, 'meg', 'mri'))
elif (coord_frame == 'mri'):
    head_trans = head_mri_t
    meg_trans = combine_transforms(dev_head_t, head_mri_t, 'meg', 'mri')
    mri_trans = Transform('mri', 'mri')
else:
    head_trans = Transform('head', 'head')
    meg_trans = info['dev_head_t']
    mri_trans = invert_transform(head_mri_t)
surfs = dict()
head = any([('outer_skin' in surfaces), ('head' in surfaces)])
if head:
    head_surf = None
    if (bem is not None):
        if isinstance(bem, ConductorModel):
            if is_sphere:
                head_surf = _complete_sphere_surf(bem, 3, 4)
            else:
                head_surf = _bem_find_surface(bem, io.constants.FIFF.FIFFV_BEM_SURF_ID_HEAD)
                complete_surface_info(head_surf, copy=False, verbose=False)
        elif (bem is not None):
            for this_surf in bem:
                if (this_surf['id'] == io.constants.FIFF.FIFFV_BEM_SURF_ID_HEAD):
                    head_surf = this_surf
                    break
            else:
                raise ValueError('Could not find the surface for head.')
    if (head_surf is None):
        subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
        surf_fname = os.path.join(subjects_dir, subject, 'bem', 'flash', 'outer_skin.surf')
        if (not os.path.exists(surf_fname)):
            surf_fname = os.path.join(subjects_dir, subject, 'bem', 'outer_skin.surf')
            if (not os.path.exists(surf_fname)):
                raise IOError(('No head surface found for subject %s.' % subject))
        utils.logger.info('Using outer_skin for head surface.')
        (rr, tris) = read_surface(surf_fname)
        head_surf = dict(rr=(rr / 1000.0), tris=tris, ntri=len(tris), np=len(rr), coord_frame=io.constants.FIFF.FIFFV_COORD_MRI)
        complete_surface_info(head_surf, copy=False, verbose=False)
    surfs['head'] = head_surf
if mri_fiducials:
    if (mri_fiducials is True):
        subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
        if (subject is None):
            raise ValueError('Subject needs to be specified to automatically find the fiducials file.')
        mri_fiducials = os.path.join(subjects_dir, subject, 'bem', (subject + '-fiducials.fif'))
    if isinstance(mri_fiducials, string_types):
        (mri_fiducials, cf) = read_fiducials(mri_fiducials)
        if (cf != io.constants.FIFF.FIFFV_COORD_MRI):
            raise ValueError('Fiducials are not in MRI space')
    fid_loc = _fiducial_coords(mri_fiducials, io.constants.FIFF.FIFFV_COORD_MRI)
    fid_loc = apply_trans(mri_trans, fid_loc)
else:
    fid_loc = []
if (('helmet' in meg) and (len(meg_picks) > 0)):
    surfs['helmet'] = get_meg_helmet_surf(info, head_mri_t)
brain = False
brain_surfs = numpy.intersect1d(surfaces, ['brain', 'pial', 'white', 'inflated'])
if (len(brain_surfs) > 1):
    raise ValueError(('Only one brain surface can be plotted. Got %s.' % brain_surfs))
elif (len(brain_surfs) == 1):
    if (brain_surfs[0] == 'brain'):
        brain = 'pial'
    else:
        brain = brain_surfs[0]
if brain:
    if is_sphere:
        if (len(bem['layers']) > 0):
            surfs['lh'] = _complete_sphere_surf(bem, 0, 4)
    else:
        subjects_dir = get_subjects_dir(subjects_dir, raise_error=True)
        for hemi in ['lh', 'rh']:
            fname = os.path.join(subjects_dir, subject, 'surf', ('%s.%s' % (hemi, brain)))
            (rr, tris) = read_surface(fname)
            rr *= 0.001
            surfs[hemi] = dict(rr=rr, ntri=len(tris), np=len(rr), coord_frame=io.constants.FIFF.FIFFV_COORD_MRI, tris=tris)
            complete_surface_info(surfs[hemi], copy=False, verbose=False)
skull_alpha = dict()
skull_colors = dict()
hemi_val = 0.5
if ((src is None) or (brain and any(((s['type'] == 'surf') for s in src)))):
    hemi_val = 1.0
tempResult = arange((len(skull) + 1))
	
===================================================================	
_make_tris_fan: 858	
----------------------------	

'Make tris given a number of vertices of a circle-like obj.'
tris = numpy.zeros(((n_vert - 2), 3), int)
tempResult = arange(2, n_vert)
	
===================================================================	
_limits_to_control_points: 927	
----------------------------	

"Convert limits (values or percentiles) to control points.\n\n    Note: If using 'mne', generate cmap control points for a directly\n    mirrored cmap for simplicity (i.e., no normalization is computed to account\n    for a 2-tailed mne cmap).\n\n    Parameters\n    ----------\n    clim : str | dict\n        Desired limits use to set cmap control points.\n\n    Returns\n    -------\n    ctrl_pts : list (length 3)\n        Array of floats corresponding to values to use as cmap control points.\n    colormap : str\n        The colormap.\n    "
if (colormap == 'auto'):
    if (clim == 'auto'):
        colormap = ('mne' if (stc_data < 0).any() else 'hot')
    elif ('lims' in clim):
        colormap = 'hot'
    else:
        colormap = 'mne'
if (clim == 'auto'):
    ctrl_pts = numpy.percentile(numpy.abs(stc_data), [96, 97.5, 99.95])
elif isinstance(clim, dict):
    limit_key = ['lims', 'pos_lims'][(colormap in ('mne', 'mne_analyze'))]
    if ((colormap != 'auto') and (limit_key not in clim.keys())):
        raise KeyError('"pos_lims" must be used with "mne" colormap')
    clim['kind'] = clim.get('kind', 'percent')
    if (clim['kind'] == 'percent'):
        ctrl_pts = numpy.percentile(numpy.abs(stc_data), list(numpy.abs(clim[limit_key])))
    elif (clim['kind'] == 'value'):
        ctrl_pts = numpy.array(clim[limit_key])
        if (np.diff(ctrl_pts) < 0).any():
            raise ValueError('value colormap limits must be strictly nondecreasing')
    else:
        raise ValueError('If clim is a dict, clim[kind] must be  "value" or "percent"')
else:
    raise ValueError('"clim" must be "auto" or dict')
if (len(ctrl_pts) != 3):
    raise ValueError(('"lims" or "pos_lims" is length %i. It must be length 3' % len(ctrl_pts)))
ctrl_pts = numpy.array(ctrl_pts, float)
if (len(set(ctrl_pts)) != 3):
    if (len(set(ctrl_pts)) == 1):
        if (ctrl_pts[0] == 0):
            warn('All data were zero')
            tempResult = arange(3, dtype=float)
	
===================================================================	
test_plot_head_positions: 38	
----------------------------	

'Test plotting of head positions.'
import matplotlib.pyplot as plt
pos = np.random.RandomState(0).randn(4, 10)
tempResult = arange(len(pos))
	
===================================================================	
test_plot_evoked: 118	
----------------------------	

'Test plotting of evoked.'
import matplotlib.pyplot as plt
evoked = _get_epochs().average()
with warnings.catch_warnings(record=True):
    fig = evoked.plot(proj=True, hline=[1], exclude=[], window_title='foo')
    ax = fig.get_axes()[0]
    line = ax.lines[0]
    _fake_click(fig, ax, [line.get_xdata()[0], line.get_ydata()[0]], 'data')
    _fake_click(fig, ax, [ax.get_xlim()[0], ax.get_ylim()[1]], 'data')
    evoked.plot(exclude='bads')
    evoked.plot(hline=[1], units=dict(mag='femto foo'))
    evoked_delayed_ssp = _get_epochs_delayed_ssp().average()
    evoked_delayed_ssp.plot(proj='interactive')
    evoked_delayed_ssp.apply_proj()
    assert_raises(RuntimeError, evoked_delayed_ssp.plot, proj='interactive')
    evoked_delayed_ssp.info['projs'] = []
    assert_raises(RuntimeError, evoked_delayed_ssp.plot, proj='interactive')
    assert_raises(RuntimeError, evoked_delayed_ssp.plot, proj='interactive', axes='foo')
    matplotlib.pyplot.close('all')
    evoked.plot(gfp='only')
    assert_raises(ValueError, evoked.plot, gfp='foo')
    evoked.plot_image(proj=True)
    evoked.plot_image(exclude='bads', cmap='interactive')
    evoked.plot_image(exclude=evoked.info['bads'])
    matplotlib.pyplot.close('all')
    evoked.plot_topo()
    _line_plot_onselect(0, 200, ['mag', 'grad'], evoked.info, evoked.data, evoked.times)
    matplotlib.pyplot.close('all')
    cov = read_cov(cov_fname)
    cov['method'] = 'empirical'
    evoked.plot_white(cov, rank={'mag': 101, 'grad': 201})
    evoked.plot_white(cov, rank={'mag': 101})
    evoked.plot_white(cov, rank={'grad': 201})
    assert_raises(ValueError, evoked.plot_white, cov, rank={'mag': 101, 'grad': 201, 'meg': 306})
    assert_raises(ValueError, evoked.plot_white, cov, rank={'meg': 306})
    evoked.plot_white([cov, cov])
    plot_compare_evokeds(evoked.copy().pick_types(meg='mag'))
    plot_compare_evokeds(evoked.copy().pick_types(meg='grad'), picks=[1, 2])
    evoked.rename_channels({'MEG 2142': 'MEG 1642'})
    assert (len(plot_compare_evokeds(evoked)) == 2)
    colors = dict(red='r', blue='b')
    linestyles = dict(red='--', blue='-')
    (red, blue) = (evoked.copy(), evoked.copy())
    red.data *= 1.1
    blue.data *= 0.9
    plot_compare_evokeds([red, blue], picks=3)
    plot_compare_evokeds([red, blue], picks=3, truncate_yaxis=True)
    plot_compare_evokeds([[red, evoked], [blue, evoked]], picks=3)
    contrast = dict()
    contrast['red/stim'] = list((evoked.copy(), red))
    contrast['blue/stim'] = list((evoked.copy(), blue))
    plot_compare_evokeds(contrast, colors=colors, linestyles=linestyles, picks=[0, 2], vlines=[0.01, (- 0.04)], invert_y=True, truncate_yaxis=False, ylim=dict(mag=((- 10), 10)), styles={'red/stim': {'linewidth': 1}}, show_sensors=True)
    assert_raises(ValueError, plot_compare_evokeds, contrast, picks='str')
    assert_raises(ValueError, plot_compare_evokeds, evoked, picks=3, colors=dict(fake=1))
    assert_raises(ValueError, plot_compare_evokeds, evoked, picks=3, styles=dict(fake=1))
    assert_raises(ValueError, plot_compare_evokeds, [[1, 2], [3, 4]], picks=3)
    assert_raises(ValueError, plot_compare_evokeds, evoked, picks=3, styles=dict(err=1))
    assert_raises(ValueError, plot_compare_evokeds, evoked, picks=3, gfp=True)
    assert_raises(TypeError, plot_compare_evokeds, evoked, picks=3, ci='fake')
    assert_raises(TypeError, plot_compare_evokeds, evoked, picks=3, show_sensors='a')
    contrast['red/stim'] = red
    contrast['blue/stim'] = blue
    plot_compare_evokeds(contrast, picks=[0], colors=['r', 'b'], ylim=dict(mag=(1, 10)), ci=_parametric_ci, truncate_yaxis='max_ticks')
    evoked_sss = evoked.copy()
    tempResult = arange(80)
	
===================================================================	
test_plot_raw: 106	
----------------------------	

'Test plotting of raw data.'
import matplotlib.pyplot as plt
raw = _get_raw()
raw.info['lowpass'] = 10.0
events = _get_events()
matplotlib.pyplot.close('all')
with warnings.catch_warnings(record=True):
    fig = raw.plot(events=events, show_options=True, order=[1, 7, 3], group_by='original')
    x = fig.get_axes()[0].lines[1].get_xdata().mean()
    y = fig.get_axes()[0].lines[1].get_ydata().mean()
    data_ax = fig.axes[0]
    _fake_click(fig, data_ax, [x, y], xform='data')
    _fake_click(fig, data_ax, [x, y], xform='data')
    _fake_click(fig, data_ax, [0.5, 0.999])
    _fake_click(fig, data_ax, [(- 0.1), 0.9])
    _fake_click(fig, fig.get_axes()[1], [0.5, 0.5])
    _fake_click(fig, fig.get_axes()[2], [0.5, 0.5])
    _fake_click(fig, fig.get_axes()[3], [0.5, 0.5])
    fig.canvas.button_press_event(1, 1, 1)
    fig.canvas.scroll_event(0.5, 0.5, (- 0.5))
    fig.canvas.scroll_event(0.5, 0.5, 0.5)
    for key in ['down', 'up', 'right', 'left', 'o', '-', '+', '=', 'pageup', 'pagedown', 'home', 'end', '?', 'f11', 'escape']:
        fig.canvas.key_press_event(key)
    fig = plot_raw(raw, events=events, group_by='selection')
    for key in ['b', 'down', 'up', 'right', 'left', 'o', '-', '+', '=', 'pageup', 'pagedown', 'home', 'end', '?', 'f11', 'b', 'escape']:
        fig.canvas.key_press_event(key)
    assert_raises(KeyError, raw.plot, event_color={0: 'r'})
    assert_raises(TypeError, raw.plot, event_color={'foo': 'r'})
    annot = Annotations([10, (10 + (raw.first_samp / raw.info['sfreq']))], [10, 10], ['test', 'test'], raw.info['meas_date'])
    raw.annotations = annot
    fig = plot_raw(raw, events=events, event_color={(- 1): 'r', 998: 'b'})
    matplotlib.pyplot.close('all')
    tempResult = arange(len(raw.ch_names))
	
===================================================================	
test_plot_raw: 127	
----------------------------	

'Test plotting of raw data.'
import matplotlib.pyplot as plt
raw = _get_raw()
raw.info['lowpass'] = 10.0
events = _get_events()
matplotlib.pyplot.close('all')
with warnings.catch_warnings(record=True):
    fig = raw.plot(events=events, show_options=True, order=[1, 7, 3], group_by='original')
    x = fig.get_axes()[0].lines[1].get_xdata().mean()
    y = fig.get_axes()[0].lines[1].get_ydata().mean()
    data_ax = fig.axes[0]
    _fake_click(fig, data_ax, [x, y], xform='data')
    _fake_click(fig, data_ax, [x, y], xform='data')
    _fake_click(fig, data_ax, [0.5, 0.999])
    _fake_click(fig, data_ax, [(- 0.1), 0.9])
    _fake_click(fig, fig.get_axes()[1], [0.5, 0.5])
    _fake_click(fig, fig.get_axes()[2], [0.5, 0.5])
    _fake_click(fig, fig.get_axes()[3], [0.5, 0.5])
    fig.canvas.button_press_event(1, 1, 1)
    fig.canvas.scroll_event(0.5, 0.5, (- 0.5))
    fig.canvas.scroll_event(0.5, 0.5, 0.5)
    for key in ['down', 'up', 'right', 'left', 'o', '-', '+', '=', 'pageup', 'pagedown', 'home', 'end', '?', 'f11', 'escape']:
        fig.canvas.key_press_event(key)
    fig = plot_raw(raw, events=events, group_by='selection')
    for key in ['b', 'down', 'up', 'right', 'left', 'o', '-', '+', '=', 'pageup', 'pagedown', 'home', 'end', '?', 'f11', 'b', 'escape']:
        fig.canvas.key_press_event(key)
    assert_raises(KeyError, raw.plot, event_color={0: 'r'})
    assert_raises(TypeError, raw.plot, event_color={'foo': 'r'})
    annot = Annotations([10, (10 + (raw.first_samp / raw.info['sfreq']))], [10, 10], ['test', 'test'], raw.info['meas_date'])
    raw.annotations = annot
    fig = plot_raw(raw, events=events, event_color={(- 1): 'r', 998: 'b'})
    matplotlib.pyplot.close('all')
    for (group_by, order) in zip(['position', 'selection'], [numpy.arange(len(raw.ch_names))[::(- 3)], [1, 2, 4, 6]]):
        fig = raw.plot(group_by=group_by, order=order)
        x = fig.get_axes()[0].lines[1].get_xdata()[10]
        y = fig.get_axes()[0].lines[1].get_ydata()[10]
        _fake_click(fig, data_ax, [x, y], xform='data')
        fig.canvas.key_press_event('down')
        _fake_click(fig, fig.get_axes()[2], [0.5, 0.5])
        sel_fig = matplotlib.pyplot.figure(1)
        topo_ax = sel_fig.axes[1]
        _fake_click(sel_fig, topo_ax, [(- 0.425), 0.20223853], xform='data')
        fig.canvas.key_press_event('down')
        fig.canvas.key_press_event('up')
        fig.canvas.scroll_event(0.5, 0.5, (- 1))
        fig.canvas.scroll_event(0.5, 0.5, 1)
        _fake_click(sel_fig, topo_ax, [(- 0.5), 0.0], xform='data')
        _fake_click(sel_fig, topo_ax, [0.5, 0.0], xform='data', kind='motion')
        _fake_click(sel_fig, topo_ax, [0.5, 0.5], xform='data', kind='motion')
        _fake_click(sel_fig, topo_ax, [(- 0.5), 0.5], xform='data', kind='release')
        matplotlib.pyplot.close('all')
    raw.info['meas_date'] = numpy.array([raw.info['meas_date'][0]], dtype=numpy.int32)
    raw.annotations = Annotations([(1 + (raw.first_samp / raw.info['sfreq']))], [5], ['bad'])
    tempResult = arange(8)
	
===================================================================	
test_plot_tfr_topo: 129	
----------------------------	

'Test plotting of TFR data.'
import matplotlib.pyplot as plt
epochs = _get_epochs()
n_freqs = 3
nave = 1
data = np.random.RandomState(0).randn(len(epochs.ch_names), n_freqs, len(epochs.times))
tempResult = arange(n_freqs)
	
===================================================================	
test_plot_tfr_topomap: 235	
----------------------------	

'Test plotting of TFR data.'
import matplotlib as mpl
import matplotlib.pyplot as plt
raw = read_raw_fif(raw_fname)
times = numpy.linspace((- 0.1), 0.1, 200)
res = 8
n_freqs = 3
nave = 1
rng = numpy.random.RandomState(42)
picks = [93, 94, 96, 97, 21, 22, 24, 25, 129, 130, 315, 316, 2, 5, 8, 11]
info = pick_info(raw.info, picks)
data = rng.randn(len(picks), n_freqs, len(times))
tempResult = arange(n_freqs)
	
===================================================================	
test_plot_tfr_topomap: 254	
----------------------------	

'Test plotting of TFR data.'
import matplotlib as mpl
import matplotlib.pyplot as plt
raw = read_raw_fif(raw_fname)
times = numpy.linspace((- 0.1), 0.1, 200)
res = 8
n_freqs = 3
nave = 1
rng = numpy.random.RandomState(42)
picks = [93, 94, 96, 97, 21, 22, 24, 25, 129, 130, 315, 316, 2, 5, 8, 11]
info = pick_info(raw.info, picks)
data = rng.randn(len(picks), n_freqs, len(times))
tfr = AverageTFR(info, data, times, numpy.arange(n_freqs), nave)
tfr.plot_topomap(ch_type='mag', tmin=0.05, tmax=0.15, fmin=0, fmax=10, res=res, contours=0)
eclick = matplotlib.backend_bases.MouseEvent('button_press_event', plt.gcf().canvas, 0, 0, 1)
eclick.xdata = eclick.ydata = 0.1
eclick.inaxes = matplotlib.pyplot.gca()
erelease = matplotlib.backend_bases.MouseEvent('button_release_event', plt.gcf().canvas, 0.9, 0.9, 1)
erelease.xdata = 0.3
erelease.ydata = 0.2
pos = [[0.11, 0.11], [0.25, 0.5], [0.0, 0.2], [0.2, 0.39]]
_onselect(eclick, erelease, tfr, pos, 'grad', 1, 3, 1, 3, 'RdBu_r', list())
_onselect(eclick, erelease, tfr, pos, 'mag', 1, 3, 1, 3, 'RdBu_r', list())
eclick.xdata = eclick.ydata = 0.0
erelease.xdata = erelease.ydata = 0.9
tfr._onselect(eclick, erelease, None, 'mean', None)
matplotlib.pyplot.close('all')
info = raw.info.copy()
chan_inds = channel_indices_by_type(info)
info = pick_info(info, chan_inds['grad'][:4])
(fig, axes) = matplotlib.pyplot.subplots()
tempResult = arange(3.0, 9.5)
	
***************************************************	
